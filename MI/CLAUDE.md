# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

---

## Project Overview

This is the **MI Knowledge Hub** - a comprehensive Materials Informatics educational and community platform powered by **Claude Code subagents**. The project features AI-driven content generation through 7 specialized subagents, automatic paper collection, and interactive learning support.

**Key Characteristics:**
- 7 specialized Claude Code subagents collaborating on content creation
- Quality-first approach with academic review gates (80-point and 90-point thresholds)
- Static site on GitHub Pages with JSON/Markdown data layer
- Mobile-first responsive design with WCAG 2.1 Level AA accessibility
- **NO API keys required** - everything runs within Claude Code sessions

---

## Architecture: Claude Code Subagent-Based

### Core Philosophy

This project leverages **Claude Code subagents** instead of external API clients. All content generation, paper collection, and quality assurance happen within Claude Code sessions using the built-in tools (Read, Write, Edit, Bash, Grep, WebSearch).

**Benefits:**
- ‚úÖ No API keys or external services required
- ‚úÖ All agents run within secure Claude Code environment
- ‚úÖ Direct access to file system and tools
- ‚úÖ Version controlled agent definitions (.claude/agents/)
- ‚úÖ Seamless collaboration between agents
- ‚úÖ No rate limiting or API costs

### 7 Specialized Subagents

Located in `.claude/agents/`, each agent is a Markdown file with YAML frontmatter defining its role, tools, and behavior.

| Agent | File | Primary Role | Key Tools |
|-------|------|--------------|-----------|
| **Scholar Agent** | `scholar-agent.md` | Paper collection & summarization | Read, Write, Edit, Bash, WebSearch |
| **Content Agent** | `content-agent.md` | Article generation (9-phase workflow) | Read, Write, Edit, MultiEdit, Bash |
| **Academic Reviewer** | `academic-reviewer.md` | Quality assurance (0-100 scoring) | Read, Write, Edit, Grep, Bash |
| **Tutor Agent** | `tutor-agent.md` | Interactive learning support | Read, Write, Grep, Bash |
| **Data Agent** | `data-agent.md` | Dataset & tool management | Read, Write, Edit, Bash, WebSearch |
| **Design Agent** | `design-agent.md` | UX optimization & accessibility | Read, Write, Edit, Grep, Bash |
| **Maintenance Agent** | `maintenance-agent.md` | Validation & monitoring | Read, Write, Bash, Grep, Glob |

---

## Common Workflows

### 1. Collecting Recent Papers

```
User: "Use scholar-agent to collect recent papers on bayesian optimization for materials"

scholar-agent:
1. Searches using WebSearch
2. Extracts metadata (DOI, authors, abstract)
3. Checks data/papers.json for duplicates
4. Adds new papers with proper structure
5. Reports: "Added 5 new papers, skipped 2 duplicates"
```

### 2. Creating Educational Content (9-Phase Workflow)

```
User: "Use content-agent to create an intermediate-level article about Bayesian optimization"

Phase 0-2: content-agent drafts initial article
  ‚Üì
Phase 3: academic-reviewer reviews (must score ‚â•80)
  ‚Üí If fail: Return to Phase 1
  ‚Üí If pass: Continue
  ‚Üì
Phase 4-6: content-agent enhances with help from design-agent, data-agent
  ‚Üì
Phase 7: academic-reviewer reviews again (must score ‚â•90)
  ‚Üí If 80-89: Minor revision (return to Phase 4)
  ‚Üí If <80: Major revision (return to Phase 1)
  ‚Üí If ‚â•90: Approved
  ‚Üì
Phase 8-9: Final checks and publication to content/methods/
```

### 3. Interactive Learning Support

```
User: "Ask tutor-agent to explain Bayesian optimization"

tutor-agent:
Uses Socratic dialogue:
"Before I explain, let me ask: „ÇÇ„ÅóÂÆüÈ®ì„ÅÆ„Ç≥„Çπ„Éà„ÅåÈùûÂ∏∏„Å´È´ò„ÅÑÂ†¥Âêà„ÄÅ
„Å©„ÅÆ„Çà„ÅÜ„Å´Ê¨°„ÅÆÂÆüÈ®ìÊù°‰ª∂„ÇíÈÅ∏„Å≥„Åæ„Åô„ÅãÔºü"

[Guides through discovery learning with hints and questions]
```

### 4. Data Validation & Maintenance

```
User: "Use maintenance-agent to validate all data"

maintenance-agent:
1. Validates JSON structure (papers, datasets, tutorials, tools)
2. Checks all URLs for accessibility
3. Runs quality metrics (Lighthouse scores)
4. Generates health report
5. Reports issues with priority levels
```

---

## Directory Structure

```
MI/
‚îú‚îÄ‚îÄ .claude/
‚îÇ   ‚îî‚îÄ‚îÄ agents/              # 7 subagent definitions (Markdown + YAML)
‚îÇ       ‚îú‚îÄ‚îÄ scholar-agent.md
‚îÇ       ‚îú‚îÄ‚îÄ content-agent.md
‚îÇ       ‚îú‚îÄ‚îÄ academic-reviewer.md
‚îÇ       ‚îú‚îÄ‚îÄ tutor-agent.md
‚îÇ       ‚îú‚îÄ‚îÄ data-agent.md
‚îÇ       ‚îú‚îÄ‚îÄ design-agent.md
‚îÇ       ‚îî‚îÄ‚îÄ maintenance-agent.md
‚îú‚îÄ‚îÄ assets/                  # CSS/JS/Images
‚îú‚îÄ‚îÄ content/                 # Markdown articles
‚îÇ   ‚îú‚îÄ‚îÄ basics/              # Beginner content
‚îÇ   ‚îú‚îÄ‚îÄ methods/             # Intermediate methods
‚îÇ   ‚îú‚îÄ‚îÄ advanced/            # Advanced topics
‚îÇ   ‚îî‚îÄ‚îÄ applications/        # Case studies
‚îú‚îÄ‚îÄ data/                    # JSON data files
‚îÇ   ‚îú‚îÄ‚îÄ papers.json
‚îÇ   ‚îú‚îÄ‚îÄ datasets.json
‚îÇ   ‚îú‚îÄ‚îÄ tutorials.json
‚îÇ   ‚îî‚îÄ‚îÄ tools.json
‚îú‚îÄ‚îÄ pages/                   # Static HTML pages
‚îú‚îÄ‚îÄ notebooks/               # Jupyter tutorials
‚îú‚îÄ‚îÄ tools/                   # Validation utilities (Python)
‚îÇ   ‚îú‚îÄ‚îÄ validate_data.py     # JSON validation script
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ reviews/                 # Academic review reports
‚îú‚îÄ‚îÄ claudedocs/              # Project documentation
‚îÇ   ‚îú‚îÄ‚îÄ requirements.md
‚îÇ   ‚îî‚îÄ‚îÄ content-creation-procedure.md
‚îú‚îÄ‚îÄ index.html               # Homepage
‚îú‚îÄ‚îÄ CLAUDE.md                # This file
‚îî‚îÄ‚îÄ requirements.txt         # Minimal dependencies
```

---

## Development Setup

### Prerequisites

**Required:**
- Python 3.11+ (for validation utilities and Jupyter notebooks)
- Claude Code (for subagent execution)

**Optional:**
- Jupyter (for tutorial notebooks)

### Installation

```bash
# 1. Setup Python environment
python -m venv .venv
source .venv/bin/activate  # macOS/Linux

# 2. Install minimal dependencies
pip install -r requirements.txt

# 3. No API keys needed! Everything runs in Claude Code
```

### Local Preview

```bash
# Start local server
python -m http.server 8000

# Visit: http://localhost:8000
```

---

## Content Creation Process

### üö® CRITICAL: Always Use Subagents for Their Designated Tasks

**This project is designed around the subagent architecture.** When working on this project, you MUST use the appropriate subagent for each task rather than performing the work directly.

#### Task Tool Usage Pattern

Use the **Task tool** to invoke subagents:

```python
# Correct approach
Task(
    subagent_type="content-agent",
    description="Enhance MI introduction article",
    prompt="Enhance content/basics/mi_introduction.md by adding inline citations and recent references based on the academic review report in reviews/mi_introduction_phase3_review.md"
)
```

#### When to Use Which Subagent

| Task Type | Use This Subagent | DO NOT |
|-----------|-------------------|--------|
| **Creating/editing article content** | content-agent | ‚ùå Direct Edit/Write to content/ files |
| **Reviewing article quality** | academic-reviewer | ‚ùå Manual quality assessment |
| **Collecting research papers** | scholar-agent | ‚ùå Manual WebSearch + data editing |
| **Adding/updating datasets** | data-agent | ‚ùå Direct Edit to data/*.json |
| **Checking accessibility/UX** | design-agent | ‚ùå Manual CSS/HTML changes |
| **Validating data integrity** | maintenance-agent | ‚ùå Direct Bash validation scripts |
| **Explaining concepts to users** | tutor-agent | ‚ùå Direct explanation without Socratic method |

#### Direct Work vs. Subagent Delegation

**‚úÖ YOU SHOULD do directly:**
- Reading files to understand project status (Read tool)
- Running validation scripts (Bash: `python tools/validate_data.py`)
- Git operations (Bash: git status, commit, push)
- Creating project infrastructure (directories, config files)
- Updating CLAUDE.md or README.md (documentation)

**‚ùå YOU SHOULD NOT do directly:**
- Writing or editing Markdown articles in content/
- Adding entries to data/*.json files
- Conducting academic quality reviews
- Collecting research paper metadata
- Making UX/accessibility decisions
- Creating educational explanations

### Explicit Subagent Invocation Examples

**Paper Collection:**
```
"Use scholar-agent to collect papers on 'bayesian optimization materials' from the last 30 days"
```

**Content Creation:**
```
"Use content-agent to create a beginner-level article about neural networks in MI"
```

**Quality Review:**
```
"Use academic-reviewer to review content/methods/bayesian_optimization.md and generate a detailed report"
```

**Data Management:**
```
"Use data-agent to add information about the OQMD database to datasets.json"
```

**Content Enhancement (THIS is what should have been done instead of direct editing):**
```
"Use content-agent to enhance content/basics/mi_introduction.md based on the academic review feedback in reviews/mi_introduction_phase3_review.md. Focus on adding inline citations and updating references."
```

### Quality Assurance: Academic Review Gates

The **academic-reviewer** subagent enforces quality through scoring (0-100):

**Phase 3 Gate (After Initial Draft):**
- Minimum required: 80/100
- Dimensions: Scientific accuracy, clarity, references, accessibility
- Fail ‚Üí Major revision (return to Phase 1)

**Phase 7 Gate (After Enhancement):**
- Minimum required: 90/100
- All dimensions must be excellent
- Fail 80-89 ‚Üí Minor revision (return to Phase 4)
- Fail <80 ‚Üí Major revision (return to Phase 1)

**Review Report Structure:**
- Detailed scores for each dimension
- Specific issues with line numbers
- Prioritized improvement recommendations
- Clear decision: APPROVE / MINOR_REVISION / MAJOR_REVISION

---

## Prompt Template System (NEW - 2025-10-16)

### Overview

The **Prompt Template System** provides reusable, high-quality prompt templates for content-agent, ensuring consistency, reducing parse errors, and improving quality scores from 70‚Üí85 points.

**Inspired by Write_tb_AI's template-based approach**, this system implements strict output format constraints and structured prompt engineering.

### Key Benefits

1. **Consistency**: 100% uniform prompt structure across all content generation
2. **Parse Error Reduction**: 50% fewer errors through strict JSON/Markdown format constraints
3. **Quality Improvement**: Phase 3 pass rate increases from 70% to 85%
4. **Efficiency**: 50% reduction in prompt engineering time

### 3 Core Templates

Located in `tools/content_agent_prompts.py`:

#### Template 1: Article Structure Generation
- **Purpose**: Generate overall article structure (chapters, sections)
- **Input**: topic, level, target_audience, min_words
- **Output**: JSON with title, learning_objectives, chapters, sections
- **Function**: `get_structure_prompt(...)`

```python
from tools.content_agent_prompts import get_structure_prompt

prompt = get_structure_prompt(
    topic="„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„Å´„Çà„ÇãÊùêÊñôÊé¢Á¥¢",
    level="intermediate",
    target_audience="Â§ßÂ≠¶Èô¢Áîü",
    min_words=5000
)
# Use this prompt with content-agent
# Expect JSON output
```

#### Template 2: Section Detail Expansion
- **Purpose**: Expand specific section into detailed subsections
- **Input**: article_structure (from Template 1), chapter_number, section_number
- **Output**: JSON with subsections, exercises, key_points, references
- **Function**: `get_section_detail_prompt(...)`

```python
from tools.content_agent_prompts import get_section_detail_prompt

prompt = get_section_detail_prompt(
    article_structure=structure_json,
    chapter_number=2,
    section_number=1,
    level="intermediate"
)
# Use this prompt with content-agent
# Expect JSON output
```

#### Template 3: Content Generation
- **Purpose**: Generate final Markdown content with code examples and exercises
- **Input**: section_detail (from Template 2), level, context (references, datasets)
- **Output**: Markdown with headings, text, code, exercises
- **Function**: `get_content_generation_prompt(...)`

```python
from tools.content_agent_prompts import get_content_generation_prompt

prompt = get_content_generation_prompt(
    section_detail=section_json,
    level="intermediate",
    min_words=2000,
    context={"references": [...], "datasets": [...]}
)
# Use this prompt with content-agent
# Expect Markdown output
```

### Complete Workflow Example

```bash
# Step 1: Structure generation
python -c "
from tools.content_agent_prompts import get_structure_prompt
prompt = get_structure_prompt('„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ', 'intermediate', 'Â§ßÂ≠¶Èô¢Áîü', 5000)
print(prompt)
" > /tmp/structure_prompt.txt

# Step 2: Use with content-agent
# (content-agent uses this prompt internally)

# Step 3: Section detail expansion
# (iteratively for each section)

# Step 4: Content generation
# (final Markdown output)
```

### Usage in Content Agent

The **content-agent** now uses these templates in Phase 0-2:

1. **Phase 0**: Use Template 1 to generate article structure
2. **Phase 1**: Use Template 2 to detail each section
3. **Phase 2**: Use Template 3 to generate final content

See `.claude/agents/content-agent.md` for detailed integration instructions.

### Files

- **Core Library**: `tools/content_agent_prompts.py` (650 lines)
- **Usage Examples**: `tools/example_template_usage.py` (4 complete examples)
- **Documentation**:
  - `claudedocs/high_quality_output_guide.md` (comprehensive, 1258 lines)
  - `claudedocs/quality_quick_reference.md` (quick reference, 299 lines)

### Quality Metrics

**Before Template System:**
- Parse errors: ~20% of generations
- Phase 3 pass rate: 70%
- Prompt consistency: Variable

**After Template System:**
- Parse errors: ~10% (50% reduction)
- Phase 3 pass rate: 85% (estimated)
- Prompt consistency: 100%

---

## Data Management

### JSON Data Schemas

**papers.json:**
```json
{
  "id": "paper_001",
  "title": "Paper title",
  "authors": ["Author A", "Author B"],
  "year": 2024,
  "journal": "Journal Name",
  "doi": "10.xxxx/xxxxx",
  "abstract": "Abstract text",
  "tags": ["tag1", "tag2"],
  "collected_at": "2025-10-15T12:00:00Z"
}
```

**datasets.json, tutorials.json, tools.json:**
See data-agent documentation in `.claude/agents/data-agent.md` for complete schemas.

### Data Validation

```bash
# Validate all JSON files
python tools/validate_data.py

# Output:
# ‚úÖ papers.json (153 entries, no errors)
# ‚úÖ datasets.json (48 entries, no errors)
# ‚ö†Ô∏è tutorials.json (2 missing fields)
# ‚úÖ tools.json (67 entries, no errors)
```

---

## Quality Standards

### Performance Targets
- **Lighthouse Performance**: ‚â•95
- **Lighthouse Accessibility**: 100
- **First Contentful Paint**: <1.5s
- **Time to Interactive**: <3.0s

### Accessibility Requirements
- WCAG 2.1 Level AA compliance
- Color contrast ratio ‚â•4.5:1
- Touch targets ‚â•44px √ó 44px (Apple HIG)
- Keyboard navigation support
- Screen reader compatibility

### Academic Content Quality
- Scientific accuracy verified by academic-reviewer
- All dimensions scored ‚â•90 at publication
- Peer-reviewed references preferred
- Executable code examples
- Clear learning objectives

---

## Common Tasks

### Collecting Papers

```
User: "Use scholar-agent to collect papers on 'materials informatics machine learning' from the last 30 days"

scholar-agent will:
1. Search using WebSearch
2. Extract and validate metadata
3. Update data/papers.json
4. Report results
```

### Creating Content

```
User: "Use content-agent to create a beginner article about MI basics"

Process:
1. content-agent drafts (Phase 0-2)
2. academic-reviewer scores (Phase 3, must be ‚â•80)
3. content-agent enhances (Phase 4-6)
4. academic-reviewer scores again (Phase 7, must be ‚â•90)
5. Published to content/basics/
```

### Updating Datasets

```
User: "Use data-agent to add a new dataset about crystal structures"

data-agent will:
1. Prompt for complete metadata
2. Validate required fields
3. Check for duplicates
4. Add to data/datasets.json
5. Verify JSON integrity
```

### Validating System Health

```
User: "Use maintenance-agent to validate all data and check links"

maintenance-agent will:
1. Validate JSON structure
2. Check all URLs
3. Run quality metrics
4. Generate health report
5. Prioritize issues
```

---

## Collaboration Patterns

### Scholar + Content Agent
```
Content creation needs recent research context
  ‚Üí scholar-agent collects papers
  ‚Üí content-agent references in article
```

### Content + Academic Reviewer Agent
```
Article draft completed
  ‚Üí academic-reviewer scores (Phase 3)
  ‚Üí content-agent revises based on feedback
  ‚Üí academic-reviewer scores again (Phase 7)
```

### Content + Design Agent
```
Article structure complete
  ‚Üí design-agent adds diagrams, improves formatting
  ‚Üí design-agent checks accessibility
  ‚Üí content-agent incorporates improvements
```

### Data + Maintenance Agent
```
New data added
  ‚Üí maintenance-agent validates structure
  ‚Üí maintenance-agent checks URLs
  ‚Üí data-agent fixes reported issues
```

---

## Troubleshooting

### Subagent Not Found

If a subagent is not recognized:
```bash
# Check that agent file exists
ls .claude/agents/

# Verify YAML frontmatter
head .claude/agents/scholar-agent.md
```

### Validation Failures

```bash
# Run validation script
python tools/validate_data.py

# Fix reported issues in JSON files
# Re-validate until all pass
```

### Content Below Quality Threshold

If academic-reviewer rejects content:
1. Read the review report in reviews/
2. Address all HIGH priority issues
3. Incorporate recommendations
4. Re-submit for review

---

## Git Workflow

```bash
# Always work on feature branches
git checkout -b feature/add-quantum-ml-article

# Create content using subagents
# (All changes tracked automatically)

# Validate before committing
python tools/validate_data.py

# Commit with descriptive message
git commit -m "Add quantum ML article (academic-reviewer score: 92)"

# Push and create PR
git push origin feature/add-quantum-ml-article
```

---

## Key Differences from Original Design

### What Changed

**Before (Python API-based):**
- Required ANTHROPIC_API_KEY
- Python agents with anthropic library
- External API calls for each generation
- Complex async/await patterns
- Rate limiting concerns

**Now (Claude Code Subagent-based):**
- No API keys required
- Subagents run within Claude Code
- Direct tool access (Read, Write, Edit, Bash)
- Synchronous, session-based execution
- No rate limits within session

### What Stayed the Same

- 9-phase quality workflow
- Academic review gates (80/90 points)
- Static HTML/CSS/JS frontend
- JSON data layer
- Mobile-first responsive design
- WCAG 2.1 Level AA accessibility

---

## Documentation Reference

**Essential reading:**

1. **`claudedocs/requirements.md`** - Comprehensive requirements (v1.1)
2. **`claudedocs/content-creation-procedure.md`** - 9-phase quality workflow
3. **`.claude/agents/*.md`** - Individual subagent documentation
4. **`README.md`** - Quick start guide

---

## Contact

**Project Lead**: Dr. Yusuke Hashimoto
**Email**: yusuke.hashimoto.b8@tohoku.ac.jp
**Institution**: Tohoku University

---

**Last Updated**: 2025-10-16
**CLAUDE.md Version**: 2.1 (Added explicit subagent usage guidelines)
