<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ナノ材料データ解析と機械学習">
    <title>Chapter 3: Python実践チュートリアル - MI Knowledge Hub</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 0.9rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        tbody {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: #666;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Details/Summary (for exercises) */
        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            padding: 0.5rem;
        }

        summary:hover {
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }

        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }

            table {
                font-size: 0.9rem;
            }
        }
    </style>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="container">
            <h1>Chapter 3: Python実践チュートリアル</h1>
            <div class="meta">
                <span>📖 読了時間: 30-40分</span>
                <span>📊 レベル: 中級</span>
            </div>
        </div>
    </header>

    <main class="container">
        <h1 id="chapter-3-python">Chapter 3: Python実践チュートリアル</h1>
<p>ナノ材料データ解析と機械学習</p>
<hr />
<h2 id="_1">本章の学習目標</h2>
<p>本章を学習することで、以下のスキルを習得できます：</p>
<p>✅ ナノ粒子データの生成・可視化・前処理の実践<br />
✅ 5種類の回帰モデルによるナノ材料物性予測<br />
✅ ベイズ最適化によるナノ材料の最適設計<br />
✅ SHAP分析による機械学習モデルの解釈<br />
✅ 多目的最適化によるトレードオフ分析<br />
✅ TEM画像解析とサイズ分布のフィッティング<br />
✅ 異常検知による品質管理への応用</p>
<hr />
<h2 id="31">3.1 環境構築</h2>
<h3 id="_2">必要なライブラリ</h3>
<p>本チュートリアルで使用する主要なPythonライブラリ：</p>
<pre class="codehilite"><code class="language-python"># データ処理・可視化
pandas, numpy, matplotlib, seaborn, scipy

# 機械学習
scikit-learn, lightgbm

# 最適化
scikit-optimize

# モデル解釈
shap

# 多目的最適化（オプション）
pymoo
</code></pre>

<h3 id="_3">インストール方法</h3>
<h4 id="option-1-anaconda">Option 1: Anaconda環境</h4>
<pre class="codehilite"><code class="language-bash"># Anacondaで新しい環境を作成
conda create -n nanomaterials python=3.10 -y
conda activate nanomaterials

# 必要なライブラリをインストール
conda install pandas numpy matplotlib seaborn scipy scikit-learn -y
conda install -c conda-forge lightgbm scikit-optimize shap -y

# 多目的最適化用（オプション）
pip install pymoo
</code></pre>

<h4 id="option-2-venv-pip">Option 2: venv + pip環境</h4>
<pre class="codehilite"><code class="language-bash"># 仮想環境を作成
python -m venv nanomaterials_env

# 仮想環境を有効化
# macOS/Linux:
source nanomaterials_env/bin/activate
# Windows:
nanomaterials_env\Scripts\activate

# 必要なライブラリをインストール
pip install pandas numpy matplotlib seaborn scipy
pip install scikit-learn lightgbm scikit-optimize shap pymoo
</code></pre>

<h4 id="option-3-google-colab">Option 3: Google Colab</h4>
<p>Google Colabを使用する場合、以下のコードをセルで実行：</p>
<pre class="codehilite"><code class="language-python"># 追加パッケージのインストール
!pip install lightgbm scikit-optimize shap pymoo

# インポートの確認
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
print(&quot;環境構築完了！&quot;)
</code></pre>

<hr />
<h2 id="32">3.2 ナノ粒子データの準備と可視化</h2>
<h3 id="1">【例1】合成データ生成：金ナノ粒子のサイズと光学特性</h3>
<p>金ナノ粒子の局在表面プラズモン共鳴（LSPR）波長は、粒子サイズに依存します。この関係を模擬データで表現します。</p>
<pre class="codehilite"><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 日本語フォント設定（必要に応じて）
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# 乱数シードの設定（再現性のため）
np.random.seed(42)

# サンプル数
n_samples = 200

# 金ナノ粒子のサイズ（nm）: 平均15 nm、標準偏差5 nm
size = np.random.normal(15, 5, n_samples)
size = np.clip(size, 5, 50)  # 5-50 nmの範囲に制限

# LSPR波長（nm）: Mie理論の簡易近似
# 基本波長520 nm + サイズ依存項 + ノイズ
lspr = 520 + 0.8 * (size - 15) + np.random.normal(0, 5, n_samples)

# 合成条件
temperature = np.random.uniform(20, 80, n_samples)  # 温度（℃）
pH = np.random.uniform(4, 10, n_samples)  # pH

# データフレームの作成
data = pd.DataFrame({
    'size_nm': size,
    'lspr_nm': lspr,
    'temperature_C': temperature,
    'pH': pH
})

print(&quot;=&quot; * 60)
print(&quot;金ナノ粒子データの生成完了&quot;)
print(&quot;=&quot; * 60)
print(data.head(10))
print(&quot;\n基本統計量:&quot;)
print(data.describe())
</code></pre>

<h3 id="2">【例2】サイズ分布のヒストグラム</h3>
<pre class="codehilite"><code class="language-python"># サイズ分布のヒストグラム
fig, ax = plt.subplots(figsize=(10, 6))

# ヒストグラムとKDE（カーネル密度推定）
ax.hist(data['size_nm'], bins=30, alpha=0.6, color='skyblue',
        edgecolor='black', density=True, label='Histogram')

# KDEプロット
from scipy.stats import gaussian_kde
kde = gaussian_kde(data['size_nm'])
x_range = np.linspace(data['size_nm'].min(), data['size_nm'].max(), 100)
ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')

ax.set_xlabel('Particle Size (nm)', fontsize=12)
ax.set_ylabel('Probability Density', fontsize=12)
ax.set_title('Gold Nanoparticle Size Distribution', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;平均サイズ: {data['size_nm'].mean():.2f} nm&quot;)
print(f&quot;標準偏差: {data['size_nm'].std():.2f} nm&quot;)
print(f&quot;中央値: {data['size_nm'].median():.2f} nm&quot;)
</code></pre>

<h3 id="3">【例3】散布図マトリックス</h3>
<pre class="codehilite"><code class="language-python"># ペアプロット（散布図マトリックス）
sns.pairplot(data, diag_kind='kde', plot_kws={'alpha': 0.6},
             height=2.5, corner=False)
plt.suptitle('Pairplot of Gold Nanoparticle Data', y=1.01, fontsize=14, fontweight='bold')
plt.show()

print(&quot;各変数間の関係を可視化しました&quot;)
</code></pre>

<h3 id="4">【例4】相関行列のヒートマップ</h3>
<pre class="codehilite"><code class="language-python"># 相関行列の計算
correlation_matrix = data.corr()

# ヒートマップ
fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm',
            center=0, square=True, linewidths=1, cbar_kws={&quot;shrink&quot;: 0.8})
ax.set_title('Correlation Matrix', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

print(&quot;相関係数:&quot;)
print(correlation_matrix)
print(f&quot;\nLSPR波長とサイズの相関: {correlation_matrix.loc['lspr_nm', 'size_nm']:.3f}&quot;)
</code></pre>

<h3 id="53d-vs-vs-lspr">【例5】3Dプロット：サイズ vs 温度 vs LSPR</h3>
<pre class="codehilite"><code class="language-python">from mpl_toolkits.mplot3d import Axes3D

# 3D散布図
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# カラーマップ
scatter = ax.scatter(data['size_nm'], data['temperature_C'], data['lspr_nm'],
                     c=data['pH'], cmap='viridis', s=50, alpha=0.6, edgecolors='k')

ax.set_xlabel('Size (nm)', fontsize=11)
ax.set_ylabel('Temperature (°C)', fontsize=11)
ax.set_zlabel('LSPR Wavelength (nm)', fontsize=11)
ax.set_title('3D Scatter: Size vs Temperature vs LSPR (colored by pH)',
             fontsize=13, fontweight='bold')

# カラーバー
cbar = plt.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)
cbar.set_label('pH', fontsize=10)

plt.tight_layout()
plt.show()

print(&quot;3Dプロットで多次元の関係を可視化しました&quot;)
</code></pre>

<hr />
<h2 id="33">3.3 前処理とデータ分割</h2>
<h3 id="6">【例6】欠損値処理</h3>
<pre class="codehilite"><code class="language-python"># 欠損値を人為的に導入（実習用）
data_with_missing = data.copy()
np.random.seed(123)

# ランダムに5%の欠損値を導入
missing_indices = np.random.choice(data.index, size=int(0.05 * len(data)), replace=False)
data_with_missing.loc[missing_indices, 'temperature_C'] = np.nan

print(&quot;=&quot; * 60)
print(&quot;欠損値の確認&quot;)
print(&quot;=&quot; * 60)
print(f&quot;欠損値の数:\n{data_with_missing.isnull().sum()}&quot;)

# 欠損値の処理方法1: 平均値で補完
data_filled_mean = data_with_missing.fillna(data_with_missing.mean())

# 欠損値の処理方法2: 中央値で補完
data_filled_median = data_with_missing.fillna(data_with_missing.median())

# 欠損値の処理方法3: 削除
data_dropped = data_with_missing.dropna()

print(f&quot;\n元のデータ: {len(data_with_missing)}行&quot;)
print(f&quot;欠損値削除後: {len(data_dropped)}行&quot;)
print(f&quot;平均値補完後: {len(data_filled_mean)}行（欠損値なし）&quot;)

# 以降の分析では元のデータ（欠損値なし）を使用
data_clean = data.copy()
print(&quot;\n→ 以降は欠損値のないデータを使用します&quot;)
</code></pre>

<h3 id="7iqr">【例7】外れ値検出（IQR法）</h3>
<pre class="codehilite"><code class="language-python"># IQR（四分位範囲）法による外れ値検出
def detect_outliers_iqr(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = (series &lt; lower_bound) | (series &gt; upper_bound)
    return outliers, lower_bound, upper_bound

# サイズについて外れ値検出
outliers, lower, upper = detect_outliers_iqr(data_clean['size_nm'])

print(&quot;=&quot; * 60)
print(&quot;外れ値検出（IQR法）&quot;)
print(&quot;=&quot; * 60)
print(f&quot;検出された外れ値の数: {outliers.sum()}&quot;)
print(f&quot;下限: {lower:.2f} nm, 上限: {upper:.2f} nm&quot;)

# 可視化
fig, ax = plt.subplots(figsize=(10, 6))
ax.boxplot([data_clean['size_nm']], labels=['Size (nm)'], vert=False)
ax.scatter(data_clean.loc[outliers, 'size_nm'],
           [1] * outliers.sum(), color='red', s=100,
           label=f'Outliers (n={outliers.sum()})', zorder=3)
ax.set_xlabel('Size (nm)', fontsize=12)
ax.set_title('Boxplot with Outliers', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()

print(&quot;→ 外れ値は除去せず、全データを使用します&quot;)
</code></pre>

<h3 id="8standardscaler">【例8】特徴量スケーリング（StandardScaler）</h3>
<pre class="codehilite"><code class="language-python">from sklearn.preprocessing import StandardScaler

# 特徴量とターゲットの分離
X = data_clean[['size_nm', 'temperature_C', 'pH']]
y = data_clean['lspr_nm']

# StandardScaler（平均0、標準偏差1に標準化）
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# スケーリング前後の比較
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)

print(&quot;=&quot; * 60)
print(&quot;スケーリング前の統計量&quot;)
print(&quot;=&quot; * 60)
print(X.describe())

print(&quot;\n&quot; + &quot;=&quot; * 60)
print(&quot;スケーリング後の統計量（平均≈0、標準偏差≈1）&quot;)
print(&quot;=&quot; * 60)
print(X_scaled_df.describe())

print(&quot;\n→ スケーリングにより各特徴量のスケールが統一されました&quot;)
</code></pre>

<h3 id="9">【例9】訓練データとテストデータの分割</h3>
<pre class="codehilite"><code class="language-python">from sklearn.model_selection import train_test_split

# 訓練データとテストデータに分割（80:20）
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

print(&quot;=&quot; * 60)
print(&quot;データ分割&quot;)
print(&quot;=&quot; * 60)
print(f&quot;全データ数: {len(X)}サンプル&quot;)
print(f&quot;訓練データ: {len(X_train)}サンプル ({len(X_train)/len(X)*100:.1f}%)&quot;)
print(f&quot;テストデータ: {len(X_test)}サンプル ({len(X_test)/len(X)*100:.1f}%)&quot;)

print(&quot;\n訓練データの統計量:&quot;)
print(pd.DataFrame(X_train, columns=X.columns).describe())
</code></pre>

<hr />
<h2 id="34">3.4 回帰モデルによるナノ粒子物性予測</h2>
<p>目標：サイズ、温度、pHからLSPR波長を予測</p>
<h3 id="10linear-regression">【例10】線形回帰（Linear Regression）</h3>
<pre class="codehilite"><code class="language-python">from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# 線形回帰モデルの構築
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)

# 予測
y_train_pred_lr = model_lr.predict(X_train)
y_test_pred_lr = model_lr.predict(X_test)

# 評価指標
r2_train_lr = r2_score(y_train, y_train_pred_lr)
r2_test_lr = r2_score(y_test, y_test_pred_lr)
rmse_test_lr = np.sqrt(mean_squared_error(y_test, y_test_pred_lr))
mae_test_lr = mean_absolute_error(y_test, y_test_pred_lr)

print(&quot;=&quot; * 60)
print(&quot;線形回帰（Linear Regression）&quot;)
print(&quot;=&quot; * 60)
print(f&quot;訓練データ R²: {r2_train_lr:.4f}&quot;)
print(f&quot;テストデータ R²: {r2_test_lr:.4f}&quot;)
print(f&quot;テストデータ RMSE: {rmse_test_lr:.4f} nm&quot;)
print(f&quot;テストデータ MAE: {mae_test_lr:.4f} nm&quot;)

# 回帰係数
print(&quot;\n回帰係数:&quot;)
for name, coef in zip(X.columns, model_lr.coef_):
    print(f&quot;  {name}: {coef:.4f}&quot;)
print(f&quot;  切片: {model_lr.intercept_:.4f}&quot;)

# 残差プロット
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# 予測値 vs 実測値
axes[0].scatter(y_test, y_test_pred_lr, alpha=0.6, edgecolors='k')
axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
             'r--', lw=2, label='Perfect Prediction')
axes[0].set_xlabel('Actual LSPR (nm)', fontsize=11)
axes[0].set_ylabel('Predicted LSPR (nm)', fontsize=11)
axes[0].set_title(f'Linear Regression (R² = {r2_test_lr:.3f})', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# 残差プロット
residuals = y_test - y_test_pred_lr
axes[1].scatter(y_test_pred_lr, residuals, alpha=0.6, edgecolors='k')
axes[1].axhline(y=0, color='r', linestyle='--', lw=2)
axes[1].set_xlabel('Predicted LSPR (nm)', fontsize=11)
axes[1].set_ylabel('Residuals (nm)', fontsize=11)
axes[1].set_title('Residual Plot', fontsize=12, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3 id="11random-forest">【例11】ランダムフォレスト回帰（Random Forest）</h3>
<pre class="codehilite"><code class="language-python">from sklearn.ensemble import RandomForestRegressor

# ランダムフォレスト回帰モデル
model_rf = RandomForestRegressor(n_estimators=100, max_depth=10,
                                 random_state=42, n_jobs=-1)
model_rf.fit(X_train, y_train)

# 予測
y_train_pred_rf = model_rf.predict(X_train)
y_test_pred_rf = model_rf.predict(X_test)

# 評価
r2_train_rf = r2_score(y_train, y_train_pred_rf)
r2_test_rf = r2_score(y_test, y_test_pred_rf)
rmse_test_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))
mae_test_rf = mean_absolute_error(y_test, y_test_pred_rf)

print(&quot;=&quot; * 60)
print(&quot;ランダムフォレスト回帰（Random Forest）&quot;)
print(&quot;=&quot; * 60)
print(f&quot;訓練データ R²: {r2_train_rf:.4f}&quot;)
print(f&quot;テストデータ R²: {r2_test_rf:.4f}&quot;)
print(f&quot;テストデータ RMSE: {rmse_test_rf:.4f} nm&quot;)
print(f&quot;テストデータ MAE: {mae_test_rf:.4f} nm&quot;)

# 特徴量重要度
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model_rf.feature_importances_
}).sort_values('Importance', ascending=False)

print(&quot;\n特徴量重要度:&quot;)
print(feature_importance)

# 特徴量重要度の可視化
fig, ax = plt.subplots(figsize=(8, 5))
ax.barh(feature_importance['Feature'], feature_importance['Importance'],
        color='steelblue', edgecolor='black')
ax.set_xlabel('Importance', fontsize=12)
ax.set_title('Feature Importance (Random Forest)', fontsize=13, fontweight='bold')
ax.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()
</code></pre>

<h3 id="12lightgbm">【例12】勾配ブースティング（LightGBM）</h3>
<pre class="codehilite"><code class="language-python">import lightgbm as lgb

# LightGBMモデルの構築
params = {
    'objective': 'regression',
    'metric': 'rmse',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'n_estimators': 200,
    'random_state': 42,
    'verbose': -1
}

model_lgb = lgb.LGBMRegressor(**params)
model_lgb.fit(X_train, y_train)

# 予測
y_train_pred_lgb = model_lgb.predict(X_train)
y_test_pred_lgb = model_lgb.predict(X_test)

# 評価
r2_train_lgb = r2_score(y_train, y_train_pred_lgb)
r2_test_lgb = r2_score(y_test, y_test_pred_lgb)
rmse_test_lgb = np.sqrt(mean_squared_error(y_test, y_test_pred_lgb))
mae_test_lgb = mean_absolute_error(y_test, y_test_pred_lgb)

print(&quot;=&quot; * 60)
print(&quot;勾配ブースティング（LightGBM）&quot;)
print(&quot;=&quot; * 60)
print(f&quot;訓練データ R²: {r2_train_lgb:.4f}&quot;)
print(f&quot;テストデータ R²: {r2_test_lgb:.4f}&quot;)
print(f&quot;テストデータ RMSE: {rmse_test_lgb:.4f} nm&quot;)
print(f&quot;テストデータ MAE: {mae_test_lgb:.4f} nm&quot;)

# 予測値 vs 実測値プロット
fig, ax = plt.subplots(figsize=(8, 6))
ax.scatter(y_test, y_test_pred_lgb, alpha=0.6, edgecolors='k', s=60)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
        'r--', lw=2, label='Perfect Prediction')
ax.set_xlabel('Actual LSPR (nm)', fontsize=12)
ax.set_ylabel('Predicted LSPR (nm)', fontsize=12)
ax.set_title(f'LightGBM Prediction (R² = {r2_test_lgb:.3f})',
             fontsize=13, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<h3 id="13svr">【例13】サポートベクター回帰（SVR）</h3>
<pre class="codehilite"><code class="language-python">from sklearn.svm import SVR

# SVRモデル（RBFカーネル）
model_svr = SVR(kernel='rbf', C=10, gamma='scale', epsilon=0.1)
model_svr.fit(X_train, y_train)

# 予測
y_train_pred_svr = model_svr.predict(X_train)
y_test_pred_svr = model_svr.predict(X_test)

# 評価
r2_train_svr = r2_score(y_train, y_train_pred_svr)
r2_test_svr = r2_score(y_test, y_test_pred_svr)
rmse_test_svr = np.sqrt(mean_squared_error(y_test, y_test_pred_svr))
mae_test_svr = mean_absolute_error(y_test, y_test_pred_svr)

print(&quot;=&quot; * 60)
print(&quot;サポートベクター回帰（SVR）&quot;)
print(&quot;=&quot; * 60)
print(f&quot;訓練データ R²: {r2_train_svr:.4f}&quot;)
print(f&quot;テストデータ R²: {r2_test_svr:.4f}&quot;)
print(f&quot;テストデータ RMSE: {rmse_test_svr:.4f} nm&quot;)
print(f&quot;テストデータ MAE: {mae_test_svr:.4f} nm&quot;)
print(f&quot;サポートベクター数: {len(model_svr.support_)}&quot;)
</code></pre>

<h3 id="14mlp-regressor">【例14】ニューラルネットワーク（MLP Regressor）</h3>
<pre class="codehilite"><code class="language-python">from sklearn.neural_network import MLPRegressor

# MLPモデル
model_mlp = MLPRegressor(hidden_layer_sizes=(100, 50),
                         activation='relu',
                         solver='adam',
                         alpha=0.001,
                         max_iter=500,
                         random_state=42,
                         early_stopping=True,
                         validation_fraction=0.1,
                         verbose=False)

model_mlp.fit(X_train, y_train)

# 予測
y_train_pred_mlp = model_mlp.predict(X_train)
y_test_pred_mlp = model_mlp.predict(X_test)

# 評価
r2_train_mlp = r2_score(y_train, y_train_pred_mlp)
r2_test_mlp = r2_score(y_test, y_test_pred_mlp)
rmse_test_mlp = np.sqrt(mean_squared_error(y_test, y_test_pred_mlp))
mae_test_mlp = mean_absolute_error(y_test, y_test_pred_mlp)

print(&quot;=&quot; * 60)
print(&quot;ニューラルネットワーク（MLP Regressor）&quot;)
print(&quot;=&quot; * 60)
print(f&quot;訓練データ R²: {r2_train_mlp:.4f}&quot;)
print(f&quot;テストデータ R²: {r2_test_mlp:.4f}&quot;)
print(f&quot;テストデータ RMSE: {rmse_test_mlp:.4f} nm&quot;)
print(f&quot;テストデータ MAE: {mae_test_mlp:.4f} nm&quot;)
print(f&quot;反復回数: {model_mlp.n_iter_}&quot;)
print(f&quot;隠れ層の構造: {model_mlp.hidden_layer_sizes}&quot;)
</code></pre>

<h3 id="15">【例15】モデル性能比較</h3>
<pre class="codehilite"><code class="language-python"># 全モデルの性能をまとめる
results = pd.DataFrame({
    'Model': ['Linear Regression', 'Random Forest', 'LightGBM', 'SVR', 'MLP'],
    'R² (Train)': [r2_train_lr, r2_train_rf, r2_train_lgb, r2_train_svr, r2_train_mlp],
    'R² (Test)': [r2_test_lr, r2_test_rf, r2_test_lgb, r2_test_svr, r2_test_mlp],
    'RMSE (Test)': [rmse_test_lr, rmse_test_rf, rmse_test_lgb, rmse_test_svr, rmse_test_mlp],
    'MAE (Test)': [mae_test_lr, mae_test_rf, mae_test_lgb, mae_test_svr, mae_test_mlp]
})

results['Overfit'] = results['R² (Train)'] - results['R² (Test)']

print(&quot;=&quot; * 80)
print(&quot;全モデルの性能比較&quot;)
print(&quot;=&quot; * 80)
print(results.to_string(index=False))

# 最良モデルの特定
best_model_idx = results['R² (Test)'].idxmax()
best_model_name = results.loc[best_model_idx, 'Model']
best_r2 = results.loc[best_model_idx, 'R² (Test)']

print(f&quot;\n最良モデル: {best_model_name} (R² = {best_r2:.4f})&quot;)

# 可視化
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# R²スコア比較
x_pos = np.arange(len(results))
axes[0].bar(x_pos, results['R² (Test)'], alpha=0.7, color='steelblue',
            edgecolor='black', label='Test R²')
axes[0].set_xticks(x_pos)
axes[0].set_xticklabels(results['Model'], rotation=15, ha='right')
axes[0].set_ylabel('R² Score', fontsize=12)
axes[0].set_title('Model Comparison: R² Score', fontsize=13, fontweight='bold')
axes[0].grid(True, alpha=0.3, axis='y')
axes[0].legend()

# RMSE比較
axes[1].bar(x_pos, results['RMSE (Test)'], alpha=0.7, color='coral',
            edgecolor='black', label='Test RMSE')
axes[1].set_xticks(x_pos)
axes[1].set_xticklabels(results['Model'], rotation=15, ha='right')
axes[1].set_ylabel('RMSE (nm)', fontsize=12)
axes[1].set_title('Model Comparison: RMSE', fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='y')
axes[1].legend()

plt.tight_layout()
plt.show()
</code></pre>

<hr />
<h2 id="35">3.5 量子ドット発光波長予測</h2>
<h3 id="16cdse">【例16】データ生成：CdSe量子ドット</h3>
<p>CdSe量子ドットの発光波長は、Brus方程式に基づきサイズに依存します。</p>
<pre class="codehilite"><code class="language-python"># CdSe量子ドットデータの生成
np.random.seed(100)

n_qd_samples = 150

# 量子ドットのサイズ（2-10 nm）
size_qd = np.random.uniform(2, 10, n_qd_samples)

# Brus方程式の簡易近似: emission = 520 + 130/(size^0.8) + noise
emission = 520 + 130 / (size_qd ** 0.8) + np.random.normal(0, 10, n_qd_samples)

# 合成条件
synthesis_time = np.random.uniform(10, 120, n_qd_samples)  # 分
precursor_ratio = np.random.uniform(0.5, 2.0, n_qd_samples)  # モル比

# データフレーム作成
data_qd = pd.DataFrame({
    'size_nm': size_qd,
    'emission_nm': emission,
    'synthesis_time_min': synthesis_time,
    'precursor_ratio': precursor_ratio
})

print(&quot;=&quot; * 60)
print(&quot;CdSe量子ドットデータの生成完了&quot;)
print(&quot;=&quot; * 60)
print(data_qd.head(10))
print(&quot;\n基本統計量:&quot;)
print(data_qd.describe())

# サイズと発光波長の関係プロット
fig, ax = plt.subplots(figsize=(10, 6))
scatter = ax.scatter(data_qd['size_nm'], data_qd['emission_nm'],
                     c=data_qd['synthesis_time_min'], cmap='plasma',
                     s=80, alpha=0.7, edgecolors='k')
ax.set_xlabel('Quantum Dot Size (nm)', fontsize=12)
ax.set_ylabel('Emission Wavelength (nm)', fontsize=12)
ax.set_title('CdSe Quantum Dot: Size vs Emission Wavelength',
             fontsize=13, fontweight='bold')
cbar = plt.colorbar(scatter, ax=ax)
cbar.set_label('Synthesis Time (min)', fontsize=10)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<h3 id="17lightgbm">【例17】量子ドットモデル（LightGBM）</h3>
<pre class="codehilite"><code class="language-python"># 特徴量とターゲットの分離
X_qd = data_qd[['size_nm', 'synthesis_time_min', 'precursor_ratio']]
y_qd = data_qd['emission_nm']

# スケーリング
scaler_qd = StandardScaler()
X_qd_scaled = scaler_qd.fit_transform(X_qd)

# 訓練/テスト分割
X_qd_train, X_qd_test, y_qd_train, y_qd_test = train_test_split(
    X_qd_scaled, y_qd, test_size=0.2, random_state=42
)

# LightGBMモデル
model_qd = lgb.LGBMRegressor(
    objective='regression',
    num_leaves=31,
    learning_rate=0.05,
    n_estimators=200,
    random_state=42,
    verbose=-1
)

model_qd.fit(X_qd_train, y_qd_train)

# 予測
y_qd_train_pred = model_qd.predict(X_qd_train)
y_qd_test_pred = model_qd.predict(X_qd_test)

# 評価
r2_qd_train = r2_score(y_qd_train, y_qd_train_pred)
r2_qd_test = r2_score(y_qd_test, y_qd_test_pred)
rmse_qd = np.sqrt(mean_squared_error(y_qd_test, y_qd_test_pred))
mae_qd = mean_absolute_error(y_qd_test, y_qd_test_pred)

print(&quot;=&quot; * 60)
print(&quot;量子ドット発光波長予測モデル（LightGBM）&quot;)
print(&quot;=&quot; * 60)
print(f&quot;訓練データ R²: {r2_qd_train:.4f}&quot;)
print(f&quot;テストデータ R²: {r2_qd_test:.4f}&quot;)
print(f&quot;テストデータ RMSE: {rmse_qd:.4f} nm&quot;)
print(f&quot;テストデータ MAE: {mae_qd:.4f} nm&quot;)
</code></pre>

<h3 id="18">【例18】予測結果の可視化</h3>
<pre class="codehilite"><code class="language-python"># 予測値 vs 実測値プロット（信頼区間付き）
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# テストデータのプロット
axes[0].scatter(y_qd_test, y_qd_test_pred, alpha=0.6, s=80,
                edgecolors='k', label='Test Data')
axes[0].plot([y_qd_test.min(), y_qd_test.max()],
             [y_qd_test.min(), y_qd_test.max()],
             'r--', lw=2, label='Perfect Prediction')

# ±10 nm の範囲を表示
axes[0].fill_between([y_qd_test.min(), y_qd_test.max()],
                     [y_qd_test.min()-10, y_qd_test.max()-10],
                     [y_qd_test.min()+10, y_qd_test.max()+10],
                     alpha=0.2, color='gray', label='±10 nm')

axes[0].set_xlabel('Actual Emission (nm)', fontsize=12)
axes[0].set_ylabel('Predicted Emission (nm)', fontsize=12)
axes[0].set_title(f'QD Emission Prediction (R² = {r2_qd_test:.3f})',
                  fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# サイズ別の予測精度
size_bins = [2, 4, 6, 8, 10]
size_labels = ['2-4 nm', '4-6 nm', '6-8 nm', '8-10 nm']
data_qd_test = pd.DataFrame({
    'size': X_qd.iloc[y_qd_test.index]['size_nm'].values,
    'actual': y_qd_test.values,
    'predicted': y_qd_test_pred
})
data_qd_test['size_bin'] = pd.cut(data_qd_test['size'], bins=size_bins, labels=size_labels)
data_qd_test['error'] = np.abs(data_qd_test['actual'] - data_qd_test['predicted'])

# サイズビンごとの平均誤差
error_by_size = data_qd_test.groupby('size_bin')['error'].mean()

axes[1].bar(range(len(error_by_size)), error_by_size.values,
            color='coral', edgecolor='black', alpha=0.7)
axes[1].set_xticks(range(len(error_by_size)))
axes[1].set_xticklabels(error_by_size.index)
axes[1].set_ylabel('Mean Absolute Error (nm)', fontsize=12)
axes[1].set_xlabel('QD Size Range', fontsize=12)
axes[1].set_title('Prediction Error by QD Size', fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print(f&quot;\n全体の平均絶対誤差: {mae_qd:.2f} nm&quot;)
print(&quot;サイズ別の平均絶対誤差:&quot;)
print(error_by_size)
</code></pre>

<hr />
<h2 id="36">3.6 特徴量重要度分析</h2>
<h3 id="19lightgbm">【例19】特徴量重要度（LightGBM）</h3>
<pre class="codehilite"><code class="language-python"># LightGBMモデルの特徴量重要度（ゲインベース）
importance_gain = model_lgb.feature_importances_
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': importance_gain
}).sort_values('Importance', ascending=False)

print(&quot;=&quot; * 60)
print(&quot;特徴量重要度（LightGBM）&quot;)
print(&quot;=&quot; * 60)
print(importance_df)

# 可視化
fig, ax = plt.subplots(figsize=(8, 5))
colors = ['steelblue', 'coral', 'lightgreen']
ax.barh(importance_df['Feature'], importance_df['Importance'],
        color=colors, edgecolor='black')
ax.set_xlabel('Feature Importance (Gain)', fontsize=12)
ax.set_title('Feature Importance: LSPR Prediction',
             fontsize=13, fontweight='bold')
ax.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()

print(f&quot;\n最も重要な特徴量: {importance_df.iloc[0]['Feature']}&quot;)
</code></pre>

<h3 id="20shap">【例20】SHAP分析：予測解釈</h3>
<pre class="codehilite"><code class="language-python">import shap

# SHAP Explainerの作成
explainer = shap.Explainer(model_lgb, X_train)
shap_values = explainer(X_test)

print(&quot;=&quot; * 60)
print(&quot;SHAP分析&quot;)
print(&quot;=&quot; * 60)
print(&quot;SHAP値の計算完了&quot;)
print(f&quot;SHAP値の形状: {shap_values.values.shape}&quot;)

# SHAP Summary Plot
fig, ax = plt.subplots(figsize=(10, 6))
shap.summary_plot(shap_values, X_test, feature_names=X.columns, show=False)
plt.title('SHAP Summary Plot: Feature Impact on LSPR Prediction',
          fontsize=13, fontweight='bold', pad=20)
plt.tight_layout()
plt.show()

# SHAP Dependence Plot（最も重要な特徴量）
top_feature_idx = importance_df.index[0]
top_feature_name = X.columns[top_feature_idx]

fig, ax = plt.subplots(figsize=(10, 6))
shap.dependence_plot(top_feature_idx, shap_values.values, X_test,
                     feature_names=X.columns, show=False)
plt.title(f'SHAP Dependence Plot: {top_feature_name}',
          fontsize=13, fontweight='bold')
plt.tight_layout()
plt.show()

print(f&quot;\nSHAP分析により、{top_feature_name}がLSPR波長予測に最も影響することが確認されました&quot;)
</code></pre>

<hr />
<h2 id="37">3.7 ベイズ最適化によるナノ材料設計</h2>
<p>目標：目標LSPR波長（550 nm）を実現する最適な合成条件を探索</p>
<h3 id="21">【例21】探索空間の定義</h3>
<pre class="codehilite"><code class="language-python">from skopt.space import Real

# 探索空間の定義
# サイズ: 10-40 nm、温度: 20-80°C、pH: 4-10
search_space = [
    Real(10, 40, name='size_nm'),
    Real(20, 80, name='temperature_C'),
    Real(4, 10, name='pH')
]

print(&quot;=&quot; * 60)
print(&quot;ベイズ最適化：探索空間の定義&quot;)
print(&quot;=&quot; * 60)
for dim in search_space:
    print(f&quot;  {dim.name}: [{dim.bounds[0]}, {dim.bounds[1]}]&quot;)

print(&quot;\n目標: LSPR波長 = 550 nm を実現する条件を探索&quot;)
</code></pre>

<h3 id="22">【例22】目的関数の設定</h3>
<pre class="codehilite"><code class="language-python"># 目的関数：予測LSPR波長と目標波長（550 nm）の差の絶対値を最小化
target_lspr = 550.0

def objective_function(params):
    &quot;&quot;&quot;
    ベイズ最適化の目的関数

    Parameters:
    -----------
    params : list
        [size_nm, temperature_C, pH]

    Returns:
    --------
    float
        目標波長との誤差（最小化する値）
    &quot;&quot;&quot;
    # パラメータを取得
    size, temp, ph = params

    # 特徴量を構築（スケーリング適用）
    features = np.array([[size, temp, ph]])
    features_scaled = scaler.transform(features)

    # LSPR波長を予測
    predicted_lspr = model_lgb.predict(features_scaled)[0]

    # 目標波長との誤差（絶対値）
    error = abs(predicted_lspr - target_lspr)

    return error

# テスト実行
test_params = [20.0, 50.0, 7.0]
test_error = objective_function(test_params)
print(f&quot;\nテスト実行:&quot;)
print(f&quot;  パラメータ: size={test_params[0]} nm, temp={test_params[1]}°C, pH={test_params[2]}&quot;)
print(f&quot;  目的関数値（誤差）: {test_error:.4f} nm&quot;)
</code></pre>

<h3 id="23scikit-optimize">【例23】ベイズ最適化の実行（scikit-optimize）</h3>
<pre class="codehilite"><code class="language-python">from skopt import gp_minimize
from skopt.plots import plot_convergence, plot_objective

# ベイズ最適化の実行
print(&quot;\n&quot; + &quot;=&quot; * 60)
print(&quot;ベイズ最適化の実行中...&quot;)
print(&quot;=&quot; * 60)

result = gp_minimize(
    func=objective_function,
    dimensions=search_space,
    n_calls=50,  # 評価回数
    n_initial_points=10,  # ランダムサンプリング回数
    random_state=42,
    verbose=False
)

print(&quot;最適化完了！&quot;)
print(&quot;\n&quot; + &quot;=&quot; * 60)
print(&quot;最適化結果&quot;)
print(&quot;=&quot; * 60)
print(f&quot;最小目的関数値（誤差）: {result.fun:.4f} nm&quot;)
print(f&quot;\n最適パラメータ:&quot;)
print(f&quot;  サイズ: {result.x[0]:.2f} nm&quot;)
print(f&quot;  温度: {result.x[1]:.2f} °C&quot;)
print(f&quot;  pH: {result.x[2]:.2f}&quot;)

# 最適条件での予測LSPR波長を計算
optimal_features = np.array([result.x])
optimal_features_scaled = scaler.transform(optimal_features)
predicted_optimal_lspr = model_lgb.predict(optimal_features_scaled)[0]

print(f&quot;\n予測されるLSPR波長: {predicted_optimal_lspr:.2f} nm&quot;)
print(f&quot;目標LSPR波長: {target_lspr} nm&quot;)
print(f&quot;達成精度: {abs(predicted_optimal_lspr - target_lspr):.2f} nm&quot;)
</code></pre>

<h3 id="24">【例24】最適化結果の可視化</h3>
<pre class="codehilite"><code class="language-python"># 最適化プロセスの可視化
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# 収束プロット
plot_convergence(result, ax=axes[0])
axes[0].set_title('Convergence Plot', fontsize=13, fontweight='bold')
axes[0].set_ylabel('Objective Value (Error, nm)', fontsize=11)
axes[0].set_xlabel('Number of Evaluations', fontsize=11)
axes[0].grid(True, alpha=0.3)

# 評価履歴のプロット
iterations = range(1, len(result.func_vals) + 1)
axes[1].plot(iterations, result.func_vals, 'o-', alpha=0.6, label='Evaluation')
axes[1].plot(iterations, np.minimum.accumulate(result.func_vals),
             'r-', linewidth=2, label='Best So Far')
axes[1].set_xlabel('Iteration', fontsize=11)
axes[1].set_ylabel('Objective Value (Error, nm)', fontsize=11)
axes[1].set_title('Optimization Progress', fontsize=13, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3 id="25">【例25】収束プロット</h3>
<pre class="codehilite"><code class="language-python"># 詳細な収束プロット（最良値の推移）
fig, ax = plt.subplots(figsize=(10, 6))

cumulative_min = np.minimum.accumulate(result.func_vals)
iterations = np.arange(1, len(cumulative_min) + 1)

ax.plot(iterations, cumulative_min, 'b-', linewidth=2, marker='o',
        markersize=4, label='Best Error')
ax.axhline(y=result.fun, color='r', linestyle='--', linewidth=2,
           label=f'Final Best: {result.fun:.2f} nm')
ax.fill_between(iterations, 0, cumulative_min, alpha=0.2, color='blue')

ax.set_xlabel('Iteration', fontsize=12)
ax.set_ylabel('Minimum Error (nm)', fontsize=12)
ax.set_title('Bayesian Optimization: Convergence to Optimal Solution',
             fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;\n{len(result.func_vals)}回の評価で最適解に収束しました&quot;)
print(f&quot;初期評価での最良誤差: {result.func_vals[0]:.2f} nm&quot;)
print(f&quot;最終的な最良誤差: {result.fun:.2f} nm&quot;)
print(f&quot;改善率: {(1 - result.fun/result.func_vals[0])*100:.1f}%&quot;)
</code></pre>

<hr />
<h2 id="38">3.8 多目的最適化：サイズと発光効率のトレードオフ</h2>
<h3 id="26paretonsga-ii">【例26】Pareto最適化（NSGA-II）</h3>
<p>多目的最適化では、複数の目的を同時に最適化します。ここでは、量子ドットのサイズを最小化しつつ、発光効率（仮想的な指標）を最大化します。</p>
<pre class="codehilite"><code class="language-python"># pymooを使用した多目的最適化
try:
    from pymoo.core.problem import Problem
    from pymoo.algorithms.moo.nsga2 import NSGA2
    from pymoo.optimize import minimize as pymoo_minimize
    from pymoo.operators.crossover.sbx import SBX
    from pymoo.operators.mutation.pm import PM
    from pymoo.operators.sampling.rnd import FloatRandomSampling

    # 多目的最適化問題の定義
    class QuantumDotOptimization(Problem):
        def __init__(self):
            super().__init__(
                n_var=3,  # 変数数（size, synthesis_time, precursor_ratio）
                n_obj=2,  # 目的関数数（size最小化、emission効率最大化）
                n_constr=0,  # 制約なし
                xl=np.array([2.0, 10.0, 0.5]),  # 下限
                xu=np.array([10.0, 120.0, 2.0])  # 上限
            )

        def _evaluate(self, X, out, *args, **kwargs):
            # 目的関数1: サイズの最小化
            obj1 = X[:, 0]  # size

            # 目的関数2: 発光効率の最大化（負の値で最小化問題に変換）
            # 効率は仮想的に、emission wavelengthが550 nmに近いほど高いと仮定
            features = X  # [size, synthesis_time, precursor_ratio]
            features_scaled = scaler_qd.transform(features)
            predicted_emission = model_qd.predict(features_scaled)

            # 効率：550 nmからのずれが小さいほど高い（負の値で最大化→最小化）
            efficiency = -np.abs(predicted_emission - 550)
            obj2 = -efficiency  # 最大化を最小化問題に変換

            out[&quot;F&quot;] = np.column_stack([obj1, obj2])

    # 問題のインスタンス化
    problem = QuantumDotOptimization()

    # NSGA-IIアルゴリズム
    algorithm = NSGA2(
        pop_size=40,
        sampling=FloatRandomSampling(),
        crossover=SBX(prob=0.9, eta=15),
        mutation=PM(eta=20),
        eliminate_duplicates=True
    )

    # 最適化実行
    print(&quot;=&quot; * 60)
    print(&quot;多目的最適化（NSGA-II）実行中...&quot;)
    print(&quot;=&quot; * 60)

    res = pymoo_minimize(
        problem,
        algorithm,
        ('n_gen', 50),  # 世代数
        seed=42,
        verbose=False
    )

    print(&quot;多目的最適化完了！&quot;)
    print(f&quot;\nパレート最適解の数: {len(res.F)}&quot;)

    # パレート最適解の表示（上位5つ）
    print(&quot;\n代表的なパレート最適解（上位5つ）:&quot;)
    pareto_solutions = pd.DataFrame({
        'Size (nm)': res.X[:, 0],
        'Synthesis Time (min)': res.X[:, 1],
        'Precursor Ratio': res.X[:, 2],
        'Obj1: Size': res.F[:, 0],
        'Obj2: -Efficiency': res.F[:, 1]
    }).head(5)
    print(pareto_solutions.to_string(index=False))

    PYMOO_AVAILABLE = True

except ImportError:
    print(&quot;=&quot; * 60)
    print(&quot;pymooがインストールされていません&quot;)
    print(&quot;=&quot; * 60)
    print(&quot;多目的最適化にはpymooが必要です:&quot;)
    print(&quot;  pip install pymoo&quot;)
    print(&quot;\n代わりに、簡易的な多目的最適化の例を表示します&quot;)

    # 簡易的なグリッドサーチによる多目的最適化の模擬
    sizes = np.linspace(2, 10, 20)
    times = np.linspace(10, 120, 20)
    ratios = np.linspace(0.5, 2.0, 20)

    # グリッドサーチ（サンプリング）
    sample_X = []
    sample_F = []

    for size in sizes[::4]:
        for time in times[::4]:
            for ratio in ratios[::4]:
                features = np.array([[size, time, ratio]])
                features_scaled = scaler_qd.transform(features)
                emission = model_qd.predict(features_scaled)[0]

                obj1 = size
                obj2 = abs(emission - 550)

                sample_X.append([size, time, ratio])
                sample_F.append([obj1, obj2])

    sample_X = np.array(sample_X)
    sample_F = np.array(sample_F)

    print(&quot;\nグリッドサーチによる解の探索完了&quot;)
    print(f&quot;探索した解の数: {len(sample_F)}&quot;)

    res = type('Result', (), {
        'X': sample_X,
        'F': sample_F
    })()

    PYMOO_AVAILABLE = False
</code></pre>

<h3 id="27pareto">【例27】Paretoフロントの可視化</h3>
<pre class="codehilite"><code class="language-python"># Paretoフロントの可視化
fig, ax = plt.subplots(figsize=(10, 7))

if PYMOO_AVAILABLE:
    # NSGA-IIの結果をプロット
    ax.scatter(res.F[:, 0], -res.F[:, 1], c='blue', s=80, alpha=0.6,
               edgecolors='black', label='Pareto Optimal Solutions')

    title_suffix = &quot;(NSGA-II)&quot;
else:
    # グリッドサーチの結果をプロット
    ax.scatter(res.F[:, 0], res.F[:, 1], c='blue', s=60, alpha=0.5,
               edgecolors='black', label='Sampled Solutions')

    title_suffix = &quot;(Grid Search)&quot;

ax.set_xlabel('Objective 1: Size (nm) [Minimize]', fontsize=12)

if PYMOO_AVAILABLE:
    ax.set_ylabel('Objective 2: Efficiency [Maximize]', fontsize=12)
else:
    ax.set_ylabel('Objective 2: Deviation from 550nm [Minimize]', fontsize=12)

ax.set_title(f'Pareto Front: Size vs Emission Efficiency {title_suffix}',
             fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;\nパレートフロント:&quot;)
print(&quot;  サイズを小さくすると効率が下がり、効率を上げるとサイズが大きくなる&quot;)
print(&quot;  → トレードオフ関係が明確に&quot;)
</code></pre>

<hr />
<h2 id="39-tem">3.9 TEM画像解析とサイズ分布</h2>
<h3 id="28tem">【例28】模擬TEMデータの生成</h3>
<p>TEM（透過型電子顕微鏡）で測定されるナノ粒子サイズは、しばしば対数正規分布に従います。</p>
<pre class="codehilite"><code class="language-python">from scipy.stats import lognorm

# 対数正規分布に従うTEMサイズデータの生成
np.random.seed(200)

# パラメータ
mean_size = 20  # 平均サイズ（nm）
cv = 0.3  # 変動係数（標準偏差/平均）

# 対数正規分布のパラメータ計算
sigma = np.sqrt(np.log(1 + cv**2))
mu = np.log(mean_size) - 0.5 * sigma**2

# サンプル生成（500粒子）
tem_sizes = lognorm.rvs(s=sigma, scale=np.exp(mu), size=500)

print(&quot;=&quot; * 60)
print(&quot;TEM測定データの生成（対数正規分布）&quot;)
print(&quot;=&quot; * 60)
print(f&quot;サンプル数: {len(tem_sizes)}粒子&quot;)
print(f&quot;平均サイズ: {tem_sizes.mean():.2f} nm&quot;)
print(f&quot;標準偏差: {tem_sizes.std():.2f} nm&quot;)
print(f&quot;中央値: {np.median(tem_sizes):.2f} nm&quot;)
print(f&quot;最小値: {tem_sizes.min():.2f} nm&quot;)
print(f&quot;最大値: {tem_sizes.max():.2f} nm&quot;)

# ヒストグラム
fig, ax = plt.subplots(figsize=(10, 6))
ax.hist(tem_sizes, bins=40, alpha=0.7, color='lightblue',
        edgecolor='black', density=True, label='TEM Data')
ax.set_xlabel('Particle Size (nm)', fontsize=12)
ax.set_ylabel('Probability Density', fontsize=12)
ax.set_title('TEM Size Distribution (Lognormal)', fontsize=13, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
</code></pre>

<h3 id="29">【例29】対数正規分布フィッティング</h3>
<pre class="codehilite"><code class="language-python"># 対数正規分布のフィッティング
shape_fit, loc_fit, scale_fit = lognorm.fit(tem_sizes, floc=0)

# フィッティングされた分布のパラメータ
fitted_mean = np.exp(np.log(scale_fit) + 0.5 * shape_fit**2)
fitted_std = fitted_mean * np.sqrt(np.exp(shape_fit**2) - 1)

print(&quot;=&quot; * 60)
print(&quot;対数正規分布フィッティング結果&quot;)
print(&quot;=&quot; * 60)
print(f&quot;形状パラメータ (sigma): {shape_fit:.4f}&quot;)
print(f&quot;スケールパラメータ: {scale_fit:.4f}&quot;)
print(f&quot;フィッティングされた平均サイズ: {fitted_mean:.2f} nm&quot;)
print(f&quot;フィッティングされた標準偏差: {fitted_std:.2f} nm&quot;)

# 実測値との比較
print(f&quot;\n実測値との比較:&quot;)
print(f&quot;  平均サイズ - 実測: {tem_sizes.mean():.2f} nm, フィット: {fitted_mean:.2f} nm&quot;)
print(f&quot;  標準偏差 - 実測: {tem_sizes.std():.2f} nm, フィット: {fitted_std:.2f} nm&quot;)
</code></pre>

<h3 id="30">【例30】フィッティング結果の可視化</h3>
<pre class="codehilite"><code class="language-python"># フィッティング結果の詳細可視化
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ヒストグラムとフィッティング曲線
axes[0].hist(tem_sizes, bins=40, alpha=0.6, color='lightblue',
             edgecolor='black', density=True, label='TEM Data')

# フィッティングされた対数正規分布
x_range = np.linspace(0, tem_sizes.max(), 200)
fitted_pdf = lognorm.pdf(x_range, shape_fit, loc=loc_fit, scale=scale_fit)
axes[0].plot(x_range, fitted_pdf, 'r-', linewidth=2,
             label=f'Lognormal Fit (μ={fitted_mean:.1f}, σ={fitted_std:.1f})')

axes[0].set_xlabel('Particle Size (nm)', fontsize=12)
axes[0].set_ylabel('Probability Density', fontsize=12)
axes[0].set_title('TEM Size Distribution with Lognormal Fit',
                  fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3, axis='y')

# Q-Qプロット（分位点プロット）
from scipy.stats import probplot

probplot(tem_sizes, dist=lognorm, sparams=(shape_fit, loc_fit, scale_fit),
         plot=axes[1])
axes[1].set_title('Q-Q Plot: Lognormal Distribution',
                  fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;\nQ-Qプロット: データが直線上にあれば、対数正規分布に良く従っています&quot;)
</code></pre>

<hr />
<h2 id="310-md">3.10 分子動力学（MD）データ解析</h2>
<h3 id="31md">【例31】MDシミュレーションデータの読み込み</h3>
<p>分子動力学シミュレーションでは、ナノ粒子の原子配置の時間発展を追跡します。</p>
<pre class="codehilite"><code class="language-python"># MDシミュレーションデータの模擬生成
# 実際のMDデータはLAMMPS, GROMACS等から取得

np.random.seed(300)

n_atoms = 100  # 原子数
n_steps = 1000  # タイムステップ数
dt = 0.001  # タイムステップ（ps）

# 初期位置（nm）
positions_initial = np.random.uniform(-1, 1, (n_atoms, 3))

# 時間発展の模擬（ランダムウォーク）
positions = np.zeros((n_steps, n_atoms, 3))
positions[0] = positions_initial

for t in range(1, n_steps):
    # ランダムな変位
    displacement = np.random.normal(0, 0.01, (n_atoms, 3))
    positions[t] = positions[t-1] + displacement

print(&quot;=&quot; * 60)
print(&quot;MDシミュレーションデータの生成&quot;)
print(&quot;=&quot; * 60)
print(f&quot;原子数: {n_atoms}&quot;)
print(f&quot;タイムステップ数: {n_steps}&quot;)
print(f&quot;シミュレーション時間: {n_steps * dt:.2f} ps&quot;)
print(f&quot;データ形状: {positions.shape} (time, atoms, xyz)&quot;)

# 中心原子（原子0）の軌跡をプロット
fig = plt.figure(figsize=(12, 5))

ax1 = fig.add_subplot(121, projection='3d')
ax1.plot(positions[:, 0, 0], positions[:, 0, 1], positions[:, 0, 2],
         'b-', alpha=0.5, linewidth=0.5)
ax1.scatter(positions[0, 0, 0], positions[0, 0, 1], positions[0, 0, 2],
            c='green', s=100, label='Start', edgecolors='k')
ax1.scatter(positions[-1, 0, 0], positions[-1, 0, 1], positions[-1, 0, 2],
            c='red', s=100, label='End', edgecolors='k')
ax1.set_xlabel('X (nm)')
ax1.set_ylabel('Y (nm)')
ax1.set_zlabel('Z (nm)')
ax1.set_title('Atom Trajectory (Atom 0)', fontweight='bold')
ax1.legend()

ax2 = fig.add_subplot(122)
ax2.plot(np.arange(n_steps) * dt, positions[:, 0, 0], label='X')
ax2.plot(np.arange(n_steps) * dt, positions[:, 0, 1], label='Y')
ax2.plot(np.arange(n_steps) * dt, positions[:, 0, 2], label='Z')
ax2.set_xlabel('Time (ps)', fontsize=11)
ax2.set_ylabel('Position (nm)', fontsize=11)
ax2.set_title('Position vs Time (Atom 0)', fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3 id="32rdf">【例32】動径分布関数（RDF）の計算</h3>
<p>動径分布関数（Radial Distribution Function, RDF）は、原子間距離の分布を表します。</p>
<pre class="codehilite"><code class="language-python"># 動径分布関数（RDF）の計算
def calculate_rdf(positions, r_max=2.0, n_bins=100):
    &quot;&quot;&quot;
    動径分布関数を計算

    Parameters:
    -----------
    positions : ndarray
        原子位置 (n_atoms, 3)
    r_max : float
        最大距離（nm）
    n_bins : int
        ビン数

    Returns:
    --------
    r_bins : ndarray
        距離ビン
    rdf : ndarray
        動径分布関数
    &quot;&quot;&quot;
    n_atoms = positions.shape[0]

    # 全原子ペア間の距離を計算
    distances = []
    for i in range(n_atoms):
        for j in range(i+1, n_atoms):
            dist = np.linalg.norm(positions[i] - positions[j])
            if dist &lt; r_max:
                distances.append(dist)

    distances = np.array(distances)

    # ヒストグラム
    hist, bin_edges = np.histogram(distances, bins=n_bins, range=(0, r_max))
    r_bins = (bin_edges[:-1] + bin_edges[1:]) / 2

    # 規格化（理想気体との比）
    dr = r_max / n_bins
    volume_shell = 4 * np.pi * r_bins**2 * dr
    n_ideal = volume_shell * (n_atoms / (4/3 * np.pi * r_max**3))

    rdf = hist / n_ideal / (n_atoms / 2)

    return r_bins, rdf

# 最終フレームでRDFを計算
final_positions = positions[-1]
r_bins, rdf = calculate_rdf(final_positions, r_max=1.5, n_bins=150)

print(&quot;=&quot; * 60)
print(&quot;動径分布関数（RDF）&quot;)
print(&quot;=&quot; * 60)
print(f&quot;計算完了: {len(r_bins)}個のビン&quot;)

# RDFのプロット
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(r_bins, rdf, 'b-', linewidth=2)
ax.axhline(y=1, color='r', linestyle='--', linewidth=1, label='Ideal Gas (g(r)=1)')
ax.set_xlabel('Distance r (nm)', fontsize=12)
ax.set_ylabel('g(r)', fontsize=12)
ax.set_title('Radial Distribution Function (RDF)', fontsize=13, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)
ax.set_ylim(0, max(rdf) * 1.1)

plt.tight_layout()
plt.show()

# ピーク位置の検出
from scipy.signal import find_peaks

peaks, _ = find_peaks(rdf, height=1.2, distance=10)
print(f&quot;\nRDFのピーク位置（特徴的な原子間距離）:&quot;)
for i, peak_idx in enumerate(peaks[:3], 1):
    print(f&quot;  ピーク{i}: r = {r_bins[peak_idx]:.3f} nm, g(r) = {rdf[peak_idx]:.2f}&quot;)
</code></pre>

<h3 id="33mean-squared-displacement">【例33】拡散係数の計算（Mean Squared Displacement）</h3>
<pre class="codehilite"><code class="language-python"># 平均二乗変位（MSD）の計算
def calculate_msd(positions):
    &quot;&quot;&quot;
    平均二乗変位を計算

    Parameters:
    -----------
    positions : ndarray
        原子位置 (n_steps, n_atoms, 3)

    Returns:
    --------
    msd : ndarray
        平均二乗変位 (n_steps,)
    &quot;&quot;&quot;
    n_steps, n_atoms, _ = positions.shape
    msd = np.zeros(n_steps)

    # 各タイムステップでのMSD
    for t in range(n_steps):
        displacement = positions[t] - positions[0]
        squared_displacement = np.sum(displacement**2, axis=1)
        msd[t] = np.mean(squared_displacement)

    return msd

# MSDの計算
msd = calculate_msd(positions)
time = np.arange(n_steps) * dt

print(&quot;=&quot; * 60)
print(&quot;平均二乗変位（MSD）と拡散係数&quot;)
print(&quot;=&quot; * 60)

# 拡散係数の計算（Einstein関係式: MSD = 6*D*t）
# 線形フィット（後半50%のデータを使用）
start_idx = n_steps // 2
fit_coeffs = np.polyfit(time[start_idx:], msd[start_idx:], 1)
slope = fit_coeffs[0]
diffusion_coefficient = slope / 6

print(f&quot;拡散係数 D = {diffusion_coefficient:.6f} nm²/ps&quot;)
print(f&quot;            = {diffusion_coefficient * 1e3:.6f} × 10⁻⁶ cm²/s&quot;)

# MSDプロット
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(time, msd, 'b-', linewidth=2, label='MSD')
ax.plot(time[start_idx:], fit_coeffs[0] * time[start_idx:] + fit_coeffs[1],
        'r--', linewidth=2, label=f'Linear Fit (D={diffusion_coefficient:.4f} nm²/ps)')
ax.set_xlabel('Time (ps)', fontsize=12)
ax.set_ylabel('MSD (nm²)', fontsize=12)
ax.set_title('Mean Squared Displacement (MSD)', fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;\n拡散係数は、ナノ粒子の移動性を定量的に評価する重要な指標です&quot;)
</code></pre>

<hr />
<h2 id="311">3.11 異常検知：品質管理への応用</h2>
<h3 id="34isolation-forest">【例34】Isolation Forestによる異常ナノ粒子検出</h3>
<p>製造プロセスで生成されたナノ粒子の品質管理に、機械学習による異常検知を適用します。</p>
<pre class="codehilite"><code class="language-python">from sklearn.ensemble import IsolationForest

# 正常データと異常データを混在させる
np.random.seed(400)

# 正常な金ナノ粒子データ（180サンプル）
normal_size = np.random.normal(15, 3, 180)
normal_lspr = 520 + 0.8 * (normal_size - 15) + np.random.normal(0, 3, 180)

# 異常なナノ粒子データ（20サンプル）：サイズが異常に大きいor小さい
anomaly_size = np.concatenate([
    np.random.uniform(5, 8, 10),  # 異常に小さい
    np.random.uniform(35, 50, 10)  # 異常に大きい
])
anomaly_lspr = 520 + 0.8 * (anomaly_size - 15) + np.random.normal(0, 8, 20)

# 全データを結合
all_size = np.concatenate([normal_size, anomaly_size])
all_lspr = np.concatenate([normal_lspr, anomaly_lspr])
all_data = np.column_stack([all_size, all_lspr])

# ラベル（正常=0、異常=1）
true_labels = np.concatenate([np.zeros(180), np.ones(20)])

print(&quot;=&quot; * 60)
print(&quot;異常検知（Isolation Forest）&quot;)
print(&quot;=&quot; * 60)
print(f&quot;全データ数: {len(all_data)}&quot;)
print(f&quot;正常データ: {int((true_labels == 0).sum())}サンプル&quot;)
print(f&quot;異常データ: {int((true_labels == 1).sum())}サンプル&quot;)

# Isolation Forestモデル
iso_forest = IsolationForest(
    contamination=0.1,  # 異常データの割合（10%と仮定）
    random_state=42,
    n_estimators=100
)

# 異常検知
predictions = iso_forest.fit_predict(all_data)
anomaly_scores = iso_forest.score_samples(all_data)

# 予測結果（1: 正常、-1: 異常）
predicted_anomalies = (predictions == -1)
true_anomalies = (true_labels == 1)

# 評価指標
from sklearn.metrics import confusion_matrix, classification_report

# 予測を0/1に変換
predicted_labels = (predictions == -1).astype(int)

print(&quot;\n混同行列:&quot;)
cm = confusion_matrix(true_labels, predicted_labels)
print(cm)

print(&quot;\n分類レポート:&quot;)
print(classification_report(true_labels, predicted_labels,
                            target_names=['Normal', 'Anomaly']))

# 検出率
detected_anomalies = np.sum(predicted_anomalies &amp; true_anomalies)
total_anomalies = np.sum(true_anomalies)
detection_rate = detected_anomalies / total_anomalies * 100

print(f&quot;\n異常検出率: {detection_rate:.1f}% ({detected_anomalies}/{total_anomalies})&quot;)
</code></pre>

<h3 id="35_1">【例35】異常サンプルの可視化</h3>
<pre class="codehilite"><code class="language-python"># 異常検知結果の可視化
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# 散布図（真のラベル）
axes[0].scatter(all_size[true_labels == 0], all_lspr[true_labels == 0],
                c='blue', s=60, alpha=0.6, label='Normal', edgecolors='k')
axes[0].scatter(all_size[true_labels == 1], all_lspr[true_labels == 1],
                c='red', s=100, alpha=0.8, marker='^', label='True Anomaly',
                edgecolors='k', linewidths=2)
axes[0].set_xlabel('Size (nm)', fontsize=12)
axes[0].set_ylabel('LSPR Wavelength (nm)', fontsize=12)
axes[0].set_title('True Labels', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# 散布図（予測結果）
normal_mask = ~predicted_anomalies
anomaly_mask = predicted_anomalies

axes[1].scatter(all_size[normal_mask], all_lspr[normal_mask],
                c='blue', s=60, alpha=0.6, label='Predicted Normal', edgecolors='k')
axes[1].scatter(all_size[anomaly_mask], all_lspr[anomaly_mask],
                c='orange', s=100, alpha=0.8, marker='X', label='Predicted Anomaly',
                edgecolors='k', linewidths=2)

# 正しく検出された異常を強調
correctly_detected = predicted_anomalies &amp; true_anomalies
axes[1].scatter(all_size[correctly_detected], all_lspr[correctly_detected],
                c='red', s=150, marker='*', label='Correctly Detected',
                edgecolors='black', linewidths=1.5, zorder=5)

axes[1].set_xlabel('Size (nm)', fontsize=12)
axes[1].set_ylabel('LSPR Wavelength (nm)', fontsize=12)
axes[1].set_title('Isolation Forest Predictions', fontsize=13, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 異常スコアの分布
fig, ax = plt.subplots(figsize=(10, 6))
ax.hist(anomaly_scores[true_labels == 0], bins=30, alpha=0.6,
        color='blue', label='Normal', edgecolor='black')
ax.hist(anomaly_scores[true_labels == 1], bins=30, alpha=0.6,
        color='red', label='Anomaly', edgecolor='black')
ax.set_xlabel('Anomaly Score', fontsize=12)
ax.set_ylabel('Frequency', fontsize=12)
ax.set_title('Anomaly Score Distribution', fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print(&quot;\n異常スコアが低い（負の値が大きい）ほど、異常である可能性が高い&quot;)
</code></pre>

<hr />
<h2 id="_4">まとめ</h2>
<p>本章では、Pythonを使ったナノ材料データ解析と機械学習の実践的手法を35個のコード例で学びました。</p>
<h3 id="_5">習得した主要技術</h3>
<ol>
<li>
<p><strong>データ生成と可視化</strong>（例1-5）<br />
   - 金ナノ粒子、量子ドットの合成データ生成<br />
   - ヒストグラム、散布図、3Dプロット、相関分析</p>
</li>
<li>
<p><strong>データ前処理</strong>（例6-9）<br />
   - 欠損値処理、外れ値検出、スケーリング、データ分割</p>
</li>
<li>
<p><strong>回帰モデルによる物性予測</strong>（例10-15）<br />
   - 線形回帰、ランダムフォレスト、LightGBM、SVR、MLP<br />
   - モデル性能比較（R²、RMSE、MAE）</p>
</li>
<li>
<p><strong>量子ドット発光予測</strong>（例16-18）<br />
   - Brus方程式に基づくデータ生成<br />
   - LightGBMによる予測モデル構築</p>
</li>
<li>
<p><strong>特徴量重要度とモデル解釈</strong>（例19-20）<br />
   - LightGBM特徴量重要度<br />
   - SHAP分析による予測の解釈</p>
</li>
<li>
<p><strong>ベイズ最適化</strong>（例21-25）<br />
   - 目標LSPR波長を実現する最適合成条件の探索<br />
   - 収束プロット、最適化プロセスの可視化</p>
</li>
<li>
<p><strong>多目的最適化</strong>（例26-27）<br />
   - NSGA-IIによるパレート最適化<br />
   - サイズと発光効率のトレードオフ分析</p>
</li>
<li>
<p><strong>TEM画像解析</strong>（例28-30）<br />
   - 対数正規分布によるサイズ分布フィッティング<br />
   - Q-Qプロットによる分布の検証</p>
</li>
<li>
<p><strong>分子動力学データ解析</strong>（例31-33）<br />
   - 原子軌跡の可視化<br />
   - 動径分布関数（RDF）の計算<br />
   - 拡散係数の算出（MSD法）</p>
</li>
<li>
<p><strong>異常検知</strong>（例34-35）</p>
<ul>
<li>Isolation Forestによる品質管理</li>
<li>異常ナノ粒子の自動検出</li>
</ul>
</li>
</ol>
<h3 id="_6">実践的な応用</h3>
<p>これらの技術は、以下のような実際のナノ材料研究に直接応用できます：</p>
<ul>
<li><strong>材料設計</strong>: 機械学習による物性予測と最適化による高効率材料探索</li>
<li><strong>プロセス最適化</strong>: ベイズ最適化による実験回数削減と最適合成条件発見</li>
<li><strong>品質管理</strong>: 異常検知による不良品の早期発見と歩留まり向上</li>
<li><strong>データ解析</strong>: TEMデータ、MDシミュレーションデータの定量的解析</li>
<li><strong>モデル解釈</strong>: SHAP分析による予測根拠の可視化と信頼性向上</li>
</ul>
<h3 id="_7">次章の予告</h3>
<p>Chapter 4では、これらの技術を実際のナノ材料研究プロジェクトに適用した5つの詳細なケーススタディを学びます。カーボンナノチューブ複合材料、量子ドット、金ナノ粒子触媒、グラフェン、ナノ医薬の実用化事例を通じて、問題解決の全体像を理解します。</p>
<hr />
<h2 id="_8">演習問題</h2>
<h3 id="1_1">演習1: カーボンナノチューブの電気伝導度予測</h3>
<p>カーボンナノチューブ（CNT）の電気伝導度は、直径、カイラリティ、長さに依存します。以下のデータを生成し、LightGBMモデルで予測してください。</p>
<p><strong>データ仕様</strong>：<br />
- サンプル数：150<br />
- 特徴量：直径（1-3 nm）、長さ（100-1000 nm）、カイラリティ指標（0-1の連続値）<br />
- ターゲット：電気伝導度（10³-10⁷ S/m、対数正規分布）</p>
<p><strong>タスク</strong>：<br />
1. データ生成<br />
2. 訓練/テストデータ分割<br />
3. LightGBMモデルの構築と評価<br />
4. 特徴量重要度の可視化</p>
<details>
<summary>解答例</summary>


<pre class="codehilite"><code class="language-python"># データ生成
np.random.seed(500)
n_samples = 150

diameter = np.random.uniform(1, 3, n_samples)
length = np.random.uniform(100, 1000, n_samples)
chirality = np.random.uniform(0, 1, n_samples)

# 電気伝導度（簡易モデル: 直径とカイラリティに強く依存）
log_conductivity = 3 + 2*diameter + 3*chirality + 0.001*length + np.random.normal(0, 0.5, n_samples)
conductivity = 10 ** log_conductivity  # S/m

data_cnt = pd.DataFrame({
    'diameter_nm': diameter,
    'length_nm': length,
    'chirality': chirality,
    'conductivity_Sm': conductivity
})

# 特徴量とターゲット
X_cnt = data_cnt[['diameter_nm', 'length_nm', 'chirality']]
y_cnt = np.log10(data_cnt['conductivity_Sm'])  # 対数変換

# スケーリング
scaler_cnt = StandardScaler()
X_cnt_scaled = scaler_cnt.fit_transform(X_cnt)

# 訓練/テスト分割
X_cnt_train, X_cnt_test, y_cnt_train, y_cnt_test = train_test_split(
    X_cnt_scaled, y_cnt, test_size=0.2, random_state=42
)

# LightGBMモデル
model_cnt = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.05, n_estimators=200, random_state=42, verbose=-1)
model_cnt.fit(X_cnt_train, y_cnt_train)

# 予測と評価
y_cnt_pred = model_cnt.predict(X_cnt_test)
r2_cnt = r2_score(y_cnt_test, y_cnt_pred)
rmse_cnt = np.sqrt(mean_squared_error(y_cnt_test, y_cnt_pred))

print(f&quot;R²: {r2_cnt:.4f}&quot;)
print(f&quot;RMSE: {rmse_cnt:.4f}&quot;)

# 特徴量重要度
importance_cnt = pd.DataFrame({
    'Feature': X_cnt.columns,
    'Importance': model_cnt.feature_importances_
}).sort_values('Importance', ascending=False)

print(&quot;\n特徴量重要度:&quot;)
print(importance_cnt)

# 可視化
fig, ax = plt.subplots(figsize=(8, 5))
ax.barh(importance_cnt['Feature'], importance_cnt['Importance'], color='steelblue', edgecolor='black')
ax.set_xlabel('Importance')
ax.set_title('Feature Importance: CNT Conductivity Prediction')
plt.tight_layout()
plt.show()
</code></pre>



</details>

<h3 id="2_1">演習2: 銀ナノ粒子の最適合成条件探索</h3>
<p>銀ナノ粒子の抗菌活性は、サイズが小さいほど高くなります。ベイズ最適化を用いて、目標サイズ（10 nm）を実現する最適な合成温度とpHを探索してください。</p>
<p><strong>条件</strong>：<br />
- 温度範囲：20-80°C<br />
- pH範囲：6-11<br />
- 目標サイズ：10 nm</p>
<details>
<summary>解答例</summary>


<pre class="codehilite"><code class="language-python"># 銀ナノ粒子データの生成
np.random.seed(600)
n_ag = 100

temp_ag = np.random.uniform(20, 80, n_ag)
pH_ag = np.random.uniform(6, 11, n_ag)

# サイズモデル（温度が高く、pHが低いほど小さくなると仮定）
size_ag = 15 - 0.1*temp_ag - 0.8*pH_ag + np.random.normal(0, 1, n_ag)
size_ag = np.clip(size_ag, 5, 30)

data_ag = pd.DataFrame({
    'temperature': temp_ag,
    'pH': pH_ag,
    'size': size_ag
})

# モデル構築（LightGBM）
X_ag = data_ag[['temperature', 'pH']]
y_ag = data_ag['size']

scaler_ag = StandardScaler()
X_ag_scaled = scaler_ag.fit_transform(X_ag)

model_ag = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.05, n_estimators=100, random_state=42, verbose=-1)
model_ag.fit(X_ag_scaled, y_ag)

# ベイズ最適化
from skopt import gp_minimize
from skopt.space import Real

space_ag = [
    Real(20, 80, name='temperature'),
    Real(6, 11, name='pH')
]

target_size = 10.0

def objective_ag(params):
    temp, ph = params
    features = scaler_ag.transform([[temp, ph]])
    predicted_size = model_ag.predict(features)[0]
    return abs(predicted_size - target_size)

result_ag = gp_minimize(objective_ag, space_ag, n_calls=40, random_state=42, verbose=False)

print(&quot;=&quot; * 60)
print(&quot;銀ナノ粒子の最適合成条件&quot;)
print(&quot;=&quot; * 60)
print(f&quot;最小誤差: {result_ag.fun:.2f} nm&quot;)
print(f&quot;最適温度: {result_ag.x[0]:.1f} °C&quot;)
print(f&quot;最適pH: {result_ag.x[1]:.2f}&quot;)

# 最適条件での予測サイズ
optimal_features = scaler_ag.transform([result_ag.x])
predicted_size = model_ag.predict(optimal_features)[0]
print(f&quot;予測サイズ: {predicted_size:.2f} nm&quot;)
</code></pre>



</details>

<h3 id="3_1">演習3: 量子ドットの多色発光設計</h3>
<p>赤（650 nm）、緑（550 nm）、青（450 nm）の3色の発光を実現するCdSe量子ドットのサイズを、ベイズ最適化で設計してください。</p>
<p><strong>ヒント</strong>：<br />
- 各色ごとに最適化を実行<br />
- 発光波長とサイズの関係を使用</p>
<details>
<summary>解答例</summary>


<pre class="codehilite"><code class="language-python"># 量子ドットデータ（例16のdata_qdを使用）
# model_qd と scaler_qd が構築済みと仮定

# 3色の目標波長
target_colors = {
    'Red': 650,
    'Green': 550,
    'Blue': 450
}

results_colors = {}

for color_name, target_emission in target_colors.items():
    # 探索空間
    space_qd = [
        Real(2, 10, name='size_nm'),
        Real(10, 120, name='synthesis_time_min'),
        Real(0.5, 2.0, name='precursor_ratio')
    ]

    # 目的関数
    def objective_qd(params):
        features = scaler_qd.transform([params])
        predicted_emission = model_qd.predict(features)[0]
        return abs(predicted_emission - target_emission)

    # 最適化
    result_qd_color = gp_minimize(objective_qd, space_qd, n_calls=30, random_state=42, verbose=False)

    # 結果保存
    optimal_features = scaler_qd.transform([result_qd_color.x])
    predicted_emission = model_qd.predict(optimal_features)[0]

    results_colors[color_name] = {
        'target': target_emission,
        'size': result_qd_color.x[0],
        'time': result_qd_color.x[1],
        'ratio': result_qd_color.x[2],
        'predicted': predicted_emission,
        'error': result_qd_color.fun
    }

# 結果表示
print(&quot;=&quot; * 80)
print(&quot;量子ドット多色発光設計&quot;)
print(&quot;=&quot; * 80)

results_df = pd.DataFrame(results_colors).T
print(results_df.to_string())

# 可視化
fig, ax = plt.subplots(figsize=(10, 6))
colors_rgb = {'Red': 'red', 'Green': 'green', 'Blue': 'blue'}

for color_name, result in results_colors.items():
    ax.scatter(result['size'], result['predicted'],
               s=200, color=colors_rgb[color_name],
               edgecolors='black', linewidths=2, label=color_name)

ax.set_xlabel('Quantum Dot Size (nm)', fontsize=12)
ax.set_ylabel('Emission Wavelength (nm)', fontsize=12)
ax.set_title('Multi-Color Quantum Dot Design', fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>



</details>

<hr />
<h2 id="_9">参考文献</h2>
<ol>
<li>
<p><strong>Pedregosa, F. et al.</strong> (2011). Scikit-learn: Machine Learning in Python. <em>Journal of Machine Learning Research</em>, 12, 2825-2830.</p>
</li>
<li>
<p><strong>Ke, G. et al.</strong> (2017). LightGBM: A highly efficient gradient boosting decision tree. <em>Advances in Neural Information Processing Systems</em>, 30, 3146-3154.</p>
</li>
<li>
<p><strong>Lundberg, S. M. &amp; Lee, S.-I.</strong> (2017). A unified approach to interpreting model predictions. <em>Advances in Neural Information Processing Systems</em>, 30, 4765-4774.</p>
</li>
<li>
<p><strong>Snoek, J., Larochelle, H., &amp; Adams, R. P.</strong> (2012). Practical Bayesian optimization of machine learning algorithms. <em>Advances in Neural Information Processing Systems</em>, 25, 2951-2959.</p>
</li>
<li>
<p><strong>Deb, K. et al.</strong> (2002). A fast and elitist multiobjective genetic algorithm: NSGA-II. <em>IEEE Transactions on Evolutionary Computation</em>, 6(2), 182-197. <a href="https://doi.org/10.1109/4235.996017">DOI: 10.1109/4235.996017</a></p>
</li>
<li>
<p><strong>Frenkel, D. &amp; Smit, B.</strong> (2001). <em>Understanding Molecular Simulation: From Algorithms to Applications</em> (2nd ed.). Academic Press.</p>
</li>
</ol>
<hr />
<p><a href="chapter2-fundamentals.html">← 前章：ナノ材料の基礎原理</a> | <a href="chapter4-real-world.html">次章：実世界の応用とキャリア →</a></p>

        <div class="nav-buttons">
            <a href="index.html" class="nav-button">← シリーズ目次に戻る</a>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 MI Knowledge Hub - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
