<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿è§£æã¨æ©Ÿæ¢°å­¦ç¿’">
    <title>Chapter 3: Pythonå®Ÿè·µãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ« - MI Knowledge Hub</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 0.9rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        tbody {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: #666;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Details/Summary (for exercises) */
        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            padding: 0.5rem;
        }

        summary:hover {
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }

        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }

            table {
                font-size: 0.9rem;
            }
        }
    </style>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="container">
            <h1>Chapter 3: Pythonå®Ÿè·µãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«</h1>
            <div class="meta">
                <span>ğŸ“– èª­äº†æ™‚é–“: 30-40åˆ†</span>
                <span>ğŸ“Š ãƒ¬ãƒ™ãƒ«: ä¸­ç´š</span>
            </div>
        </div>
    </header>

    <main class="container">
        <h1 id="chapter-3-python">Chapter 3: Pythonå®Ÿè·µãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«</h1>
<p>ãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿è§£æã¨æ©Ÿæ¢°å­¦ç¿’</p>
<hr />
<h2 id="_1">æœ¬ç« ã®å­¦ç¿’ç›®æ¨™</h2>
<p>æœ¬ç« ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ«ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š</p>
<p>âœ… ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆãƒ»å¯è¦–åŒ–ãƒ»å‰å‡¦ç†ã®å®Ÿè·µ<br />
âœ… 5ç¨®é¡ã®å›å¸°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒŠãƒææ–™ç‰©æ€§äºˆæ¸¬<br />
âœ… ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚‹ãƒŠãƒææ–™ã®æœ€é©è¨­è¨ˆ<br />
âœ… SHAPåˆ†æã«ã‚ˆã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆ<br />
âœ… å¤šç›®çš„æœ€é©åŒ–ã«ã‚ˆã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æ<br />
âœ… TEMç”»åƒè§£æã¨ã‚µã‚¤ã‚ºåˆ†å¸ƒã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°<br />
âœ… ç•°å¸¸æ¤œçŸ¥ã«ã‚ˆã‚‹å“è³ªç®¡ç†ã¸ã®å¿œç”¨</p>
<hr />
<h2 id="31">3.1 ç’°å¢ƒæ§‹ç¯‰</h2>
<h3 id="_2">å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª</h3>
<p>æœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ä½¿ç”¨ã™ã‚‹ä¸»è¦ãªPythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼š</p>
<pre class="codehilite"><code class="language-python"># ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»å¯è¦–åŒ–
pandas, numpy, matplotlib, seaborn, scipy

# æ©Ÿæ¢°å­¦ç¿’
scikit-learn, lightgbm

# æœ€é©åŒ–
scikit-optimize

# ãƒ¢ãƒ‡ãƒ«è§£é‡ˆ
shap

# å¤šç›®çš„æœ€é©åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
pymoo
</code></pre>

<h3 id="_3">ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•</h3>
<h4 id="option-1-anaconda">Option 1: Anacondaç’°å¢ƒ</h4>
<pre class="codehilite"><code class="language-bash"># Anacondaã§æ–°ã—ã„ç’°å¢ƒã‚’ä½œæˆ
conda create -n nanomaterials python=3.10 -y
conda activate nanomaterials

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
conda install pandas numpy matplotlib seaborn scipy scikit-learn -y
conda install -c conda-forge lightgbm scikit-optimize shap -y

# å¤šç›®çš„æœ€é©åŒ–ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
pip install pymoo
</code></pre>

<h4 id="option-2-venv-pip">Option 2: venv + pipç’°å¢ƒ</h4>
<pre class="codehilite"><code class="language-bash"># ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆ
python -m venv nanomaterials_env

# ä»®æƒ³ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–
# macOS/Linux:
source nanomaterials_env/bin/activate
# Windows:
nanomaterials_env\Scripts\activate

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install pandas numpy matplotlib seaborn scipy
pip install scikit-learn lightgbm scikit-optimize shap pymoo
</code></pre>

<h4 id="option-3-google-colab">Option 3: Google Colab</h4>
<p>Google Colabã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’ã‚»ãƒ«ã§å®Ÿè¡Œï¼š</p>
<pre class="codehilite"><code class="language-python"># è¿½åŠ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
!pip install lightgbm scikit-optimize shap pymoo

# ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®ç¢ºèª
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
print(&quot;ç’°å¢ƒæ§‹ç¯‰å®Œäº†ï¼&quot;)
</code></pre>

<hr />
<h2 id="32">3.2 ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã¨å¯è¦–åŒ–</h2>
<h3 id="1">ã€ä¾‹1ã€‘åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼šé‡‘ãƒŠãƒç²’å­ã®ã‚µã‚¤ã‚ºã¨å…‰å­¦ç‰¹æ€§</h3>
<p>é‡‘ãƒŠãƒç²’å­ã®å±€åœ¨è¡¨é¢ãƒ—ãƒ©ã‚ºãƒ¢ãƒ³å…±é³´ï¼ˆLSPRï¼‰æ³¢é•·ã¯ã€ç²’å­ã‚µã‚¤ã‚ºã«ä¾å­˜ã—ã¾ã™ã€‚ã“ã®é–¢ä¿‚ã‚’æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã§è¡¨ç¾ã—ã¾ã™ã€‚</p>
<pre class="codehilite"><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã®è¨­å®šï¼ˆå†ç¾æ€§ã®ãŸã‚ï¼‰
np.random.seed(42)

# ã‚µãƒ³ãƒ—ãƒ«æ•°
n_samples = 200

# é‡‘ãƒŠãƒç²’å­ã®ã‚µã‚¤ã‚ºï¼ˆnmï¼‰: å¹³å‡15 nmã€æ¨™æº–åå·®5 nm
size = np.random.normal(15, 5, n_samples)
size = np.clip(size, 5, 50)  # 5-50 nmã®ç¯„å›²ã«åˆ¶é™

# LSPRæ³¢é•·ï¼ˆnmï¼‰: Mieç†è«–ã®ç°¡æ˜“è¿‘ä¼¼
# åŸºæœ¬æ³¢é•·520 nm + ã‚µã‚¤ã‚ºä¾å­˜é … + ãƒã‚¤ã‚º
lspr = 520 + 0.8 * (size - 15) + np.random.normal(0, 5, n_samples)

# åˆæˆæ¡ä»¶
temperature = np.random.uniform(20, 80, n_samples)  # æ¸©åº¦ï¼ˆâ„ƒï¼‰
pH = np.random.uniform(4, 10, n_samples)  # pH

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½œæˆ
data = pd.DataFrame({
    'size_nm': size,
    'lspr_nm': lspr,
    'temperature_C': temperature,
    'pH': pH
})

print(&quot;=&quot; * 60)
print(&quot;é‡‘ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆå®Œäº†&quot;)
print(&quot;=&quot; * 60)
print(data.head(10))
print(&quot;\nåŸºæœ¬çµ±è¨ˆé‡:&quot;)
print(data.describe())
</code></pre>

<h3 id="2">ã€ä¾‹2ã€‘ã‚µã‚¤ã‚ºåˆ†å¸ƒã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ </h3>
<pre class="codehilite"><code class="language-python"># ã‚µã‚¤ã‚ºåˆ†å¸ƒã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
fig, ax = plt.subplots(figsize=(10, 6))

# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¨KDEï¼ˆã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šï¼‰
ax.hist(data['size_nm'], bins=30, alpha=0.6, color='skyblue',
        edgecolor='black', density=True, label='Histogram')

# KDEãƒ—ãƒ­ãƒƒãƒˆ
from scipy.stats import gaussian_kde
kde = gaussian_kde(data['size_nm'])
x_range = np.linspace(data['size_nm'].min(), data['size_nm'].max(), 100)
ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')

ax.set_xlabel('Particle Size (nm)', fontsize=12)
ax.set_ylabel('Probability Density', fontsize=12)
ax.set_title('Gold Nanoparticle Size Distribution', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;å¹³å‡ã‚µã‚¤ã‚º: {data['size_nm'].mean():.2f} nm&quot;)
print(f&quot;æ¨™æº–åå·®: {data['size_nm'].std():.2f} nm&quot;)
print(f&quot;ä¸­å¤®å€¤: {data['size_nm'].median():.2f} nm&quot;)
</code></pre>

<h3 id="3">ã€ä¾‹3ã€‘æ•£å¸ƒå›³ãƒãƒˆãƒªãƒƒã‚¯ã‚¹</h3>
<pre class="codehilite"><code class="language-python"># ãƒšã‚¢ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ•£å¸ƒå›³ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼‰
sns.pairplot(data, diag_kind='kde', plot_kws={'alpha': 0.6},
             height=2.5, corner=False)
plt.suptitle('Pairplot of Gold Nanoparticle Data', y=1.01, fontsize=14, fontweight='bold')
plt.show()

print(&quot;å„å¤‰æ•°é–“ã®é–¢ä¿‚ã‚’å¯è¦–åŒ–ã—ã¾ã—ãŸ&quot;)
</code></pre>

<h3 id="4">ã€ä¾‹4ã€‘ç›¸é–¢è¡Œåˆ—ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—</h3>
<pre class="codehilite"><code class="language-python"># ç›¸é–¢è¡Œåˆ—ã®è¨ˆç®—
correlation_matrix = data.corr()

# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm',
            center=0, square=True, linewidths=1, cbar_kws={&quot;shrink&quot;: 0.8})
ax.set_title('Correlation Matrix', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

print(&quot;ç›¸é–¢ä¿‚æ•°:&quot;)
print(correlation_matrix)
print(f&quot;\nLSPRæ³¢é•·ã¨ã‚µã‚¤ã‚ºã®ç›¸é–¢: {correlation_matrix.loc['lspr_nm', 'size_nm']:.3f}&quot;)
</code></pre>

<h3 id="53d-vs-vs-lspr">ã€ä¾‹5ã€‘3Dãƒ—ãƒ­ãƒƒãƒˆï¼šã‚µã‚¤ã‚º vs æ¸©åº¦ vs LSPR</h3>
<pre class="codehilite"><code class="language-python">from mpl_toolkits.mplot3d import Axes3D

# 3Dæ•£å¸ƒå›³
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—
scatter = ax.scatter(data['size_nm'], data['temperature_C'], data['lspr_nm'],
                     c=data['pH'], cmap='viridis', s=50, alpha=0.6, edgecolors='k')

ax.set_xlabel('Size (nm)', fontsize=11)
ax.set_ylabel('Temperature (Â°C)', fontsize=11)
ax.set_zlabel('LSPR Wavelength (nm)', fontsize=11)
ax.set_title('3D Scatter: Size vs Temperature vs LSPR (colored by pH)',
             fontsize=13, fontweight='bold')

# ã‚«ãƒ©ãƒ¼ãƒãƒ¼
cbar = plt.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)
cbar.set_label('pH', fontsize=10)

plt.tight_layout()
plt.show()

print(&quot;3Dãƒ—ãƒ­ãƒƒãƒˆã§å¤šæ¬¡å…ƒã®é–¢ä¿‚ã‚’å¯è¦–åŒ–ã—ã¾ã—ãŸ&quot;)
</code></pre>

<hr />
<h2 id="33">3.3 å‰å‡¦ç†ã¨ãƒ‡ãƒ¼ã‚¿åˆ†å‰²</h2>
<h3 id="6">ã€ä¾‹6ã€‘æ¬ æå€¤å‡¦ç†</h3>
<pre class="codehilite"><code class="language-python"># æ¬ æå€¤ã‚’äººç‚ºçš„ã«å°å…¥ï¼ˆå®Ÿç¿’ç”¨ï¼‰
data_with_missing = data.copy()
np.random.seed(123)

# ãƒ©ãƒ³ãƒ€ãƒ ã«5%ã®æ¬ æå€¤ã‚’å°å…¥
missing_indices = np.random.choice(data.index, size=int(0.05 * len(data)), replace=False)
data_with_missing.loc[missing_indices, 'temperature_C'] = np.nan

print(&quot;=&quot; * 60)
print(&quot;æ¬ æå€¤ã®ç¢ºèª&quot;)
print(&quot;=&quot; * 60)
print(f&quot;æ¬ æå€¤ã®æ•°:\n{data_with_missing.isnull().sum()}&quot;)

# æ¬ æå€¤ã®å‡¦ç†æ–¹æ³•1: å¹³å‡å€¤ã§è£œå®Œ
data_filled_mean = data_with_missing.fillna(data_with_missing.mean())

# æ¬ æå€¤ã®å‡¦ç†æ–¹æ³•2: ä¸­å¤®å€¤ã§è£œå®Œ
data_filled_median = data_with_missing.fillna(data_with_missing.median())

# æ¬ æå€¤ã®å‡¦ç†æ–¹æ³•3: å‰Šé™¤
data_dropped = data_with_missing.dropna()

print(f&quot;\nå…ƒã®ãƒ‡ãƒ¼ã‚¿: {len(data_with_missing)}è¡Œ&quot;)
print(f&quot;æ¬ æå€¤å‰Šé™¤å¾Œ: {len(data_dropped)}è¡Œ&quot;)
print(f&quot;å¹³å‡å€¤è£œå®Œå¾Œ: {len(data_filled_mean)}è¡Œï¼ˆæ¬ æå€¤ãªã—ï¼‰&quot;)

# ä»¥é™ã®åˆ†æã§ã¯å…ƒã®ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¬ æå€¤ãªã—ï¼‰ã‚’ä½¿ç”¨
data_clean = data.copy()
print(&quot;\nâ†’ ä»¥é™ã¯æ¬ æå€¤ã®ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™&quot;)
</code></pre>

<h3 id="7iqr">ã€ä¾‹7ã€‘å¤–ã‚Œå€¤æ¤œå‡ºï¼ˆIQRæ³•ï¼‰</h3>
<pre class="codehilite"><code class="language-python"># IQRï¼ˆå››åˆ†ä½ç¯„å›²ï¼‰æ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º
def detect_outliers_iqr(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = (series &lt; lower_bound) | (series &gt; upper_bound)
    return outliers, lower_bound, upper_bound

# ã‚µã‚¤ã‚ºã«ã¤ã„ã¦å¤–ã‚Œå€¤æ¤œå‡º
outliers, lower, upper = detect_outliers_iqr(data_clean['size_nm'])

print(&quot;=&quot; * 60)
print(&quot;å¤–ã‚Œå€¤æ¤œå‡ºï¼ˆIQRæ³•ï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;æ¤œå‡ºã•ã‚ŒãŸå¤–ã‚Œå€¤ã®æ•°: {outliers.sum()}&quot;)
print(f&quot;ä¸‹é™: {lower:.2f} nm, ä¸Šé™: {upper:.2f} nm&quot;)

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(10, 6))
ax.boxplot([data_clean['size_nm']], labels=['Size (nm)'], vert=False)
ax.scatter(data_clean.loc[outliers, 'size_nm'],
           [1] * outliers.sum(), color='red', s=100,
           label=f'Outliers (n={outliers.sum()})', zorder=3)
ax.set_xlabel('Size (nm)', fontsize=12)
ax.set_title('Boxplot with Outliers', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()

print(&quot;â†’ å¤–ã‚Œå€¤ã¯é™¤å»ã›ãšã€å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™&quot;)
</code></pre>

<h3 id="8standardscaler">ã€ä¾‹8ã€‘ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆStandardScalerï¼‰</h3>
<pre class="codehilite"><code class="language-python">from sklearn.preprocessing import StandardScaler

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
X = data_clean[['size_nm', 'temperature_C', 'pH']]
y = data_clean['lspr_nm']

# StandardScalerï¼ˆå¹³å‡0ã€æ¨™æº–åå·®1ã«æ¨™æº–åŒ–ï¼‰
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰å¾Œã®æ¯”è¼ƒ
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)

print(&quot;=&quot; * 60)
print(&quot;ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰ã®çµ±è¨ˆé‡&quot;)
print(&quot;=&quot; * 60)
print(X.describe())

print(&quot;\n&quot; + &quot;=&quot; * 60)
print(&quot;ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®çµ±è¨ˆé‡ï¼ˆå¹³å‡â‰ˆ0ã€æ¨™æº–åå·®â‰ˆ1ï¼‰&quot;)
print(&quot;=&quot; * 60)
print(X_scaled_df.describe())

print(&quot;\nâ†’ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šå„ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒçµ±ä¸€ã•ã‚Œã¾ã—ãŸ&quot;)
</code></pre>

<h3 id="9">ã€ä¾‹9ã€‘è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²</h3>
<pre class="codehilite"><code class="language-python">from sklearn.model_selection import train_test_split

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²ï¼ˆ80:20ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

print(&quot;=&quot; * 60)
print(&quot;ãƒ‡ãƒ¼ã‚¿åˆ†å‰²&quot;)
print(&quot;=&quot; * 60)
print(f&quot;å…¨ãƒ‡ãƒ¼ã‚¿æ•°: {len(X)}ã‚µãƒ³ãƒ—ãƒ«&quot;)
print(f&quot;è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}ã‚µãƒ³ãƒ—ãƒ« ({len(X_train)/len(X)*100:.1f}%)&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}ã‚µãƒ³ãƒ—ãƒ« ({len(X_test)/len(X)*100:.1f}%)&quot;)

print(&quot;\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡:&quot;)
print(pd.DataFrame(X_train, columns=X.columns).describe())
</code></pre>

<hr />
<h2 id="34">3.4 å›å¸°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒŠãƒç²’å­ç‰©æ€§äºˆæ¸¬</h2>
<p>ç›®æ¨™ï¼šã‚µã‚¤ã‚ºã€æ¸©åº¦ã€pHã‹ã‚‰LSPRæ³¢é•·ã‚’äºˆæ¸¬</p>
<h3 id="10linear-regression">ã€ä¾‹10ã€‘ç·šå½¢å›å¸°ï¼ˆLinear Regressionï¼‰</h3>
<pre class="codehilite"><code class="language-python">from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)

# äºˆæ¸¬
y_train_pred_lr = model_lr.predict(X_train)
y_test_pred_lr = model_lr.predict(X_test)

# è©•ä¾¡æŒ‡æ¨™
r2_train_lr = r2_score(y_train, y_train_pred_lr)
r2_test_lr = r2_score(y_test, y_test_pred_lr)
rmse_test_lr = np.sqrt(mean_squared_error(y_test, y_test_pred_lr))
mae_test_lr = mean_absolute_error(y_test, y_test_pred_lr)

print(&quot;=&quot; * 60)
print(&quot;ç·šå½¢å›å¸°ï¼ˆLinear Regressionï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_train_lr:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_test_lr:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_test_lr:.4f} nm&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_test_lr:.4f} nm&quot;)

# å›å¸°ä¿‚æ•°
print(&quot;\nå›å¸°ä¿‚æ•°:&quot;)
for name, coef in zip(X.columns, model_lr.coef_):
    print(f&quot;  {name}: {coef:.4f}&quot;)
print(f&quot;  åˆ‡ç‰‡: {model_lr.intercept_:.4f}&quot;)

# æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤
axes[0].scatter(y_test, y_test_pred_lr, alpha=0.6, edgecolors='k')
axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
             'r--', lw=2, label='Perfect Prediction')
axes[0].set_xlabel('Actual LSPR (nm)', fontsize=11)
axes[0].set_ylabel('Predicted LSPR (nm)', fontsize=11)
axes[0].set_title(f'Linear Regression (RÂ² = {r2_test_lr:.3f})', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
residuals = y_test - y_test_pred_lr
axes[1].scatter(y_test_pred_lr, residuals, alpha=0.6, edgecolors='k')
axes[1].axhline(y=0, color='r', linestyle='--', lw=2)
axes[1].set_xlabel('Predicted LSPR (nm)', fontsize=11)
axes[1].set_ylabel('Residuals (nm)', fontsize=11)
axes[1].set_title('Residual Plot', fontsize=12, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3 id="11random-forest">ã€ä¾‹11ã€‘ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°ï¼ˆRandom Forestï¼‰</h3>
<pre class="codehilite"><code class="language-python">from sklearn.ensemble import RandomForestRegressor

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°ãƒ¢ãƒ‡ãƒ«
model_rf = RandomForestRegressor(n_estimators=100, max_depth=10,
                                 random_state=42, n_jobs=-1)
model_rf.fit(X_train, y_train)

# äºˆæ¸¬
y_train_pred_rf = model_rf.predict(X_train)
y_test_pred_rf = model_rf.predict(X_test)

# è©•ä¾¡
r2_train_rf = r2_score(y_train, y_train_pred_rf)
r2_test_rf = r2_score(y_test, y_test_pred_rf)
rmse_test_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))
mae_test_rf = mean_absolute_error(y_test, y_test_pred_rf)

print(&quot;=&quot; * 60)
print(&quot;ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°ï¼ˆRandom Forestï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_train_rf:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_test_rf:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_test_rf:.4f} nm&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_test_rf:.4f} nm&quot;)

# ç‰¹å¾´é‡é‡è¦åº¦
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model_rf.feature_importances_
}).sort_values('Importance', ascending=False)

print(&quot;\nç‰¹å¾´é‡é‡è¦åº¦:&quot;)
print(feature_importance)

# ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(8, 5))
ax.barh(feature_importance['Feature'], feature_importance['Importance'],
        color='steelblue', edgecolor='black')
ax.set_xlabel('Importance', fontsize=12)
ax.set_title('Feature Importance (Random Forest)', fontsize=13, fontweight='bold')
ax.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()
</code></pre>

<h3 id="12lightgbm">ã€ä¾‹12ã€‘å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼ˆLightGBMï¼‰</h3>
<pre class="codehilite"><code class="language-python">import lightgbm as lgb

# LightGBMãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
params = {
    'objective': 'regression',
    'metric': 'rmse',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'n_estimators': 200,
    'random_state': 42,
    'verbose': -1
}

model_lgb = lgb.LGBMRegressor(**params)
model_lgb.fit(X_train, y_train)

# äºˆæ¸¬
y_train_pred_lgb = model_lgb.predict(X_train)
y_test_pred_lgb = model_lgb.predict(X_test)

# è©•ä¾¡
r2_train_lgb = r2_score(y_train, y_train_pred_lgb)
r2_test_lgb = r2_score(y_test, y_test_pred_lgb)
rmse_test_lgb = np.sqrt(mean_squared_error(y_test, y_test_pred_lgb))
mae_test_lgb = mean_absolute_error(y_test, y_test_pred_lgb)

print(&quot;=&quot; * 60)
print(&quot;å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼ˆLightGBMï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_train_lgb:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_test_lgb:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_test_lgb:.4f} nm&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_test_lgb:.4f} nm&quot;)

# äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤ãƒ—ãƒ­ãƒƒãƒˆ
fig, ax = plt.subplots(figsize=(8, 6))
ax.scatter(y_test, y_test_pred_lgb, alpha=0.6, edgecolors='k', s=60)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
        'r--', lw=2, label='Perfect Prediction')
ax.set_xlabel('Actual LSPR (nm)', fontsize=12)
ax.set_ylabel('Predicted LSPR (nm)', fontsize=12)
ax.set_title(f'LightGBM Prediction (RÂ² = {r2_test_lgb:.3f})',
             fontsize=13, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<h3 id="13svr">ã€ä¾‹13ã€‘ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼å›å¸°ï¼ˆSVRï¼‰</h3>
<pre class="codehilite"><code class="language-python">from sklearn.svm import SVR

# SVRãƒ¢ãƒ‡ãƒ«ï¼ˆRBFã‚«ãƒ¼ãƒãƒ«ï¼‰
model_svr = SVR(kernel='rbf', C=10, gamma='scale', epsilon=0.1)
model_svr.fit(X_train, y_train)

# äºˆæ¸¬
y_train_pred_svr = model_svr.predict(X_train)
y_test_pred_svr = model_svr.predict(X_test)

# è©•ä¾¡
r2_train_svr = r2_score(y_train, y_train_pred_svr)
r2_test_svr = r2_score(y_test, y_test_pred_svr)
rmse_test_svr = np.sqrt(mean_squared_error(y_test, y_test_pred_svr))
mae_test_svr = mean_absolute_error(y_test, y_test_pred_svr)

print(&quot;=&quot; * 60)
print(&quot;ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼å›å¸°ï¼ˆSVRï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_train_svr:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_test_svr:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_test_svr:.4f} nm&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_test_svr:.4f} nm&quot;)
print(f&quot;ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼æ•°: {len(model_svr.support_)}&quot;)
</code></pre>

<h3 id="14mlp-regressor">ã€ä¾‹14ã€‘ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆMLP Regressorï¼‰</h3>
<pre class="codehilite"><code class="language-python">from sklearn.neural_network import MLPRegressor

# MLPãƒ¢ãƒ‡ãƒ«
model_mlp = MLPRegressor(hidden_layer_sizes=(100, 50),
                         activation='relu',
                         solver='adam',
                         alpha=0.001,
                         max_iter=500,
                         random_state=42,
                         early_stopping=True,
                         validation_fraction=0.1,
                         verbose=False)

model_mlp.fit(X_train, y_train)

# äºˆæ¸¬
y_train_pred_mlp = model_mlp.predict(X_train)
y_test_pred_mlp = model_mlp.predict(X_test)

# è©•ä¾¡
r2_train_mlp = r2_score(y_train, y_train_pred_mlp)
r2_test_mlp = r2_score(y_test, y_test_pred_mlp)
rmse_test_mlp = np.sqrt(mean_squared_error(y_test, y_test_pred_mlp))
mae_test_mlp = mean_absolute_error(y_test, y_test_pred_mlp)

print(&quot;=&quot; * 60)
print(&quot;ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆMLP Regressorï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_train_mlp:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_test_mlp:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_test_mlp:.4f} nm&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_test_mlp:.4f} nm&quot;)
print(f&quot;åå¾©å›æ•°: {model_mlp.n_iter_}&quot;)
print(f&quot;éš ã‚Œå±¤ã®æ§‹é€ : {model_mlp.hidden_layer_sizes}&quot;)
</code></pre>

<h3 id="15">ã€ä¾‹15ã€‘ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ</h3>
<pre class="codehilite"><code class="language-python"># å…¨ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ã¾ã¨ã‚ã‚‹
results = pd.DataFrame({
    'Model': ['Linear Regression', 'Random Forest', 'LightGBM', 'SVR', 'MLP'],
    'RÂ² (Train)': [r2_train_lr, r2_train_rf, r2_train_lgb, r2_train_svr, r2_train_mlp],
    'RÂ² (Test)': [r2_test_lr, r2_test_rf, r2_test_lgb, r2_test_svr, r2_test_mlp],
    'RMSE (Test)': [rmse_test_lr, rmse_test_rf, rmse_test_lgb, rmse_test_svr, rmse_test_mlp],
    'MAE (Test)': [mae_test_lr, mae_test_rf, mae_test_lgb, mae_test_svr, mae_test_mlp]
})

results['Overfit'] = results['RÂ² (Train)'] - results['RÂ² (Test)']

print(&quot;=&quot; * 80)
print(&quot;å…¨ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¯”è¼ƒ&quot;)
print(&quot;=&quot; * 80)
print(results.to_string(index=False))

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å®š
best_model_idx = results['RÂ² (Test)'].idxmax()
best_model_name = results.loc[best_model_idx, 'Model']
best_r2 = results.loc[best_model_idx, 'RÂ² (Test)']

print(f&quot;\næœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model_name} (RÂ² = {best_r2:.4f})&quot;)

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# RÂ²ã‚¹ã‚³ã‚¢æ¯”è¼ƒ
x_pos = np.arange(len(results))
axes[0].bar(x_pos, results['RÂ² (Test)'], alpha=0.7, color='steelblue',
            edgecolor='black', label='Test RÂ²')
axes[0].set_xticks(x_pos)
axes[0].set_xticklabels(results['Model'], rotation=15, ha='right')
axes[0].set_ylabel('RÂ² Score', fontsize=12)
axes[0].set_title('Model Comparison: RÂ² Score', fontsize=13, fontweight='bold')
axes[0].grid(True, alpha=0.3, axis='y')
axes[0].legend()

# RMSEæ¯”è¼ƒ
axes[1].bar(x_pos, results['RMSE (Test)'], alpha=0.7, color='coral',
            edgecolor='black', label='Test RMSE')
axes[1].set_xticks(x_pos)
axes[1].set_xticklabels(results['Model'], rotation=15, ha='right')
axes[1].set_ylabel('RMSE (nm)', fontsize=12)
axes[1].set_title('Model Comparison: RMSE', fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='y')
axes[1].legend()

plt.tight_layout()
plt.show()
</code></pre>

<hr />
<h2 id="35">3.5 é‡å­ãƒ‰ãƒƒãƒˆç™ºå…‰æ³¢é•·äºˆæ¸¬</h2>
<h3 id="16cdse">ã€ä¾‹16ã€‘ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼šCdSeé‡å­ãƒ‰ãƒƒãƒˆ</h3>
<p>CdSeé‡å­ãƒ‰ãƒƒãƒˆã®ç™ºå…‰æ³¢é•·ã¯ã€Brusæ–¹ç¨‹å¼ã«åŸºã¥ãã‚µã‚¤ã‚ºã«ä¾å­˜ã—ã¾ã™ã€‚</p>
<pre class="codehilite"><code class="language-python"># CdSeé‡å­ãƒ‰ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
np.random.seed(100)

n_qd_samples = 150

# é‡å­ãƒ‰ãƒƒãƒˆã®ã‚µã‚¤ã‚ºï¼ˆ2-10 nmï¼‰
size_qd = np.random.uniform(2, 10, n_qd_samples)

# Brusæ–¹ç¨‹å¼ã®ç°¡æ˜“è¿‘ä¼¼: emission = 520 + 130/(size^0.8) + noise
emission = 520 + 130 / (size_qd ** 0.8) + np.random.normal(0, 10, n_qd_samples)

# åˆæˆæ¡ä»¶
synthesis_time = np.random.uniform(10, 120, n_qd_samples)  # åˆ†
precursor_ratio = np.random.uniform(0.5, 2.0, n_qd_samples)  # ãƒ¢ãƒ«æ¯”

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
data_qd = pd.DataFrame({
    'size_nm': size_qd,
    'emission_nm': emission,
    'synthesis_time_min': synthesis_time,
    'precursor_ratio': precursor_ratio
})

print(&quot;=&quot; * 60)
print(&quot;CdSeé‡å­ãƒ‰ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆå®Œäº†&quot;)
print(&quot;=&quot; * 60)
print(data_qd.head(10))
print(&quot;\nåŸºæœ¬çµ±è¨ˆé‡:&quot;)
print(data_qd.describe())

# ã‚µã‚¤ã‚ºã¨ç™ºå…‰æ³¢é•·ã®é–¢ä¿‚ãƒ—ãƒ­ãƒƒãƒˆ
fig, ax = plt.subplots(figsize=(10, 6))
scatter = ax.scatter(data_qd['size_nm'], data_qd['emission_nm'],
                     c=data_qd['synthesis_time_min'], cmap='plasma',
                     s=80, alpha=0.7, edgecolors='k')
ax.set_xlabel('Quantum Dot Size (nm)', fontsize=12)
ax.set_ylabel('Emission Wavelength (nm)', fontsize=12)
ax.set_title('CdSe Quantum Dot: Size vs Emission Wavelength',
             fontsize=13, fontweight='bold')
cbar = plt.colorbar(scatter, ax=ax)
cbar.set_label('Synthesis Time (min)', fontsize=10)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<h3 id="17lightgbm">ã€ä¾‹17ã€‘é‡å­ãƒ‰ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBMï¼‰</h3>
<pre class="codehilite"><code class="language-python"># ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
X_qd = data_qd[['size_nm', 'synthesis_time_min', 'precursor_ratio']]
y_qd = data_qd['emission_nm']

# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
scaler_qd = StandardScaler()
X_qd_scaled = scaler_qd.fit_transform(X_qd)

# è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²
X_qd_train, X_qd_test, y_qd_train, y_qd_test = train_test_split(
    X_qd_scaled, y_qd, test_size=0.2, random_state=42
)

# LightGBMãƒ¢ãƒ‡ãƒ«
model_qd = lgb.LGBMRegressor(
    objective='regression',
    num_leaves=31,
    learning_rate=0.05,
    n_estimators=200,
    random_state=42,
    verbose=-1
)

model_qd.fit(X_qd_train, y_qd_train)

# äºˆæ¸¬
y_qd_train_pred = model_qd.predict(X_qd_train)
y_qd_test_pred = model_qd.predict(X_qd_test)

# è©•ä¾¡
r2_qd_train = r2_score(y_qd_train, y_qd_train_pred)
r2_qd_test = r2_score(y_qd_test, y_qd_test_pred)
rmse_qd = np.sqrt(mean_squared_error(y_qd_test, y_qd_test_pred))
mae_qd = mean_absolute_error(y_qd_test, y_qd_test_pred)

print(&quot;=&quot; * 60)
print(&quot;é‡å­ãƒ‰ãƒƒãƒˆç™ºå…‰æ³¢é•·äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBMï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;è¨“ç·´ãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_qd_train:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RÂ²: {r2_qd_test:.4f}&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ RMSE: {rmse_qd:.4f} nm&quot;)
print(f&quot;ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ MAE: {mae_qd:.4f} nm&quot;)
</code></pre>

<h3 id="18">ã€ä¾‹18ã€‘äºˆæ¸¬çµæœã®å¯è¦–åŒ–</h3>
<pre class="codehilite"><code class="language-python"># äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤ãƒ—ãƒ­ãƒƒãƒˆï¼ˆä¿¡é ¼åŒºé–“ä»˜ãï¼‰
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ­ãƒƒãƒˆ
axes[0].scatter(y_qd_test, y_qd_test_pred, alpha=0.6, s=80,
                edgecolors='k', label='Test Data')
axes[0].plot([y_qd_test.min(), y_qd_test.max()],
             [y_qd_test.min(), y_qd_test.max()],
             'r--', lw=2, label='Perfect Prediction')

# Â±10 nm ã®ç¯„å›²ã‚’è¡¨ç¤º
axes[0].fill_between([y_qd_test.min(), y_qd_test.max()],
                     [y_qd_test.min()-10, y_qd_test.max()-10],
                     [y_qd_test.min()+10, y_qd_test.max()+10],
                     alpha=0.2, color='gray', label='Â±10 nm')

axes[0].set_xlabel('Actual Emission (nm)', fontsize=12)
axes[0].set_ylabel('Predicted Emission (nm)', fontsize=12)
axes[0].set_title(f'QD Emission Prediction (RÂ² = {r2_qd_test:.3f})',
                  fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# ã‚µã‚¤ã‚ºåˆ¥ã®äºˆæ¸¬ç²¾åº¦
size_bins = [2, 4, 6, 8, 10]
size_labels = ['2-4 nm', '4-6 nm', '6-8 nm', '8-10 nm']
data_qd_test = pd.DataFrame({
    'size': X_qd.iloc[y_qd_test.index]['size_nm'].values,
    'actual': y_qd_test.values,
    'predicted': y_qd_test_pred
})
data_qd_test['size_bin'] = pd.cut(data_qd_test['size'], bins=size_bins, labels=size_labels)
data_qd_test['error'] = np.abs(data_qd_test['actual'] - data_qd_test['predicted'])

# ã‚µã‚¤ã‚ºãƒ“ãƒ³ã”ã¨ã®å¹³å‡èª¤å·®
error_by_size = data_qd_test.groupby('size_bin')['error'].mean()

axes[1].bar(range(len(error_by_size)), error_by_size.values,
            color='coral', edgecolor='black', alpha=0.7)
axes[1].set_xticks(range(len(error_by_size)))
axes[1].set_xticklabels(error_by_size.index)
axes[1].set_ylabel('Mean Absolute Error (nm)', fontsize=12)
axes[1].set_xlabel('QD Size Range', fontsize=12)
axes[1].set_title('Prediction Error by QD Size', fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print(f&quot;\nå…¨ä½“ã®å¹³å‡çµ¶å¯¾èª¤å·®: {mae_qd:.2f} nm&quot;)
print(&quot;ã‚µã‚¤ã‚ºåˆ¥ã®å¹³å‡çµ¶å¯¾èª¤å·®:&quot;)
print(error_by_size)
</code></pre>

<hr />
<h2 id="36">3.6 ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ</h2>
<h3 id="19lightgbm">ã€ä¾‹19ã€‘ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆLightGBMï¼‰</h3>
<pre class="codehilite"><code class="language-python"># LightGBMãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆã‚²ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ï¼‰
importance_gain = model_lgb.feature_importances_
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': importance_gain
}).sort_values('Importance', ascending=False)

print(&quot;=&quot; * 60)
print(&quot;ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆLightGBMï¼‰&quot;)
print(&quot;=&quot; * 60)
print(importance_df)

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(8, 5))
colors = ['steelblue', 'coral', 'lightgreen']
ax.barh(importance_df['Feature'], importance_df['Importance'],
        color=colors, edgecolor='black')
ax.set_xlabel('Feature Importance (Gain)', fontsize=12)
ax.set_title('Feature Importance: LSPR Prediction',
             fontsize=13, fontweight='bold')
ax.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()

print(f&quot;\næœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡: {importance_df.iloc[0]['Feature']}&quot;)
</code></pre>

<h3 id="20shap">ã€ä¾‹20ã€‘SHAPåˆ†æï¼šäºˆæ¸¬è§£é‡ˆ</h3>
<pre class="codehilite"><code class="language-python">import shap

# SHAP Explainerã®ä½œæˆ
explainer = shap.Explainer(model_lgb, X_train)
shap_values = explainer(X_test)

print(&quot;=&quot; * 60)
print(&quot;SHAPåˆ†æ&quot;)
print(&quot;=&quot; * 60)
print(&quot;SHAPå€¤ã®è¨ˆç®—å®Œäº†&quot;)
print(f&quot;SHAPå€¤ã®å½¢çŠ¶: {shap_values.values.shape}&quot;)

# SHAP Summary Plot
fig, ax = plt.subplots(figsize=(10, 6))
shap.summary_plot(shap_values, X_test, feature_names=X.columns, show=False)
plt.title('SHAP Summary Plot: Feature Impact on LSPR Prediction',
          fontsize=13, fontweight='bold', pad=20)
plt.tight_layout()
plt.show()

# SHAP Dependence Plotï¼ˆæœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡ï¼‰
top_feature_idx = importance_df.index[0]
top_feature_name = X.columns[top_feature_idx]

fig, ax = plt.subplots(figsize=(10, 6))
shap.dependence_plot(top_feature_idx, shap_values.values, X_test,
                     feature_names=X.columns, show=False)
plt.title(f'SHAP Dependence Plot: {top_feature_name}',
          fontsize=13, fontweight='bold')
plt.tight_layout()
plt.show()

print(f&quot;\nSHAPåˆ†æã«ã‚ˆã‚Šã€{top_feature_name}ãŒLSPRæ³¢é•·äºˆæ¸¬ã«æœ€ã‚‚å½±éŸ¿ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¾ã—ãŸ&quot;)
</code></pre>

<hr />
<h2 id="37">3.7 ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚‹ãƒŠãƒææ–™è¨­è¨ˆ</h2>
<p>ç›®æ¨™ï¼šç›®æ¨™LSPRæ³¢é•·ï¼ˆ550 nmï¼‰ã‚’å®Ÿç¾ã™ã‚‹æœ€é©ãªåˆæˆæ¡ä»¶ã‚’æ¢ç´¢</p>
<h3 id="21">ã€ä¾‹21ã€‘æ¢ç´¢ç©ºé–“ã®å®šç¾©</h3>
<pre class="codehilite"><code class="language-python">from skopt.space import Real

# æ¢ç´¢ç©ºé–“ã®å®šç¾©
# ã‚µã‚¤ã‚º: 10-40 nmã€æ¸©åº¦: 20-80Â°Cã€pH: 4-10
search_space = [
    Real(10, 40, name='size_nm'),
    Real(20, 80, name='temperature_C'),
    Real(4, 10, name='pH')
]

print(&quot;=&quot; * 60)
print(&quot;ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼šæ¢ç´¢ç©ºé–“ã®å®šç¾©&quot;)
print(&quot;=&quot; * 60)
for dim in search_space:
    print(f&quot;  {dim.name}: [{dim.bounds[0]}, {dim.bounds[1]}]&quot;)

print(&quot;\nç›®æ¨™: LSPRæ³¢é•· = 550 nm ã‚’å®Ÿç¾ã™ã‚‹æ¡ä»¶ã‚’æ¢ç´¢&quot;)
</code></pre>

<h3 id="22">ã€ä¾‹22ã€‘ç›®çš„é–¢æ•°ã®è¨­å®š</h3>
<pre class="codehilite"><code class="language-python"># ç›®çš„é–¢æ•°ï¼šäºˆæ¸¬LSPRæ³¢é•·ã¨ç›®æ¨™æ³¢é•·ï¼ˆ550 nmï¼‰ã®å·®ã®çµ¶å¯¾å€¤ã‚’æœ€å°åŒ–
target_lspr = 550.0

def objective_function(params):
    &quot;&quot;&quot;
    ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ç›®çš„é–¢æ•°

    Parameters:
    -----------
    params : list
        [size_nm, temperature_C, pH]

    Returns:
    --------
    float
        ç›®æ¨™æ³¢é•·ã¨ã®èª¤å·®ï¼ˆæœ€å°åŒ–ã™ã‚‹å€¤ï¼‰
    &quot;&quot;&quot;
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
    size, temp, ph = params

    # ç‰¹å¾´é‡ã‚’æ§‹ç¯‰ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é©ç”¨ï¼‰
    features = np.array([[size, temp, ph]])
    features_scaled = scaler.transform(features)

    # LSPRæ³¢é•·ã‚’äºˆæ¸¬
    predicted_lspr = model_lgb.predict(features_scaled)[0]

    # ç›®æ¨™æ³¢é•·ã¨ã®èª¤å·®ï¼ˆçµ¶å¯¾å€¤ï¼‰
    error = abs(predicted_lspr - target_lspr)

    return error

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
test_params = [20.0, 50.0, 7.0]
test_error = objective_function(test_params)
print(f&quot;\nãƒ†ã‚¹ãƒˆå®Ÿè¡Œ:&quot;)
print(f&quot;  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: size={test_params[0]} nm, temp={test_params[1]}Â°C, pH={test_params[2]}&quot;)
print(f&quot;  ç›®çš„é–¢æ•°å€¤ï¼ˆèª¤å·®ï¼‰: {test_error:.4f} nm&quot;)
</code></pre>

<h3 id="23scikit-optimize">ã€ä¾‹23ã€‘ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®å®Ÿè¡Œï¼ˆscikit-optimizeï¼‰</h3>
<pre class="codehilite"><code class="language-python">from skopt import gp_minimize
from skopt.plots import plot_convergence, plot_objective

# ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®å®Ÿè¡Œ
print(&quot;\n&quot; + &quot;=&quot; * 60)
print(&quot;ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®å®Ÿè¡Œä¸­...&quot;)
print(&quot;=&quot; * 60)

result = gp_minimize(
    func=objective_function,
    dimensions=search_space,
    n_calls=50,  # è©•ä¾¡å›æ•°
    n_initial_points=10,  # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å›æ•°
    random_state=42,
    verbose=False
)

print(&quot;æœ€é©åŒ–å®Œäº†ï¼&quot;)
print(&quot;\n&quot; + &quot;=&quot; * 60)
print(&quot;æœ€é©åŒ–çµæœ&quot;)
print(&quot;=&quot; * 60)
print(f&quot;æœ€å°ç›®çš„é–¢æ•°å€¤ï¼ˆèª¤å·®ï¼‰: {result.fun:.4f} nm&quot;)
print(f&quot;\næœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:&quot;)
print(f&quot;  ã‚µã‚¤ã‚º: {result.x[0]:.2f} nm&quot;)
print(f&quot;  æ¸©åº¦: {result.x[1]:.2f} Â°C&quot;)
print(f&quot;  pH: {result.x[2]:.2f}&quot;)

# æœ€é©æ¡ä»¶ã§ã®äºˆæ¸¬LSPRæ³¢é•·ã‚’è¨ˆç®—
optimal_features = np.array([result.x])
optimal_features_scaled = scaler.transform(optimal_features)
predicted_optimal_lspr = model_lgb.predict(optimal_features_scaled)[0]

print(f&quot;\näºˆæ¸¬ã•ã‚Œã‚‹LSPRæ³¢é•·: {predicted_optimal_lspr:.2f} nm&quot;)
print(f&quot;ç›®æ¨™LSPRæ³¢é•·: {target_lspr} nm&quot;)
print(f&quot;é”æˆç²¾åº¦: {abs(predicted_optimal_lspr - target_lspr):.2f} nm&quot;)
</code></pre>

<h3 id="24">ã€ä¾‹24ã€‘æœ€é©åŒ–çµæœã®å¯è¦–åŒ–</h3>
<pre class="codehilite"><code class="language-python"># æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# åæŸãƒ—ãƒ­ãƒƒãƒˆ
plot_convergence(result, ax=axes[0])
axes[0].set_title('Convergence Plot', fontsize=13, fontweight='bold')
axes[0].set_ylabel('Objective Value (Error, nm)', fontsize=11)
axes[0].set_xlabel('Number of Evaluations', fontsize=11)
axes[0].grid(True, alpha=0.3)

# è©•ä¾¡å±¥æ­´ã®ãƒ—ãƒ­ãƒƒãƒˆ
iterations = range(1, len(result.func_vals) + 1)
axes[1].plot(iterations, result.func_vals, 'o-', alpha=0.6, label='Evaluation')
axes[1].plot(iterations, np.minimum.accumulate(result.func_vals),
             'r-', linewidth=2, label='Best So Far')
axes[1].set_xlabel('Iteration', fontsize=11)
axes[1].set_ylabel('Objective Value (Error, nm)', fontsize=11)
axes[1].set_title('Optimization Progress', fontsize=13, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3 id="25">ã€ä¾‹25ã€‘åæŸãƒ—ãƒ­ãƒƒãƒˆ</h3>
<pre class="codehilite"><code class="language-python"># è©³ç´°ãªåæŸãƒ—ãƒ­ãƒƒãƒˆï¼ˆæœ€è‰¯å€¤ã®æ¨ç§»ï¼‰
fig, ax = plt.subplots(figsize=(10, 6))

cumulative_min = np.minimum.accumulate(result.func_vals)
iterations = np.arange(1, len(cumulative_min) + 1)

ax.plot(iterations, cumulative_min, 'b-', linewidth=2, marker='o',
        markersize=4, label='Best Error')
ax.axhline(y=result.fun, color='r', linestyle='--', linewidth=2,
           label=f'Final Best: {result.fun:.2f} nm')
ax.fill_between(iterations, 0, cumulative_min, alpha=0.2, color='blue')

ax.set_xlabel('Iteration', fontsize=12)
ax.set_ylabel('Minimum Error (nm)', fontsize=12)
ax.set_title('Bayesian Optimization: Convergence to Optimal Solution',
             fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;\n{len(result.func_vals)}å›ã®è©•ä¾¡ã§æœ€é©è§£ã«åæŸã—ã¾ã—ãŸ&quot;)
print(f&quot;åˆæœŸè©•ä¾¡ã§ã®æœ€è‰¯èª¤å·®: {result.func_vals[0]:.2f} nm&quot;)
print(f&quot;æœ€çµ‚çš„ãªæœ€è‰¯èª¤å·®: {result.fun:.2f} nm&quot;)
print(f&quot;æ”¹å–„ç‡: {(1 - result.fun/result.func_vals[0])*100:.1f}%&quot;)
</code></pre>

<hr />
<h2 id="38">3.8 å¤šç›®çš„æœ€é©åŒ–ï¼šã‚µã‚¤ã‚ºã¨ç™ºå…‰åŠ¹ç‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•</h2>
<h3 id="26paretonsga-ii">ã€ä¾‹26ã€‘Paretoæœ€é©åŒ–ï¼ˆNSGA-IIï¼‰</h3>
<p>å¤šç›®çš„æœ€é©åŒ–ã§ã¯ã€è¤‡æ•°ã®ç›®çš„ã‚’åŒæ™‚ã«æœ€é©åŒ–ã—ã¾ã™ã€‚ã“ã“ã§ã¯ã€é‡å­ãƒ‰ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã‚’æœ€å°åŒ–ã—ã¤ã¤ã€ç™ºå…‰åŠ¹ç‡ï¼ˆä»®æƒ³çš„ãªæŒ‡æ¨™ï¼‰ã‚’æœ€å¤§åŒ–ã—ã¾ã™ã€‚</p>
<pre class="codehilite"><code class="language-python"># pymooã‚’ä½¿ç”¨ã—ãŸå¤šç›®çš„æœ€é©åŒ–
try:
    from pymoo.core.problem import Problem
    from pymoo.algorithms.moo.nsga2 import NSGA2
    from pymoo.optimize import minimize as pymoo_minimize
    from pymoo.operators.crossover.sbx import SBX
    from pymoo.operators.mutation.pm import PM
    from pymoo.operators.sampling.rnd import FloatRandomSampling

    # å¤šç›®çš„æœ€é©åŒ–å•é¡Œã®å®šç¾©
    class QuantumDotOptimization(Problem):
        def __init__(self):
            super().__init__(
                n_var=3,  # å¤‰æ•°æ•°ï¼ˆsize, synthesis_time, precursor_ratioï¼‰
                n_obj=2,  # ç›®çš„é–¢æ•°æ•°ï¼ˆsizeæœ€å°åŒ–ã€emissionåŠ¹ç‡æœ€å¤§åŒ–ï¼‰
                n_constr=0,  # åˆ¶ç´„ãªã—
                xl=np.array([2.0, 10.0, 0.5]),  # ä¸‹é™
                xu=np.array([10.0, 120.0, 2.0])  # ä¸Šé™
            )

        def _evaluate(self, X, out, *args, **kwargs):
            # ç›®çš„é–¢æ•°1: ã‚µã‚¤ã‚ºã®æœ€å°åŒ–
            obj1 = X[:, 0]  # size

            # ç›®çš„é–¢æ•°2: ç™ºå…‰åŠ¹ç‡ã®æœ€å¤§åŒ–ï¼ˆè² ã®å€¤ã§æœ€å°åŒ–å•é¡Œã«å¤‰æ›ï¼‰
            # åŠ¹ç‡ã¯ä»®æƒ³çš„ã«ã€emission wavelengthãŒ550 nmã«è¿‘ã„ã»ã©é«˜ã„ã¨ä»®å®š
            features = X  # [size, synthesis_time, precursor_ratio]
            features_scaled = scaler_qd.transform(features)
            predicted_emission = model_qd.predict(features_scaled)

            # åŠ¹ç‡ï¼š550 nmã‹ã‚‰ã®ãšã‚ŒãŒå°ã•ã„ã»ã©é«˜ã„ï¼ˆè² ã®å€¤ã§æœ€å¤§åŒ–â†’æœ€å°åŒ–ï¼‰
            efficiency = -np.abs(predicted_emission - 550)
            obj2 = -efficiency  # æœ€å¤§åŒ–ã‚’æœ€å°åŒ–å•é¡Œã«å¤‰æ›

            out[&quot;F&quot;] = np.column_stack([obj1, obj2])

    # å•é¡Œã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    problem = QuantumDotOptimization()

    # NSGA-IIã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    algorithm = NSGA2(
        pop_size=40,
        sampling=FloatRandomSampling(),
        crossover=SBX(prob=0.9, eta=15),
        mutation=PM(eta=20),
        eliminate_duplicates=True
    )

    # æœ€é©åŒ–å®Ÿè¡Œ
    print(&quot;=&quot; * 60)
    print(&quot;å¤šç›®çš„æœ€é©åŒ–ï¼ˆNSGA-IIï¼‰å®Ÿè¡Œä¸­...&quot;)
    print(&quot;=&quot; * 60)

    res = pymoo_minimize(
        problem,
        algorithm,
        ('n_gen', 50),  # ä¸–ä»£æ•°
        seed=42,
        verbose=False
    )

    print(&quot;å¤šç›®çš„æœ€é©åŒ–å®Œäº†ï¼&quot;)
    print(f&quot;\nãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã®æ•°: {len(res.F)}&quot;)

    # ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã®è¡¨ç¤ºï¼ˆä¸Šä½5ã¤ï¼‰
    print(&quot;\nä»£è¡¨çš„ãªãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ï¼ˆä¸Šä½5ã¤ï¼‰:&quot;)
    pareto_solutions = pd.DataFrame({
        'Size (nm)': res.X[:, 0],
        'Synthesis Time (min)': res.X[:, 1],
        'Precursor Ratio': res.X[:, 2],
        'Obj1: Size': res.F[:, 0],
        'Obj2: -Efficiency': res.F[:, 1]
    }).head(5)
    print(pareto_solutions.to_string(index=False))

    PYMOO_AVAILABLE = True

except ImportError:
    print(&quot;=&quot; * 60)
    print(&quot;pymooãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“&quot;)
    print(&quot;=&quot; * 60)
    print(&quot;å¤šç›®çš„æœ€é©åŒ–ã«ã¯pymooãŒå¿…è¦ã§ã™:&quot;)
    print(&quot;  pip install pymoo&quot;)
    print(&quot;\nä»£ã‚ã‚Šã«ã€ç°¡æ˜“çš„ãªå¤šç›®çš„æœ€é©åŒ–ã®ä¾‹ã‚’è¡¨ç¤ºã—ã¾ã™&quot;)

    # ç°¡æ˜“çš„ãªã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ã‚ˆã‚‹å¤šç›®çš„æœ€é©åŒ–ã®æ¨¡æ“¬
    sizes = np.linspace(2, 10, 20)
    times = np.linspace(10, 120, 20)
    ratios = np.linspace(0.5, 2.0, 20)

    # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
    sample_X = []
    sample_F = []

    for size in sizes[::4]:
        for time in times[::4]:
            for ratio in ratios[::4]:
                features = np.array([[size, time, ratio]])
                features_scaled = scaler_qd.transform(features)
                emission = model_qd.predict(features_scaled)[0]

                obj1 = size
                obj2 = abs(emission - 550)

                sample_X.append([size, time, ratio])
                sample_F.append([obj1, obj2])

    sample_X = np.array(sample_X)
    sample_F = np.array(sample_F)

    print(&quot;\nã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ã‚ˆã‚‹è§£ã®æ¢ç´¢å®Œäº†&quot;)
    print(f&quot;æ¢ç´¢ã—ãŸè§£ã®æ•°: {len(sample_F)}&quot;)

    res = type('Result', (), {
        'X': sample_X,
        'F': sample_F
    })()

    PYMOO_AVAILABLE = False
</code></pre>

<h3 id="27pareto">ã€ä¾‹27ã€‘Paretoãƒ•ãƒ­ãƒ³ãƒˆã®å¯è¦–åŒ–</h3>
<pre class="codehilite"><code class="language-python"># Paretoãƒ•ãƒ­ãƒ³ãƒˆã®å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(10, 7))

if PYMOO_AVAILABLE:
    # NSGA-IIã®çµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    ax.scatter(res.F[:, 0], -res.F[:, 1], c='blue', s=80, alpha=0.6,
               edgecolors='black', label='Pareto Optimal Solutions')

    title_suffix = &quot;(NSGA-II)&quot;
else:
    # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã®çµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆ
    ax.scatter(res.F[:, 0], res.F[:, 1], c='blue', s=60, alpha=0.5,
               edgecolors='black', label='Sampled Solutions')

    title_suffix = &quot;(Grid Search)&quot;

ax.set_xlabel('Objective 1: Size (nm) [Minimize]', fontsize=12)

if PYMOO_AVAILABLE:
    ax.set_ylabel('Objective 2: Efficiency [Maximize]', fontsize=12)
else:
    ax.set_ylabel('Objective 2: Deviation from 550nm [Minimize]', fontsize=12)

ax.set_title(f'Pareto Front: Size vs Emission Efficiency {title_suffix}',
             fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;\nãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆ:&quot;)
print(&quot;  ã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹ã¨åŠ¹ç‡ãŒä¸‹ãŒã‚Šã€åŠ¹ç‡ã‚’ä¸Šã’ã‚‹ã¨ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹&quot;)
print(&quot;  â†’ ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•é–¢ä¿‚ãŒæ˜ç¢ºã«&quot;)
</code></pre>

<hr />
<h2 id="39-tem">3.9 TEMç”»åƒè§£æã¨ã‚µã‚¤ã‚ºåˆ†å¸ƒ</h2>
<h3 id="28tem">ã€ä¾‹28ã€‘æ¨¡æ“¬TEMãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ</h3>
<p>TEMï¼ˆé€éå‹é›»å­é¡•å¾®é¡ï¼‰ã§æ¸¬å®šã•ã‚Œã‚‹ãƒŠãƒç²’å­ã‚µã‚¤ã‚ºã¯ã€ã—ã°ã—ã°å¯¾æ•°æ­£è¦åˆ†å¸ƒã«å¾“ã„ã¾ã™ã€‚</p>
<pre class="codehilite"><code class="language-python">from scipy.stats import lognorm

# å¯¾æ•°æ­£è¦åˆ†å¸ƒã«å¾“ã†TEMã‚µã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
np.random.seed(200)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
mean_size = 20  # å¹³å‡ã‚µã‚¤ã‚ºï¼ˆnmï¼‰
cv = 0.3  # å¤‰å‹•ä¿‚æ•°ï¼ˆæ¨™æº–åå·®/å¹³å‡ï¼‰

# å¯¾æ•°æ­£è¦åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨ˆç®—
sigma = np.sqrt(np.log(1 + cv**2))
mu = np.log(mean_size) - 0.5 * sigma**2

# ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆï¼ˆ500ç²’å­ï¼‰
tem_sizes = lognorm.rvs(s=sigma, scale=np.exp(mu), size=500)

print(&quot;=&quot; * 60)
print(&quot;TEMæ¸¬å®šãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆå¯¾æ•°æ­£è¦åˆ†å¸ƒï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(tem_sizes)}ç²’å­&quot;)
print(f&quot;å¹³å‡ã‚µã‚¤ã‚º: {tem_sizes.mean():.2f} nm&quot;)
print(f&quot;æ¨™æº–åå·®: {tem_sizes.std():.2f} nm&quot;)
print(f&quot;ä¸­å¤®å€¤: {np.median(tem_sizes):.2f} nm&quot;)
print(f&quot;æœ€å°å€¤: {tem_sizes.min():.2f} nm&quot;)
print(f&quot;æœ€å¤§å€¤: {tem_sizes.max():.2f} nm&quot;)

# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
fig, ax = plt.subplots(figsize=(10, 6))
ax.hist(tem_sizes, bins=40, alpha=0.7, color='lightblue',
        edgecolor='black', density=True, label='TEM Data')
ax.set_xlabel('Particle Size (nm)', fontsize=12)
ax.set_ylabel('Probability Density', fontsize=12)
ax.set_title('TEM Size Distribution (Lognormal)', fontsize=13, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
</code></pre>

<h3 id="29">ã€ä¾‹29ã€‘å¯¾æ•°æ­£è¦åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°</h3>
<pre class="codehilite"><code class="language-python"># å¯¾æ•°æ­£è¦åˆ†å¸ƒã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
shape_fit, loc_fit, scale_fit = lognorm.fit(tem_sizes, floc=0)

# ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã•ã‚ŒãŸåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
fitted_mean = np.exp(np.log(scale_fit) + 0.5 * shape_fit**2)
fitted_std = fitted_mean * np.sqrt(np.exp(shape_fit**2) - 1)

print(&quot;=&quot; * 60)
print(&quot;å¯¾æ•°æ­£è¦åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœ&quot;)
print(&quot;=&quot; * 60)
print(f&quot;å½¢çŠ¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (sigma): {shape_fit:.4f}&quot;)
print(f&quot;ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {scale_fit:.4f}&quot;)
print(f&quot;ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã•ã‚ŒãŸå¹³å‡ã‚µã‚¤ã‚º: {fitted_mean:.2f} nm&quot;)
print(f&quot;ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã•ã‚ŒãŸæ¨™æº–åå·®: {fitted_std:.2f} nm&quot;)

# å®Ÿæ¸¬å€¤ã¨ã®æ¯”è¼ƒ
print(f&quot;\nå®Ÿæ¸¬å€¤ã¨ã®æ¯”è¼ƒ:&quot;)
print(f&quot;  å¹³å‡ã‚µã‚¤ã‚º - å®Ÿæ¸¬: {tem_sizes.mean():.2f} nm, ãƒ•ã‚£ãƒƒãƒˆ: {fitted_mean:.2f} nm&quot;)
print(f&quot;  æ¨™æº–åå·® - å®Ÿæ¸¬: {tem_sizes.std():.2f} nm, ãƒ•ã‚£ãƒƒãƒˆ: {fitted_std:.2f} nm&quot;)
</code></pre>

<h3 id="30">ã€ä¾‹30ã€‘ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã®å¯è¦–åŒ–</h3>
<pre class="codehilite"><code class="language-python"># ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã®è©³ç´°å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¨ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°æ›²ç·š
axes[0].hist(tem_sizes, bins=40, alpha=0.6, color='lightblue',
             edgecolor='black', density=True, label='TEM Data')

# ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã•ã‚ŒãŸå¯¾æ•°æ­£è¦åˆ†å¸ƒ
x_range = np.linspace(0, tem_sizes.max(), 200)
fitted_pdf = lognorm.pdf(x_range, shape_fit, loc=loc_fit, scale=scale_fit)
axes[0].plot(x_range, fitted_pdf, 'r-', linewidth=2,
             label=f'Lognormal Fit (Î¼={fitted_mean:.1f}, Ïƒ={fitted_std:.1f})')

axes[0].set_xlabel('Particle Size (nm)', fontsize=12)
axes[0].set_ylabel('Probability Density', fontsize=12)
axes[0].set_title('TEM Size Distribution with Lognormal Fit',
                  fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3, axis='y')

# Q-Qãƒ—ãƒ­ãƒƒãƒˆï¼ˆåˆ†ä½ç‚¹ãƒ—ãƒ­ãƒƒãƒˆï¼‰
from scipy.stats import probplot

probplot(tem_sizes, dist=lognorm, sparams=(shape_fit, loc_fit, scale_fit),
         plot=axes[1])
axes[1].set_title('Q-Q Plot: Lognormal Distribution',
                  fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;\nQ-Qãƒ—ãƒ­ãƒƒãƒˆ: ãƒ‡ãƒ¼ã‚¿ãŒç›´ç·šä¸Šã«ã‚ã‚Œã°ã€å¯¾æ•°æ­£è¦åˆ†å¸ƒã«è‰¯ãå¾“ã£ã¦ã„ã¾ã™&quot;)
</code></pre>

<hr />
<h2 id="310-md">3.10 åˆ†å­å‹•åŠ›å­¦ï¼ˆMDï¼‰ãƒ‡ãƒ¼ã‚¿è§£æ</h2>
<h3 id="31md">ã€ä¾‹31ã€‘MDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿</h3>
<p>åˆ†å­å‹•åŠ›å­¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒŠãƒç²’å­ã®åŸå­é…ç½®ã®æ™‚é–“ç™ºå±•ã‚’è¿½è·¡ã—ã¾ã™ã€‚</p>
<pre class="codehilite"><code class="language-python"># MDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®æ¨¡æ“¬ç”Ÿæˆ
# å®Ÿéš›ã®MDãƒ‡ãƒ¼ã‚¿ã¯LAMMPS, GROMACSç­‰ã‹ã‚‰å–å¾—

np.random.seed(300)

n_atoms = 100  # åŸå­æ•°
n_steps = 1000  # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°
dt = 0.001  # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆpsï¼‰

# åˆæœŸä½ç½®ï¼ˆnmï¼‰
positions_initial = np.random.uniform(-1, 1, (n_atoms, 3))

# æ™‚é–“ç™ºå±•ã®æ¨¡æ“¬ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼‰
positions = np.zeros((n_steps, n_atoms, 3))
positions[0] = positions_initial

for t in range(1, n_steps):
    # ãƒ©ãƒ³ãƒ€ãƒ ãªå¤‰ä½
    displacement = np.random.normal(0, 0.01, (n_atoms, 3))
    positions[t] = positions[t-1] + displacement

print(&quot;=&quot; * 60)
print(&quot;MDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ&quot;)
print(&quot;=&quot; * 60)
print(f&quot;åŸå­æ•°: {n_atoms}&quot;)
print(f&quot;ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°: {n_steps}&quot;)
print(f&quot;ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚é–“: {n_steps * dt:.2f} ps&quot;)
print(f&quot;ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {positions.shape} (time, atoms, xyz)&quot;)

# ä¸­å¿ƒåŸå­ï¼ˆåŸå­0ï¼‰ã®è»Œè·¡ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
fig = plt.figure(figsize=(12, 5))

ax1 = fig.add_subplot(121, projection='3d')
ax1.plot(positions[:, 0, 0], positions[:, 0, 1], positions[:, 0, 2],
         'b-', alpha=0.5, linewidth=0.5)
ax1.scatter(positions[0, 0, 0], positions[0, 0, 1], positions[0, 0, 2],
            c='green', s=100, label='Start', edgecolors='k')
ax1.scatter(positions[-1, 0, 0], positions[-1, 0, 1], positions[-1, 0, 2],
            c='red', s=100, label='End', edgecolors='k')
ax1.set_xlabel('X (nm)')
ax1.set_ylabel('Y (nm)')
ax1.set_zlabel('Z (nm)')
ax1.set_title('Atom Trajectory (Atom 0)', fontweight='bold')
ax1.legend()

ax2 = fig.add_subplot(122)
ax2.plot(np.arange(n_steps) * dt, positions[:, 0, 0], label='X')
ax2.plot(np.arange(n_steps) * dt, positions[:, 0, 1], label='Y')
ax2.plot(np.arange(n_steps) * dt, positions[:, 0, 2], label='Z')
ax2.set_xlabel('Time (ps)', fontsize=11)
ax2.set_ylabel('Position (nm)', fontsize=11)
ax2.set_title('Position vs Time (Atom 0)', fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<h3 id="32rdf">ã€ä¾‹32ã€‘å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRDFï¼‰ã®è¨ˆç®—</h3>
<p>å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRadial Distribution Function, RDFï¼‰ã¯ã€åŸå­é–“è·é›¢ã®åˆ†å¸ƒã‚’è¡¨ã—ã¾ã™ã€‚</p>
<pre class="codehilite"><code class="language-python"># å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRDFï¼‰ã®è¨ˆç®—
def calculate_rdf(positions, r_max=2.0, n_bins=100):
    &quot;&quot;&quot;
    å‹•å¾„åˆ†å¸ƒé–¢æ•°ã‚’è¨ˆç®—

    Parameters:
    -----------
    positions : ndarray
        åŸå­ä½ç½® (n_atoms, 3)
    r_max : float
        æœ€å¤§è·é›¢ï¼ˆnmï¼‰
    n_bins : int
        ãƒ“ãƒ³æ•°

    Returns:
    --------
    r_bins : ndarray
        è·é›¢ãƒ“ãƒ³
    rdf : ndarray
        å‹•å¾„åˆ†å¸ƒé–¢æ•°
    &quot;&quot;&quot;
    n_atoms = positions.shape[0]

    # å…¨åŸå­ãƒšã‚¢é–“ã®è·é›¢ã‚’è¨ˆç®—
    distances = []
    for i in range(n_atoms):
        for j in range(i+1, n_atoms):
            dist = np.linalg.norm(positions[i] - positions[j])
            if dist &lt; r_max:
                distances.append(dist)

    distances = np.array(distances)

    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    hist, bin_edges = np.histogram(distances, bins=n_bins, range=(0, r_max))
    r_bins = (bin_edges[:-1] + bin_edges[1:]) / 2

    # è¦æ ¼åŒ–ï¼ˆç†æƒ³æ°—ä½“ã¨ã®æ¯”ï¼‰
    dr = r_max / n_bins
    volume_shell = 4 * np.pi * r_bins**2 * dr
    n_ideal = volume_shell * (n_atoms / (4/3 * np.pi * r_max**3))

    rdf = hist / n_ideal / (n_atoms / 2)

    return r_bins, rdf

# æœ€çµ‚ãƒ•ãƒ¬ãƒ¼ãƒ ã§RDFã‚’è¨ˆç®—
final_positions = positions[-1]
r_bins, rdf = calculate_rdf(final_positions, r_max=1.5, n_bins=150)

print(&quot;=&quot; * 60)
print(&quot;å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRDFï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;è¨ˆç®—å®Œäº†: {len(r_bins)}å€‹ã®ãƒ“ãƒ³&quot;)

# RDFã®ãƒ—ãƒ­ãƒƒãƒˆ
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(r_bins, rdf, 'b-', linewidth=2)
ax.axhline(y=1, color='r', linestyle='--', linewidth=1, label='Ideal Gas (g(r)=1)')
ax.set_xlabel('Distance r (nm)', fontsize=12)
ax.set_ylabel('g(r)', fontsize=12)
ax.set_title('Radial Distribution Function (RDF)', fontsize=13, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)
ax.set_ylim(0, max(rdf) * 1.1)

plt.tight_layout()
plt.show()

# ãƒ”ãƒ¼ã‚¯ä½ç½®ã®æ¤œå‡º
from scipy.signal import find_peaks

peaks, _ = find_peaks(rdf, height=1.2, distance=10)
print(f&quot;\nRDFã®ãƒ”ãƒ¼ã‚¯ä½ç½®ï¼ˆç‰¹å¾´çš„ãªåŸå­é–“è·é›¢ï¼‰:&quot;)
for i, peak_idx in enumerate(peaks[:3], 1):
    print(f&quot;  ãƒ”ãƒ¼ã‚¯{i}: r = {r_bins[peak_idx]:.3f} nm, g(r) = {rdf[peak_idx]:.2f}&quot;)
</code></pre>

<h3 id="33mean-squared-displacement">ã€ä¾‹33ã€‘æ‹¡æ•£ä¿‚æ•°ã®è¨ˆç®—ï¼ˆMean Squared Displacementï¼‰</h3>
<pre class="codehilite"><code class="language-python"># å¹³å‡äºŒä¹—å¤‰ä½ï¼ˆMSDï¼‰ã®è¨ˆç®—
def calculate_msd(positions):
    &quot;&quot;&quot;
    å¹³å‡äºŒä¹—å¤‰ä½ã‚’è¨ˆç®—

    Parameters:
    -----------
    positions : ndarray
        åŸå­ä½ç½® (n_steps, n_atoms, 3)

    Returns:
    --------
    msd : ndarray
        å¹³å‡äºŒä¹—å¤‰ä½ (n_steps,)
    &quot;&quot;&quot;
    n_steps, n_atoms, _ = positions.shape
    msd = np.zeros(n_steps)

    # å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§ã®MSD
    for t in range(n_steps):
        displacement = positions[t] - positions[0]
        squared_displacement = np.sum(displacement**2, axis=1)
        msd[t] = np.mean(squared_displacement)

    return msd

# MSDã®è¨ˆç®—
msd = calculate_msd(positions)
time = np.arange(n_steps) * dt

print(&quot;=&quot; * 60)
print(&quot;å¹³å‡äºŒä¹—å¤‰ä½ï¼ˆMSDï¼‰ã¨æ‹¡æ•£ä¿‚æ•°&quot;)
print(&quot;=&quot; * 60)

# æ‹¡æ•£ä¿‚æ•°ã®è¨ˆç®—ï¼ˆEinsteiné–¢ä¿‚å¼: MSD = 6*D*tï¼‰
# ç·šå½¢ãƒ•ã‚£ãƒƒãƒˆï¼ˆå¾ŒåŠ50%ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
start_idx = n_steps // 2
fit_coeffs = np.polyfit(time[start_idx:], msd[start_idx:], 1)
slope = fit_coeffs[0]
diffusion_coefficient = slope / 6

print(f&quot;æ‹¡æ•£ä¿‚æ•° D = {diffusion_coefficient:.6f} nmÂ²/ps&quot;)
print(f&quot;            = {diffusion_coefficient * 1e3:.6f} Ã— 10â»â¶ cmÂ²/s&quot;)

# MSDãƒ—ãƒ­ãƒƒãƒˆ
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(time, msd, 'b-', linewidth=2, label='MSD')
ax.plot(time[start_idx:], fit_coeffs[0] * time[start_idx:] + fit_coeffs[1],
        'r--', linewidth=2, label=f'Linear Fit (D={diffusion_coefficient:.4f} nmÂ²/ps)')
ax.set_xlabel('Time (ps)', fontsize=12)
ax.set_ylabel('MSD (nmÂ²)', fontsize=12)
ax.set_title('Mean Squared Displacement (MSD)', fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(&quot;\næ‹¡æ•£ä¿‚æ•°ã¯ã€ãƒŠãƒç²’å­ã®ç§»å‹•æ€§ã‚’å®šé‡çš„ã«è©•ä¾¡ã™ã‚‹é‡è¦ãªæŒ‡æ¨™ã§ã™&quot;)
</code></pre>

<hr />
<h2 id="311">3.11 ç•°å¸¸æ¤œçŸ¥ï¼šå“è³ªç®¡ç†ã¸ã®å¿œç”¨</h2>
<h3 id="34isolation-forest">ã€ä¾‹34ã€‘Isolation Forestã«ã‚ˆã‚‹ç•°å¸¸ãƒŠãƒç²’å­æ¤œå‡º</h3>
<p>è£½é€ ãƒ—ãƒ­ã‚»ã‚¹ã§ç”Ÿæˆã•ã‚ŒãŸãƒŠãƒç²’å­ã®å“è³ªç®¡ç†ã«ã€æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ã‚’é©ç”¨ã—ã¾ã™ã€‚</p>
<pre class="codehilite"><code class="language-python">from sklearn.ensemble import IsolationForest

# æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã¨ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã‚’æ··åœ¨ã•ã›ã‚‹
np.random.seed(400)

# æ­£å¸¸ãªé‡‘ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ï¼ˆ180ã‚µãƒ³ãƒ—ãƒ«ï¼‰
normal_size = np.random.normal(15, 3, 180)
normal_lspr = 520 + 0.8 * (normal_size - 15) + np.random.normal(0, 3, 180)

# ç•°å¸¸ãªãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ï¼ˆ20ã‚µãƒ³ãƒ—ãƒ«ï¼‰ï¼šã‚µã‚¤ã‚ºãŒç•°å¸¸ã«å¤§ãã„orå°ã•ã„
anomaly_size = np.concatenate([
    np.random.uniform(5, 8, 10),  # ç•°å¸¸ã«å°ã•ã„
    np.random.uniform(35, 50, 10)  # ç•°å¸¸ã«å¤§ãã„
])
anomaly_lspr = 520 + 0.8 * (anomaly_size - 15) + np.random.normal(0, 8, 20)

# å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
all_size = np.concatenate([normal_size, anomaly_size])
all_lspr = np.concatenate([normal_lspr, anomaly_lspr])
all_data = np.column_stack([all_size, all_lspr])

# ãƒ©ãƒ™ãƒ«ï¼ˆæ­£å¸¸=0ã€ç•°å¸¸=1ï¼‰
true_labels = np.concatenate([np.zeros(180), np.ones(20)])

print(&quot;=&quot; * 60)
print(&quot;ç•°å¸¸æ¤œçŸ¥ï¼ˆIsolation Forestï¼‰&quot;)
print(&quot;=&quot; * 60)
print(f&quot;å…¨ãƒ‡ãƒ¼ã‚¿æ•°: {len(all_data)}&quot;)
print(f&quot;æ­£å¸¸ãƒ‡ãƒ¼ã‚¿: {int((true_labels == 0).sum())}ã‚µãƒ³ãƒ—ãƒ«&quot;)
print(f&quot;ç•°å¸¸ãƒ‡ãƒ¼ã‚¿: {int((true_labels == 1).sum())}ã‚µãƒ³ãƒ—ãƒ«&quot;)

# Isolation Forestãƒ¢ãƒ‡ãƒ«
iso_forest = IsolationForest(
    contamination=0.1,  # ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆï¼ˆ10%ã¨ä»®å®šï¼‰
    random_state=42,
    n_estimators=100
)

# ç•°å¸¸æ¤œçŸ¥
predictions = iso_forest.fit_predict(all_data)
anomaly_scores = iso_forest.score_samples(all_data)

# äºˆæ¸¬çµæœï¼ˆ1: æ­£å¸¸ã€-1: ç•°å¸¸ï¼‰
predicted_anomalies = (predictions == -1)
true_anomalies = (true_labels == 1)

# è©•ä¾¡æŒ‡æ¨™
from sklearn.metrics import confusion_matrix, classification_report

# äºˆæ¸¬ã‚’0/1ã«å¤‰æ›
predicted_labels = (predictions == -1).astype(int)

print(&quot;\næ··åŒè¡Œåˆ—:&quot;)
cm = confusion_matrix(true_labels, predicted_labels)
print(cm)

print(&quot;\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:&quot;)
print(classification_report(true_labels, predicted_labels,
                            target_names=['Normal', 'Anomaly']))

# æ¤œå‡ºç‡
detected_anomalies = np.sum(predicted_anomalies &amp; true_anomalies)
total_anomalies = np.sum(true_anomalies)
detection_rate = detected_anomalies / total_anomalies * 100

print(f&quot;\nç•°å¸¸æ¤œå‡ºç‡: {detection_rate:.1f}% ({detected_anomalies}/{total_anomalies})&quot;)
</code></pre>

<h3 id="35_1">ã€ä¾‹35ã€‘ç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®å¯è¦–åŒ–</h3>
<pre class="codehilite"><code class="language-python"># ç•°å¸¸æ¤œçŸ¥çµæœã®å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# æ•£å¸ƒå›³ï¼ˆçœŸã®ãƒ©ãƒ™ãƒ«ï¼‰
axes[0].scatter(all_size[true_labels == 0], all_lspr[true_labels == 0],
                c='blue', s=60, alpha=0.6, label='Normal', edgecolors='k')
axes[0].scatter(all_size[true_labels == 1], all_lspr[true_labels == 1],
                c='red', s=100, alpha=0.8, marker='^', label='True Anomaly',
                edgecolors='k', linewidths=2)
axes[0].set_xlabel('Size (nm)', fontsize=12)
axes[0].set_ylabel('LSPR Wavelength (nm)', fontsize=12)
axes[0].set_title('True Labels', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# æ•£å¸ƒå›³ï¼ˆäºˆæ¸¬çµæœï¼‰
normal_mask = ~predicted_anomalies
anomaly_mask = predicted_anomalies

axes[1].scatter(all_size[normal_mask], all_lspr[normal_mask],
                c='blue', s=60, alpha=0.6, label='Predicted Normal', edgecolors='k')
axes[1].scatter(all_size[anomaly_mask], all_lspr[anomaly_mask],
                c='orange', s=100, alpha=0.8, marker='X', label='Predicted Anomaly',
                edgecolors='k', linewidths=2)

# æ­£ã—ãæ¤œå‡ºã•ã‚ŒãŸç•°å¸¸ã‚’å¼·èª¿
correctly_detected = predicted_anomalies &amp; true_anomalies
axes[1].scatter(all_size[correctly_detected], all_lspr[correctly_detected],
                c='red', s=150, marker='*', label='Correctly Detected',
                edgecolors='black', linewidths=1.5, zorder=5)

axes[1].set_xlabel('Size (nm)', fontsize=12)
axes[1].set_ylabel('LSPR Wavelength (nm)', fontsize=12)
axes[1].set_title('Isolation Forest Predictions', fontsize=13, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ç•°å¸¸ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
fig, ax = plt.subplots(figsize=(10, 6))
ax.hist(anomaly_scores[true_labels == 0], bins=30, alpha=0.6,
        color='blue', label='Normal', edgecolor='black')
ax.hist(anomaly_scores[true_labels == 1], bins=30, alpha=0.6,
        color='red', label='Anomaly', edgecolor='black')
ax.set_xlabel('Anomaly Score', fontsize=12)
ax.set_ylabel('Frequency', fontsize=12)
ax.set_title('Anomaly Score Distribution', fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print(&quot;\nç•°å¸¸ã‚¹ã‚³ã‚¢ãŒä½ã„ï¼ˆè² ã®å€¤ãŒå¤§ãã„ï¼‰ã»ã©ã€ç•°å¸¸ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„&quot;)
</code></pre>

<hr />
<h2 id="_4">ã¾ã¨ã‚</h2>
<p>æœ¬ç« ã§ã¯ã€Pythonã‚’ä½¿ã£ãŸãƒŠãƒææ–™ãƒ‡ãƒ¼ã‚¿è§£æã¨æ©Ÿæ¢°å­¦ç¿’ã®å®Ÿè·µçš„æ‰‹æ³•ã‚’35å€‹ã®ã‚³ãƒ¼ãƒ‰ä¾‹ã§å­¦ã³ã¾ã—ãŸã€‚</p>
<h3 id="_5">ç¿’å¾—ã—ãŸä¸»è¦æŠ€è¡“</h3>
<ol>
<li>
<p><strong>ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¨å¯è¦–åŒ–</strong>ï¼ˆä¾‹1-5ï¼‰<br />
   - é‡‘ãƒŠãƒç²’å­ã€é‡å­ãƒ‰ãƒƒãƒˆã®åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ<br />
   - ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã€æ•£å¸ƒå›³ã€3Dãƒ—ãƒ­ãƒƒãƒˆã€ç›¸é–¢åˆ†æ</p>
</li>
<li>
<p><strong>ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†</strong>ï¼ˆä¾‹6-9ï¼‰<br />
   - æ¬ æå€¤å‡¦ç†ã€å¤–ã‚Œå€¤æ¤œå‡ºã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€ãƒ‡ãƒ¼ã‚¿åˆ†å‰²</p>
</li>
<li>
<p><strong>å›å¸°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç‰©æ€§äºˆæ¸¬</strong>ï¼ˆä¾‹10-15ï¼‰<br />
   - ç·šå½¢å›å¸°ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€LightGBMã€SVRã€MLP<br />
   - ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒï¼ˆRÂ²ã€RMSEã€MAEï¼‰</p>
</li>
<li>
<p><strong>é‡å­ãƒ‰ãƒƒãƒˆç™ºå…‰äºˆæ¸¬</strong>ï¼ˆä¾‹16-18ï¼‰<br />
   - Brusæ–¹ç¨‹å¼ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ç”Ÿæˆ<br />
   - LightGBMã«ã‚ˆã‚‹äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰</p>
</li>
<li>
<p><strong>ç‰¹å¾´é‡é‡è¦åº¦ã¨ãƒ¢ãƒ‡ãƒ«è§£é‡ˆ</strong>ï¼ˆä¾‹19-20ï¼‰<br />
   - LightGBMç‰¹å¾´é‡é‡è¦åº¦<br />
   - SHAPåˆ†æã«ã‚ˆã‚‹äºˆæ¸¬ã®è§£é‡ˆ</p>
</li>
<li>
<p><strong>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–</strong>ï¼ˆä¾‹21-25ï¼‰<br />
   - ç›®æ¨™LSPRæ³¢é•·ã‚’å®Ÿç¾ã™ã‚‹æœ€é©åˆæˆæ¡ä»¶ã®æ¢ç´¢<br />
   - åæŸãƒ—ãƒ­ãƒƒãƒˆã€æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–</p>
</li>
<li>
<p><strong>å¤šç›®çš„æœ€é©åŒ–</strong>ï¼ˆä¾‹26-27ï¼‰<br />
   - NSGA-IIã«ã‚ˆã‚‹ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©åŒ–<br />
   - ã‚µã‚¤ã‚ºã¨ç™ºå…‰åŠ¹ç‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æ</p>
</li>
<li>
<p><strong>TEMç”»åƒè§£æ</strong>ï¼ˆä¾‹28-30ï¼‰<br />
   - å¯¾æ•°æ­£è¦åˆ†å¸ƒã«ã‚ˆã‚‹ã‚µã‚¤ã‚ºåˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°<br />
   - Q-Qãƒ—ãƒ­ãƒƒãƒˆã«ã‚ˆã‚‹åˆ†å¸ƒã®æ¤œè¨¼</p>
</li>
<li>
<p><strong>åˆ†å­å‹•åŠ›å­¦ãƒ‡ãƒ¼ã‚¿è§£æ</strong>ï¼ˆä¾‹31-33ï¼‰<br />
   - åŸå­è»Œè·¡ã®å¯è¦–åŒ–<br />
   - å‹•å¾„åˆ†å¸ƒé–¢æ•°ï¼ˆRDFï¼‰ã®è¨ˆç®—<br />
   - æ‹¡æ•£ä¿‚æ•°ã®ç®—å‡ºï¼ˆMSDæ³•ï¼‰</p>
</li>
<li>
<p><strong>ç•°å¸¸æ¤œçŸ¥</strong>ï¼ˆä¾‹34-35ï¼‰</p>
<ul>
<li>Isolation Forestã«ã‚ˆã‚‹å“è³ªç®¡ç†</li>
<li>ç•°å¸¸ãƒŠãƒç²’å­ã®è‡ªå‹•æ¤œå‡º</li>
</ul>
</li>
</ol>
<h3 id="_6">å®Ÿè·µçš„ãªå¿œç”¨</h3>
<p>ã“ã‚Œã‚‰ã®æŠ€è¡“ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå®Ÿéš›ã®ãƒŠãƒææ–™ç ”ç©¶ã«ç›´æ¥å¿œç”¨ã§ãã¾ã™ï¼š</p>
<ul>
<li><strong>ææ–™è¨­è¨ˆ</strong>: æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ç‰©æ€§äºˆæ¸¬ã¨æœ€é©åŒ–ã«ã‚ˆã‚‹é«˜åŠ¹ç‡ææ–™æ¢ç´¢</li>
<li><strong>ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–</strong>: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚‹å®Ÿé¨“å›æ•°å‰Šæ¸›ã¨æœ€é©åˆæˆæ¡ä»¶ç™ºè¦‹</li>
<li><strong>å“è³ªç®¡ç†</strong>: ç•°å¸¸æ¤œçŸ¥ã«ã‚ˆã‚‹ä¸è‰¯å“ã®æ—©æœŸç™ºè¦‹ã¨æ­©ç•™ã¾ã‚Šå‘ä¸Š</li>
<li><strong>ãƒ‡ãƒ¼ã‚¿è§£æ</strong>: TEMãƒ‡ãƒ¼ã‚¿ã€MDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®å®šé‡çš„è§£æ</li>
<li><strong>ãƒ¢ãƒ‡ãƒ«è§£é‡ˆ</strong>: SHAPåˆ†æã«ã‚ˆã‚‹äºˆæ¸¬æ ¹æ‹ ã®å¯è¦–åŒ–ã¨ä¿¡é ¼æ€§å‘ä¸Š</li>
</ul>
<h3 id="_7">æ¬¡ç« ã®äºˆå‘Š</h3>
<p>Chapter 4ã§ã¯ã€ã“ã‚Œã‚‰ã®æŠ€è¡“ã‚’å®Ÿéš›ã®ãƒŠãƒææ–™ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«é©ç”¨ã—ãŸ5ã¤ã®è©³ç´°ãªã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ã‚’å­¦ã³ã¾ã™ã€‚ã‚«ãƒ¼ãƒœãƒ³ãƒŠãƒãƒãƒ¥ãƒ¼ãƒ–è¤‡åˆææ–™ã€é‡å­ãƒ‰ãƒƒãƒˆã€é‡‘ãƒŠãƒç²’å­è§¦åª’ã€ã‚°ãƒ©ãƒ•ã‚§ãƒ³ã€ãƒŠãƒåŒ»è–¬ã®å®Ÿç”¨åŒ–äº‹ä¾‹ã‚’é€šã˜ã¦ã€å•é¡Œè§£æ±ºã®å…¨ä½“åƒã‚’ç†è§£ã—ã¾ã™ã€‚</p>
<hr />
<h2 id="_8">æ¼”ç¿’å•é¡Œ</h2>
<h3 id="1_1">æ¼”ç¿’1: ã‚«ãƒ¼ãƒœãƒ³ãƒŠãƒãƒãƒ¥ãƒ¼ãƒ–ã®é›»æ°—ä¼å°åº¦äºˆæ¸¬</h3>
<p>ã‚«ãƒ¼ãƒœãƒ³ãƒŠãƒãƒãƒ¥ãƒ¼ãƒ–ï¼ˆCNTï¼‰ã®é›»æ°—ä¼å°åº¦ã¯ã€ç›´å¾„ã€ã‚«ã‚¤ãƒ©ãƒªãƒ†ã‚£ã€é•·ã•ã«ä¾å­˜ã—ã¾ã™ã€‚ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã€LightGBMãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>ãƒ‡ãƒ¼ã‚¿ä»•æ§˜</strong>ï¼š<br />
- ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼š150<br />
- ç‰¹å¾´é‡ï¼šç›´å¾„ï¼ˆ1-3 nmï¼‰ã€é•·ã•ï¼ˆ100-1000 nmï¼‰ã€ã‚«ã‚¤ãƒ©ãƒªãƒ†ã‚£æŒ‡æ¨™ï¼ˆ0-1ã®é€£ç¶šå€¤ï¼‰<br />
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼šé›»æ°—ä¼å°åº¦ï¼ˆ10Â³-10â· S/mã€å¯¾æ•°æ­£è¦åˆ†å¸ƒï¼‰</p>
<p><strong>ã‚¿ã‚¹ã‚¯</strong>ï¼š<br />
1. ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ<br />
2. è¨“ç·´/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²<br />
3. LightGBMãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨è©•ä¾¡<br />
4. ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–</p>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre class="codehilite"><code class="language-python"># ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(500)
n_samples = 150

diameter = np.random.uniform(1, 3, n_samples)
length = np.random.uniform(100, 1000, n_samples)
chirality = np.random.uniform(0, 1, n_samples)

# é›»æ°—ä¼å°åº¦ï¼ˆç°¡æ˜“ãƒ¢ãƒ‡ãƒ«: ç›´å¾„ã¨ã‚«ã‚¤ãƒ©ãƒªãƒ†ã‚£ã«å¼·ãä¾å­˜ï¼‰
log_conductivity = 3 + 2*diameter + 3*chirality + 0.001*length + np.random.normal(0, 0.5, n_samples)
conductivity = 10 ** log_conductivity  # S/m

data_cnt = pd.DataFrame({
    'diameter_nm': diameter,
    'length_nm': length,
    'chirality': chirality,
    'conductivity_Sm': conductivity
})

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
X_cnt = data_cnt[['diameter_nm', 'length_nm', 'chirality']]
y_cnt = np.log10(data_cnt['conductivity_Sm'])  # å¯¾æ•°å¤‰æ›

# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
scaler_cnt = StandardScaler()
X_cnt_scaled = scaler_cnt.fit_transform(X_cnt)

# è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²
X_cnt_train, X_cnt_test, y_cnt_train, y_cnt_test = train_test_split(
    X_cnt_scaled, y_cnt, test_size=0.2, random_state=42
)

# LightGBMãƒ¢ãƒ‡ãƒ«
model_cnt = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.05, n_estimators=200, random_state=42, verbose=-1)
model_cnt.fit(X_cnt_train, y_cnt_train)

# äºˆæ¸¬ã¨è©•ä¾¡
y_cnt_pred = model_cnt.predict(X_cnt_test)
r2_cnt = r2_score(y_cnt_test, y_cnt_pred)
rmse_cnt = np.sqrt(mean_squared_error(y_cnt_test, y_cnt_pred))

print(f&quot;RÂ²: {r2_cnt:.4f}&quot;)
print(f&quot;RMSE: {rmse_cnt:.4f}&quot;)

# ç‰¹å¾´é‡é‡è¦åº¦
importance_cnt = pd.DataFrame({
    'Feature': X_cnt.columns,
    'Importance': model_cnt.feature_importances_
}).sort_values('Importance', ascending=False)

print(&quot;\nç‰¹å¾´é‡é‡è¦åº¦:&quot;)
print(importance_cnt)

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(8, 5))
ax.barh(importance_cnt['Feature'], importance_cnt['Importance'], color='steelblue', edgecolor='black')
ax.set_xlabel('Importance')
ax.set_title('Feature Importance: CNT Conductivity Prediction')
plt.tight_layout()
plt.show()
</code></pre>



</details>

<h3 id="2_1">æ¼”ç¿’2: éŠ€ãƒŠãƒç²’å­ã®æœ€é©åˆæˆæ¡ä»¶æ¢ç´¢</h3>
<p>éŠ€ãƒŠãƒç²’å­ã®æŠ—èŒæ´»æ€§ã¯ã€ã‚µã‚¤ã‚ºãŒå°ã•ã„ã»ã©é«˜ããªã‚Šã¾ã™ã€‚ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ç”¨ã„ã¦ã€ç›®æ¨™ã‚µã‚¤ã‚ºï¼ˆ10 nmï¼‰ã‚’å®Ÿç¾ã™ã‚‹æœ€é©ãªåˆæˆæ¸©åº¦ã¨pHã‚’æ¢ç´¢ã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>æ¡ä»¶</strong>ï¼š<br />
- æ¸©åº¦ç¯„å›²ï¼š20-80Â°C<br />
- pHç¯„å›²ï¼š6-11<br />
- ç›®æ¨™ã‚µã‚¤ã‚ºï¼š10 nm</p>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre class="codehilite"><code class="language-python"># éŠ€ãƒŠãƒç²’å­ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
np.random.seed(600)
n_ag = 100

temp_ag = np.random.uniform(20, 80, n_ag)
pH_ag = np.random.uniform(6, 11, n_ag)

# ã‚µã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ï¼ˆæ¸©åº¦ãŒé«˜ãã€pHãŒä½ã„ã»ã©å°ã•ããªã‚‹ã¨ä»®å®šï¼‰
size_ag = 15 - 0.1*temp_ag - 0.8*pH_ag + np.random.normal(0, 1, n_ag)
size_ag = np.clip(size_ag, 5, 30)

data_ag = pd.DataFrame({
    'temperature': temp_ag,
    'pH': pH_ag,
    'size': size_ag
})

# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆLightGBMï¼‰
X_ag = data_ag[['temperature', 'pH']]
y_ag = data_ag['size']

scaler_ag = StandardScaler()
X_ag_scaled = scaler_ag.fit_transform(X_ag)

model_ag = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.05, n_estimators=100, random_state=42, verbose=-1)
model_ag.fit(X_ag_scaled, y_ag)

# ãƒ™ã‚¤ã‚ºæœ€é©åŒ–
from skopt import gp_minimize
from skopt.space import Real

space_ag = [
    Real(20, 80, name='temperature'),
    Real(6, 11, name='pH')
]

target_size = 10.0

def objective_ag(params):
    temp, ph = params
    features = scaler_ag.transform([[temp, ph]])
    predicted_size = model_ag.predict(features)[0]
    return abs(predicted_size - target_size)

result_ag = gp_minimize(objective_ag, space_ag, n_calls=40, random_state=42, verbose=False)

print(&quot;=&quot; * 60)
print(&quot;éŠ€ãƒŠãƒç²’å­ã®æœ€é©åˆæˆæ¡ä»¶&quot;)
print(&quot;=&quot; * 60)
print(f&quot;æœ€å°èª¤å·®: {result_ag.fun:.2f} nm&quot;)
print(f&quot;æœ€é©æ¸©åº¦: {result_ag.x[0]:.1f} Â°C&quot;)
print(f&quot;æœ€é©pH: {result_ag.x[1]:.2f}&quot;)

# æœ€é©æ¡ä»¶ã§ã®äºˆæ¸¬ã‚µã‚¤ã‚º
optimal_features = scaler_ag.transform([result_ag.x])
predicted_size = model_ag.predict(optimal_features)[0]
print(f&quot;äºˆæ¸¬ã‚µã‚¤ã‚º: {predicted_size:.2f} nm&quot;)
</code></pre>



</details>

<h3 id="3_1">æ¼”ç¿’3: é‡å­ãƒ‰ãƒƒãƒˆã®å¤šè‰²ç™ºå…‰è¨­è¨ˆ</h3>
<p>èµ¤ï¼ˆ650 nmï¼‰ã€ç·‘ï¼ˆ550 nmï¼‰ã€é’ï¼ˆ450 nmï¼‰ã®3è‰²ã®ç™ºå…‰ã‚’å®Ÿç¾ã™ã‚‹CdSeé‡å­ãƒ‰ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã‚’ã€ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚</p>
<p><strong>ãƒ’ãƒ³ãƒˆ</strong>ï¼š<br />
- å„è‰²ã”ã¨ã«æœ€é©åŒ–ã‚’å®Ÿè¡Œ<br />
- ç™ºå…‰æ³¢é•·ã¨ã‚µã‚¤ã‚ºã®é–¢ä¿‚ã‚’ä½¿ç”¨</p>
<details>
<summary>è§£ç­”ä¾‹</summary>


<pre class="codehilite"><code class="language-python"># é‡å­ãƒ‰ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹16ã®data_qdã‚’ä½¿ç”¨ï¼‰
# model_qd ã¨ scaler_qd ãŒæ§‹ç¯‰æ¸ˆã¿ã¨ä»®å®š

# 3è‰²ã®ç›®æ¨™æ³¢é•·
target_colors = {
    'Red': 650,
    'Green': 550,
    'Blue': 450
}

results_colors = {}

for color_name, target_emission in target_colors.items():
    # æ¢ç´¢ç©ºé–“
    space_qd = [
        Real(2, 10, name='size_nm'),
        Real(10, 120, name='synthesis_time_min'),
        Real(0.5, 2.0, name='precursor_ratio')
    ]

    # ç›®çš„é–¢æ•°
    def objective_qd(params):
        features = scaler_qd.transform([params])
        predicted_emission = model_qd.predict(features)[0]
        return abs(predicted_emission - target_emission)

    # æœ€é©åŒ–
    result_qd_color = gp_minimize(objective_qd, space_qd, n_calls=30, random_state=42, verbose=False)

    # çµæœä¿å­˜
    optimal_features = scaler_qd.transform([result_qd_color.x])
    predicted_emission = model_qd.predict(optimal_features)[0]

    results_colors[color_name] = {
        'target': target_emission,
        'size': result_qd_color.x[0],
        'time': result_qd_color.x[1],
        'ratio': result_qd_color.x[2],
        'predicted': predicted_emission,
        'error': result_qd_color.fun
    }

# çµæœè¡¨ç¤º
print(&quot;=&quot; * 80)
print(&quot;é‡å­ãƒ‰ãƒƒãƒˆå¤šè‰²ç™ºå…‰è¨­è¨ˆ&quot;)
print(&quot;=&quot; * 80)

results_df = pd.DataFrame(results_colors).T
print(results_df.to_string())

# å¯è¦–åŒ–
fig, ax = plt.subplots(figsize=(10, 6))
colors_rgb = {'Red': 'red', 'Green': 'green', 'Blue': 'blue'}

for color_name, result in results_colors.items():
    ax.scatter(result['size'], result['predicted'],
               s=200, color=colors_rgb[color_name],
               edgecolors='black', linewidths=2, label=color_name)

ax.set_xlabel('Quantum Dot Size (nm)', fontsize=12)
ax.set_ylabel('Emission Wavelength (nm)', fontsize=12)
ax.set_title('Multi-Color Quantum Dot Design', fontsize=13, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>



</details>

<hr />
<h2 id="_9">å‚è€ƒæ–‡çŒ®</h2>
<ol>
<li>
<p><strong>Pedregosa, F. et al.</strong> (2011). Scikit-learn: Machine Learning in Python. <em>Journal of Machine Learning Research</em>, 12, 2825-2830.</p>
</li>
<li>
<p><strong>Ke, G. et al.</strong> (2017). LightGBM: A highly efficient gradient boosting decision tree. <em>Advances in Neural Information Processing Systems</em>, 30, 3146-3154.</p>
</li>
<li>
<p><strong>Lundberg, S. M. &amp; Lee, S.-I.</strong> (2017). A unified approach to interpreting model predictions. <em>Advances in Neural Information Processing Systems</em>, 30, 4765-4774.</p>
</li>
<li>
<p><strong>Snoek, J., Larochelle, H., &amp; Adams, R. P.</strong> (2012). Practical Bayesian optimization of machine learning algorithms. <em>Advances in Neural Information Processing Systems</em>, 25, 2951-2959.</p>
</li>
<li>
<p><strong>Deb, K. et al.</strong> (2002). A fast and elitist multiobjective genetic algorithm: NSGA-II. <em>IEEE Transactions on Evolutionary Computation</em>, 6(2), 182-197. <a href="https://doi.org/10.1109/4235.996017">DOI: 10.1109/4235.996017</a></p>
</li>
<li>
<p><strong>Frenkel, D. &amp; Smit, B.</strong> (2001). <em>Understanding Molecular Simulation: From Algorithms to Applications</em> (2nd ed.). Academic Press.</p>
</li>
</ol>
<hr />
<p><a href="chapter2-fundamentals.html">â† å‰ç« ï¼šãƒŠãƒææ–™ã®åŸºç¤åŸç†</a> | <a href="chapter4-real-world.html">æ¬¡ç« ï¼šå®Ÿä¸–ç•Œã®å¿œç”¨ã¨ã‚­ãƒ£ãƒªã‚¢ â†’</a></p>

        <div class="nav-buttons">
            <a href="index.html" class="nav-button">â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 MI Knowledge Hub - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
