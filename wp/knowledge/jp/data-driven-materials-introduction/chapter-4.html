<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Chapter</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-25åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 0å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 0å•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 4: è§£é‡ˆå¯èƒ½AI (XAI)</h1>

---

<h2>å­¦ç¿’ç›®æ¨™</h2>

ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š

âœ… è§£é‡ˆå¯èƒ½æ€§ã®é‡è¦æ€§ã¨ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹å•é¡Œã®ç†è§£
âœ… SHAPï¼ˆShapleyå€¤ï¼‰ã«ã‚ˆã‚‹äºˆæ¸¬ã®å®šé‡çš„è§£é‡ˆ
âœ… LIMEã«ã‚ˆã‚‹å±€æ‰€çš„ãªç·šå½¢è¿‘ä¼¼ã¨èª¬æ˜ç”Ÿæˆ
âœ… Attentionå¯è¦–åŒ–ã«ã‚ˆã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è§£é‡ˆ
âœ… ãƒˆãƒ¨ã‚¿ãƒ»IBMãƒ»Citrineãªã©å®Ÿä¸–ç•Œå¿œç”¨äº‹ä¾‹ã®å­¦ç¿’
âœ… ææ–™ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã®ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹ã¨å¹´åæƒ…å ±

---

<h2>4.1 è§£é‡ˆå¯èƒ½æ€§ã®é‡è¦æ€§</h2>

æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’ç†è§£ã—ã€ç‰©ç†çš„æ„å‘³ã‚’æŠ½å‡ºã™ã‚‹ã“ã¨ãŒææ–™ç§‘å­¦ã§ã¯ä¸å¯æ¬ ã§ã™ã€‚

<h3>ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹å•é¡Œ</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge

<h1>ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿</h1>
np.random.seed(42)
X = np.random.randn(200, 10)
y = 2*X[:, 0] + 3*X[:, 1] - 1.5*X[:, 2] + np.random.normal(0, 0.5, 200)

<h1>è§£é‡ˆå¯èƒ½ãƒ¢ãƒ‡ãƒ« vs ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«</h1>
ridge = Ridge(alpha=1.0)
rf = RandomForestRegressor(n_estimators=100, random_state=42)

ridge.fit(X, y)
rf.fit(X, y)

<h1>Ridgeä¿‚æ•°ï¼ˆè§£é‡ˆå¯èƒ½ï¼‰</h1>
ridge_coefs = ridge.coef_

<h1>å¯è¦–åŒ–ï¼šãƒ¢ãƒ‡ãƒ«è§£é‡ˆæ€§ã®é•ã„</h1>
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

<h1>Ridge: ç·šå½¢ä¿‚æ•°ã§æ˜ç¢º</h1>
axes[0].bar(range(len(ridge_coefs)), ridge_coefs,
            color='steelblue', alpha=0.7)
axes[0].set_xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=11)
axes[0].set_ylabel('ä¿‚æ•°', fontsize=11)
axes[0].set_title('Ridgeå›å¸°ï¼ˆè§£é‡ˆå¯èƒ½ï¼‰', fontsize=12, fontweight='bold')
axes[0].axhline(y=0, color='red', linestyle='--', linewidth=1)
axes[0].grid(alpha=0.3)

<h1>Random Forest: è¤‡é›‘ãªéç·šå½¢é–¢ä¿‚ï¼ˆãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ï¼‰</h1>
axes[1].text(0.5, 0.5, 'â“\nãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹\n\n100æœ¬ã®æ±ºå®šæœ¨\nè¤‡é›‘ãªéç·šå½¢é–¢ä¿‚\nè§£é‡ˆå›°é›£',
             ha='center', va='center', fontsize=16,
             bbox=dict(boxstyle='round', facecolor='gray', alpha=0.3),
             transform=axes[1].transAxes)
axes[1].set_title('Random Forestï¼ˆãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ï¼‰',
                  fontsize=12, fontweight='bold')
axes[1].axis('off')

plt.tight_layout()
plt.show()

print("è§£é‡ˆå¯èƒ½æ€§ã®èª²é¡Œï¼š")
print("- ç·šå½¢ãƒ¢ãƒ‡ãƒ«: ä¿‚æ•°ã§å½±éŸ¿åº¦ãŒæ˜ç¢ºã ãŒã€ç²¾åº¦ãŒä½ã„")
print("- éç·šå½¢ãƒ¢ãƒ‡ãƒ«: é«˜ç²¾åº¦ã ãŒã€ãªãœãã®äºˆæ¸¬ã«ãªã£ãŸã‹ä¸æ˜")
print("â†’ XAIï¼ˆè§£é‡ˆå¯èƒ½AIï¼‰ã§ä¸¡ç«‹ã‚’ç›®æŒ‡ã™")</code></pre>

<h3>ææ–™ç§‘å­¦ã«ãŠã‘ã‚‹ç‰©ç†çš„è§£é‡ˆã®å¿…è¦æ€§</h3>

<pre><code class="language-python"><h1>ææ–™ç§‘å­¦ã§ã®è§£é‡ˆå¯èƒ½æ€§ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹</h1>
use_cases = pd.DataFrame({
    'ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹': [
        'æ–°ææ–™ç™ºè¦‹',
        'åˆæˆæ¡ä»¶æœ€é©åŒ–',
        'ãƒ—ãƒ­ã‚»ã‚¹ç•°å¸¸æ¤œå‡º',
        'ç‰©æ€§äºˆæ¸¬',
        'ææ–™è¨­è¨ˆã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³'
    ],
    'è§£é‡ˆæ€§ã®é‡è¦åº¦': [10, 9, 8, 7, 10],
    'ç†ç”±': [
        'ç‰©ç†çš„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®ç†è§£ãŒæ–°ç™ºè¦‹ã«ã¤ãªãŒã‚‹',
        'ã©ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒé‡è¦ã‹ã‚’ç‰¹å®š',
        'ç•°å¸¸ã®åŸå› ç‰¹å®šãŒå¿…è¦',
        'äºˆæ¸¬æ ¹æ‹ ã®æ¤œè¨¼',
        'è¨­è¨ˆæŒ‡é‡ã®æŠ½å‡º'
    ]
})

<h1>å¯è¦–åŒ–</h1>
fig, ax = plt.subplots(figsize=(12, 6))

colors = plt.cm.YlOrRd(np.linspace(0.3, 0.9, len(use_cases)))

bars = ax.barh(use_cases['ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹'],
               use_cases['è§£é‡ˆæ€§ã®é‡è¦åº¦'],
               color=colors, alpha=0.7)

ax.set_xlabel('è§£é‡ˆæ€§ã®é‡è¦åº¦ï¼ˆ1-10ï¼‰', fontsize=12)
ax.set_xlim(0, 10)
ax.set_title('ææ–™ç§‘å­¦ã«ãŠã‘ã‚‹è§£é‡ˆå¯èƒ½æ€§ã®é‡è¦åº¦',
             fontsize=13, fontweight='bold')
ax.grid(axis='x', alpha=0.3)

<h1>ç†ç”±ã‚’æ³¨é‡ˆ</h1>
for idx, row in use_cases.iterrows():
    ax.text(row['è§£é‡ˆæ€§ã®é‡è¦åº¦'] + 0.3, idx,
            row['ç†ç”±'], va='center', fontsize=9, style='italic')

plt.tight_layout()
plt.show()

print("ææ–™ç§‘å­¦ã§XAIãŒå¿…è¦ãªç†ç”±ï¼š")
print("1. ç‰©ç†æ³•å‰‡ã¨ã®æ•´åˆæ€§æ¤œè¨¼")
print("2. å®Ÿé¨“è¨ˆç”»ã¸ã®åæ˜ ")
print("3. å°‚é–€å®¶çŸ¥è­˜ã¨ã®çµ±åˆ")
print("4. è«–æ–‡ãƒ»ç‰¹è¨±ã§ã®èª¬æ˜è²¬ä»»")</code></pre>

<h3>ä¿¡é ¼æ€§ã¨ãƒ‡ãƒãƒƒã‚°</h3>

<pre><code class="language-python"><h1>ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãƒŸã‚¹ã‚’è§£é‡ˆã§ç™ºè¦‹ã™ã‚‹ä¾‹</h1>
from sklearn.model_data import train_test_split
from sklearn.metrics import mean_absolute_error

<h1>ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆæ„å›³çš„ã«ãƒã‚¤ã‚ºã‚’å«ã‚€ï¼‰</h1>
X_data = np.random.randn(300, 5)
<h1>æ­£ã—ã„é–¢ä¿‚: y = 2*X0 + 3*X1</h1>
y_true = 2*X_data[:, 0] + 3*X_data[:, 1] + np.random.normal(0, 0.3, 300)

<h1>ä¸€éƒ¨ã®ã‚µãƒ³ãƒ—ãƒ«ã«ãƒã‚¤ã‚ºæ··å…¥ï¼ˆæ¸¬å®šã‚¨ãƒ©ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰</h1>
noise_idx = np.random.choice(300, 30, replace=False)
y_data = y_true.copy()
y_data[noise_idx] += np.random.normal(0, 5, 30)

<h1>è¨“ç·´</h1>
X_train, X_test, y_train, y_test = train_test_split(
    X_data, y_data, test_size=0.2, random_state=42
)

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

<h1>äºˆæ¸¬</h1>
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)

<h1>èª¤å·®ãŒå¤§ãã„ã‚µãƒ³ãƒ—ãƒ«ã‚’ç‰¹å®š</h1>
errors = np.abs(y_test - y_pred)
high_error_idx = np.where(errors > np.percentile(errors, 90))[0]

print(f"ãƒ¢ãƒ‡ãƒ«MAE: {mae:.4f}")
print(f"é«˜èª¤å·®ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(high_error_idx)}")
print("\nâ†’ XAIã§é«˜èª¤å·®ã‚µãƒ³ãƒ—ãƒ«ã®åŸå› ã‚’åˆ†æ")
print("  - ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œã®ç™ºè¦‹")
print("  - ãƒ¢ãƒ‡ãƒ«ã®å¼±ç‚¹ç‰¹å®š")
print("  - ç‰©ç†çš„å¦¥å½“æ€§ã®æ¤œè¨¼")</code></pre>

---

<h2>4.2 SHAP (SHapley Additive exPlanations)</h2>

Shapleyå€¤ã«åŸºã¥ãå”åŠ›ã‚²ãƒ¼ãƒ ç†è«–ã‹ã‚‰ã®è§£é‡ˆæ‰‹æ³•ã§ã™ã€‚

<h3>Shapleyå€¤ã®ç†è«–</h3>

<pre><code class="language-python">import shap

<h1>SHAPåŸºæœ¬æ¦‚å¿µã®å¯è¦–åŒ–</h1>
shap.initjs()

<h1>ãƒ¢ãƒ‡ãƒ«è¨“ç·´</h1>
model_shap = RandomForestRegressor(n_estimators=100, random_state=42)
model_shap.fit(X_train, y_train)

<h1>SHAP Explainer</h1>
explainer = shap.TreeExplainer(model_shap)
shap_values = explainer.shap_values(X_test)

print("SHAPå€¤ã®æ„å‘³ï¼š")
print("- å„ç‰¹å¾´é‡ãŒäºˆæ¸¬å€¤ã«ã©ã‚Œã ã‘å¯„ä¸ã—ãŸã‹")
print("- Shapleyå€¤: å”åŠ›ã‚²ãƒ¼ãƒ ç†è«–ã®å…¬å¹³ãªåˆ†é…")
print("- åŸºæº–å€¤ï¼ˆbase valueï¼‰ã‹ã‚‰ã®åå·®ã¨ã—ã¦è¡¨ç¾")
print(f"\nSHAPå€¤ã®å½¢çŠ¶: {shap_values.shape}")
print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {shap_values.shape[0]}")
print(f"  ç‰¹å¾´é‡æ•°: {shap_values.shape[1]}")

<h1>å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«ã®èª¬æ˜</h1>
sample_idx = 0
base_value = explainer.expected_value
prediction = model_shap.predict(X_test[sample_idx:sample_idx+1])[0]

print(f"\nã‚µãƒ³ãƒ—ãƒ« {sample_idx} ã®äºˆæ¸¬:")
print(f"åŸºæº–å€¤: {base_value:.4f}")
print(f"SHAPå€¤åˆè¨ˆ: {shap_values[sample_idx].sum():.4f}")
print(f"äºˆæ¸¬å€¤: {prediction:.4f}")
print(f"æ¤œè¨¼: {base_value + shap_values[sample_idx].sum():.4f} â‰ˆ {prediction:.4f}")</code></pre>

<h3>SHAPå€¤ã®è¨ˆç®—ï¼ˆTree SHAP, Kernel SHAPï¼‰</h3>

<pre><code class="language-python"><h1>Tree SHAPï¼ˆé«˜é€Ÿã€æœ¨ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å°‚ç”¨ï¼‰</h1>
explainer_tree = shap.TreeExplainer(model_shap)
shap_values_tree = explainer_tree.shap_values(X_test)

<h1>Kernel SHAPï¼ˆãƒ¢ãƒ‡ãƒ«éä¾å­˜ã€é…ã„ï¼‰</h1>
<h1>å°ã‚µãƒ³ãƒ—ãƒ«ã§ãƒ‡ãƒ¢</h1>
X_test_small = X_test[:10]
explainer_kernel = shap.KernelExplainer(
    model_shap.predict,
    shap.sample(X_train, 50)
)
shap_values_kernel = explainer_kernel.shap_values(X_test_small)

print("SHAPè¨ˆç®—æ‰‹æ³•ã®æ¯”è¼ƒï¼š")
print("\nTree SHAP:")
print(f"  å¯¾è±¡ãƒ¢ãƒ‡ãƒ«: Tree-based (RF, XGBoost, LightGBM)")
print(f"  è¨ˆç®—é€Ÿåº¦: é«˜é€Ÿ")
print(f"  ç²¾åº¦: å³å¯†è§£")

print("\nKernel SHAP:")
print(f"  å¯¾è±¡ãƒ¢ãƒ‡ãƒ«: ä»»æ„ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚‚å¯ï¼‰")
print(f"  è¨ˆç®—é€Ÿåº¦: é…ã„")
print(f"  ç²¾åº¦: è¿‘ä¼¼è§£ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ï¼‰")

<h1>è¨ˆç®—æ™‚é–“æ¯”è¼ƒï¼ˆç°¡æ˜“ï¼‰</h1>
import time

start = time.time()
_ = explainer_tree.shap_values(X_test)
tree_time = time.time() - start

print(f"\nTree SHAPè¨ˆç®—æ™‚é–“: {tree_time:.3f}ç§’ ({len(X_test)}ã‚µãƒ³ãƒ—ãƒ«)")</code></pre>

<h3>Global vs Localè§£é‡ˆ</h3>

<pre><code class="language-python"><h1>Globalè§£é‡ˆ: å…¨ã‚µãƒ³ãƒ—ãƒ«ã§ã®å¹³å‡çš„é‡è¦åº¦</h1>
mean_abs_shap = np.abs(shap_values).mean(axis=0)

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

<h1>Globalè§£é‡ˆ</h1>
axes[0].bar(range(len(mean_abs_shap)), mean_abs_shap,
            color='steelblue', alpha=0.7)
axes[0].set_xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=11)
axes[0].set_ylabel('å¹³å‡|SHAPå€¤|', fontsize=11)
axes[0].set_title('Globalè§£é‡ˆï¼ˆå…¨ä½“çš„é‡è¦åº¦ï¼‰',
                  fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3)

<h1>Localè§£é‡ˆ: ç‰¹å®šã‚µãƒ³ãƒ—ãƒ«</h1>
sample_idx = 0
axes[1].bar(range(len(shap_values[sample_idx])),
            shap_values[sample_idx],
            color='coral', alpha=0.7)
axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1)
axes[1].set_xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=11)
axes[1].set_ylabel('SHAPå€¤', fontsize=11)
axes[1].set_title(f'Localè§£é‡ˆï¼ˆã‚µãƒ³ãƒ—ãƒ«{sample_idx}ã®èª¬æ˜ï¼‰',
                  fontsize=12, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("Globalè§£é‡ˆ vs Localè§£é‡ˆï¼š")
print("\nGlobal:")
print("  - å…¨ã‚µãƒ³ãƒ—ãƒ«ã§ã®å¹³å‡çš„ãªç‰¹å¾´é‡é‡è¦åº¦")
print("  - ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®æŒ™å‹•ç†è§£")
print("  - æ–°ææ–™è¨­è¨ˆã®ä¸€èˆ¬çš„ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³")

print("\nLocal:")
print("  - å€‹ã€…ã®äºˆæ¸¬ã®æ ¹æ‹ èª¬æ˜")
print("  - ç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ã®åŸå› ç‰¹å®š")
print("  - ç‰¹å®šææ–™ã®æœ€é©åŒ–æ–¹å‘")</code></pre>

<h3>Summary plot, Dependence plot</h3>

<pre><code class="language-python"><h1>Summary plotï¼ˆå…¨ä½“åƒï¼‰</h1>
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values, X_test, plot_type="dot", show=False)
plt.title('SHAP Summary Plot', fontsize=13, fontweight='bold', pad=20)
plt.tight_layout()
plt.show()

print("Summary Plotã®èª­ã¿æ–¹ï¼š")
print("- ç¸¦è»¸: ç‰¹å¾´é‡ï¼ˆé‡è¦åº¦é †ï¼‰")
print("- æ¨ªè»¸: SHAPå€¤ï¼ˆäºˆæ¸¬ã¸ã®å½±éŸ¿ï¼‰")
print("- è‰²: ç‰¹å¾´é‡ã®å€¤ï¼ˆèµ¤=é«˜ã€é’=ä½ï¼‰")
print("- åˆ†å¸ƒ: å„ç‰¹å¾´é‡ã®å½±éŸ¿ã®å¤šæ§˜æ€§")

<h1>Dependence plotï¼ˆå€‹åˆ¥ç‰¹å¾´é‡ã®è©³ç´°ï¼‰</h1>
feature_idx = 0

plt.figure(figsize=(10, 6))
shap.dependence_plot(
    feature_idx,
    shap_values,
    X_test,
    show=False
)
plt.title(f'SHAP Dependence Plot (ç‰¹å¾´é‡ {feature_idx})',
          fontsize=13, fontweight='bold')
plt.tight_layout()
plt.show()

print("\nDependence Plotã®èª­ã¿æ–¹ï¼š")
print("- æ¨ªè»¸: ç‰¹å¾´é‡ã®å€¤")
print("- ç¸¦è»¸: SHAPå€¤ï¼ˆäºˆæ¸¬ã¸ã®å½±éŸ¿ï¼‰")
print("- è‰²: ç›¸äº’ä½œç”¨ã™ã‚‹ä»–ã®ç‰¹å¾´é‡")
print("- å‚¾å‘: éç·šå½¢é–¢ä¿‚ã®å¯è¦–åŒ–")</code></pre>

---

<h2>4.3 LIME (Local Interpretable Model-agnostic Explanations)</h2>

å±€æ‰€çš„ãªç·šå½¢è¿‘ä¼¼ã«ã‚ˆã‚‹èª¬æ˜ç”Ÿæˆæ‰‹æ³•ã§ã™ã€‚

<h3>å±€æ‰€ç·šå½¢è¿‘ä¼¼</h3>

<pre><code class="language-python">from lime import lime_tabular

<h1>LIME Explainer</h1>
lime_explainer = lime_tabular.LimeTabularExplainer(
    X_train,
    mode='regression',
    feature_names=[f'Feature_{i}' for i in range(X_train.shape[1])],
    verbose=False
)

<h1>å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«ã®èª¬æ˜</h1>
sample_idx = 0
explanation = lime_explainer.explain_instance(
    X_test[sample_idx],
    model_shap.predict,
    num_features=5
)

<h1>å¯è¦–åŒ–</h1>
fig = explanation.as_pyplot_figure()
plt.title(f'LIME Explanation (ã‚µãƒ³ãƒ—ãƒ« {sample_idx})',
          fontsize=13, fontweight='bold')
plt.tight_layout()
plt.show()

print("LIMEã®ä»•çµ„ã¿ï¼š")
print("1. å¯¾è±¡ã‚µãƒ³ãƒ—ãƒ«å‘¨è¾ºã§ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°")
print("2. ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬")
print("3. è·é›¢ã«åŸºã¥ãé‡ã¿ä»˜ã‘")
print("4. å±€æ‰€çš„ãªç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’")
print("5. ç·šå½¢ä¿‚æ•°ã§èª¬æ˜")

<h1>èª¬æ˜ã®æ•°å€¤è¡¨ç¤º</h1>
print("\nèª¬æ˜ï¼ˆé‡è¦åº¦é †ï¼‰:")
for feature, weight in explanation.as_list():
    print(f"  {feature}: {weight:.4f}")</code></pre>

<h3>Tabular LIME</h3>

<pre><code class="language-python"><h1>è¤‡æ•°ã‚µãƒ³ãƒ—ãƒ«ã§LIMEå®Ÿè¡Œ</h1>
n_samples_lime = 5
lime_results = []

for i in range(n_samples_lime):
    exp = lime_explainer.explain_instance(
        X_test[i],
        model_shap.predict,
        num_features=X_train.shape[1]
    )

    # èª¬æ˜ã‚’è¾æ›¸ã«å¤‰æ›
    exp_dict = dict(exp.as_list())
    lime_results.append(exp_dict)

<h1>ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ åŒ–</h1>
lime_df = pd.DataFrame(lime_results)

print(f"\n{n_samples_lime}ã‚µãƒ³ãƒ—ãƒ«ã®LIMEèª¬æ˜:")
print(lime_df.head())

<h1>ä¸€è²«æ€§ã®è©•ä¾¡ï¼ˆåŒã˜ç‰¹å¾´é‡ãŒå¸¸ã«é‡è¦ã‹ï¼‰</h1>
feature_importance_consistency = lime_df.abs().mean()
print("\nç‰¹å¾´é‡ã®å¹³å‡çš„é‡è¦åº¦ï¼ˆLIMEï¼‰:")
print(feature_importance_consistency.sort_values(ascending=False))</code></pre>

<h3>äºˆæ¸¬ã®èª¬æ˜ç”Ÿæˆ</h3>

<pre><code class="language-python"><h1>SHAP vs LIMEæ¯”è¼ƒ</h1>
def compare_shap_lime(sample_idx):
    """
    åŒä¸€ã‚µãƒ³ãƒ—ãƒ«ã®SHAP vs LIMEèª¬æ˜æ¯”è¼ƒ
    """
    # SHAP
    shap_exp = shap_values[sample_idx]

    # LIME
    lime_exp = lime_explainer.explain_instance(
        X_test[sample_idx],
        model_shap.predict,
        num_features=X_train.shape[1]
    )
    lime_dict = dict(lime_exp.as_list())

    # LIMEèª¬æ˜ã‚’SHAPã¨åŒã˜é †åºã«æ•´åˆ—
    lime_exp_ordered = []
    for i in range(len(shap_exp)):
        feature_name = f'Feature_{i}'
        # LIMEã®èª¬æ˜ã‹ã‚‰è©²å½“ç‰¹å¾´é‡ã‚’æ¢ã™
        for key, value in lime_dict.items():
            if feature_name in key:
                lime_exp_ordered.append(value)
                break
        else:
            lime_exp_ordered.append(0)

    return shap_exp, np.array(lime_exp_ordered)

<h1>æ¯”è¼ƒ</h1>
sample_idx = 0
shap_exp, lime_exp = compare_shap_lime(sample_idx)

<h1>å¯è¦–åŒ–</h1>
fig, ax = plt.subplots(figsize=(12, 6))

x_pos = np.arange(len(shap_exp))
width = 0.35

ax.bar(x_pos - width/2, shap_exp, width,
       label='SHAP', color='steelblue', alpha=0.7)
ax.bar(x_pos + width/2, lime_exp, width,
       label='LIME', color='coral', alpha=0.7)

ax.set_xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=12)
ax.set_ylabel('é‡è¦åº¦', fontsize=12)
ax.set_title(f'SHAP vs LIME (ã‚µãƒ³ãƒ—ãƒ« {sample_idx})',
             fontsize=13, fontweight='bold')
ax.set_xticks(x_pos)
ax.legend()
ax.grid(alpha=0.3)
ax.axhline(y=0, color='black', linestyle='-', linewidth=1)

plt.tight_layout()
plt.show()

<h1>ç›¸é–¢åˆ†æ</h1>
correlation = np.corrcoef(shap_exp, lime_exp)[0, 1]
print(f"\nSHAP-LIMEç›¸é–¢: {correlation:.4f}")
print("é«˜ç›¸é–¢ â†’ ä¸¡æ‰‹æ³•ã§ä¸€è²«ã—ãŸèª¬æ˜")</code></pre>

---

<h2>4.4 Attentionå¯è¦–åŒ–ï¼ˆNN/GNNç”¨ï¼‰</h2>

ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®Attentionæ©Ÿæ§‹ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚

<h3>Attention weightsã®å¯è¦–åŒ–</h3>

<pre><code class="language-python"><h1>ç°¡æ˜“çš„ãªAttentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®ãƒ‡ãƒ¢</h1>
from sklearn.neural_network import MLPRegressor

<h1>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨“ç·´</h1>
nn_model = MLPRegressor(
    hidden_layer_sizes=(50, 50),
    max_iter=1000,
    random_state=42
)
nn_model.fit(X_train, y_train)

<h1>ä¸­é–“å±¤ã®æ´»æ€§åŒ–ã‚’å–å¾—ï¼ˆç°¡æ˜“ç‰ˆï¼‰</h1>
def get_activation(model, X, layer_idx=0):
    """
    æŒ‡å®šå±¤ã®æ´»æ€§åŒ–ã‚’å–å¾—
    """
    # é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹
    W = model.coefs_[layer_idx]
    b = model.intercepts_[layer_idx]

    # æ´»æ€§åŒ–ï¼ˆReLUï¼‰
    activation = np.maximum(0, X @ W + b)

    return activation

<h1>ç¬¬1å±¤ã®æ´»æ€§åŒ–</h1>
activation_layer1 = get_activation(nn_model, X_test, layer_idx=0)

<h1>Attention-like weightsï¼ˆæ´»æ€§åŒ–ã®å¤§ãã•ã‚’é‡ã¿ã¨è¦‹åšã™ï¼‰</h1>
attention_weights = np.abs(activation_layer1).mean(axis=1)

<h1>å¯è¦–åŒ–</h1>
plt.figure(figsize=(12, 6))
plt.scatter(range(len(attention_weights)), attention_weights,
            c=y_test, cmap='viridis', s=100, alpha=0.6)
plt.colorbar(label='Target Value')
plt.xlabel('ã‚µãƒ³ãƒ—ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=12)
plt.ylabel('Attention Weight (æ´»æ€§åŒ–å¼·åº¦)', fontsize=12)
plt.title('Attention-like Weightsï¼ˆç¬¬1å±¤æ´»æ€§åŒ–ï¼‰',
          fontsize=13, fontweight='bold')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print("Attentionå¯è¦–åŒ–ã®æ„ç¾©ï¼š")
print("- ãƒ¢ãƒ‡ãƒ«ãŒã©ã®å…¥åŠ›ã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹")
print("- é‡è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚„ç‰¹å¾´ã®ç‰¹å®š")
print("- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å†…éƒ¨å‹•ä½œç†è§£")</code></pre>

<h3>Grad-CAM for materials</h3>

<pre><code class="language-python"><h1>Grad-CAMé¢¨ã®å‹¾é…ãƒ™ãƒ¼ã‚¹é‡è¦åº¦ï¼ˆç°¡æ˜“ç‰ˆï¼‰</h1>
def gradient_based_importance(model, X_sample):
    """
    å‹¾é…ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡é‡è¦åº¦
    """
    # æ•°å€¤å¾®åˆ†ã§è¿‘ä¼¼
    epsilon = 1e-5
    base_pred = model.predict(X_sample.reshape(1, -1))[0]

    importances = []
    for i in range(len(X_sample)):
        X_perturbed = X_sample.copy()
        X_perturbed[i] += epsilon

        perturbed_pred = model.predict(X_perturbed.reshape(1, -1))[0]

        # å‹¾é…è¿‘ä¼¼
        gradient = (perturbed_pred - base_pred) / epsilon
        importances.append(gradient)

    return np.array(importances)

<h1>ã‚µãƒ³ãƒ—ãƒ«ã§å®Ÿè¡Œ</h1>
sample_idx = 0
grad_importances = gradient_based_importance(nn_model, X_test[sample_idx])

<h1>SHAP, LIME, Gradientã®æ¯”è¼ƒ</h1>
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

<h1>SHAP</h1>
axes[0].bar(range(len(shap_exp)), shap_exp,
            color='steelblue', alpha=0.7)
axes[0].axhline(y=0, color='black', linestyle='-', linewidth=1)
axes[0].set_xlabel('ç‰¹å¾´é‡', fontsize=11)
axes[0].set_ylabel('é‡è¦åº¦', fontsize=11)
axes[0].set_title('SHAP', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3)

<h1>LIME</h1>
axes[1].bar(range(len(lime_exp)), lime_exp,
            color='coral', alpha=0.7)
axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1)
axes[1].set_xlabel('ç‰¹å¾´é‡', fontsize=11)
axes[1].set_ylabel('é‡è¦åº¦', fontsize=11)
axes[1].set_title('LIME', fontsize=12, fontweight='bold')
axes[1].grid(alpha=0.3)

<h1>Gradient</h1>
axes[2].bar(range(len(grad_importances)), grad_importances,
            color='green', alpha=0.7)
axes[2].axhline(y=0, color='black', linestyle='-', linewidth=1)
axes[2].set_xlabel('ç‰¹å¾´é‡', fontsize=11)
axes[2].set_ylabel('å‹¾é…', fontsize=11)
axes[2].set_title('Gradient-based', fontsize=12, fontweight='bold')
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("3æ‰‹æ³•ã®ç‰¹å¾´ï¼š")
print("SHAP: ã‚²ãƒ¼ãƒ ç†è«–çš„å…¬å¹³æ€§ã€å…¨ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ")
print("LIME: å±€æ‰€ç·šå½¢è¿‘ä¼¼ã€ç›´æ„Ÿçš„")
print("Gradient: å‹¾é…æƒ…å ±ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç‰¹åŒ–")</code></pre>

<h3>ã©ã®åŸå­/çµåˆãŒé‡è¦ã‹</h3>

<pre><code class="language-python"><h1>ææ–™ç§‘å­¦ã§ã®å¿œç”¨ä¾‹ï¼šçµ„æˆã®é‡è¦åº¦</h1>
composition_features = ['Li', 'Co', 'Ni', 'Mn', 'O']

<h1>ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿</h1>
X_composition = pd.DataFrame({
    'Li': np.random.uniform(0.9, 1.1, 100),
    'Co': np.random.uniform(0, 0.6, 100),
    'Ni': np.random.uniform(0, 0.8, 100),
    'Mn': np.random.uniform(0, 0.4, 100),
    'O': np.random.uniform(1.9, 2.1, 100)
})

<h1>å®¹é‡ï¼ˆNiãŒé‡è¦ï¼‰</h1>
y_capacity = (
    150 * X_composition['Ni'] +
    120 * X_composition['Co'] +
    80 * X_composition['Mn'] +
    np.random.normal(0, 5, 100)
)

<h1>ãƒ¢ãƒ‡ãƒ«è¨“ç·´</h1>
model_comp = RandomForestRegressor(n_estimators=100, random_state=42)
model_comp.fit(X_composition, y_capacity)

<h1>SHAPè§£æ</h1>
explainer_comp = shap.TreeExplainer(model_comp)
shap_values_comp = explainer_comp.shap_values(X_composition)

<h1>å…ƒç´ åˆ¥é‡è¦åº¦</h1>
mean_abs_shap_comp = np.abs(shap_values_comp).mean(axis=0)

<h1>å¯è¦–åŒ–</h1>
plt.figure(figsize=(10, 6))
plt.bar(composition_features, mean_abs_shap_comp,
        color=['#FFD700', '#4169E1', '#32CD32', '#FF69B4', '#FF6347'],
        alpha=0.7, edgecolor='black', linewidth=1.5)
plt.xlabel('å…ƒç´ ', fontsize=12)
plt.ylabel('å¹³å‡|SHAPå€¤|', fontsize=12)
plt.title('é›»æ± å®¹é‡ã¸ã®å…ƒç´ å¯„ä¸åº¦ï¼ˆSHAPè§£æï¼‰',
          fontsize=13, fontweight='bold')
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

print("å…ƒç´ åˆ¥é‡è¦åº¦:")
for elem, importance in zip(composition_features, mean_abs_shap_comp):
    print(f"  {elem}: {importance:.2f}")

print("\nææ–™è¨­è¨ˆã¸ã®ç¤ºå”†:")
print("â†’ Niå«æœ‰é‡ã‚’å¢—ã‚„ã™ã“ã¨ã§å®¹é‡å‘ä¸ŠãŒæœŸå¾…ã§ãã‚‹")</code></pre>

---

<h2>4.5 å®Ÿä¸–ç•Œå¿œç”¨ã¨ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹</h2>

XAIã®ç”£æ¥­å¿œç”¨äº‹ä¾‹ã¨ã€ææ–™ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã®ã‚­ãƒ£ãƒªã‚¢æƒ…å ±ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

<h3>ãƒˆãƒ¨ã‚¿ï¼šææ–™é–‹ç™ºã«ãŠã‘ã‚‹XAIæ´»ç”¨</h3>

<pre><code class="language-python"><h1>ãƒˆãƒ¨ã‚¿ã®äº‹ä¾‹ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰</h1>
print("=== ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Š ææ–™é–‹ç™ºäº‹ä¾‹ ===")
print("\nèª²é¡Œ:")
print("  - é›»æ± ææ–™ã®åŠ£åŒ–ãƒ¡ã‚«ãƒ‹ã‚ºãƒ è§£æ˜")
print("  - æ•°åƒã®å€™è£œææ–™ã‹ã‚‰æœ€é©ææ–™é¸å®š")

print("\nXAIé©ç”¨:")
print("  - SHAPè§£æã§åŠ£åŒ–ã«å¯„ä¸ã™ã‚‹å› å­ã‚’ç‰¹å®š")
print("  - æ¸©åº¦ã€é›»åœ§ã€ã‚µã‚¤ã‚¯ãƒ«æ•°ã®ç›¸äº’ä½œç”¨ã‚’å¯è¦–åŒ–")
print("  - ç‰©ç†ãƒ¢ãƒ‡ãƒ«ã¨ã®æ•´åˆæ€§æ¤œè¨¼")

print("\næˆæœ:")
print("  - é–‹ç™ºæœŸé–“ 40% çŸ­ç¸®")
print("  - é›»æ± å¯¿å‘½ 20% å‘ä¸Š")
print("  - ç ”ç©¶è€…ã®ç‰©ç†çš„æ´å¯Ÿç²å¾—")

<h1>ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: é›»æ± åŠ£åŒ–äºˆæ¸¬</h1>
battery_aging = pd.DataFrame({
    'æ¸©åº¦': np.random.uniform(20, 60, 200),
    'é›»åœ§': np.random.uniform(3.0, 4.5, 200),
    'ã‚µã‚¤ã‚¯ãƒ«æ•°': np.random.uniform(0, 1000, 200),
    'å……é›»ãƒ¬ãƒ¼ãƒˆ': np.random.uniform(0.5, 2.0, 200)
})

<h1>åŠ£åŒ–ç‡ï¼ˆæ¸©åº¦ã¨ã‚µã‚¤ã‚¯ãƒ«ãŒä¸»è¦å› ï¼‰</h1>
degradation = (
    0.5 * battery_aging['æ¸©åº¦'] +
    0.3 * battery_aging['ã‚µã‚¤ã‚¯ãƒ«æ•°'] / 100 +
    0.2 * battery_aging['é›»åœ§'] * battery_aging['å……é›»ãƒ¬ãƒ¼ãƒˆ'] +
    np.random.normal(0, 2, 200)
)

<h1>ãƒ¢ãƒ‡ãƒ«</h1>
model_aging = RandomForestRegressor(n_estimators=100, random_state=42)
model_aging.fit(battery_aging, degradation)

<h1>SHAPåˆ†æ</h1>
explainer_aging = shap.TreeExplainer(model_aging)
shap_values_aging = explainer_aging.shap_values(battery_aging)

<h1>å¯è¦–åŒ–</h1>
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values_aging, battery_aging, show=False)
plt.title('é›»æ± åŠ£åŒ–è¦å› ã®SHAPåˆ†æï¼ˆãƒˆãƒ¨ã‚¿äº‹ä¾‹é¢¨ï¼‰',
          fontsize=13, fontweight='bold', pad=20)
plt.tight_layout()
plt.show()</code></pre>

<h3>IBM Researchï¼šAIææ–™è¨­è¨ˆã®è§£é‡ˆæ€§</h3>

<pre><code class="language-python">print("\n=== IBM Research ææ–™è¨­è¨ˆäº‹ä¾‹ ===")
print("\nãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: RoboRXN (è‡ªå‹•åŒ–å­¦å®Ÿé¨“)")
print("\nç‰¹å¾´:")
print("  - åå¿œæ¡ä»¶æœ€é©åŒ–ã«XAIçµ±åˆ")
print("  - SHAP + Attentionã§åå¿œãƒ¡ã‚«ãƒ‹ã‚ºãƒ äºˆæ¸¬")
print("  - åŒ–å­¦è€…ã¸ã®èª¬æ˜å¯èƒ½ãªææ¡ˆç”Ÿæˆ")

print("\næŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯:")
print("  - Graph Neural Network (GNN)")
print("  - Attention mechanism")
print("  - SHAP for molecular graphs")

print("\næˆæœ:")
print("  - åå¿œåç‡äºˆæ¸¬ç²¾åº¦ 95%")
print("  - åŒ–å­¦è€…ã®ä¿¡é ¼ç²å¾—")
print("  - æ–°è¦åå¿œçµŒè·¯ã®ç™ºè¦‹")

<h1>åˆ†å­ã‚°ãƒ©ãƒ•ã®é‡è¦åº¦å¯è¦–åŒ–ï¼ˆæ¦‚å¿µå›³ï¼‰</h1>
fig, ax = plt.subplots(figsize=(10, 8))

<h1>ãƒ€ãƒŸãƒ¼ã®åˆ†å­ã‚°ãƒ©ãƒ•</h1>
import networkx as nx

G = nx.Graph()
G.add_edges_from([
    (0, 1), (1, 2), (2, 3), (3, 4), (4, 0),
    (1, 5), (3, 6)
])

pos = nx.spring_layout(G, seed=42)

<h1>ãƒãƒ¼ãƒ‰é‡è¦åº¦ï¼ˆAttention weightsé¢¨ï¼‰</h1>
node_importance = np.random.rand(len(G.nodes))
node_importance = node_importance / node_importance.sum()

nx.draw(
    G, pos,
    node_color=node_importance,
    node_size=1000 * node_importance / node_importance.max(),
    cmap='YlOrRd',
    with_labels=True,
    font_size=12,
    font_weight='bold',
    edge_color='gray',
    width=2,
    ax=ax
)

sm = plt.cm.ScalarMappable(
    cmap='YlOrRd',
    norm=plt.Normalize(vmin=0, vmax=node_importance.max())
)
sm.set_array([])
cbar = plt.colorbar(sm, ax=ax, label='Attention Weight')

ax.set_title('åˆ†å­ã‚°ãƒ©ãƒ•ã®Attentionå¯è¦–åŒ–ï¼ˆIBMé¢¨ï¼‰',
             fontsize=13, fontweight='bold')
plt.tight_layout()
plt.show()</code></pre>

<h3>ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ï¼šCitrine Informaticsï¼ˆèª¬æ˜å¯èƒ½ãªAIï¼‰</h3>

<pre><code class="language-python">print("\n=== Citrine Informatics äº‹ä¾‹ ===")
print("\nãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«:")
print("  - ææ–™é–‹ç™ºãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æä¾›")
print("  - èª¬æ˜å¯èƒ½AIã‚’ä¸­æ ¸æŠ€è¡“ã¨ã™ã‚‹")
print("  - å¤§æ‰‹è£½é€ æ¥­ã¸ã®SaaSå±•é–‹")

print("\næŠ€è¡“çš„ç‰¹å¾´:")
print("  - ãƒ™ã‚¤ã‚ºæœ€é©åŒ– + XAI")
print("  - ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–")
print("  - ç‰©ç†åˆ¶ç´„ã®çµ±åˆ")

print("\né¡§å®¢äº‹ä¾‹:")
print("  - ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯: é›»æ± ææ–™é–‹ç™º 50% é«˜é€ŸåŒ–")
print("  - 3M: æ¥ç€å‰¤æ€§èƒ½ 30% å‘ä¸Š")
print("  - Michelin: ã‚¿ã‚¤ãƒ¤ã‚´ãƒ æœ€é©åŒ–")

print("\nå·®åˆ¥åŒ–è¦å› :")
print("  - èª¬æ˜å¯èƒ½æ€§ã«ã‚ˆã‚‹å°‚é–€å®¶ã®ä¿¡é ¼ç²å¾—")
print("  - ç‰©ç†ãƒ¢ãƒ‡ãƒ«ã¨ã®çµ±åˆ")
print("  - å°ãƒ‡ãƒ¼ã‚¿ã§ã‚‚é«˜ç²¾åº¦")

<h1>Citrineã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰</h1>
<h1>ä¸ç¢ºå®Ÿæ€§ã¤ãäºˆæ¸¬ + SHAP</h1>

from sklearn.ensemble import GradientBoostingRegressor

<h1>ãƒ¢ãƒ‡ãƒ«ï¼ˆåˆ†ä½ç‚¹å›å¸°é¢¨ï¼‰</h1>
model_citrine_lower = GradientBoostingRegressor(
    loss='quantile', alpha=0.1, n_estimators=100, random_state=42
)
model_citrine_median = GradientBoostingRegressor(
    n_estimators=100, random_state=42
)
model_citrine_upper = GradientBoostingRegressor(
    loss='quantile', alpha=0.9, n_estimators=100, random_state=42
)

X_citrine = X_composition
y_citrine = y_capacity

model_citrine_lower.fit(X_citrine, y_citrine)
model_citrine_median.fit(X_citrine, y_citrine)
model_citrine_upper.fit(X_citrine, y_citrine)

<h1>äºˆæ¸¬</h1>
X_new = X_citrine.iloc[:20]
y_pred_lower = model_citrine_lower.predict(X_new)
y_pred_median = model_citrine_median.predict(X_new)
y_pred_upper = model_citrine_upper.predict(X_new)

<h1>å¯è¦–åŒ–</h1>
fig, ax = plt.subplots(figsize=(12, 6))

x_axis = range(len(X_new))

ax.fill_between(x_axis, y_pred_lower, y_pred_upper,
                alpha=0.3, color='steelblue',
                label='80% äºˆæ¸¬åŒºé–“')
ax.plot(x_axis, y_pred_median, 'o-',
        color='steelblue', linewidth=2, label='äºˆæ¸¬ä¸­å¤®å€¤')

ax.set_xlabel('ææ–™ã‚µãƒ³ãƒ—ãƒ«', fontsize=12)
ax.set_ylabel('å®¹é‡ (mAh/g)', fontsize=12)
ax.set_title('Citrineé¢¨ä¸ç¢ºå®Ÿæ€§ã¤ãäºˆæ¸¬',
             fontsize=13, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\nä¸ç¢ºå®Ÿæ€§ã®åˆ©ç‚¹:")
print("  - ãƒªã‚¹ã‚¯è©•ä¾¡")
print("  - è¿½åŠ å®Ÿé¨“ã®å„ªå…ˆé †ä½ã¥ã‘")
print("  - æ„æ€æ±ºå®šã®ä¿¡é ¼æ€§å‘ä¸Š")</code></pre>

<h3>ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹ï¼šææ–™ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã€XAIç ”ç©¶è€…</h3>

<pre><code class="language-python"><h1>ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹æƒ…å ±</h1>
career_paths = pd.DataFrame({
    'ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹': [
        'ææ–™ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆ',
        'XAIç ”ç©¶è€…ï¼ˆã‚¢ã‚«ãƒ‡ãƒŸã‚¢ï¼‰',
        'MLã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ï¼ˆææ–™ç‰¹åŒ–ï¼‰',
        'R&D Managerï¼ˆAIæ´»ç”¨ï¼‰',
        'ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ'
    ],
    'å¿…è¦ã‚¹ã‚­ãƒ«': [
        'ææ–™ç§‘å­¦+ML+Python',
        'çµ±è¨ˆ+MLç†è«–+è«–æ–‡åŸ·ç­†',
        'MLå®Ÿè£…+MLOps',
        'ææ–™ç§‘å­¦+ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†',
        'ææ–™ç§‘å­¦+ML+ãƒ“ã‚¸ãƒã‚¹'
    ],
    'å‹¤å‹™å…ˆä¾‹': [
        'ãƒˆãƒ¨ã‚¿ã€ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯ã€ä¸‰è±ã‚±ãƒŸã‚«ãƒ«',
        'å¤§å­¦ã€ç”£ç·ç ”ã€ç†ç ”',
        'Citrine, Materials Zone',
        'å¤§æ‰‹è£½é€ æ¥­R&Déƒ¨é–€',
        'ã‚¢ã‚¯ã‚»ãƒ³ãƒãƒ¥ã‚¢ã€ãƒ‡ãƒ­ã‚¤ãƒˆ'
    ]
})

print("\n=== ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹ ===")
print(career_paths.to_string(index=False))</code></pre>

<h3>å¹´åï¼š700-1,500ä¸‡å††ï¼ˆæ—¥æœ¬ï¼‰ã€$90-180Kï¼ˆç±³å›½ï¼‰</h3>

<pre><code class="language-python"><h1>å¹´åãƒ‡ãƒ¼ã‚¿</h1>
salary_data = pd.DataFrame({
    'ãƒã‚¸ã‚·ãƒ§ãƒ³': [
        'ã‚¸ãƒ¥ãƒ‹ã‚¢ï¼ˆã€œ3å¹´ï¼‰',
        'ãƒŸãƒ‰ãƒ«ï¼ˆ3-7å¹´ï¼‰',
        'ã‚·ãƒ‹ã‚¢ï¼ˆ7-15å¹´ï¼‰',
        'ãƒªãƒ¼ãƒ‰ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆ',
        'ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼'
    ],
    'æ—¥æœ¬_æœ€ä½': [500, 700, 1000, 1200, 1500],
    'æ—¥æœ¬_æœ€é«˜': [700, 1000, 1500, 2000, 2500],
    'ç±³å›½_æœ€ä½': [70, 90, 130, 150, 180],
    'ç±³å›½_æœ€é«˜': [90, 130, 180, 220, 300]
})

<h1>å¯è¦–åŒ–</h1>
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

<h1>æ—¥æœ¬</h1>
axes[0].barh(salary_data['ãƒã‚¸ã‚·ãƒ§ãƒ³'],
             salary_data['æ—¥æœ¬_æœ€é«˜'] - salary_data['æ—¥æœ¬_æœ€ä½'],
             left=salary_data['æ—¥æœ¬_æœ€ä½'],
             color='steelblue', alpha=0.7)

for idx, row in salary_data.iterrows():
    axes[0].text(row['æ—¥æœ¬_æœ€ä½'] - 50, idx,
                 f"{row['æ—¥æœ¬_æœ€ä½']}", va='center', ha='right', fontsize=9)
    axes[0].text(row['æ—¥æœ¬_æœ€é«˜'] + 50, idx,
                 f"{row['æ—¥æœ¬_æœ€é«˜']}", va='center', ha='left', fontsize=9)

axes[0].set_xlabel('å¹´åï¼ˆä¸‡å††ï¼‰', fontsize=12)
axes[0].set_title('æ—¥æœ¬ã®å¹´åãƒ¬ãƒ³ã‚¸', fontsize=13, fontweight='bold')
axes[0].grid(axis='x', alpha=0.3)

<h1>ç±³å›½</h1>
axes[1].barh(salary_data['ãƒã‚¸ã‚·ãƒ§ãƒ³'],
             salary_data['ç±³å›½_æœ€é«˜'] - salary_data['ç±³å›½_æœ€ä½'],
             left=salary_data['ç±³å›½_æœ€ä½'],
             color='coral', alpha=0.7)

for idx, row in salary_data.iterrows():
    axes[1].text(row['ç±³å›½_æœ€ä½'] - 5, idx,
                 f"${row['ç±³å›½_æœ€ä½']}K", va='center', ha='right', fontsize=9)
    axes[1].text(row['ç±³å›½_æœ€é«˜'] + 5, idx,
                 f"${row['ç±³å›½_æœ€é«˜']}K", va='center', ha='left', fontsize=9)

axes[1].set_xlabel('å¹´åï¼ˆåƒãƒ‰ãƒ«ï¼‰', fontsize=12)
axes[1].set_title('ç±³å›½ã®å¹´åãƒ¬ãƒ³ã‚¸', fontsize=13, fontweight='bold')
axes[1].grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.show()

print("\nå¹´åã«å½±éŸ¿ã™ã‚‹è¦å› :")
print("  - å­¦ä½ï¼ˆä¿®å£« vs åšå£«ï¼‰")
print("  - æ¥­ç•Œï¼ˆè£½é€ æ¥­ vs ITï¼‰")
print("  - åœ°åŸŸï¼ˆæ±äº¬ vs åœ°æ–¹ã€ã‚·ãƒªã‚³ãƒ³ãƒãƒ¬ãƒ¼ vs ãã®ä»–ï¼‰")
print("  - ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆï¼ˆææ–™ç§‘å­¦ + ML + ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ï¼‰")
print("  - å®Ÿç¸¾ï¼ˆè«–æ–‡ã€ç‰¹è¨±ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæˆåŠŸï¼‰")

print("\nã‚¹ã‚­ãƒ«ã‚¢ãƒƒãƒ—æˆ¦ç•¥:")
print("  1. ææ–™ç§‘å­¦ã®åŸºç¤å›ºã‚ï¼ˆå­¦ä½å–å¾—ï¼‰")
print("  2. ML/DLã®å®Ÿè·µã‚¹ã‚­ãƒ«ï¼ˆKaggleã€GitHubï¼‰")
print("  3. XAIæ‰‹æ³•ã®ç¿’å¾—ï¼ˆSHAPã€LIMEï¼‰")
print("  4. è«–æ–‡ç™ºè¡¨ãƒ»OSSã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³")
print("  5. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ï¼ˆå­¦ä¼šã€å‹‰å¼·ä¼šï¼‰")</code></pre>

---

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦: easyï¼‰</h3>

SHAPã¨LIMEã‚’ç”¨ã„ã¦ã€åŒä¸€ã‚µãƒ³ãƒ—ãƒ«ã®èª¬æ˜ã‚’ç”Ÿæˆã—ã€ç‰¹å¾´é‡é‡è¦åº¦ã®ç›¸é–¢ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚ç›¸é–¢ãŒé«˜ã„å ´åˆã¨ä½ã„å ´åˆã€ãã‚Œãã‚Œä½•ã‚’æ„å‘³ã™ã‚‹ã‹è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import shap
from lime import lime_tabular
from sklearn.ensemble import RandomForestRegressor
import numpy as np

<h1>ãƒ¢ãƒ‡ãƒ«è¨“ç·´</h1>
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

<h1>SHAP</h1>
explainer_shap = shap.TreeExplainer(model)
shap_values = explainer_shap.shap_values(X_test)

<h1>LIME</h1>
explainer_lime = lime_tabular.LimeTabularExplainer(
    X_train, mode='regression'
)

sample_idx = 0

<h1>LIMEèª¬æ˜</h1>
lime_exp = explainer_lime.explain_instance(
    X_test[sample_idx], model.predict, num_features=X_train.shape[1]
)
lime_dict = dict(lime_exp.as_list())

<h1>ç›¸é–¢è¨ˆç®—</h1>
shap_importances = shap_values[sample_idx]
lime_importances = [lime_dict.get(f'Feature_{i}', 0)
                    for i in range(len(shap_importances))]

correlation = np.corrcoef(shap_importances, lime_importances)[0, 1]
print(f"SHAP-LIMEç›¸é–¢: {correlation:.4f}")

if correlation > 0.7:
    print("é«˜ç›¸é–¢: ä¸¡æ‰‹æ³•ã§ä¸€è²«ã—ãŸèª¬æ˜ â†’ ä¿¡é ¼æ€§é«˜ã„")
else:
    print("ä½ç›¸é–¢: èª¬æ˜ã®ä¸ä¸€è‡´ â†’ æ…é‡ã«è§£é‡ˆãŒå¿…è¦")</code></pre>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦: mediumï¼‰</h3>

SHAP Dependence Plotã‚’ç”¨ã„ã¦ã€2ã¤ã®ç‰¹å¾´é‡é–“ã®ç›¸äº’ä½œç”¨ã‚’å¯è¦–åŒ–ã—ã¦ãã ã•ã„ã€‚éç·šå½¢ãªé–¢ä¿‚ã‚„ç›¸äº’ä½œç”¨ãŒè¦‹ã‚‰ã‚Œã‚‹ã‹åˆ†æã—ã¦ãã ã•ã„ã€‚

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import shap
import matplotlib.pyplot as plt

<h1>SHAPè¨ˆç®—</h1>
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

<h1>Dependence Plotï¼ˆç‰¹å¾´é‡0ã¨ç‰¹å¾´é‡1ã®ç›¸äº’ä½œç”¨ï¼‰</h1>
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

shap.dependence_plot(0, shap_values, X_test, interaction_index=1,
                     ax=axes[0], show=False)
axes[0].set_title('Feature 0 (interaction with Feature 1)')

shap.dependence_plot(1, shap_values, X_test, interaction_index=0,
                     ax=axes[1], show=False)
axes[1].set_title('Feature 1 (interaction with Feature 0)')

plt.tight_layout()
plt.show()

print("åˆ†æãƒã‚¤ãƒ³ãƒˆ:")
print("- è‰²ã®å¤‰åŒ–: ç›¸äº’ä½œç”¨ã®å¼·ã•")
print("- éç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³: è¤‡é›‘ãªé–¢ä¿‚æ€§")
print("- å‚¾å‘: æ­£/è² ã®å½±éŸ¿")</code></pre>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦: hardï¼‰</h3>

ãƒˆãƒ¨ã‚¿ã®é›»æ± åŠ£åŒ–äºˆæ¸¬äº‹ä¾‹ã‚’æ¨¡å€£ã—ã€æ¸©åº¦ãƒ»é›»åœ§ãƒ»ã‚µã‚¤ã‚¯ãƒ«æ•°ã®3è¦å› ã§SHAPåˆ†æã‚’è¡Œã„ã€ã©ã®è¦å› ãŒæœ€ã‚‚åŠ£åŒ–ã«å¯„ä¸ã™ã‚‹ã‹å®šé‡è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€ç‰©ç†çš„ã«å¦¥å½“ã‹ã©ã†ã‹è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
import shap

<h1>ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ</h1>
battery_data = pd.DataFrame({
    'æ¸©åº¦': np.random.uniform(20, 60, 300),
    'é›»åœ§': np.random.uniform(3.0, 4.5, 300),
    'ã‚µã‚¤ã‚¯ãƒ«æ•°': np.random.uniform(0, 1000, 300)
})

<h1>åŠ£åŒ–ç‡ï¼ˆç‰©ç†çš„ã«å¦¥å½“ãªãƒ¢ãƒ‡ãƒ«ï¼‰</h1>
<h1>é«˜æ¸©ã€é«˜é›»åœ§ã€å¤šã‚µã‚¤ã‚¯ãƒ«ã§åŠ£åŒ–åŠ é€Ÿ</h1>
degradation = (
    0.8 * (battery_data['æ¸©åº¦'] - 20) +  # é«˜æ¸©ã§åŠ£åŒ–
    2.0 * (battery_data['é›»åœ§'] - 3.0)**2 +  # é«˜é›»åœ§ã§åŠ£åŒ–
    0.05 * battery_data['ã‚µã‚¤ã‚¯ãƒ«æ•°'] +  # ã‚µã‚¤ã‚¯ãƒ«åŠ£åŒ–
    0.01 * battery_data['æ¸©åº¦'] * battery_data['ã‚µã‚¤ã‚¯ãƒ«æ•°'] / 100 +  # ç›¸äº’ä½œç”¨
    np.random.normal(0, 3, 300)
)

<h1>ãƒ¢ãƒ‡ãƒ«è¨“ç·´</h1>
model = GradientBoostingRegressor(n_estimators=100, random_state=42)
model.fit(battery_data, degradation)

<h1>SHAPåˆ†æ</h1>
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(battery_data)

<h1>é‡è¦åº¦é›†è¨ˆ</h1>
mean_abs_shap = np.abs(shap_values).mean(axis=0)
feature_names = battery_data.columns

print("åŠ£åŒ–è¦å› ã®é‡è¦åº¦ï¼ˆSHAPï¼‰:")
for name, importance in zip(feature_names, mean_abs_shap):
    print(f"  {name}: {importance:.2f}")

<h1>Summary Plot</h1>
shap.summary_plot(shap_values, battery_data, show=False)
plt.title('é›»æ± åŠ£åŒ–è¦å› ã®SHAPåˆ†æ', fontsize=13, fontweight='bold')
plt.tight_layout()
plt.show()

print("\nç‰©ç†çš„å¦¥å½“æ€§:")
print("- æ¸©åº¦: ã‚¢ãƒ¬ãƒ‹ã‚¦ã‚¹å‰‡ã«ã‚ˆã‚Šé«˜æ¸©ã§åå¿œé€Ÿåº¦ä¸Šæ˜‡ â†’ å¦¥å½“")
print("- é›»åœ§: é«˜é›»åœ§ã§å‰¯åå¿œä¿ƒé€² â†’ å¦¥å½“")
print("- ã‚µã‚¤ã‚¯ãƒ«æ•°: å……æ”¾é›»ç¹°ã‚Šè¿”ã—ã§åŠ£åŒ– â†’ å¦¥å½“")</code></pre>

</details>

---

<h2>ã¾ã¨ã‚</h2>

ã“ã®ç« ã§ã¯ã€<strong>è§£é‡ˆå¯èƒ½AIï¼ˆXAIï¼‰</strong> ã®ç†è«–ã¨å®Ÿè·µã‚’å­¦ã³ã¾ã—ãŸã€‚

<strong>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</strong>ï¼š

1. <strong>ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹å•é¡Œ</strong>ï¼šé«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ã¯è§£é‡ˆå›°é›£ â†’ XAIã§è§£æ±º
2. <strong>SHAP</strong>ï¼šShapleyå€¤ã«ã‚ˆã‚‹å…¬å¹³ãªç‰¹å¾´é‡å¯„ä¸åº¦è©•ä¾¡
3. <strong>LIME</strong>ï¼šå±€æ‰€ç·šå½¢è¿‘ä¼¼ã§å€‹åˆ¥äºˆæ¸¬ã®èª¬æ˜ç”Ÿæˆ
4. <strong>Attentionå¯è¦–åŒ–</strong>ï¼šãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å†…éƒ¨å‹•ä½œç†è§£
5. <strong>å®Ÿä¸–ç•Œå¿œç”¨</strong>ï¼šãƒˆãƒ¨ã‚¿ã€IBMã€Citrineã®æˆåŠŸäº‹ä¾‹
6. <strong>ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹</strong>ï¼šææ–™ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã®éœ€è¦æ‹¡å¤§ã€å¹´å700-2500ä¸‡å††

<strong>ã‚·ãƒªãƒ¼ã‚ºç·ã¾ã¨ã‚</strong>ï¼š

- <strong>Chapter 1</strong>: ãƒ‡ãƒ¼ã‚¿åé›†æˆ¦ç•¥ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° â†’ é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
- <strong>Chapter 2</strong>: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° â†’ 200æ¬¡å…ƒâ†’20æ¬¡å…ƒã¸ã®åŠ¹ç‡åŒ–
- <strong>Chapter 3</strong>: ãƒ¢ãƒ‡ãƒ«é¸æŠã¨æœ€é©åŒ– â†’ Optunaã§è‡ªå‹•æœ€é©åŒ–
- <strong>Chapter 4</strong>: è§£é‡ˆå¯èƒ½AI â†’ äºˆæ¸¬ã®ç‰©ç†çš„æ„å‘³ã¥ã‘

<strong>æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—</strong>ï¼š

1. å®Ÿãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å…¨å·¥ç¨‹ã‚’å®Ÿè·µ
2. è«–æ–‡æŠ•ç¨¿ã‚„OSSã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³
3. å­¦ä¼šå‚åŠ ã¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚­ãƒ³ã‚°
4. ã‚­ãƒ£ãƒªã‚¢æ§‹ç¯‰ï¼ˆææ–™ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆï¼‰

---

<h2>å‚è€ƒæ–‡çŒ®</h2>

1. <strong>Lundberg, S. M. & Lee, S. I.</strong> (2017). A unified approach to interpreting model predictions. *Advances in Neural Information Processing Systems*, 30, 4765-4774.

2. <strong>Ribeiro, M. T., Singh, S., & Guestrin, C.</strong> (2016). "Why should I trust you?": Explaining the predictions of any classifier. *Proceedings of the 22nd ACM SIGKDD*, 1135-1144. [DOI: 10.1145/2939672.2939778](https://doi.org/10.1145/2939672.2939778)

3. <strong>Molnar, C.</strong> (2022). *Interpretable Machine Learning: A Guide for Making Black Box Models Explainable* (2nd ed.). [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)

4. <strong>Vaswani, A., Shazeer, N., Parmar, N., et al.</strong> (2017). Attention is all you need. *Advances in Neural Information Processing Systems*, 30, 5998-6008.

5. <strong>Citrine Informatics.</strong> (2023). Materials Informatics Platform. [https://citrine.io/](https://citrine.io/)

---

[â† Chapter 3ã«æˆ»ã‚‹](chapter-3.md) | [ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹](index.md)

---

<h2>ã‚·ãƒªãƒ¼ã‚ºå®Œäº†ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼</h2>

ãƒ‡ãƒ¼ã‚¿é§†å‹•ææ–™ç§‘å­¦ã®å®Ÿè·µçš„ã‚¹ã‚­ãƒ«ã‚’ç¿’å¾—ã•ã‚Œã¾ã—ãŸã€‚ä»Šå¾Œã®ã”æ´»èºã‚’æœŸå¾…ã—ã¦ã„ã¾ã™ã€‚

<strong>ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ»è³ªå•</strong>:
- Email: yusuke.hashimoto.b8@tohoku.ac.jp
- GitHub: [AI_Homepage Repository](https://github.com/YusukeHashimotoPhD/AI_Homepage)

<strong>é–¢é€£ã‚·ãƒªãƒ¼ã‚º</strong>:
- [ãƒ™ã‚¤ã‚ºæœ€é©åŒ–å…¥é–€](../bayesian-optimization-introduction/)
- [Active Learningå…¥é–€](../active-learning-introduction/)
- [ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å…¥é–€](../gnn-introduction/)
<div class="navigation">
    <a href="chapter-3.html" class="nav-button">â† ç¬¬3ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
</div>
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>
