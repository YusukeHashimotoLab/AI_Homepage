<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Chapter</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 20-25ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ÂàùÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 0ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 0Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 3: „É¢„Éá„É´ÈÅ∏Êäû„Å®„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊúÄÈÅ©Âåñ</h1>

---

<h2>Â≠¶ÁøíÁõÆÊ®ô</h2>

„Åì„ÅÆÁ´†„ÇíË™≠„ÇÄ„Åì„Å®„Åß„ÄÅ‰ª•‰∏ã„ÇíÁøíÂæó„Åß„Åç„Åæ„ÅôÔºö

‚úÖ „Éá„Éº„Çø„Çµ„Ç§„Ç∫„Å´Âøú„Åò„ÅüÈÅ©Âàá„Å™„É¢„Éá„É´ÈÅ∏ÊäûÔºàÁ∑öÂΩ¢„ÄÅÊú®„Éô„Éº„Çπ„ÄÅNN„ÄÅGNNÔºâ
‚úÖ ‰∫§Â∑ÆÊ§úË®ºÔºàK-Fold„ÄÅStratified„ÄÅTime Series SplitÔºâ„ÅÆÂÆüË∑µ
‚úÖ Optuna„Å´„Çà„Çã„Éô„Ç§„Ç∫ÊúÄÈÅ©Âåñ„ÇíÁî®„ÅÑ„Åü„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøËá™ÂãïÊúÄÈÅ©Âåñ
‚úÖ „Ç¢„É≥„Çµ„É≥„Éñ„É´Â≠¶ÁøíÔºàBagging„ÄÅBoosting„ÄÅStackingÔºâ„ÅÆÂÆüË£Ö
‚úÖ Li-ionÈõªÊ±†ÂÆπÈáè‰∫àÊ∏¨„Å´„Åä„Åë„ÇãÂÆüË∑µÁöÑ„ÉØ„Éº„ÇØ„Éï„É≠„Éº

---

<h2>3.1 „É¢„Éá„É´ÈÅ∏Êäû„ÅÆÊà¶Áï•</h2>

ÊùêÊñôÁßëÂ≠¶„Å´„Åä„Åë„ÇãÊ©üÊ¢∞Â≠¶Áøí„Åß„ÅØ„ÄÅ„Éá„Éº„Çø„ÅÆÁâπÊÄß„Å´Âøú„Åò„ÅüÈÅ©Âàá„Å™„É¢„Éá„É´ÈÅ∏Êäû„ÅåÈáçË¶Å„Åß„Åô„ÄÇ

<h3>„Éá„Éº„Çø„Çµ„Ç§„Ç∫„Å®„É¢„Éá„É´Ë§áÈõëÂ∫¶</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import learning_curve
from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor

<h1>„Çµ„É≥„Éó„É´„Éá„Éº„ÇøÁîüÊàê</h1>
np.random.seed(42)

def generate_material_data(n_samples, n_features=20):
    """ÊùêÊñô„Éá„Éº„Çø„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥"""
    X = np.random.randn(n_samples, n_features)
    # ÈùûÁ∑öÂΩ¢Èñ¢‰øÇ
    y = (
        2 * X[:, 0]**2 +
        3 * X[:, 1] * X[:, 2] -
        1.5 * X[:, 3] +
        np.random.normal(0, 0.5, n_samples)
    )
    return X, y

<h1>„É¢„Éá„É´Ë§áÈõëÂ∫¶„Å®„Çµ„É≥„Éó„É´„Çµ„Ç§„Ç∫„ÅÆÈñ¢‰øÇ</h1>
sample_sizes = [50, 100, 200, 500, 1000]
models = {
    'Ridge': Ridge(),
    'Random Forest': RandomForestRegressor(n_estimators=50, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=50, random_state=42),
    'Neural Network': MLPRegressor(hidden_layers=(50, 50), max_iter=1000, random_state=42)
}

<h1>Â≠¶ÁøíÊõ≤Á∑ö</h1>
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for idx, (model_name, model) in enumerate(models.items()):
    X, y = generate_material_data(1000, n_features=20)

    train_sizes, train_scores, val_scores = learning_curve(
        model, X, y,
        train_sizes=np.linspace(0.1, 1.0, 10),
        cv=5,
        scoring='neg_mean_squared_error',
        n_jobs=-1
    )

    train_mean = -train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    val_mean = -val_scores.mean(axis=1)
    val_std = val_scores.std(axis=1)

    axes[idx].plot(train_sizes, train_mean, 'o-',
                   color='steelblue', label='Training Error')
    axes[idx].fill_between(train_sizes,
                           train_mean - train_std,
                           train_mean + train_std,
                           alpha=0.2, color='steelblue')

    axes[idx].plot(train_sizes, val_mean, 'o-',
                   color='coral', label='Validation Error')
    axes[idx].fill_between(train_sizes,
                           val_mean - val_std,
                           val_mean + val_std,
                           alpha=0.2, color='coral')

    axes[idx].set_xlabel('Training Size', fontsize=11)
    axes[idx].set_ylabel('MSE', fontsize=11)
    axes[idx].set_title(f'{model_name}', fontsize=12, fontweight='bold')
    axes[idx].legend(loc='upper right')
    axes[idx].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("„É¢„Éá„É´ÈÅ∏Êäû„Ç¨„Ç§„Éâ„É©„Ç§„É≥Ôºö")
print("- Â∞è„Éá„Éº„Çø (<100): Ridge, LassoÔºàÊ≠£ÂâáÂåñÁ∑öÂΩ¢„É¢„Éá„É´Ôºâ")
print("- ‰∏≠„Éá„Éº„Çø (100-1000): Random Forest, Gradient Boosting")
print("- Â§ß„Éá„Éº„Çø (>1000): Neural Network, Deep Learning")</code></pre>

<h3>Ëß£ÈáàÊÄß vs Á≤æÂ∫¶„ÅÆ„Éà„É¨„Éº„Éâ„Ç™„Éï</h3>

<pre><code class="language-python"><h1>„É¢„Éá„É´„ÅÆËß£ÈáàÊÄß„Å®Á≤æÂ∫¶„ÅÆÊØîËºÉ</h1>
model_comparison = pd.DataFrame({
    '„É¢„Éá„É´': [
        'Linear Regression',
        'Ridge/Lasso',
        'Decision Tree',
        'Random Forest',
        'Gradient Boosting',
        'Neural Network',
        'GNN'
    ],
    'Ëß£ÈáàÊÄß': [10, 9, 7, 4, 3, 2, 1],
    'Á≤æÂ∫¶': [4, 5, 5, 8, 9, 9, 10],
    'Ë®ìÁ∑¥ÈÄüÂ∫¶': [10, 9, 8, 6, 5, 3, 2],
    'Êé®Ë´ñÈÄüÂ∫¶': [10, 10, 9, 7, 6, 8, 4]
})

<h1>„É¨„Éº„ÉÄ„Éº„ÉÅ„É£„Éº„Éà</h1>
from math import pi

categories = ['Ëß£ÈáàÊÄß', 'Á≤æÂ∫¶', 'Ë®ìÁ∑¥ÈÄüÂ∫¶', 'Êé®Ë´ñÈÄüÂ∫¶']
N = len(categories)

angles = [n / float(N) * 2 * pi for n in range(N)]
angles += angles[:1]

fig, axes = plt.subplots(2, 4, figsize=(18, 10),
                         subplot_kw=dict(projection='polar'))
axes = axes.flatten()

for idx, row in model_comparison.iterrows():
    values = row[categories].tolist()
    values += values[:1]

    axes[idx].plot(angles, values, 'o-', linewidth=2)
    axes[idx].fill(angles, values, alpha=0.25)
    axes[idx].set_xticks(angles[:-1])
    axes[idx].set_xticklabels(categories, size=9)
    axes[idx].set_ylim(0, 10)
    axes[idx].set_title(row['„É¢„Éá„É´'], size=11, fontweight='bold', pad=20)
    axes[idx].grid(True)

plt.tight_layout()
plt.show()

print("\n„É¢„Éá„É´ÈÅ∏Êäû„ÅÆÂà§Êñ≠Âü∫Ê∫ñÔºö")
print("- Ëß£ÈáàÊÄßÈáçË¶ñ: Ridge, Lasso, Decision Tree")
print("- Á≤æÂ∫¶ÈáçË¶ñ: Gradient Boosting, Neural Network, GNN")
print("- „Éê„É©„É≥„ÇπÂûã: Random Forest")</code></pre>

<h3>Á∑öÂΩ¢„É¢„Éá„É´„ÄÅÊú®„Éô„Éº„Çπ„ÄÅNN„ÄÅGNN„ÅÆ‰Ωø„ÅÑÂàÜ„Åë</h3>

<pre><code class="language-python"><h1>ÂÆü„Éá„Éº„Çø„Åß„ÅÆÊÄßËÉΩÊØîËºÉ</h1>
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_absolute_error, r2_score

X, y = generate_material_data(500, n_features=20)

models_benchmark = {
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=0.1),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'MLP': MLPRegressor(hidden_layers=(100, 50), max_iter=1000, random_state=42)
}

results = []

for model_name, model in models_benchmark.items():
    # ‰∫§Â∑ÆÊ§úË®º
    cv_scores = cross_val_score(model, X, y, cv=5,
                                scoring='neg_mean_absolute_error')
    mae = -cv_scores.mean()
    mae_std = cv_scores.std()

    # R¬≤
    cv_r2 = cross_val_score(model, X, y, cv=5, scoring='r2')
    r2 = cv_r2.mean()

    results.append({
        'Model': model_name,
        'MAE': mae,
        'MAE_std': mae_std,
        'R¬≤': r2
    })

results_df = pd.DataFrame(results)
print("\n„É¢„Éá„É´ÊÄßËÉΩÊØîËºÉÔºö")
print(results_df.to_string(index=False))

<h1>ÂèØË¶ñÂåñ</h1>
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

<h1>MAE</h1>
axes[0].barh(results_df['Model'], results_df['MAE'],
             xerr=results_df['MAE_std'],
             color='steelblue', alpha=0.7)
axes[0].set_xlabel('MAE (lower is better)', fontsize=11)
axes[0].set_title('‰∫àÊ∏¨Ë™§Â∑ÆÔºàMAEÔºâ', fontsize=12, fontweight='bold')
axes[0].grid(axis='x', alpha=0.3)

<h1>R¬≤</h1>
axes[1].barh(results_df['Model'], results_df['R¬≤'],
             color='coral', alpha=0.7)
axes[1].set_xlabel('R¬≤ (higher is better)', fontsize=11)
axes[1].set_title('Ê±∫ÂÆö‰øÇÊï∞ÔºàR¬≤Ôºâ', fontsize=12, fontweight='bold')
axes[1].set_xlim(0, 1)
axes[1].grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.show()</code></pre>

---

<h2>3.2 ‰∫§Â∑ÆÊ§úË®ºÔºàCross-ValidationÔºâ</h2>

„É¢„Éá„É´„ÅÆÊ±éÂåñÊÄßËÉΩ„ÇíÈÅ©Âàá„Å´Ë©ï‰æ°„Åô„Çã„Åü„ÇÅ„ÅÆÊâãÊ≥ï„Åß„Åô„ÄÇ

<h3>K-Fold CV</h3>

<pre><code class="language-python">from sklearn.model_selection import KFold, cross_validate

def kfold_cv_demo(X, y, model, k=5):
    """
    K-Fold‰∫§Â∑ÆÊ§úË®º„ÅÆ„Éá„É¢
    """
    kf = KFold(n_splits=k, shuffle=True, random_state=42)

    fold_results = []

    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X)):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)

        mae = mean_absolute_error(y_val, y_pred)
        r2 = r2_score(y_val, y_pred)

        fold_results.append({
            'Fold': fold_idx + 1,
            'MAE': mae,
            'R¬≤': r2
        })

    return pd.DataFrame(fold_results)

<h1>K-FoldÂÆüË°å</h1>
model = RandomForestRegressor(n_estimators=100, random_state=42)
fold_results = kfold_cv_demo(X, y, model, k=5)

print("K-Fold CVÁµêÊûúÔºö")
print(fold_results.to_string(index=False))
print(f"\nÂπ≥ÂùáMAE: {fold_results['MAE'].mean():.4f} ¬± {fold_results['MAE'].std():.4f}")
print(f"Âπ≥ÂùáR¬≤: {fold_results['R¬≤'].mean():.4f} ¬± {fold_results['R¬≤'].std():.4f}")

<h1>ÂèØË¶ñÂåñ</h1>
fig, ax = plt.subplots(figsize=(10, 6))
x_pos = np.arange(len(fold_results))

ax.bar(x_pos, fold_results['MAE'], color='steelblue', alpha=0.7,
       label='MAE per fold')
ax.axhline(y=fold_results['MAE'].mean(), color='red',
           linestyle='--', linewidth=2, label='Mean MAE')
ax.fill_between(x_pos,
                fold_results['MAE'].mean() - fold_results['MAE'].std(),
                fold_results['MAE'].mean() + fold_results['MAE'].std(),
                color='red', alpha=0.2, label='¬±1 Std')

ax.set_xlabel('Fold', fontsize=12)
ax.set_ylabel('MAE', fontsize=12)
ax.set_title('K-Fold‰∫§Â∑ÆÊ§úË®ºÁµêÊûú', fontsize=13, fontweight='bold')
ax.set_xticks(x_pos)
ax.set_xticklabels(fold_results['Fold'])
ax.legend()
ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()</code></pre>

<h3>Stratified K-Fold</h3>

<pre><code class="language-python">from sklearn.model_selection import StratifiedKFold

<h1>ÂàÜÈ°ûÂïèÈ°åÁî®„Éá„Éº„Çø</h1>
X_class, _ = generate_material_data(500, n_features=20)
<h1>3„ÇØ„É©„ÇπÂàÜÈ°û</h1>
y_class = np.digitize(y, bins=np.percentile(y, [33, 67]))

def compare_kfold_strategies(X, y):
    """
    ÈÄöÂ∏∏K-Fold vs Stratified K-Fold
    """
    # ÈÄöÂ∏∏K-Fold
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    normal_distributions = []

    for train_idx, _ in kf.split(X):
        y_train = y[train_idx]
        class_dist = np.bincount(y_train) / len(y_train)
        normal_distributions.append(class_dist)

    # Stratified K-Fold
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    stratified_distributions = []

    for train_idx, _ in skf.split(X, y):
        y_train = y[train_idx]
        class_dist = np.bincount(y_train) / len(y_train)
        stratified_distributions.append(class_dist)

    return normal_distributions, stratified_distributions

normal_dist, stratified_dist = compare_kfold_strategies(X_class, y_class)

<h1>ÂèØË¶ñÂåñ</h1>
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

<h1>ÈÄöÂ∏∏K-Fold</h1>
normal_array = np.array(normal_dist)
axes[0].bar(range(len(normal_array)), normal_array[:, 0],
            label='Class 0', alpha=0.7)
axes[0].bar(range(len(normal_array)), normal_array[:, 1],
            bottom=normal_array[:, 0],
            label='Class 1', alpha=0.7)
axes[0].bar(range(len(normal_array)), normal_array[:, 2],
            bottom=normal_array[:, 0] + normal_array[:, 1],
            label='Class 2', alpha=0.7)
axes[0].set_xlabel('Fold', fontsize=11)
axes[0].set_ylabel('Class Distribution', fontsize=11)
axes[0].set_title('ÈÄöÂ∏∏K-Fold', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].set_ylim(0, 1)

<h1>Stratified K-Fold</h1>
stratified_array = np.array(stratified_dist)
axes[1].bar(range(len(stratified_array)), stratified_array[:, 0],
            label='Class 0', alpha=0.7)
axes[1].bar(range(len(stratified_array)), stratified_array[:, 1],
            bottom=stratified_array[:, 0],
            label='Class 1', alpha=0.7)
axes[1].bar(range(len(stratified_array)), stratified_array[:, 2],
            bottom=stratified_array[:, 0] + stratified_array[:, 1],
            label='Class 2', alpha=0.7)
axes[1].set_xlabel('Fold', fontsize=11)
axes[1].set_ylabel('Class Distribution', fontsize=11)
axes[1].set_title('Stratified K-Fold', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].set_ylim(0, 1)

plt.tight_layout()
plt.show()

print("Stratified K-Fold„ÅÆÂà©ÁÇπÔºö")
print("- ÂêÑFold„Åß„ÇØ„É©„ÇπÂàÜÂ∏É„ÅåÂùá‰∏Ä")
print("- ‰∏çÂùáË°°„Éá„Éº„Çø„Åß„ÇÇÂÆâÂÆö„Åó„ÅüË©ï‰æ°")</code></pre>

<h3>Time Series SplitÔºàÈÄêÊ¨°„Éá„Éº„ÇøÁî®Ôºâ</h3>

<pre><code class="language-python">from sklearn.model_selection import TimeSeriesSplit

<h1>ÊôÇÁ≥ªÂàó„Éá„Éº„Çø„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥</h1>
n_time_points = 200
time = np.arange(n_time_points)
<h1>„Éà„É¨„É≥„Éâ + Â≠£ÁØÄÊÄß + „Éé„Ç§„Ç∫</h1>
y_timeseries = (
    0.05 * time +
    10 * np.sin(2 * np.pi * time / 50) +
    np.random.normal(0, 2, n_time_points)
)
X_timeseries = np.column_stack([time, np.sin(2 * np.pi * time / 50)])

<h1>Time Series Split</h1>
tscv = TimeSeriesSplit(n_splits=5)

<h1>ÂèØË¶ñÂåñ</h1>
fig, ax = plt.subplots(figsize=(14, 6))

colors = plt.cm.viridis(np.linspace(0, 1, 5))

for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(X_timeseries)):
    # Train
    ax.scatter(time[train_idx], y_timeseries[train_idx],
               c=[colors[fold_idx]], s=20, alpha=0.3,
               label=f'Fold {fold_idx+1} Train')
    # Test
    ax.scatter(time[test_idx], y_timeseries[test_idx],
               c=[colors[fold_idx]], s=50, marker='s',
               label=f'Fold {fold_idx+1} Test')

ax.plot(time, y_timeseries, 'k-', alpha=0.3, linewidth=1)
ax.set_xlabel('Time', fontsize=12)
ax.set_ylabel('Value', fontsize=12)
ax.set_title('Time Series Split', fontsize=13, fontweight='bold')
ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("Time Series Split„ÅÆÁâπÂæ¥Ôºö")
print("- Ë®ìÁ∑¥„Éá„Éº„Çø„ÅØÂ∏∏„Å´„ÉÜ„Çπ„Éà„Éá„Éº„Çø„Çà„ÇäÂâç")
print("- Êú™Êù•„ÅÆ„Éá„Éº„Çø„Åß„É¢„Éá„É´„ÇíË®ìÁ∑¥„Åó„Å™„ÅÑÔºà„Éá„Éº„Çø„É™„Éº„ÇØÈò≤Ê≠¢Ôºâ")</code></pre>

<h3>Leave-One-Out CVÔºàÂ∞èË¶èÊ®°„Éá„Éº„ÇøÔºâ</h3>

<pre><code class="language-python">from sklearn.model_selection import LeaveOneOut

def loo_cv_demo(X, y, model):
    """
    Leave-One-Out CV
    Â∞èË¶èÊ®°„Éá„Éº„ÇøÁî®
    """
    loo = LeaveOneOut()
    predictions = []
    actuals = []

    for train_idx, test_idx in loo.split(X):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        predictions.append(y_pred[0])
        actuals.append(y_test[0])

    return np.array(actuals), np.array(predictions)

<h1>Â∞èË¶èÊ®°„Éá„Éº„Çø</h1>
X_small, y_small = generate_material_data(50, n_features=10)
model_small = Ridge(alpha=1.0)

y_actual, y_pred_loo = loo_cv_demo(X_small, y_small, model_small)

mae_loo = mean_absolute_error(y_actual, y_pred_loo)
r2_loo = r2_score(y_actual, y_pred_loo)

print(f"LOO CVÁµêÊûú (n={len(X_small)}):")
print(f"MAE: {mae_loo:.4f}")
print(f"R¬≤: {r2_loo:.4f}")

<h1>‰∫àÊ∏¨ vs ÂÆüÊ∏¨</h1>
plt.figure(figsize=(8, 8))
plt.scatter(y_actual, y_pred_loo, c='steelblue', s=50, alpha=0.6)
plt.plot([y_actual.min(), y_actual.max()],
         [y_actual.min(), y_actual.max()],
         'r--', linewidth=2, label='Perfect Prediction')
plt.xlabel('Actual', fontsize=12)
plt.ylabel('Predicted', fontsize=12)
plt.title('Leave-One-Out CV Results', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()</code></pre>

---

<h2>3.3 „Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊúÄÈÅ©Âåñ</h2>

„É¢„Éá„É´„ÅÆÊÄßËÉΩ„ÇíÊúÄÂ§ßÂåñ„Åô„Çã„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÇíÊé¢Á¥¢„Åó„Åæ„Åô„ÄÇ

<h3>Grid SearchÔºàÂÖ®Êé¢Á¥¢Ôºâ</h3>

<pre><code class="language-python">from sklearn.model_selection import GridSearchCV

def grid_search_demo(X, y):
    """
    Grid Search„Å´„Çà„ÇãÂÖ®Êé¢Á¥¢
    """
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, 15, None],
        'min_samples_split': [2, 5, 10]
    }

    model = RandomForestRegressor(random_state=42)

    grid_search = GridSearchCV(
        model,
        param_grid,
        cv=5,
        scoring='neg_mean_absolute_error',
        n_jobs=-1,
        verbose=1
    )

    grid_search.fit(X, y)

    return grid_search

<h1>ÂÆüË°å</h1>
grid_result = grid_search_demo(X, y)

print("Grid SearchÁµêÊûúÔºö")
print(f"ÊúÄÈÅ©„Éë„É©„É°„Éº„Çø: {grid_result.best_params_}")
print(f"ÊúÄËâØ„Çπ„Ç≥„Ç¢ (MAE): {-grid_result.best_score_:.4f}")
print(f"\nÊé¢Á¥¢Á©∫Èñì„Çµ„Ç§„Ç∫: {len(grid_result.cv_results_['params'])}")

<h1>ÁµêÊûúÂèØË¶ñÂåñÔºà2Ê¨°ÂÖÉ„Éí„Éº„Éà„Éû„ÉÉ„ÉóÔºâ</h1>
results = pd.DataFrame(grid_result.cv_results_)

<h1>n_estimators vs max_depth</h1>
pivot_table = results.pivot_table(
    values='mean_test_score',
    index='param_max_depth',
    columns='param_n_estimators',
    aggfunc='mean'
)

plt.figure(figsize=(10, 8))
sns.heatmap(-pivot_table, annot=True, fmt='.3f',
            cmap='YlOrRd', cbar_kws={'label': 'MAE'})
plt.xlabel('n_estimators', fontsize=12)
plt.ylabel('max_depth', fontsize=12)
plt.title('Grid SearchÁµêÊûúÔºàMAEÔºâ', fontsize=13, fontweight='bold')
plt.tight_layout()
plt.show()</code></pre>

<h3>Random SearchÔºà„É©„É≥„ÉÄ„É†Êé¢Á¥¢Ôºâ</h3>

<pre><code class="language-python">from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

def random_search_demo(X, y, n_iter=50):
    """
    Random Search„Å´„Çà„Çã„É©„É≥„ÉÄ„É†Êé¢Á¥¢
    """
    param_distributions = {
        'n_estimators': randint(50, 300),
        'max_depth': randint(5, 30),
        'min_samples_split': randint(2, 20),
        'min_samples_leaf': randint(1, 10),
        'max_features': uniform(0.3, 0.7)
    }

    model = RandomForestRegressor(random_state=42)

    random_search = RandomizedSearchCV(
        model,
        param_distributions,
        n_iter=n_iter,
        cv=5,
        scoring='neg_mean_absolute_error',
        n_jobs=-1,
        random_state=42,
        verbose=1
    )

    random_search.fit(X, y)

    return random_search

<h1>ÂÆüË°å</h1>
random_result = random_search_demo(X, y, n_iter=50)

print("\nRandom SearchÁµêÊûúÔºö")
print(f"ÊúÄÈÅ©„Éë„É©„É°„Éº„Çø: {random_result.best_params_}")
print(f"ÊúÄËâØ„Çπ„Ç≥„Ç¢ (MAE): {-random_result.best_score_:.4f}")

<h1>Grid vs RandomÊØîËºÉ</h1>
print(f"\nGrid SearchÊúÄËâØ„Çπ„Ç≥„Ç¢: {-grid_result.best_score_:.4f}")
print(f"Random SearchÊúÄËâØ„Çπ„Ç≥„Ç¢: {-random_result.best_score_:.4f}")
print(f"\nRandom Search„ÅÆÊé¢Á¥¢ÂäπÁéá: {50 / len(grid_result.cv_results_['params']) * 100:.1f}%„ÅÆÊé¢Á¥¢„ÅßÂêåÁ≠âÊÄßËÉΩ")</code></pre>

<h3>Bayesian OptimizationÔºàOptuna, HyperoptÔºâ</h3>

<pre><code class="language-python">import optuna

def objective(trial, X, y):
    """
    OptunaÁõÆÁöÑÈñ¢Êï∞
    """
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 5, 30),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
        'max_features': trial.suggest_float('max_features', 0.3, 1.0),
        'random_state': 42
    }

    model = RandomForestRegressor(**params)

    # ‰∫§Â∑ÆÊ§úË®º
    cv_scores = cross_val_score(
        model, X, y, cv=5,
        scoring='neg_mean_absolute_error'
    )

    return -cv_scores.mean()  # ÊúÄÂ∞èÂåñ„Åô„Çã„Åü„ÇÅË≤†„Å´„Åô„Çã

<h1>OptunaÊúÄÈÅ©Âåñ</h1>
study = optuna.create_study(direction='minimize')
study.optimize(lambda trial: objective(trial, X, y), n_trials=100, show_progress_bar=True)

print("\nOptuna Bayesian OptimizationÁµêÊûúÔºö")
print(f"ÊúÄÈÅ©„Éë„É©„É°„Éº„Çø: {study.best_params}")
print(f"ÊúÄËâØ„Çπ„Ç≥„Ç¢ (MAE): {study.best_value:.4f}")

<h1>ÊúÄÈÅ©ÂåñÂ±•Ê≠¥</h1>
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

<h1>ÊúÄÈÅ©ÂåñÂ±•Ê≠¥</h1>
axes[0].plot(study.trials_dataframe()['number'],
             study.trials_dataframe()['value'],
             'o-', color='steelblue', alpha=0.6, label='Trial Score')
axes[0].plot(study.trials_dataframe()['number'],
             study.trials_dataframe()['value'].cummin(),
             'r-', linewidth=2, label='Best Score')
axes[0].set_xlabel('Trial', fontsize=11)
axes[0].set_ylabel('MAE', fontsize=11)
axes[0].set_title('ÊúÄÈÅ©ÂåñÂ±•Ê≠¥', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

<h1>„Éë„É©„É°„Éº„ÇøÈáçË¶ÅÂ∫¶</h1>
importances = optuna.importance.get_param_importances(study)
axes[1].barh(list(importances.keys()), list(importances.values()),
             color='coral', alpha=0.7)
axes[1].set_xlabel('Importance', fontsize=11)
axes[1].set_title('„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÈáçË¶ÅÂ∫¶', fontsize=12, fontweight='bold')
axes[1].grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.show()</code></pre>

<h3>Êó©ÊúüÁµÇ‰∫ÜÔºàEarly StoppingÔºâ</h3>

<pre><code class="language-python">from sklearn.ensemble import GradientBoostingRegressor

def early_stopping_demo(X, y):
    """
    Early Stopping„ÅÆ„Éá„É¢
    """
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # GradientBoosting„Åß„Çπ„ÉÜ„Éº„Ç∏„Åî„Å®„ÅÆÊÄßËÉΩËøΩË∑°
    model = GradientBoostingRegressor(
        n_estimators=500,
        learning_rate=0.1,
        max_depth=5,
        random_state=42
    )

    model.fit(X_train, y_train)

    # „Çπ„ÉÜ„Éº„Ç∏„Åî„Å®„ÅÆ‰∫àÊ∏¨
    train_scores = []
    val_scores = []

    for i, y_pred_train in enumerate(model.staged_predict(X_train)):
        y_pred_val = list(model.staged_predict(X_val))[i]

        train_mae = mean_absolute_error(y_train, y_pred_train)
        val_mae = mean_absolute_error(y_val, y_pred_val)

        train_scores.append(train_mae)
        val_scores.append(val_mae)

    # ÊúÄÈÅ©„Å™n_estimatorsÔºàÊ§úË®ºË™§Â∑ÆÊúÄÂ∞èÔºâ
    best_n_estimators = np.argmin(val_scores) + 1

    return train_scores, val_scores, best_n_estimators

train_curve, val_curve, best_n = early_stopping_demo(X, y)

<h1>ÂèØË¶ñÂåñ</h1>
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(train_curve)+1), train_curve,
         'b-', label='Training Error', linewidth=2)
plt.plot(range(1, len(val_curve)+1), val_curve,
         'r-', label='Validation Error', linewidth=2)
plt.axvline(x=best_n, color='green', linestyle='--',
            label=f'Best n_estimators={best_n}', linewidth=2)
plt.xlabel('n_estimators', fontsize=12)
plt.ylabel('MAE', fontsize=12)
plt.title('Early Stopping', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f"Early StoppingÁµêÊûúÔºö")
print(f"ÊúÄÈÅ©n_estimators: {best_n}")
print(f"Ê§úË®ºMAE: {val_curve[best_n-1]:.4f}")
print(f"ÈÅéÂ≠¶ÁøíÈò≤Ê≠¢: {500 - best_n} „Ç§„ÉÜ„É¨„Éº„Ç∑„Éß„É≥ÂâäÊ∏õ")</code></pre>

---

<h2>3.4 „Ç¢„É≥„Çµ„É≥„Éñ„É´Â≠¶Áøí</h2>

Ë§áÊï∞„ÅÆ„É¢„Éá„É´„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Å¶‰∫àÊ∏¨Á≤æÂ∫¶„ÇíÂêë‰∏ä„Åï„Åõ„Åæ„Åô„ÄÇ

<h3>BaggingÔºàBootstrap AggregatingÔºâ</h3>

<pre><code class="language-python">from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor

<h1>Bagging</h1>
bagging = BaggingRegressor(
    estimator=DecisionTreeRegressor(max_depth=10),
    n_estimators=50,
    max_samples=0.8,
    random_state=42
)

<h1>ÊØîËºÉÁî®ÔºöÂçò‰∏ÄDecision Tree</h1>
single_tree = DecisionTreeRegressor(max_depth=10, random_state=42)

<h1>Ë©ï‰æ°</h1>
cv_bagging = cross_val_score(bagging, X, y, cv=5,
                             scoring='neg_mean_absolute_error')
cv_single = cross_val_score(single_tree, X, y, cv=5,
                            scoring='neg_mean_absolute_error')

print("BaggingÁµêÊûúÔºö")
print(f"Âçò‰∏ÄDecision Tree MAE: {-cv_single.mean():.4f} ¬± {cv_single.std():.4f}")
print(f"Bagging MAE: {-cv_bagging.mean():.4f} ¬± {cv_bagging.std():.4f}")
print(f"ÊîπÂñÑÁéá: {(cv_single.mean() - cv_bagging.mean()) / cv_single.mean() * 100:.1f}%")</code></pre>

<h3>BoostingÔºàAdaBoost, Gradient Boosting, LightGBM, XGBoostÔºâ</h3>

<pre><code class="language-python">from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor
import lightgbm as lgb
<h1>import xgboost as xgb  # Optional</h1>

<h1>ÂêÑÁ®ÆBoosting„Ç¢„É´„Ç¥„É™„Ç∫„É†</h1>
boosting_models = {
    'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1),
    # 'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42)
}

boosting_results = []

for model_name, model in boosting_models.items():
    cv_scores = cross_val_score(model, X, y, cv=5,
                                scoring='neg_mean_absolute_error')
    mae = -cv_scores.mean()
    mae_std = cv_scores.std()

    boosting_results.append({
        'Model': model_name,
        'MAE': mae,
        'MAE_std': mae_std
    })

boosting_df = pd.DataFrame(boosting_results)

print("\nBoostingÊâãÊ≥ï„ÅÆÊØîËºÉÔºö")
print(boosting_df.to_string(index=False))

<h1>ÂèØË¶ñÂåñ</h1>
plt.figure(figsize=(10, 6))
plt.barh(boosting_df['Model'], boosting_df['MAE'],
         xerr=boosting_df['MAE_std'],
         color='steelblue', alpha=0.7)
plt.xlabel('MAE', fontsize=12)
plt.title('BoostingÊâãÊ≥ï„ÅÆÊÄßËÉΩÊØîËºÉ', fontsize=13, fontweight='bold')
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()</code></pre>

<h3>Stacking</h3>

<pre><code class="language-python">from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import Ridge

<h1>Base models</h1>
base_models = [
    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),
    ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42)),
    ('lgbm', lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1))
]

<h1>Meta model</h1>
meta_model = Ridge(alpha=1.0)

<h1>Stacking</h1>
stacking = StackingRegressor(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5
)

<h1>Ë©ï‰æ°</h1>
cv_stacking = cross_val_score(stacking, X, y, cv=5,
                              scoring='neg_mean_absolute_error')

print("\nStackingÁµêÊûúÔºö")
for name, _ in base_models:
    model_cv = cross_val_score(dict(base_models)[name], X, y, cv=5,
                               scoring='neg_mean_absolute_error')
    print(f"{name} MAE: {-model_cv.mean():.4f}")

print(f"Stacking Ensemble MAE: {-cv_stacking.mean():.4f}")
print(f"\nÊîπÂñÑÂäπÊûú: Stacking„ÅåÊúÄËâØÂçò‰∏Ä„É¢„Éá„É´„Çà„Çä "
      f"{((-cv_stacking.mean() / min([cross_val_score(m, X, y, cv=5, scoring='neg_mean_absolute_error').mean() for _, m in base_models])) - 1) * -100:.1f}% ÊîπÂñÑ")</code></pre>

<h3>Voting</h3>

<pre><code class="language-python">from sklearn.ensemble import VotingRegressor

<h1>Voting Ensemble</h1>
voting = VotingRegressor(
    estimators=base_models,
    weights=[1, 1.5, 1]  # GB„Å´È´ò„ÅÑÈáç„Åø
)

cv_voting = cross_val_score(voting, X, y, cv=5,
                            scoring='neg_mean_absolute_error')

print(f"\nVoting Ensemble MAE: {-cv_voting.mean():.4f}")

<h1>„Ç¢„É≥„Çµ„É≥„Éñ„É´ÊâãÊ≥ï„ÅÆÊØîËºÉ</h1>
ensemble_comparison = pd.DataFrame({
    'Method': ['Single Best', 'Bagging', 'Boosting (LightGBM)',
               'Stacking', 'Voting'],
    'MAE': [
        min([cross_val_score(m, X, y, cv=5, scoring='neg_mean_absolute_error').mean() for _, m in base_models]),
        cv_bagging.mean(),
        boosting_df[boosting_df['Model'] == 'LightGBM']['MAE'].values[0],
        cv_stacking.mean(),
        cv_voting.mean()
    ]
})

ensemble_comparison['MAE'] = -ensemble_comparison['MAE']

plt.figure(figsize=(10, 6))
plt.barh(ensemble_comparison['Method'], ensemble_comparison['MAE'],
         color='coral', alpha=0.7)
plt.xlabel('MAE (lower is better)', fontsize=12)
plt.title('„Ç¢„É≥„Çµ„É≥„Éñ„É´ÊâãÊ≥ï„ÅÆÊÄßËÉΩÊØîËºÉ', fontsize=13, fontweight='bold')
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()</code></pre>

---

<h2>3.5 „Ç±„Éº„Çπ„Çπ„Çø„Éá„Ç£ÔºöLi-ionÈõªÊ±†ÂÆπÈáè‰∫àÊ∏¨</h2>

ÂÆüÈöõ„ÅÆLi-ionÈõªÊ±†„Éá„Éº„Çø„Åß„É¢„Éá„É´ÈÅ∏Êäû„Å®ÊúÄÈÅ©Âåñ„ÅÆÂÖ®Â∑•Á®ã„ÇíÂÆüË∑µ„Åó„Åæ„Åô„ÄÇ

<pre><code class="language-python"><h1>Li-ionÈõªÊ±†ÂÆπÈáè„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºà„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥Ôºâ</h1>
np.random.seed(42)
n_batteries = 300

battery_data = pd.DataFrame({
    'Ê≠£Ê•µÊùêÊñôÁµÑÊàê_Li': np.random.uniform(0.9, 1.1, n_batteries),
    'Ê≠£Ê•µÊùêÊñôÁµÑÊàê_Co': np.random.uniform(0, 0.6, n_batteries),
    'Ê≠£Ê•µÊùêÊñôÁµÑÊàê_Ni': np.random.uniform(0, 0.8, n_batteries),
    'Ê≠£Ê•µÊùêÊñôÁµÑÊàê_Mn': np.random.uniform(0, 0.4, n_batteries),
    'Ë≤†Ê•µÊùêÊñô_ÈªíÈâõÂâ≤Âêà': np.random.uniform(0.8, 1.0, n_batteries),
    'ÈõªËß£Ë≥™ÊøÉÂ∫¶': np.random.uniform(0.5, 2.0, n_batteries),
    'ÈõªÊ•µÂéö„Åï': np.random.uniform(50, 200, n_batteries),
    'Á≤íÂ≠ê„Çµ„Ç§„Ç∫': np.random.uniform(1, 20, n_batteries),
    'ÁÑºÊàêÊ∏©Â∫¶': np.random.uniform(700, 1000, n_batteries),
    'BETË°®Èù¢Á©ç': np.random.uniform(1, 50, n_batteries)
})

<h1>ÂÆπÈáèÔºàÁúü„ÅÆÈñ¢‰øÇ„ÅØË§áÈõë„Å™ÈùûÁ∑öÂΩ¢Ôºâ</h1>
capacity = (
    150 * battery_data['Ê≠£Ê•µÊùêÊñôÁµÑÊàê_Ni'] +
    120 * battery_data['Ê≠£Ê•µÊùêÊñôÁµÑÊàê_Co'] +
    80 * battery_data['Ê≠£Ê•µÊùêÊñôÁµÑÊàê_Mn'] +
    30 * battery_data['ÈõªËß£Ë≥™ÊøÉÂ∫¶'] -
    0.5 * battery_data['ÈõªÊ•µÂéö„Åï'] +
    2 * battery_data['BETË°®Èù¢Á©ç'] +
    0.1 * battery_data['ÁÑºÊàêÊ∏©Â∫¶'] +
    20 * battery_data['Ê≠£Ê•µÊùêÊñôÁµÑÊàê_Ni'] * battery_data['ÈõªËß£Ë≥™ÊøÉÂ∫¶'] +
    np.random.normal(0, 5, n_batteries)
)

battery_data['ÂÆπÈáè_mAh/g'] = capacity

print("=== Li-ionÈõªÊ±†ÂÆπÈáè‰∫àÊ∏¨„Éá„Éº„Çø„Çª„ÉÉ„Éà ===")
print(f"„Çµ„É≥„Éó„É´Êï∞: {len(battery_data)}")
print(f"ÁâπÂæ¥ÈáèÊï∞: {battery_data.shape[1] - 1}")
print(f"\nÂÆπÈáèÁµ±Ë®à:")
print(battery_data['ÂÆπÈáè_mAh/g'].describe())

X_battery = battery_data.drop('ÂÆπÈáè_mAh/g', axis=1)
y_battery = battery_data['ÂÆπÈáè_mAh/g']</code></pre>

<h3>Step 1: 5„Å§„ÅÆ„É¢„Éá„É´„ÅÆÊØîËºÉ</h3>

<pre><code class="language-python">from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_battery, y_battery, test_size=0.2, random_state=42
)

<h1>5„Å§„ÅÆ„É¢„Éá„É´</h1>
models_to_compare = {
    'Ridge': Ridge(alpha=1.0),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1),
    'Stacking': StackingRegressor(
        estimators=[
            ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),
            ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42)),
            ('lgbm', lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1))
        ],
        final_estimator=Ridge(alpha=1.0),
        cv=5
    )
}

comparison_results = []

for model_name, model in models_to_compare.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    comparison_results.append({
        'Model': model_name,
        'MAE': mae,
        'RMSE': rmse,
        'R¬≤': r2
    })

comparison_df = pd.DataFrame(comparison_results)
print("\n=== Step 1: „É¢„Éá„É´ÊØîËºÉÁµêÊûú ===")
print(comparison_df.to_string(index=False))</code></pre>

<h3>Step 2: Optuna„Å´„Çà„Çã„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøËá™ÂãïÊúÄÈÅ©Âåñ</h3>

<pre><code class="language-python">def objective_lightgbm(trial, X, y):
    """
    LightGBM„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊúÄÈÅ©Âåñ
    """
    param = {
        'objective': 'regression',
        'metric': 'mae',
        'verbosity': -1,
        'boosting_type': 'gbdt',
        'num_leaves': trial.suggest_int('num_leaves', 20, 100),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
        'random_state': 42
    }

    model = lgb.LGBMRegressor(**param)

    cv_scores = cross_val_score(
        model, X, y, cv=5, scoring='neg_mean_absolute_error'
    )

    return -cv_scores.mean()

<h1>OptunaÊúÄÈÅ©Âåñ</h1>
study_battery = optuna.create_study(direction='minimize')
study_battery.optimize(
    lambda trial: objective_lightgbm(trial, X_battery, y_battery),
    n_trials=100,
    show_progress_bar=True
)

print("\n=== Step 2: Optuna„Å´„Çà„ÇãÊúÄÈÅ©Âåñ ===")
print(f"ÊúÄÈÅ©„Éë„É©„É°„Éº„Çø:")
for key, value in study_battery.best_params.items():
    print(f"  {key}: {value}")
print(f"\nÊúÄËâØMAE: {study_battery.best_value:.4f} mAh/g")

<h1>ÊúÄÈÅ©Âåñ„Åï„Çå„Åü„É¢„Éá„É´„ÅßÂÜçË©ï‰æ°</h1>
best_model = lgb.LGBMRegressor(**study_battery.best_params, random_state=42)
best_model.fit(X_train, y_train)
y_pred_best = best_model.predict(X_test)

mae_best = mean_absolute_error(y_test, y_pred_best)
r2_best = r2_score(y_test, y_pred_best)

print(f"\nÊúÄÈÅ©ÂåñÂæå„ÅÆÊÄßËÉΩ:")
print(f"MAE: {mae_best:.4f} mAh/g")
print(f"R¬≤: {r2_best:.4f}")</code></pre>

<h3>Step 3: Stacking ensemble„ÅßÊúÄÈ´òÊÄßËÉΩÈÅîÊàê</h3>

<pre><code class="language-python"><h1>ÊúÄÈÅ©Âåñ„Åï„Çå„ÅüLightGBM„ÇíÂê´„ÇÄStacking</h1>
optimized_stacking = StackingRegressor(
    estimators=[
        ('rf_tuned', RandomForestRegressor(
            n_estimators=200, max_depth=15, min_samples_split=5,
            random_state=42
        )),
        ('lgbm_tuned', lgb.LGBMRegressor(**study_battery.best_params, random_state=42)),
        ('gb_tuned', GradientBoostingRegressor(
            n_estimators=150, learning_rate=0.1, max_depth=7,
            random_state=42
        ))
    ],
    final_estimator=Ridge(alpha=0.5),
    cv=5
)

optimized_stacking.fit(X_train, y_train)
y_pred_stack = optimized_stacking.predict(X_test)

mae_stack = mean_absolute_error(y_test, y_pred_stack)
r2_stack = r2_score(y_test, y_pred_stack)

print("\n=== Step 3: ÊúÄÁµÇStacking Ensemble ===")
print(f"MAE: {mae_stack:.4f} mAh/g")
print(f"R¬≤: {r2_stack:.4f}")

<h1>ÂÖ®Â∑•Á®ã„ÅÆÊØîËºÉ</h1>
final_comparison = pd.DataFrame({
    'Stage': [
        'Baseline (Ridge)',
        'Best Single Model',
        'Optuna Optimized',
        'Final Stacking'
    ],
    'MAE': [
        comparison_df[comparison_df['Model'] == 'Ridge']['MAE'].values[0],
        comparison_df['MAE'].min(),
        mae_best,
        mae_stack
    ],
    'R¬≤': [
        comparison_df[comparison_df['Model'] == 'Ridge']['R¬≤'].values[0],
        comparison_df['R¬≤'].max(),
        r2_best,
        r2_stack
    ]
})

print("\n=== ÂÖ®Â∑•Á®ã„ÅÆÊÄßËÉΩÊé®Áßª ===")
print(final_comparison.to_string(index=False))

<h1>‰∫àÊ∏¨ vs ÂÆüÊ∏¨</h1>
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

<h1>ÊúÄÈÅ©ÂåñÂâç„ÅÆÊúÄËâØ„É¢„Éá„É´</h1>
best_single_idx = comparison_df['MAE'].idxmin()
best_single_model = list(models_to_compare.values())[best_single_idx]
y_pred_single = best_single_model.predict(X_test)

axes[0].scatter(y_test, y_pred_single, c='steelblue', s=50, alpha=0.6)
axes[0].plot([y_test.min(), y_test.max()],
             [y_test.min(), y_test.max()],
             'r--', linewidth=2, label='Perfect')
axes[0].set_xlabel('Actual Capacity (mAh/g)', fontsize=11)
axes[0].set_ylabel('Predicted Capacity (mAh/g)', fontsize=11)
axes[0].set_title(f'ÊúÄËâØÂçò‰∏Ä„É¢„Éá„É´ (MAE={comparison_df["MAE"].min():.2f})',
                  fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

<h1>ÊúÄÁµÇStacking</h1>
axes[1].scatter(y_test, y_pred_stack, c='coral', s=50, alpha=0.6)
axes[1].plot([y_test.min(), y_test.max()],
             [y_test.min(), y_test.max()],
             'r--', linewidth=2, label='Perfect')
axes[1].set_xlabel('Actual Capacity (mAh/g)', fontsize=11)
axes[1].set_ylabel('Predicted Capacity (mAh/g)', fontsize=11)
axes[1].set_title(f'ÊúÄÁµÇStacking (MAE={mae_stack:.2f})',
                  fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

improvement = (comparison_df['MAE'].min() - mae_stack) / comparison_df['MAE'].min() * 100
print(f"\nÊúÄÁµÇÊîπÂñÑÁéá: {improvement:.1f}%")</code></pre>

---

<h2>ÊºîÁøíÂïèÈ°å</h2>

<h3>ÂïèÈ°å1ÔºàÈõ£ÊòìÂ∫¶: easyÔºâ</h3>

K-Fold CV„Å®Stratified K-Fold CV„ÇíÁî®„ÅÑ„Å¶„ÄÅ„ÇØ„É©„Çπ‰∏çÂùáË°°„Éá„Éº„Çø„Åß„ÅÆÊÄßËÉΩ„ÇíÊØîËºÉ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

<details>
<summary>Ëß£Á≠î‰æã</summary>

<pre><code class="language-python">from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier

<h1>‰∏çÂùáË°°„Éá„Éº„ÇøÁîüÊàê</h1>
X, y = make_classification(n_samples=200, n_features=20,
                          weights=[0.9, 0.1], random_state=42)

model = RandomForestClassifier(n_estimators=100, random_state=42)

<h1>K-Fold</h1>
kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores_kfold = cross_val_score(model, X, y, cv=kf, scoring='f1')

<h1>Stratified K-Fold</h1>
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores_stratified = cross_val_score(model, X, y, cv=skf, scoring='f1')

print(f"K-Fold F1: {scores_kfold.mean():.4f} ¬± {scores_kfold.std():.4f}")
print(f"Stratified K-Fold F1: {scores_stratified.mean():.4f} ¬± {scores_stratified.std():.4f}")</code></pre>

</details>

<h3>ÂïèÈ°å2ÔºàÈõ£ÊòìÂ∫¶: mediumÔºâ</h3>

Optuna„ÇíÁî®„ÅÑ„Å¶„ÄÅRandom Forest„ÅÆ„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÇíÊúÄÈÅ©Âåñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÊé¢Á¥¢Á©∫Èñì„ÅØ<code>n_estimators</code>, <code>max_depth</code>, <code>min_samples_split</code>, <code>max_features</code>„ÅÆ4„Å§„Å®„Åó„Åæ„Åô„ÄÇ

<details>
<summary>Ëß£Á≠î‰æã</summary>

<pre><code class="language-python">import optuna
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

def objective_rf(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 5, 30),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'max_features': trial.suggest_float('max_features', 0.3, 1.0),
        'random_state': 42
    }

    model = RandomForestRegressor(**params)
    cv_scores = cross_val_score(model, X, y, cv=5,
                                scoring='neg_mean_absolute_error')

    return -cv_scores.mean()

study = optuna.create_study(direction='minimize')
study.optimize(objective_rf, n_trials=50)

print(f"ÊúÄÈÅ©„Éë„É©„É°„Éº„Çø: {study.best_params}")
print(f"ÊúÄËâØ„Çπ„Ç≥„Ç¢: {study.best_value:.4f}")</code></pre>

</details>

<h3>ÂïèÈ°å3ÔºàÈõ£ÊòìÂ∫¶: hardÔºâ</h3>

Stacking Ensemble„ÇíÊßãÁØâ„Åó„ÄÅ3„Å§„ÅÆ„Éô„Éº„Çπ„É¢„Éá„É´ÔºàRidge, Random Forest, LightGBMÔºâ„Å®2„Å§„ÅÆ„É°„Çø„É¢„Éá„É´ÔºàRidge, LassoÔºâ„ÇíÊØîËºÉ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Å©„ÅÆ„É°„Çø„É¢„Éá„É´„ÅåÊúÄËâØ„ÅÆÊÄßËÉΩ„ÇíÁ§∫„Åô„ÅãË©ï‰æ°„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

<details>
<summary>Ëß£Á≠î‰æã</summary>

<pre><code class="language-python">from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import Ridge, Lasso
import lightgbm as lgb

base_models = [
    ('ridge', Ridge(alpha=1.0)),
    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),
    ('lgbm', lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1))
]

meta_models = {
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=0.1)
}

results = []

for meta_name, meta_model in meta_models.items():
    stacking = StackingRegressor(
        estimators=base_models,
        final_estimator=meta_model,
        cv=5
    )

    cv_scores = cross_val_score(stacking, X, y, cv=5,
                                scoring='neg_mean_absolute_error')
    mae = -cv_scores.mean()

    results.append({
        'Meta Model': meta_name,
        'MAE': mae
    })

results_df = pd.DataFrame(results)
print(results_df.to_string(index=False))</code></pre>

</details>

---

<h2>„Åæ„Å®„ÇÅ</h2>

„Åì„ÅÆÁ´†„Åß„ÅØ„ÄÅ<strong>„É¢„Éá„É´ÈÅ∏Êäû„Å®„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøÊúÄÈÅ©Âåñ</strong>„ÇíÂ≠¶„Å≥„Åæ„Åó„Åü„ÄÇ

<strong>ÈáçË¶Å„Éù„Ç§„É≥„Éà</strong>Ôºö

1. <strong>„É¢„Éá„É´ÈÅ∏Êäû</strong>Ôºö„Éá„Éº„Çø„Çµ„Ç§„Ç∫„Å´Âøú„Åò„ÅüÈÅ©Âàá„Å™„É¢„Éá„É´ÔºàÂ∞è: Ridge„ÄÅ‰∏≠: RF„ÄÅÂ§ß: NNÔºâ
2. <strong>‰∫§Â∑ÆÊ§úË®º</strong>ÔºöK-Fold„ÄÅStratified„ÄÅTime Series„Çí‰Ωø„ÅÑÂàÜ„Åë
3. <strong>ÊúÄÈÅ©ÂåñÊâãÊ≥ï</strong>ÔºöGrid < Random < BayesianÔºàOptunaÔºâ„ÅÆÈ†Ü„ÅßÂäπÁéáÂåñ
4. <strong>„Ç¢„É≥„Çµ„É≥„Éñ„É´</strong>ÔºöStacking > Voting > Boosting > Bagging
5. <strong>ÂÆüË∑µ‰∫ã‰æã</strong>ÔºöLi-ionÈõªÊ±†ÂÆπÈáè‰∫àÊ∏¨„Åß30%‰ª•‰∏ä„ÅÆÊÄßËÉΩÊîπÂñÑ

<strong>Ê¨°Á´†‰∫àÂëä</strong>Ôºö
Chapter 4„Åß„ÅØ„ÄÅËß£ÈáàÂèØËÉΩAIÔºàXAIÔºâ„ÇíÂ≠¶„Å≥„Åæ„Åô„ÄÇSHAP„ÄÅLIME„ÄÅAttentionÂèØË¶ñÂåñ„Å´„Çà„Çä„ÄÅ‰∫àÊ∏¨„ÅÆÁâ©ÁêÜÁöÑÊÑèÂë≥„ÇíÁêÜËß£„Åó„ÄÅÂÆü‰∏ñÁïåÂøúÁî®„Å®„Ç≠„É£„É™„Ç¢„Éë„Çπ„ÇíÊé¢„Çä„Åæ„Åô„ÄÇ

---

<h2>ÂèÇËÄÉÊñáÁåÆ</h2>

1. <strong>Akiba, T., Sano, S., Yanase, T., et al.</strong> (2019). Optuna: A Next-generation Hyperparameter Optimization Framework. *Proceedings of the 25th ACM SIGKDD*, 2623-2631. [DOI: 10.1145/3292500.3330701](https://doi.org/10.1145/3292500.3330701)

2. <strong>Bergstra, J. & Bengio, Y.</strong> (2012). Random search for hyper-parameter optimization. *Journal of Machine Learning Research*, 13, 281-305.

3. <strong>Dietterich, T. G.</strong> (2000). Ensemble methods in machine learning. *International Workshop on Multiple Classifier Systems*, 1-15. Springer.

4. <strong>Wolpert, D. H.</strong> (1992). Stacked generalization. *Neural Networks*, 5(2), 241-259. [DOI: 10.1016/S0893-6080(05)80023-1](https://doi.org/10.1016/S0893-6080(05)80023-1)

---

[‚Üê Chapter 2„Å´Êàª„Çã](chapter-2.md) | [Chapter 4„Å∏ÈÄ≤„ÇÄ ‚Üí](chapter-4.md)
<div class="navigation">
    <a href="chapter-2.html" class="nav-button">‚Üê Á¨¨2Á´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter-4.html" class="nav-button">Á¨¨4Á´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-17</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>
