<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Chapter</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-25åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 0å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 0å•</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Chapter 2: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</h1>

---

<h2>å­¦ç¿’ç›®æ¨™</h2>

ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š

âœ… ææ–™è¨˜è¿°å­ï¼ˆçµ„æˆãƒ»æ§‹é€ ãƒ»é›»å­æ§‹é€ ï¼‰ã®é¸æŠã¨è¨­è¨ˆ
âœ… matminerã‚’æ´»ç”¨ã—ãŸææ–™ç‰¹å¾´é‡ã®è‡ªå‹•ç”Ÿæˆ
âœ… ç‰¹å¾´é‡å¤‰æ›ï¼ˆæ­£è¦åŒ–ã€å¯¾æ•°å¤‰æ›ã€å¤šé …å¼ç‰¹å¾´é‡ï¼‰ã®å®Ÿè·µ
âœ… æ¬¡å…ƒå‰Šæ¸›ï¼ˆPCAã€t-SNEã€UMAPï¼‰ã«ã‚ˆã‚‹å¯è¦–åŒ–ã¨è§£é‡ˆ
âœ… ç‰¹å¾´é‡é¸æŠï¼ˆFilter/Wrapper/Embedded/SHAP-basedï¼‰ã®ä½¿ã„åˆ†ã‘
âœ… ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ã«ãŠã‘ã‚‹200æ¬¡å…ƒâ†’20æ¬¡å…ƒã¸ã®åŠ¹æœçš„å‰Šæ¸›

---

<h2>2.1 ææ–™è¨˜è¿°å­ã®é¸æŠã¨è¨­è¨ˆ</h2>

ææ–™ã®æ€§è³ªã‚’æ©Ÿæ¢°å­¦ç¿’ã§äºˆæ¸¬ã™ã‚‹ã«ã¯ã€é©åˆ‡ãª<strong>ææ–™è¨˜è¿°å­ï¼ˆMaterial Descriptorsï¼‰</strong>ãŒå¿…è¦ã§ã™ã€‚

<h3>çµ„æˆè¨˜è¿°å­</h3>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def calculate_composition_descriptors(formula_dict):
    """
    çµ„æˆè¨˜è¿°å­ã®è¨ˆç®—

    Parameters:
    -----------
    formula_dict : dict
        {'å…ƒç´ è¨˜å·': å‰²åˆ} ä¾‹: {'Fe': 0.7, 'Ni': 0.3}

    Returns:
    --------
    dict : çµ„æˆè¨˜è¿°å­
    """
    # å…ƒç´ ã®ç‰©æ€§å€¤ï¼ˆç°¡ç•¥ç‰ˆï¼‰
    element_properties = {
        'Fe': {'atomic_mass': 55.845, 'electronegativity': 1.83,
               'atomic_radius': 1.26},
        'Ni': {'atomic_mass': 58.693, 'electronegativity': 1.91,
               'atomic_radius': 1.24},
        'Cu': {'atomic_mass': 63.546, 'electronegativity': 1.90,
               'atomic_radius': 1.28},
        'Zn': {'atomic_mass': 65.38, 'electronegativity': 1.65,
               'atomic_radius': 1.34}
    }

    descriptors = {}

    # å¹³å‡åŸå­é‡
    avg_mass = sum(
        element_properties[el]['atomic_mass'] * frac
        for el, frac in formula_dict.items()
    )
    descriptors['å¹³å‡åŸå­é‡'] = avg_mass

    # å¹³å‡é›»æ°—é™°æ€§åº¦
    avg_electronegativity = sum(
        element_properties[el]['electronegativity'] * frac
        for el, frac in formula_dict.items()
    )
    descriptors['å¹³å‡é›»æ°—é™°æ€§åº¦'] = avg_electronegativity

    # é›»æ°—é™°æ€§åº¦å·®ï¼ˆæœ€å¤§ - æœ€å°ï¼‰
    electronegativities = [
        element_properties[el]['electronegativity']
        for el in formula_dict.keys()
    ]
    descriptors['é›»æ°—é™°æ€§åº¦å·®'] = max(electronegativities) - min(electronegativities)

    # å¹³å‡åŸå­åŠå¾„
    avg_radius = sum(
        element_properties[el]['atomic_radius'] * frac
        for el, frac in formula_dict.items()
    )
    descriptors['å¹³å‡åŸå­åŠå¾„'] = avg_radius

    return descriptors

<h1>ä¾‹ï¼šFe-Niåˆé‡‘</h1>
formula = {'Fe': 0.7, 'Ni': 0.3}
descriptors = calculate_composition_descriptors(formula)

print("çµ„æˆè¨˜è¿°å­ï¼ˆFeâ‚€.â‚‡Niâ‚€.â‚ƒï¼‰ï¼š")
for key, value in descriptors.items():
    print(f"  {key}: {value:.4f}")</code></pre>

<strong>å‡ºåŠ›</strong>ï¼š
<pre><code>çµ„æˆè¨˜è¿°å­ï¼ˆFeâ‚€.â‚‡Niâ‚€.â‚ƒï¼‰ï¼š
  å¹³å‡åŸå­é‡: 56.6984
  å¹³å‡é›»æ°—é™°æ€§åº¦: 1.8540
  é›»æ°—é™°æ€§åº¦å·®: 0.0800
  å¹³å‡åŸå­åŠå¾„: 1.2540</code></pre>

<h3>matminerã®æ´»ç”¨</h3>

<pre><code class="language-python"><h1>matminerã«ã‚ˆã‚‹ææ–™è¨˜è¿°å­ã®è‡ªå‹•ç”Ÿæˆ</h1>
<h1>!pip install matminer pymatgen</h1>

from matminer.featurizers.composition import (
    ElementProperty,
    Stoichiometry,
    ValenceOrbital,
    IonProperty
)
from pymatgen.core import Composition

def generate_matminer_features(formula_str):
    """
    matminerã§ææ–™è¨˜è¿°å­ã‚’ç”Ÿæˆ

    Parameters:
    -----------
    formula_str : str
        åŒ–å­¦å¼ï¼ˆä¾‹: "Fe2O3"ï¼‰

    Returns:
    --------
    pd.DataFrame : ç‰¹å¾´é‡
    """
    comp = Composition(formula_str)

    # å…ƒç´ ç‰©æ€§è¨˜è¿°å­
    ep_feat = ElementProperty.from_preset("magpie")
    features_ep = ep_feat.featurize(comp)

    # åŒ–å­¦é‡è«–è¨˜è¿°å­
    stoich_feat = Stoichiometry()
    features_stoich = stoich_feat.featurize(comp)

    # ä¾¡é›»å­è»Œé“è¨˜è¿°å­
    valence_feat = ValenceOrbital()
    features_valence = valence_feat.featurize(comp)

    # ç‰¹å¾´é‡åå–å¾—
    feature_names = (
        ep_feat.feature_labels() +
        stoich_feat.feature_labels() +
        valence_feat.feature_labels()
    )

    # DataFrameã«å¤‰æ›
    all_features = features_ep + features_stoich + features_valence
    df = pd.DataFrame([all_features], columns=feature_names)

    return df

<h1>ä¾‹ï¼šé…¸åŒ–é‰„</h1>
formula = "Fe2O3"
features = generate_matminer_features(formula)

print(f"matminerã«ã‚ˆã‚‹ç‰¹å¾´é‡ç”Ÿæˆï¼ˆ{formula}ï¼‰ï¼š")
print(f"ç‰¹å¾´é‡æ•°: {features.shape[1]}")
print(f"\næœ€åˆã®10ç‰¹å¾´é‡ï¼š")
print(features.iloc[:, :10].T)</code></pre>

<strong>matminerã®ä¸»ãªè¨˜è¿°å­</strong>ï¼š

<pre><code class="language-python"><h1>è¨˜è¿°å­ã‚¿ã‚¤ãƒ—ã®æ¯”è¼ƒ</h1>
descriptor_types = pd.DataFrame({
    'è¨˜è¿°å­ã‚¿ã‚¤ãƒ—': [
        'ElementProperty',
        'Stoichiometry',
        'ValenceOrbital',
        'IonProperty',
        'OxidationStates',
        'ElectronAffinity'
    ],
    'ç‰¹å¾´é‡æ•°': [132, 7, 10, 32, 3, 1],
    'ç”¨é€”': [
        'å…ƒç´ ã®ç‰©ç†åŒ–å­¦çš„æ€§è³ª',
        'åŒ–å­¦é‡è«–æ¯”',
        'ä¾¡é›»å­è»Œé“',
        'ã‚¤ã‚ªãƒ³ç‰¹æ€§',
        'é…¸åŒ–çŠ¶æ…‹',
        'é›»å­è¦ªå’ŒåŠ›'
    ]
})

<h1>å¯è¦–åŒ–</h1>
fig, ax = plt.subplots(figsize=(10, 6))
ax.barh(descriptor_types['è¨˜è¿°å­ã‚¿ã‚¤ãƒ—'],
        descriptor_types['ç‰¹å¾´é‡æ•°'],
        color='steelblue', alpha=0.7)
ax.set_xlabel('ç‰¹å¾´é‡æ•°', fontsize=12)
ax.set_title('matminerã®è¨˜è¿°å­ã‚¿ã‚¤ãƒ—', fontsize=13, fontweight='bold')
ax.grid(axis='x', alpha=0.3)

for idx, row in descriptor_types.iterrows():
    ax.text(row['ç‰¹å¾´é‡æ•°'] + 5, idx, row['ç”¨é€”'],
            va='center', fontsize=9, style='italic')

plt.tight_layout()
plt.show()

print(descriptor_types.to_string(index=False))</code></pre>

<h3>æ§‹é€ è¨˜è¿°å­</h3>

<pre><code class="language-python">def calculate_structure_descriptors(lattice_params):
    """
    çµæ™¶æ§‹é€ è¨˜è¿°å­

    Parameters:
    -----------
    lattice_params : dict
        {'a': float, 'b': float, 'c': float,
         'alpha': float, 'beta': float, 'gamma': float}

    Returns:
    --------
    dict : æ§‹é€ è¨˜è¿°å­
    """
    a = lattice_params['a']
    b = lattice_params['b']
    c = lattice_params['c']
    alpha = np.radians(lattice_params['alpha'])
    beta = np.radians(lattice_params['beta'])
    gamma = np.radians(lattice_params['gamma'])

    # ä½“ç©
    volume = a * b * c * np.sqrt(
        1 - np.cos(alpha)<strong>2 - np.cos(beta)</strong>2 - np.cos(gamma)**2 +
        2 * np.cos(alpha) * np.cos(beta) * np.cos(gamma)
    )

    # ãƒ‘ãƒƒã‚­ãƒ³ã‚°å¯†åº¦ï¼ˆç°¡ç•¥åŒ–ï¼‰
    packing_density = 0.74  # ä¾‹ï¼šFCCã®å ´åˆ

    descriptors = {
        'æ ¼å­å®šæ•°a': a,
        'æ ¼å­å®šæ•°b': b,
        'æ ¼å­å®šæ•°c': c,
        'ä½“ç©': volume,
        'ãƒ‘ãƒƒã‚­ãƒ³ã‚°å¯†åº¦': packing_density
    }

    return descriptors

<h1>ä¾‹ï¼šç«‹æ–¹æ™¶</h1>
lattice = {'a': 5.43, 'b': 5.43, 'c': 5.43,
           'alpha': 90, 'beta': 90, 'gamma': 90}
struct_desc = calculate_structure_descriptors(lattice)

print("æ§‹é€ è¨˜è¿°å­ï¼ˆç«‹æ–¹æ™¶ï¼‰ï¼š")
for key, value in struct_desc.items():
    print(f"  {key}: {value:.4f}")</code></pre>

<h3>é›»å­æ§‹é€ è¨˜è¿°å­</h3>

<pre><code class="language-python"><h1>é›»å­æ§‹é€ è¨˜è¿°å­ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</h1>
def simulate_electronic_descriptors(n_samples=100):
    """
    é›»å­æ§‹é€ è¨˜è¿°å­ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    """
    np.random.seed(42)

    data = pd.DataFrame({
        'ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—': np.random.uniform(0, 5, n_samples),
        'ãƒ•ã‚§ãƒ«ãƒŸã‚¨ãƒãƒ«ã‚®ãƒ¼': np.random.uniform(-5, 5, n_samples),
        'çŠ¶æ…‹å¯†åº¦_ä¾¡é›»å­å¸¯': np.random.uniform(10, 100, n_samples),
        'çŠ¶æ…‹å¯†åº¦_ä¼å°å¸¯': np.random.uniform(5, 50, n_samples),
        'æœ‰åŠ¹è³ªé‡_é›»å­': np.random.uniform(0.1, 2, n_samples),
        'æœ‰åŠ¹è³ªé‡_æ­£å­”': np.random.uniform(0.1, 2, n_samples)
    })

    return data

<h1>ç”Ÿæˆ</h1>
electronic_data = simulate_electronic_descriptors(100)

<h1>å¯è¦–åŒ–</h1>
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for idx, col in enumerate(electronic_data.columns):
    axes[idx].hist(electronic_data[col], bins=20,
                   color='steelblue', alpha=0.7, edgecolor='black')
    axes[idx].set_xlabel(col, fontsize=11)
    axes[idx].set_ylabel('é »åº¦', fontsize=11)
    axes[idx].set_title(f'{col}ã®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    axes[idx].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("é›»å­æ§‹é€ è¨˜è¿°å­ã®çµ±è¨ˆï¼š")
print(electronic_data.describe())</code></pre>

---

<h2>2.2 ç‰¹å¾´é‡å¤‰æ›</h2>

ç”Ÿã®ç‰¹å¾´é‡ã‚’æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«é©ã—ãŸå½¢ã«å¤‰æ›ã—ã¾ã™ã€‚

<h3>æ­£è¦åŒ–ï¼ˆMin-Max, Z-scoreï¼‰</h3>

<pre><code class="language-python">from sklearn.preprocessing import MinMaxScaler, StandardScaler

def compare_normalization(data):
    """
    æ­£è¦åŒ–æ‰‹æ³•ã®æ¯”è¼ƒ
    """
    # Min-Maxæ­£è¦åŒ–ï¼ˆ0-1ï¼‰
    minmax_scaler = MinMaxScaler()
    data_minmax = pd.DataFrame(
        minmax_scaler.fit_transform(data),
        columns=data.columns
    )

    # Z-scoreæ­£è¦åŒ–ï¼ˆå¹³å‡0ã€æ¨™æº–åå·®1ï¼‰
    standard_scaler = StandardScaler()
    data_standard = pd.DataFrame(
        standard_scaler.fit_transform(data),
        columns=data.columns
    )

    return data_minmax, data_standard

<h1>ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿</h1>
np.random.seed(42)
sample_data = pd.DataFrame({
    'æ ¼å­å®šæ•°': np.random.uniform(3, 7, 100),
    'é›»æ°—ä¼å°åº¦': np.random.lognormal(10, 2, 100),
    'ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—': np.random.uniform(0, 3, 100)
})

<h1>æ­£è¦åŒ–</h1>
data_minmax, data_standard = compare_normalization(sample_data)

<h1>å¯è¦–åŒ–</h1>
fig, axes = plt.subplots(3, 3, figsize=(15, 12))

for idx, col in enumerate(sample_data.columns):
    # å…ƒãƒ‡ãƒ¼ã‚¿
    axes[idx, 0].hist(sample_data[col], bins=20,
                      color='gray', alpha=0.7, edgecolor='black')
    axes[idx, 0].set_title(f'å…ƒãƒ‡ãƒ¼ã‚¿: {col}', fontsize=11, fontweight='bold')
    axes[idx, 0].set_ylabel('é »åº¦', fontsize=10)

    # Min-Max
    axes[idx, 1].hist(data_minmax[col], bins=20,
                      color='steelblue', alpha=0.7, edgecolor='black')
    axes[idx, 1].set_title(f'Min-Max: {col}', fontsize=11, fontweight='bold')

    # Z-score
    axes[idx, 2].hist(data_standard[col], bins=20,
                      color='coral', alpha=0.7, edgecolor='black')
    axes[idx, 2].set_title(f'Z-score: {col}', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()

print("æ­£è¦åŒ–å¾Œã®çµ±è¨ˆï¼š")
print("\nMin-Maxæ­£è¦åŒ–ï¼š")
print(data_minmax.describe())
print("\nZ-scoreæ­£è¦åŒ–ï¼š")
print(data_standard.describe())</code></pre>

<h3>å¯¾æ•°å¤‰æ›ã€Box-Coxå¤‰æ›</h3>

<pre><code class="language-python">from scipy.stats import boxcox

def apply_transformations(data):
    """
    å„ç¨®å¤‰æ›ã®é©ç”¨
    """
    # å¯¾æ•°å¤‰æ›
    data_log = np.log1p(data)  # log(1+x)ã§0ã‚’æ‰±ãˆã‚‹

    # Box-Coxå¤‰æ›ï¼ˆæ­£å€¤ã®ã¿ï¼‰
    data_boxcox, lambda_param = boxcox(data + 1)  # +1ã§ã‚¼ãƒ­ã‚’å›é¿

    return data_log, data_boxcox, lambda_param

<h1>åã£ãŸãƒ‡ãƒ¼ã‚¿ï¼ˆé›»æ°—ä¼å°åº¦ãªã©ï¼‰</h1>
np.random.seed(42)
conductivity = np.random.lognormal(10, 2, 100)

<h1>å¤‰æ›</h1>
cond_log, cond_boxcox, lambda_val = apply_transformations(conductivity)

<h1>å¯è¦–åŒ–</h1>
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

<h1>å…ƒãƒ‡ãƒ¼ã‚¿</h1>
axes[0].hist(conductivity, bins=30, color='gray',
             alpha=0.7, edgecolor='black')
axes[0].set_xlabel('é›»æ°—ä¼å°åº¦ (S/m)', fontsize=11)
axes[0].set_ylabel('é »åº¦', fontsize=11)
axes[0].set_title('å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆæ­ªåº¦ã‚ã‚Šï¼‰', fontsize=12, fontweight='bold')

<h1>å¯¾æ•°å¤‰æ›</h1>
axes[1].hist(cond_log, bins=30, color='steelblue',
             alpha=0.7, edgecolor='black')
axes[1].set_xlabel('log(é›»æ°—ä¼å°åº¦+1)', fontsize=11)
axes[1].set_ylabel('é »åº¦', fontsize=11)
axes[1].set_title('å¯¾æ•°å¤‰æ›', fontsize=12, fontweight='bold')

<h1>Box-Coxå¤‰æ›</h1>
axes[2].hist(cond_boxcox, bins=30, color='coral',
             alpha=0.7, edgecolor='black')
axes[2].set_xlabel(f'Box-Cox (Î»={lambda_val:.3f})', fontsize=11)
axes[2].set_ylabel('é »åº¦', fontsize=11)
axes[2].set_title('Box-Coxå¤‰æ›', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()

<h1>æ­ªåº¦ã®æ¯”è¼ƒ</h1>
from scipy.stats import skew
print(f"å…ƒãƒ‡ãƒ¼ã‚¿ã®æ­ªåº¦: {skew(conductivity):.3f}")
print(f"å¯¾æ•°å¤‰æ›å¾Œã®æ­ªåº¦: {skew(cond_log):.3f}")
print(f"Box-Coxå¤‰æ›å¾Œã®æ­ªåº¦: {skew(cond_boxcox):.3f}")</code></pre>

<h3>å¤šé …å¼ç‰¹å¾´é‡</h3>

<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures

def create_polynomial_features(X, degree=2):
    """
    å¤šé …å¼ç‰¹å¾´é‡ã®ç”Ÿæˆ

    Parameters:
    -----------
    X : array-like, shape (n_samples, n_features)
    degree : int
        å¤šé …å¼ã®æ¬¡æ•°

    Returns:
    --------
    X_poly : å¤šé …å¼ç‰¹å¾´é‡
    feature_names : ç‰¹å¾´é‡å
    """
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    X_poly = poly.fit_transform(X)
    feature_names = poly.get_feature_names_out()

    return X_poly, feature_names

<h1>ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿</h1>
np.random.seed(42)
X_original = pd.DataFrame({
    'x1': np.random.uniform(0, 1, 50),
    'x2': np.random.uniform(0, 1, 50)
})

<h1>2æ¬¡å¤šé …å¼ç‰¹å¾´é‡</h1>
X_poly, feature_names = create_polynomial_features(
    X_original.values, degree=2
)

print(f"å…ƒã®ç‰¹å¾´é‡æ•°: {X_original.shape[1]}")
print(f"å¤šé …å¼ç‰¹å¾´é‡æ•°: {X_poly.shape[1]}")
print(f"\nç”Ÿæˆã•ã‚ŒãŸç‰¹å¾´é‡ï¼š")
for name in feature_names:
    print(f"  {name}")

<h1>éç·šå½¢é–¢ä¿‚ã®å­¦ç¿’ä¾‹</h1>
<h1>y = 2*x1^2 + 3*x1*x2 - x2^2 + noise</h1>
y_true = (
    2 * X_original['x1']**2 +
    3 * X_original['x1'] * X_original['x2'] -
    X_original['x2']**2 +
    np.random.normal(0, 0.1, 50)
)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

<h1>ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆå…ƒã®ç‰¹å¾´é‡ï¼‰</h1>
model_linear = LinearRegression()
model_linear.fit(X_original, y_true)
y_pred_linear = model_linear.predict(X_original)
r2_linear = r2_score(y_true, y_pred_linear)

<h1>ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆå¤šé …å¼ç‰¹å¾´é‡ï¼‰</h1>
model_poly = LinearRegression()
model_poly.fit(X_poly, y_true)
y_pred_poly = model_poly.predict(X_poly)
r2_poly = r2_score(y_true, y_pred_poly)

print(f"\nç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆå…ƒç‰¹å¾´é‡ï¼‰RÂ²: {r2_linear:.4f}")
print(f"ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆå¤šé …å¼ç‰¹å¾´é‡ï¼‰RÂ²: {r2_poly:.4f}")
print(f"æ”¹å–„ç‡: {(r2_poly - r2_linear) / r2_linear * 100:.1f}%")</code></pre>

<h3>äº¤äº’ä½œç”¨é …ã®ç”Ÿæˆ</h3>

<pre><code class="language-python">from itertools import combinations

def create_interaction_features(df):
    """
    äº¤äº’ä½œç”¨é …ã®ç”Ÿæˆ

    Parameters:
    -----------
    df : pd.DataFrame
        å…ƒã®ç‰¹å¾´é‡

    Returns:
    --------
    df_with_interactions : äº¤äº’ä½œç”¨é …ã‚’è¿½åŠ ã—ãŸDataFrame
    """
    df_new = df.copy()

    # 2å¤‰æ•°ã®äº¤äº’ä½œç”¨ï¼ˆç©ï¼‰
    for col1, col2 in combinations(df.columns, 2):
        interaction_name = f"{col1}Ã—{col2}"
        df_new[interaction_name] = df[col1] * df[col2]

    return df_new

<h1>ä¾‹ï¼šç†±é›»ææ–™ã®ç‰¹å¾´é‡</h1>
thermoelectric_features = pd.DataFrame({
    'é›»æ°—ä¼å°åº¦': np.random.lognormal(8, 1, 50),
    'ã‚¼ãƒ¼ãƒ™ãƒƒã‚¯ä¿‚æ•°': np.random.normal(200, 50, 50),
    'ç†±ä¼å°åº¦': np.random.uniform(1, 10, 50)
})

<h1>äº¤äº’ä½œç”¨é …è¿½åŠ </h1>
features_with_interactions = create_interaction_features(
    thermoelectric_features
)

print(f"å…ƒã®ç‰¹å¾´é‡: {thermoelectric_features.columns.tolist()}")
print(f"\nè¿½åŠ ã•ã‚ŒãŸäº¤äº’ä½œç”¨é …:")
new_cols = [col for col in features_with_interactions.columns
            if col not in thermoelectric_features.columns]
for col in new_cols:
    print(f"  {col}")

<h1>ZTå€¤äºˆæ¸¬ï¼ˆZT = Ïƒ*SÂ²/Îº ã«è¿‘ã„ï¼‰</h1>
thermoelectric_features['ZT'] = (
    thermoelectric_features['é›»æ°—ä¼å°åº¦'] *
    thermoelectric_features['ã‚¼ãƒ¼ãƒ™ãƒƒã‚¯ä¿‚æ•°']**2 /
    thermoelectric_features['ç†±ä¼å°åº¦'] / 1e6 +
    np.random.normal(0, 0.1, 50)
)

print(f"\nç›¸é–¢åˆ†æï¼ˆäº¤äº’ä½œç”¨é …ã¨ã®ç›¸é–¢ï¼‰ï¼š")
correlations = features_with_interactions.corrwith(
    thermoelectric_features['ZT']
).sort_values(ascending=False)
print(correlations)</code></pre>

---

<h2>2.3 æ¬¡å…ƒå‰Šæ¸›</h2>

é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä½æ¬¡å…ƒã«åœ§ç¸®ã—ã€å¯è¦–åŒ–ãƒ»è§£é‡ˆã‚’å®¹æ˜“ã«ã—ã¾ã™ã€‚

<h3>PCA (Principal Component Analysis)</h3>

<pre><code class="language-python">from sklearn.decomposition import PCA

def apply_pca(X, n_components=2):
    """
    PCAã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›

    Parameters:
    -----------
    X : array-like
        å…ƒã®ç‰¹å¾´é‡
    n_components : int
        å‰Šæ¸›å¾Œã®æ¬¡å…ƒæ•°

    Returns:
    --------
    X_pca : ä¸»æˆåˆ†å¾—ç‚¹
    pca : PCãŠ object
    """
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X)

    return X_pca, pca

<h1>é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ100æ¬¡å…ƒï¼‰</h1>
np.random.seed(42)
n_samples = 200
n_features = 100

<h1>æ½œåœ¨çš„ãª2æ¬¡å…ƒæ§‹é€ ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿</h1>
latent = np.random.randn(n_samples, 2)
X_high_dim = latent @ np.random.randn(2, n_features) + np.random.randn(n_samples, n_features) * 0.5

<h1>PCAé©ç”¨</h1>
X_pca, pca_model = apply_pca(X_high_dim, n_components=10)

<h1>å¯„ä¸ç‡</h1>
explained_var = pca_model.explained_variance_ratio_

<h1>å¯è¦–åŒ–</h1>
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

<h1>å¯„ä¸ç‡</h1>
axes[0].bar(range(1, 11), explained_var * 100,
            color='steelblue', alpha=0.7)
axes[0].plot(range(1, 11), np.cumsum(explained_var) * 100,
             'ro-', linewidth=2, label='ç´¯ç©å¯„ä¸ç‡')
axes[0].set_xlabel('ä¸»æˆåˆ†', fontsize=12)
axes[0].set_ylabel('å¯„ä¸ç‡ (%)', fontsize=12)
axes[0].set_title('PCAå¯„ä¸ç‡', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

<h1>2æ¬¡å…ƒãƒ—ãƒ­ãƒƒãƒˆ</h1>
axes[1].scatter(X_pca[:, 0], X_pca[:, 1],
                c='steelblue', s=50, alpha=0.6, edgecolors='k')
axes[1].set_xlabel('PC1', fontsize=12)
axes[1].set_ylabel('PC2', fontsize=12)
axes[1].set_title('PCAå¯è¦–åŒ–ï¼ˆ2æ¬¡å…ƒï¼‰', fontsize=13, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(f"å…ƒã®æ¬¡å…ƒæ•°: {n_features}")
print(f"å‰Šæ¸›å¾Œã®æ¬¡å…ƒæ•°: {X_pca.shape[1]}")
print(f"PC1-PC2ã®ç´¯ç©å¯„ä¸ç‡: {np.sum(explained_var[:2]) * 100:.2f}%")
print(f"PC1-PC10ã®ç´¯ç©å¯„ä¸ç‡: {np.sum(explained_var) * 100:.2f}%")</code></pre>

<h3>t-SNE, UMAP</h3>

<pre><code class="language-python">from sklearn.manifold import TSNE
<h1>!pip install umap-learn</h1>
from umap import UMAP

def compare_dimensionality_reduction(X, labels=None):
    """
    PCA, t-SNE, UMAPã®æ¯”è¼ƒ
    """
    # PCA
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)

    # t-SNE
    tsne = TSNE(n_components=2, random_state=42)
    X_tsne = tsne.fit_transform(X)

    # UMAP
    umap_model = UMAP(n_components=2, random_state=42)
    X_umap = umap_model.fit_transform(X)

    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    # PCA
    axes[0].scatter(X_pca[:, 0], X_pca[:, 1],
                    c=labels, cmap='viridis', s=50, alpha=0.6)
    axes[0].set_xlabel('PC1', fontsize=11)
    axes[0].set_ylabel('PC2', fontsize=11)
    axes[0].set_title('PCA', fontsize=12, fontweight='bold')
    axes[0].grid(alpha=0.3)

    # t-SNE
    axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1],
                    c=labels, cmap='viridis', s=50, alpha=0.6)
    axes[1].set_xlabel('t-SNE1', fontsize=11)
    axes[1].set_ylabel('t-SNE2', fontsize=11)
    axes[1].set_title('t-SNE', fontsize=12, fontweight='bold')
    axes[1].grid(alpha=0.3)

    # UMAP
    im = axes[2].scatter(X_umap[:, 0], X_umap[:, 1],
                         c=labels, cmap='viridis', s=50, alpha=0.6)
    axes[2].set_xlabel('UMAP1', fontsize=11)
    axes[2].set_ylabel('UMAP2', fontsize=11)
    axes[2].set_title('UMAP', fontsize=12, fontweight='bold')
    axes[2].grid(alpha=0.3)

    if labels is not None:
        plt.colorbar(im, ax=axes[2], label='ãƒ©ãƒ™ãƒ«')

    plt.tight_layout()
    plt.show()

<h1>ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆ3ã‚¯ãƒ©ã‚¹ï¼‰</h1>
np.random.seed(42)
class1 = np.random.randn(100, 50) + [2, 2] + np.zeros(48)
class2 = np.random.randn(100, 50) + [-2, 2] + np.zeros(48)
class3 = np.random.randn(100, 50) + [0, -2] + np.zeros(48)

X_multi_class = np.vstack([class1, class2, class3])
labels = np.array([0]*100 + [1]*100 + [2]*100)

<h1>æ¯”è¼ƒ</h1>
compare_dimensionality_reduction(X_multi_class, labels)

print("æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã®ç‰¹å¾´ï¼š")
print("PCA: ç·šå½¢å¤‰æ›ã€å¤§åŸŸçš„æ§‹é€ ä¿æŒã€é«˜é€Ÿ")
print("t-SNE: éç·šå½¢å¤‰æ›ã€å±€æ‰€çš„æ§‹é€ ä¿æŒã€é…ã„")
print("UMAP: éç·šå½¢å¤‰æ›ã€å¤§åŸŸ+å±€æ‰€æ§‹é€ ä¿æŒã€ä¸­é€Ÿ")</code></pre>

<h3>LDA (Linear Discriminant Analysis)</h3>

<pre><code class="language-python">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

def apply_lda(X, y, n_components=2):
    """
    LDAã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›ï¼ˆæ•™å¸«ã‚ã‚Šï¼‰

    Parameters:
    -----------
    X : array-like
        ç‰¹å¾´é‡
    y : array-like
        ãƒ©ãƒ™ãƒ«

    Returns:
    --------
    X_lda : LDAå¤‰æ›å¾Œã®ç‰¹å¾´é‡
    lda : LDAãƒ¢ãƒ‡ãƒ«
    """
    lda = LinearDiscriminantAnalysis(n_components=n_components)
    X_lda = lda.fit_transform(X, y)

    return X_lda, lda

<h1>LDAé©ç”¨</h1>
X_lda, lda_model = apply_lda(X_multi_class, labels, n_components=2)

<h1>PCA vs LDAæ¯”è¼ƒ</h1>
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

<h1>PCAï¼ˆæ•™å¸«ãªã—ï¼‰</h1>
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_multi_class)

axes[0].scatter(X_pca[:, 0], X_pca[:, 1],
                c=labels, cmap='viridis', s=50, alpha=0.6)
axes[0].set_xlabel('PC1', fontsize=11)
axes[0].set_ylabel('PC2', fontsize=11)
axes[0].set_title('PCAï¼ˆæ•™å¸«ãªã—ï¼‰', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3)

<h1>LDAï¼ˆæ•™å¸«ã‚ã‚Šï¼‰</h1>
im = axes[1].scatter(X_lda[:, 0], X_lda[:, 1],
                     c=labels, cmap='viridis', s=50, alpha=0.6)
axes[1].set_xlabel('LD1', fontsize=11)
axes[1].set_ylabel('LD2', fontsize=11)
axes[1].set_title('LDAï¼ˆæ•™å¸«ã‚ã‚Šï¼‰', fontsize=12, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.colorbar(im, ax=axes[1], label='ã‚¯ãƒ©ã‚¹')
plt.tight_layout()
plt.show()

print("LDAã®åˆ©ç‚¹ï¼š")
print("- ã‚¯ãƒ©ã‚¹åˆ†é›¢ã‚’æœ€å¤§åŒ–ã™ã‚‹å°„å½±è»¸ã‚’è¦‹ã¤ã‘ã‚‹")
print("- åˆ†é¡å•é¡Œã«é©ã—ã¦ã„ã‚‹")
print(f"- æœ€å¤§æ¬¡å…ƒæ•°: min(n_features, n_classes-1) = {lda_model.n_components}")</code></pre>

---

<h2>2.4 ç‰¹å¾´é‡é¸æŠ</h2>

é‡è¦ãªç‰¹å¾´é‡ã®ã¿ã‚’é¸æŠã—ã€ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã¨è§£é‡ˆæ€§ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚

<h3>Filteræ³•ï¼šç›¸é–¢ä¿‚æ•°ã€åˆ†æ•£åˆ†æ</h3>

<pre><code class="language-python">from sklearn.feature_selection import (
    VarianceThreshold,
    SelectKBest,
    f_regression,
    mutual_info_regression
)

def filter_method_selection(X, y, k=10):
    """
    Filteræ³•ã«ã‚ˆã‚‹ç‰¹å¾´é‡é¸æŠ

    Parameters:
    -----------
    X : pd.DataFrame
        ç‰¹å¾´é‡
    y : array-like
        ç›®çš„å¤‰æ•°

    Returns:
    --------
    selected_features : é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡å
    scores : å„ç‰¹å¾´é‡ã®ã‚¹ã‚³ã‚¢
    """
    # ä½åˆ†æ•£ç‰¹å¾´é‡é™¤å»
    var_threshold = VarianceThreshold(threshold=0.01)
    X_var = var_threshold.fit_transform(X)
    selected_by_var = X.columns[var_threshold.get_support()]

    # Få€¤çµ±è¨ˆé‡
    selector_f = SelectKBest(f_regression, k=k)
    selector_f.fit(X, y)
    scores_f = selector_f.scores_
    selected_by_f = X.columns[selector_f.get_support()]

    # ç›¸äº’æƒ…å ±é‡
    selector_mi = SelectKBest(mutual_info_regression, k=k)
    selector_mi.fit(X, y)
    scores_mi = selector_mi.scores_
    selected_by_mi = X.columns[selector_mi.get_support()]

    return {
        'variance': selected_by_var,
        'f_stat': selected_by_f,
        'mutual_info': selected_by_mi,
        'scores_f': scores_f,
        'scores_mi': scores_mi
    }

<h1>ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿</h1>
np.random.seed(42)
n_samples = 200
X_data = pd.DataFrame(
    np.random.randn(n_samples, 30),
    columns=[f'feature_{i}' for i in range(30)]
)

<h1>ç›®çš„å¤‰æ•°ï¼ˆä¸€éƒ¨ã®ç‰¹å¾´é‡ã®ã¿é–¢é€£ï¼‰</h1>
y_data = (
    2 * X_data['feature_0'] +
    3 * X_data['feature_5'] -
    1.5 * X_data['feature_10'] +
    np.random.normal(0, 0.5, n_samples)
)

<h1>Filteræ³•å®Ÿè¡Œ</h1>
selection_results = filter_method_selection(X_data, y_data, k=10)

<h1>ã‚¹ã‚³ã‚¢å¯è¦–åŒ–</h1>
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

<h1>Få€¤ã‚¹ã‚³ã‚¢</h1>
axes[0].bar(range(len(selection_results['scores_f'])),
            selection_results['scores_f'],
            color='steelblue', alpha=0.7)
axes[0].set_xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=11)
axes[0].set_ylabel('Få€¤ã‚¹ã‚³ã‚¢', fontsize=11)
axes[0].set_title('Få€¤çµ±è¨ˆé‡ã«ã‚ˆã‚‹ç‰¹å¾´é‡è©•ä¾¡', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3)

<h1>ç›¸äº’æƒ…å ±é‡ã‚¹ã‚³ã‚¢</h1>
axes[1].bar(range(len(selection_results['scores_mi'])),
            selection_results['scores_mi'],
            color='coral', alpha=0.7)
axes[1].set_xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=11)
axes[1].set_ylabel('ç›¸äº’æƒ…å ±é‡', fontsize=11)
axes[1].set_title('ç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹ç‰¹å¾´é‡è©•ä¾¡', fontsize=12, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("Få€¤çµ±è¨ˆé‡ã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡:")
print(selection_results['f_stat'].tolist())
print("\nç›¸äº’æƒ…å ±é‡ã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡:")
print(selection_results['mutual_info'].tolist())</code></pre>

<h3>Wrapperæ³•ï¼šRFE</h3>

<pre><code class="language-python">from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor

def rfe_selection(X, y, n_features_to_select=10):
    """
    RFEï¼ˆRecursive Feature Eliminationï¼‰
    """
    estimator = RandomForestRegressor(n_estimators=50, random_state=42)
    selector = RFE(estimator, n_features_to_select=n_features_to_select)
    selector.fit(X, y)

    selected_features = X.columns[selector.support_]
    feature_ranking = selector.ranking_

    return selected_features, feature_ranking

<h1>RFEå®Ÿè¡Œ</h1>
selected_rfe, ranking_rfe = rfe_selection(X_data, y_data, n_features_to_select=10)

<h1>ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¯è¦–åŒ–</h1>
plt.figure(figsize=(12, 6))
plt.bar(range(len(ranking_rfe)), ranking_rfe,
        color='steelblue', alpha=0.7)
plt.axhline(y=1, color='red', linestyle='--',
            label='é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ (rank=1)', linewidth=2)
plt.xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=12)
plt.ylabel('ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä½ã„ã»ã©é‡è¦ï¼‰', fontsize=12)
plt.title('RFEã«ã‚ˆã‚‹ç‰¹å¾´é‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print("RFEã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡:")
print(selected_rfe.tolist())</code></pre>

<h3>Embeddedæ³•ï¼šLasso, Random Forest importances</h3>

<pre><code class="language-python">from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestRegressor

def embedded_selection(X, y):
    """
    Embeddedæ³•ï¼ˆLasso + Random Forestï¼‰
    """
    # Lassoï¼ˆL1æ­£å‰‡åŒ–ï¼‰
    lasso = Lasso(alpha=0.1, random_state=42)
    lasso.fit(X, y)
    lasso_coefs = np.abs(lasso.coef_)
    selected_lasso = X.columns[lasso_coefs > 0]

    # Random Forest importances
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X, y)
    rf_importances = rf.feature_importances_
    # ä¸Šä½10å€‹é¸æŠ
    top_10_idx = np.argsort(rf_importances)[-10:]
    selected_rf = X.columns[top_10_idx]

    return {
        'lasso': selected_lasso,
        'lasso_coefs': lasso_coefs,
        'rf': selected_rf,
        'rf_importances': rf_importances
    }

<h1>Embeddedæ³•å®Ÿè¡Œ</h1>
embedded_results = embedded_selection(X_data, y_data)

<h1>å¯è¦–åŒ–</h1>
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

<h1>Lassoä¿‚æ•°</h1>
axes[0].bar(range(len(embedded_results['lasso_coefs'])),
            embedded_results['lasso_coefs'],
            color='steelblue', alpha=0.7)
axes[0].set_xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=11)
axes[0].set_ylabel('|Lassoä¿‚æ•°|', fontsize=11)
axes[0].set_title('Lassoç‰¹å¾´é‡é¸æŠ', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3)

<h1>Random Forest importances</h1>
axes[1].bar(range(len(embedded_results['rf_importances'])),
            embedded_results['rf_importances'],
            color='coral', alpha=0.7)
axes[1].set_xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=11)
axes[1].set_ylabel('Feature Importance', fontsize=11)
axes[1].set_title('Random Foresté‡è¦åº¦', fontsize=12, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("Lassoã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡:")
print(embedded_results['lasso'].tolist())
print("\nRandom Forestã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ï¼ˆä¸Šä½10ï¼‰:")
print(embedded_results['rf'].tolist())</code></pre>

<h3>SHAP-based selection</h3>

<pre><code class="language-python">import shap

def shap_based_selection(X, y, top_k=10):
    """
    SHAPã«ã‚ˆã‚‹ç‰¹å¾´é‡é¸æŠ
    """
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X, y)

    # SHAPå€¤è¨ˆç®—
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X)

    # å¹³å‡çµ¶å¯¾SHAPå€¤ã§é‡è¦åº¦è©•ä¾¡
    mean_abs_shap = np.abs(shap_values).mean(axis=0)

    # ä¸Šä½kå€‹é¸æŠ
    top_k_idx = np.argsort(mean_abs_shap)[-top_k:]
    selected_features = X.columns[top_k_idx]

    return selected_features, mean_abs_shap, shap_values

<h1>SHAPé¸æŠ</h1>
selected_shap, mean_shap, shap_vals = shap_based_selection(X_data, y_data, top_k=10)

<h1>SHAP Summary Plot</h1>
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_vals, X_data, plot_type="bar", show=False)
plt.title('SHAPç‰¹å¾´é‡é‡è¦åº¦', fontsize=13, fontweight='bold')
plt.tight_layout()
plt.show()

print("SHAPã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ï¼ˆä¸Šä½10ï¼‰:")
print(selected_shap.tolist())

<h1>æ‰‹æ³•æ¯”è¼ƒ</h1>
print("\nç‰¹å¾´é‡é¸æŠæ‰‹æ³•ã®æ¯”è¼ƒï¼š")
print(f"Filteræ³•ï¼ˆFå€¤ï¼‰: {len(selection_results['f_stat'])} ç‰¹å¾´é‡")
print(f"Wrapperæ³•ï¼ˆRFEï¼‰: {len(selected_rfe)} ç‰¹å¾´é‡")
print(f"Embeddedæ³•ï¼ˆLassoï¼‰: {len(embedded_results['lasso'])} ç‰¹å¾´é‡")
print(f"SHAP-based: {len(selected_shap)} ç‰¹å¾´é‡")</code></pre>

---

<h2>2.5 ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ï¼šãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬</h2>

å®Ÿéš›ã®ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã§ã€200æ¬¡å…ƒã‹ã‚‰20æ¬¡å…ƒã¸ã®åŠ¹æœçš„ãªå‰Šæ¸›ã‚’å®Ÿè·µã—ã¾ã™ã€‚

<pre><code class="language-python"><h1>ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰</h1>
np.random.seed(42)
n_materials = 500

<h1>200æ¬¡å…ƒã®ææ–™è¨˜è¿°å­ï¼ˆçµ„æˆãƒ»æ§‹é€ ãƒ»é›»å­æ§‹é€ ï¼‰</h1>
descriptor_names = (
    [f'çµ„æˆ_{i}' for i in range(80)] +
    [f'æ§‹é€ _{i}' for i in range(60)] +
    [f'é›»å­_{i}' for i in range(60)]
)

X_bandgap = pd.DataFrame(
    np.random.randn(n_materials, 200),
    columns=descriptor_names
)

<h1>ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ï¼ˆä¸€éƒ¨ã®è¨˜è¿°å­ã®ã¿ä¾å­˜ï¼‰</h1>
important_features = [
    'çµ„æˆ_5', 'çµ„æˆ_12', 'çµ„æˆ_25',
    'æ§‹é€ _10', 'æ§‹é€ _23',
    'é›»å­_8', 'é›»å­_15', 'é›»å­_30'
]

y_bandgap = np.zeros(n_materials)
for feat in important_features:
    idx = descriptor_names.index(feat)
    y_bandgap += np.random.uniform(0.5, 1.5) * X_bandgap[feat]

y_bandgap = np.abs(y_bandgap) + np.random.normal(0, 0.3, n_materials)
y_bandgap = np.clip(y_bandgap, 0, 6)  # 0-6 eVã®ç¯„å›²

print("=== ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ===")
print(f"ææ–™æ•°: {n_materials}")
print(f"ç‰¹å¾´é‡æ•°: {X_bandgap.shape[1]}")
print(f"ç›®æ¨™: 200æ¬¡å…ƒ â†’ 20æ¬¡å…ƒã«å‰Šæ¸›")</code></pre>

<h3>Step 1: Filteræ³•ã§100æ¬¡å…ƒã«å‰Šæ¸›</h3>

<pre><code class="language-python"><h1>ç›¸äº’æƒ…å ±é‡ã§ãƒˆãƒƒãƒ—100é¸æŠ</h1>
selector_mi = SelectKBest(mutual_info_regression, k=100)
X_filtered = selector_mi.fit_transform(X_bandgap, y_bandgap)
selected_features_100 = X_bandgap.columns[selector_mi.get_support()]

print(f"\nStep 1: Filteræ³•ï¼ˆç›¸äº’æƒ…å ±é‡ï¼‰")
print(f"200æ¬¡å…ƒ â†’ {X_filtered.shape[1]}æ¬¡å…ƒ")</code></pre>

<h3>Step 2: PCAã§50æ¬¡å…ƒã«å‰Šæ¸›</h3>

<pre><code class="language-python"><h1>PCAé©ç”¨</h1>
pca = PCA(n_components=50)
X_pca = pca.fit_transform(X_filtered)

<h1>ç´¯ç©å¯„ä¸ç‡</h1>
cumsum_var = np.cumsum(pca.explained_variance_ratio_)

<h1>90%ç´¯ç©å¯„ä¸ç‡ã‚’é”æˆã™ã‚‹æ¬¡å…ƒæ•°</h1>
n_components_90 = np.argmax(cumsum_var >= 0.90) + 1

plt.figure(figsize=(10, 6))
plt.plot(range(1, 51), cumsum_var * 100, 'b-', linewidth=2)
plt.axhline(y=90, color='red', linestyle='--',
            label='90%ç´¯ç©å¯„ä¸ç‡', linewidth=2)
plt.axvline(x=n_components_90, color='green', linestyle='--',
            label=f'{n_components_90}æ¬¡å…ƒã§90%é”æˆ', linewidth=2)
plt.xlabel('ä¸»æˆåˆ†æ•°', fontsize=12)
plt.ylabel('ç´¯ç©å¯„ä¸ç‡ (%)', fontsize=12)
plt.title('PCAã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nStep 2: PCA")
print(f"100æ¬¡å…ƒ â†’ {X_pca.shape[1]}æ¬¡å…ƒ")
print(f"90%ç´¯ç©å¯„ä¸ç‡é”æˆ: {n_components_90}æ¬¡å…ƒ")</code></pre>

<h3>Step 3: Random Forest Importanceã§20æ¬¡å…ƒã«å‰Šæ¸›</h3>

<pre><code class="language-python"><h1>50æ¬¡å…ƒã®PCAç‰¹å¾´é‡ã§Random Forestè¨“ç·´</h1>
X_pca_df = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(50)])

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_pca_df, y_bandgap)

<h1>é‡è¦åº¦ãƒˆãƒƒãƒ—20é¸æŠ</h1>
importances = rf.feature_importances_
top_20_idx = np.argsort(importances)[-20:]
X_final = X_pca_df.iloc[:, top_20_idx]

<h1>é‡è¦åº¦å¯è¦–åŒ–</h1>
plt.figure(figsize=(12, 6))
plt.bar(range(50), importances, color='steelblue', alpha=0.7)
plt.bar(top_20_idx, importances[top_20_idx],
        color='coral', alpha=0.9, label='é¸æŠã•ã‚ŒãŸ20æ¬¡å…ƒ')
plt.xlabel('ä¸»æˆåˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', fontsize=12)
plt.ylabel('Feature Importance', fontsize=12)
plt.title('Random Forestã«ã‚ˆã‚‹æœ€çµ‚é¸æŠ', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nStep 3: Random Forest Importance")
print(f"50æ¬¡å…ƒ â†’ {X_final.shape[1]}æ¬¡å…ƒ")
print(f"\næœ€çµ‚é¸æŠã•ã‚ŒãŸä¸»æˆåˆ†:")
print(X_final.columns.tolist())</code></pre>

<h3>Step 4: äºˆæ¸¬æ€§èƒ½ã®æ¤œè¨¼</h3>

<pre><code class="language-python">from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_absolute_error, r2_score

def evaluate_dimension_reduction(X, y, name):
    """
    æ¬¡å…ƒå‰Šæ¸›å¾Œã®äºˆæ¸¬æ€§èƒ½è©•ä¾¡
    """
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # äº¤å·®æ¤œè¨¼
    cv_scores = cross_val_score(
        model, X, y, cv=5,
        scoring='neg_mean_absolute_error'
    )
    cv_mae = -cv_scores.mean()

    return {
        'name': name,
        'dimensions': X.shape[1],
        'MAE': mae,
        'R2': r2,
        'CV_MAE': cv_mae
    }

<h1>å„æ®µéšã®æ€§èƒ½è©•ä¾¡</h1>
results = []

<h1>å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆ200æ¬¡å…ƒï¼‰</h1>
results.append(evaluate_dimension_reduction(X_bandgap, y_bandgap, 'å…ƒãƒ‡ãƒ¼ã‚¿'))

<h1>Filterå¾Œï¼ˆ100æ¬¡å…ƒï¼‰</h1>
X_filtered_df = pd.DataFrame(X_filtered)
results.append(evaluate_dimension_reduction(X_filtered_df, y_bandgap, 'Filteræ³•'))

<h1>PCAå¾Œï¼ˆ50æ¬¡å…ƒï¼‰</h1>
results.append(evaluate_dimension_reduction(X_pca_df, y_bandgap, 'PCA'))

<h1>æœ€çµ‚ï¼ˆ20æ¬¡å…ƒï¼‰</h1>
results.append(evaluate_dimension_reduction(X_final, y_bandgap, 'æœ€çµ‚é¸æŠ'))

<h1>çµæœè¡¨ç¤º</h1>
results_df = pd.DataFrame(results)
print("\n=== æ¬¡å…ƒå‰Šæ¸›ã®å½±éŸ¿è©•ä¾¡ ===")
print(results_df.to_string(index=False))

<h1>å¯è¦–åŒ–</h1>
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

<h1>MAEæ¯”è¼ƒ</h1>
axes[0].bar(results_df['name'], results_df['MAE'],
            color=['gray', 'steelblue', 'coral', 'green'], alpha=0.7)
axes[0].set_ylabel('MAE (eV)', fontsize=12)
axes[0].set_title('äºˆæ¸¬èª¤å·®ï¼ˆMAEï¼‰', fontsize=13, fontweight='bold')
axes[0].tick_params(axis='x', rotation=15)
axes[0].grid(axis='y', alpha=0.3)

<h1>RÂ²æ¯”è¼ƒ</h1>
axes[1].bar(results_df['name'], results_df['R2'],
            color=['gray', 'steelblue', 'coral', 'green'], alpha=0.7)
axes[1].set_ylabel('RÂ²', fontsize=12)
axes[1].set_ylim(0, 1)
axes[1].set_title('æ±ºå®šä¿‚æ•°ï¼ˆRÂ²ï¼‰', fontsize=13, fontweight='bold')
axes[1].tick_params(axis='x', rotation=15)
axes[1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\næ€§èƒ½ç¶­æŒç‡ï¼ˆæœ€çµ‚20æ¬¡å…ƒ vs å…ƒ200æ¬¡å…ƒï¼‰:")
print(f"RÂ²ç¶­æŒç‡: {results_df.iloc[3]['R2'] / results_df.iloc[0]['R2'] * 100:.1f}%")
print(f"æ¬¡å…ƒå‰Šæ¸›ç‡: {(1 - 20/200) * 100:.0f}%")</code></pre>

<h3>ç‰©ç†çš„æ„å‘³ã¥ã‘</h3>

<pre><code class="language-python"><h1>é¸æŠã•ã‚ŒãŸ20æ¬¡å…ƒã¨å…ƒã®è¨˜è¿°å­ã®å¯¾å¿œã‚’åˆ†æ</h1>
def interpret_selected_components(pca_model, original_features, selected_pcs):
    """
    é¸æŠã•ã‚ŒãŸä¸»æˆåˆ†ã®ç‰©ç†çš„è§£é‡ˆ
    """
    # PCA loadingsï¼ˆä¸»æˆåˆ†è² è·é‡ï¼‰
    loadings = pca_model.components_.T

    interpretations = []

    for pc_name in selected_pcs:
        pc_idx = int(pc_name.replace('PC', '')) - 1

        # ã“ã®ä¸»æˆåˆ†ã¸ã®å¯„ä¸ãŒå¤§ãã„å…ƒã®ç‰¹å¾´é‡
        loading_vector = np.abs(loadings[:, pc_idx])
        top_5_idx = np.argsort(loading_vector)[-5:]

        top_features = [original_features[i] for i in top_5_idx]
        top_loadings = loading_vector[top_5_idx]

        interpretations.append({
            'PC': pc_name,
            'Top_Features': top_features,
            'Loadings': top_loadings
        })

    return interpretations

<h1>è§£é‡ˆå®Ÿè¡Œ</h1>
selected_pc_names = X_final.columns.tolist()
interpretations = interpret_selected_components(
    pca,
    selected_features_100.tolist(),
    selected_pc_names[:5]  # æœ€åˆã®5å€‹ã®ã¿è¡¨ç¤º
)

print("\n=== é¸æŠã•ã‚ŒãŸä¸»æˆåˆ†ã®ç‰©ç†çš„è§£é‡ˆ ===")
for interp in interpretations:
    print(f"\n{interp['PC']}:")
    for feat, loading in zip(interp['Top_Features'], interp['Loadings']):
        print(f"  {feat}: {loading:.3f}")</code></pre>

---

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦: easyï¼‰</h3>

matminerã‚’ä½¿ã£ã¦ã€åŒ–å­¦å¼"Fe2O3"ã¨"TiO2"ã®ææ–™è¨˜è¿°å­ã‚’ç”Ÿæˆã—ã€ã©ã®è¨˜è¿°å­ãŒæœ€ã‚‚ç•°ãªã‚‹ã‹ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

1. <code>ElementProperty.from_preset("magpie")</code>ã‚’ä½¿ç”¨
2. å„åŒ–å­¦å¼ã§ç‰¹å¾´é‡ç”Ÿæˆ
3. å·®ã®çµ¶å¯¾å€¤ã‚’è¨ˆç®—ã—ã€ä¸Šä½10å€‹ã‚’è¡¨ç¤º

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from matminer.featurizers.composition import ElementProperty
from pymatgen.core import Composition

<h1>è¨˜è¿°å­ç”Ÿæˆ</h1>
ep_feat = ElementProperty.from_preset("magpie")

comp1 = Composition("Fe2O3")
comp2 = Composition("TiO2")

features1 = ep_feat.featurize(comp1)
features2 = ep_feat.featurize(comp2)

feature_names = ep_feat.feature_labels()

<h1>å·®åˆ†è¨ˆç®—</h1>
df_comparison = pd.DataFrame({
    'Feature': feature_names,
    'Fe2O3': features1,
    'TiO2': features2,
    'Difference': np.abs(np.array(features1) - np.array(features2))
})

df_comparison_sorted = df_comparison.sort_values(
    'Difference', ascending=False
)

print("æœ€ã‚‚ç•°ãªã‚‹è¨˜è¿°å­ï¼ˆä¸Šä½10ï¼‰:")
print(df_comparison_sorted.head(10).to_string(index=False))</code></pre>

</details>

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦: mediumï¼‰</h3>

PCAã¨UMAPã‚’ç”¨ã„ã¦ã€é«˜æ¬¡å…ƒææ–™ãƒ‡ãƒ¼ã‚¿ã‚’2æ¬¡å…ƒã«å‰Šæ¸›ã—ã€ã©ã¡ã‚‰ãŒã‚¯ãƒ©ã‚¹ã‚¿æ§‹é€ ã‚’ã‚ˆã‚Šæ˜ç¢ºã«å¯è¦–åŒ–ã§ãã‚‹ã‹æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from sklearn.decomposition import PCA
from umap import UMAP
from sklearn.datasets import make_blobs

<h1>ã‚¯ãƒ©ã‚¹ã‚¿æ§‹é€ ã‚’æŒã¤é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ</h1>
X, y = make_blobs(n_samples=300, n_features=50,
                  centers=3, random_state=42)

<h1>PCA</h1>
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

<h1>UMAP</h1>
umap_model = UMAP(n_components=2, random_state=42)
X_umap = umap_model.fit_transform(X)

<h1>å¯è¦–åŒ–</h1>
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

axes[0].scatter(X_pca[:, 0], X_pca[:, 1],
                c=y, cmap='viridis', s=50, alpha=0.6)
axes[0].set_title('PCA', fontsize=12, fontweight='bold')

axes[1].scatter(X_umap[:, 0], X_umap[:, 1],
                c=y, cmap='viridis', s=50, alpha=0.6)
axes[1].set_title('UMAP', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()</code></pre>

</details>

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦: hardï¼‰</h3>

Filteræ³•ã€Wrapperæ³•ã€Embeddedæ³•ã€SHAP-basedã®4ã¤ã®ç‰¹å¾´é‡é¸æŠæ‰‹æ³•ã‚’ç”¨ã„ã¦ã€åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ãã‚Œãã‚Œä¸Šä½10ç‰¹å¾´é‡ã‚’é¸æŠã—ã€é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã®é‡è¤‡åº¦ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Lasso
import shap

<h1>ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿</h1>
np.random.seed(42)
X = pd.DataFrame(np.random.randn(200, 30),
                 columns=[f'feat_{i}' for i in range(30)])
y = (2*X['feat_0'] + 3*X['feat_5'] - X['feat_10'] +
     np.random.normal(0, 0.5, 200))

<h1>1. Filteræ³•</h1>
selector_filter = SelectKBest(f_regression, k=10)
selector_filter.fit(X, y)
selected_filter = set(X.columns[selector_filter.get_support()])

<h1>2. Wrapperæ³•ï¼ˆRFEï¼‰</h1>
model_rfe = RandomForestRegressor(n_estimators=50, random_state=42)
selector_rfe = RFE(model_rfe, n_features_to_select=10)
selector_rfe.fit(X, y)
selected_rfe = set(X.columns[selector_rfe.support_])

<h1>3. Embeddedæ³•ï¼ˆLassoï¼‰</h1>
lasso = Lasso(alpha=0.1, random_state=42)
lasso.fit(X, y)
lasso_coefs = np.abs(lasso.coef_)
top_10_lasso_idx = np.argsort(lasso_coefs)[-10:]
selected_lasso = set(X.columns[top_10_lasso_idx])

<h1>4. SHAP-based</h1>
rf_shap = RandomForestRegressor(n_estimators=100, random_state=42)
rf_shap.fit(X, y)
explainer = shap.TreeExplainer(rf_shap)
shap_values = explainer.shap_values(X)
mean_abs_shap = np.abs(shap_values).mean(axis=0)
top_10_shap_idx = np.argsort(mean_abs_shap)[-10:]
selected_shap = set(X.columns[top_10_shap_idx])

<h1>é‡è¤‡åˆ†æ</h1>
all_methods = {
    'Filter': selected_filter,
    'Wrapper': selected_rfe,
    'Embedded': selected_lasso,
    'SHAP': selected_shap
}

<h1>ãƒ™ãƒ³å›³çš„ãªé‡è¤‡è¨ˆç®—</h1>
common_all = selected_filter & selected_rfe & selected_lasso & selected_shap

print("å„æ‰‹æ³•ã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡:")
for method, features in all_methods.items():
    print(f"{method}: {sorted(features)}")

print(f"\nå…¨æ‰‹æ³•å…±é€š: {sorted(common_all)}")
print(f"å…±é€šç‰¹å¾´é‡æ•°: {len(common_all)} / 10")</code></pre>

</details>

---

<h2>ã¾ã¨ã‚</h2>

ã“ã®ç« ã§ã¯ã€<strong>ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</strong>ã®å®Ÿè·µæ‰‹æ³•ã‚’å­¦ã³ã¾ã—ãŸã€‚

<strong>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</strong>ï¼š

1. <strong>ææ–™è¨˜è¿°å­</strong>ï¼šçµ„æˆãƒ»æ§‹é€ ãƒ»é›»å­æ§‹é€ è¨˜è¿°å­ã‚’matminerã§åŠ¹ç‡çš„ã«ç”Ÿæˆ
2. <strong>ç‰¹å¾´é‡å¤‰æ›</strong>ï¼šæ­£è¦åŒ–ãƒ»å¯¾æ•°å¤‰æ›ãƒ»å¤šé …å¼ç‰¹å¾´é‡ã§éç·šå½¢é–¢ä¿‚ã‚’æ‰ãˆã‚‹
3. <strong>æ¬¡å…ƒå‰Šæ¸›</strong>ï¼šPCAã€t-SNEã€UMAPã§å¯è¦–åŒ–ã¨è¨ˆç®—åŠ¹ç‡åŒ–
4. <strong>ç‰¹å¾´é‡é¸æŠ</strong>ï¼šFilter < Wrapper < Embedded < SHAP ã®é †ã§ç²¾åº¦å‘ä¸Š
5. <strong>å®Ÿè·µäº‹ä¾‹</strong>ï¼š200æ¬¡å…ƒâ†’20æ¬¡å…ƒã§æ€§èƒ½ã‚’ç¶­æŒã—ã¤ã¤è§£é‡ˆæ€§å‘ä¸Š

<strong>æ¬¡ç« äºˆå‘Š</strong>ï¼š
Chapter 3ã§ã¯ã€æœ€é©ãªãƒ¢ãƒ‡ãƒ«é¸æŠã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å­¦ã³ã¾ã™ã€‚Optunaã‚’ç”¨ã„ãŸè‡ªå‹•æœ€é©åŒ–ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã§äºˆæ¸¬ç²¾åº¦ã‚’æœ€å¤§åŒ–ã—ã¾ã™ã€‚

---

<h2>å‚è€ƒæ–‡çŒ®</h2>

1. <strong>Ward, L., Dunn, A., Faghaninia, A., et al.</strong> (2018). Matminer: An open source toolkit for materials data mining. *Computational Materials Science*, 152, 60-69. [DOI: 10.1016/j.commatsci.2018.05.018](https://doi.org/10.1016/j.commatsci.2018.05.018)

2. <strong>Jolliffe, I. T. & Cadima, J.</strong> (2016). Principal component analysis: a review and recent developments. *Philosophical Transactions of the Royal Society A*, 374(2065), 20150202. [DOI: 10.1098/rsta.2015.0202](https://doi.org/10.1098/rsta.2015.0202)

3. <strong>McInnes, L., Healy, J., & Melville, J.</strong> (2018). UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. *arXiv preprint arXiv:1802.03426*.

4. <strong>Guyon, I. & Elisseeff, A.</strong> (2003). An introduction to variable and feature selection. *Journal of Machine Learning Research*, 3, 1157-1182.

---

[â† Chapter 1ã«æˆ»ã‚‹](chapter-1.md) | [Chapter 3ã¸é€²ã‚€ â†’](chapter-3.md)
<div class="navigation">
    <a href="chapter-1.html" class="nav-button">â† ç¬¬1ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-3.html" class="nav-button">ç¬¬3ç«  â†’</a>
</div>
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>
