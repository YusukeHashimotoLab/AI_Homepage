<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Á¨¨3Á´†ÔºöÁîªÂÉè„Éá„Éº„ÇøËß£Êûê - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Á¨¨3Á´†ÔºöÁîªÂÉè„Éá„Éº„ÇøËß£Êûê</h1>
            <p class="subtitle">SEM„ÉªTEMÁîªÂÉè„ÅÆËá™ÂãïËß£Êûê - Á≤íÂ≠êÊ§úÂá∫„Åã„ÇâÊ∑±Â±§Â≠¶Áøí„Åæ„Åß</p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 30-35ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ‰∏≠Á¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 13ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 3Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">

<h1>Á¨¨3Á´†ÔºöÁîªÂÉè„Éá„Éº„ÇøËß£Êûê</h1>

<strong>SEM„ÉªTEMÁîªÂÉè„ÅÆËá™ÂãïËß£Êûê - Á≤íÂ≠êÊ§úÂá∫„Åã„ÇâÊ∑±Â±§Â≠¶Áøí„Åæ„Åß</strong>

<h2>Â≠¶ÁøíÁõÆÊ®ô</h2>

„Åì„ÅÆÁ´†„ÇíË™≠„ÇÄ„Åì„Å®„Åß„ÄÅ‰ª•‰∏ã„ÇíÁøíÂæó„Åß„Åç„Åæ„ÅôÔºö

- ‚úÖ SEM„ÉªTEMÁîªÂÉè„ÅÆÂâçÂá¶ÁêÜÔºà„Éé„Ç§„Ç∫Èô§Âéª„ÄÅ„Ç≥„É≥„Éà„É©„Çπ„ÉàË™øÊï¥Ôºâ„ÇíÂÆüË°å„Åß„Åç„Çã
- ‚úÖ WatershedÊ≥ï„Å´„Çà„ÇãÁ≤íÂ≠êÊ§úÂá∫„ÇíÂÆüË£Ö„Åß„Åç„Çã
- ‚úÖ Á≤íÂæÑÂàÜÂ∏É„ÄÅÂΩ¢Áä∂„Éë„É©„É°„Éº„ÇøÔºàÂÜÜÂΩ¢Â∫¶„ÄÅ„Ç¢„Çπ„Éö„ÇØ„ÉàÊØîÔºâ„ÇíÂÆöÈáè„Åß„Åç„Çã
- ‚úÖ CNN„Å´„Çà„ÇãÊùêÊñôÁîªÂÉèÂàÜÈ°ûÔºàËª¢ÁßªÂ≠¶ÁøíÔºâ„ÇíÂÆüË°å„Åß„Åç„Çã
- ‚úÖ OpenCV„Éªscikit-image„Çí‰Ωø„Å£„ÅüÁîªÂÉèËß£Êûê„Éë„Ç§„Éó„É©„Ç§„É≥„ÇíÊßãÁØâ„Åß„Åç„Çã

<strong>Ë™≠‰∫ÜÊôÇÈñì</strong>: 30-35ÂàÜ
<strong>„Ç≥„Éº„Éâ‰æã</strong>: 13ÂÄã
<strong>ÊºîÁøíÂïèÈ°å</strong>: 3Âïè

---

<h2>3.1 ÁîªÂÉè„Éá„Éº„Çø„ÅÆÁâπÂæ¥„Å®ÂâçÂá¶ÁêÜÊà¶Áï•</h2>

<h3>SEM„ÉªTEMÁîªÂÉè„ÅÆÁâπÂæ¥</h3>

ÈõªÂ≠êÈ°ïÂæÆÈè°ÁîªÂÉè„ÅØÊùêÊñô„ÅÆ„Éä„Éé„Äú„Éû„Ç§„ÇØ„É≠„Çπ„Ç±„Éº„É´ÊßãÈÄ†„ÇíÂèØË¶ñÂåñ„Åô„ÇãÂº∑Âäõ„Å™„ÉÑ„Éº„É´„Åß„Åô„ÄÇ

| Ê∏¨ÂÆöÊäÄË°ì | Á©∫ÈñìÂàÜËß£ËÉΩ | ÂÖ∏ÂûãÁöÑ„Å™Ë¶ñÈáé | ‰∏ª„Å™ÊÉÖÂ†± | ÁîªÂÉèÁâπÊÄß |
|---------|----------|-----------|---------|---------|
| <strong>SEM</strong> | Êï∞nmÔΩûÊï∞Œºm | 10ŒºmÔΩû1mm | Ë°®Èù¢ÂΩ¢ÊÖã„ÄÅÁµÑÁπî | Ê∑±„ÅÑË¢´ÂÜôÁïåÊ∑±Â∫¶„ÄÅÂΩ±ÂäπÊûú |
| <strong>TEM</strong> | ÂéüÂ≠ê„É¨„Éô„É´ | Êï∞ÂçÅnmÔΩûÊï∞Œºm | ÂÜÖÈÉ®ÊßãÈÄ†„ÄÅÁµêÊô∂ÊÄß | È´ò„Ç≥„É≥„Éà„É©„Çπ„Éà„ÄÅÂõûÊäòÂÉè |
| <strong>STEM</strong> | „Çµ„Éñnm | Êï∞ÂçÅnmÔΩûÊï∞Áôænm | ÂéüÂ≠êÈÖçÂàó„ÄÅÂÖÉÁ¥†ÂàÜÂ∏É | ÂéüÂ≠êÂàÜËß£ËÉΩ |

<h3>ÁîªÂÉèËß£Êûê„ÅÆÂÖ∏ÂûãÁöÑ„ÉØ„Éº„ÇØ„Éï„É≠„Éº</h3>

<div class="mermaid">flowchart TD
    A[ÁîªÂÉèÂèñÂæó] --> B[ÂâçÂá¶ÁêÜ]
    B --> C[„Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥]
    C --> D[ÁâπÂæ¥ÊäΩÂá∫]
    D --> E[ÂÆöÈáèËß£Êûê]
    E --> F[Áµ±Ë®àÂá¶ÁêÜ]
    F --> G[ÂèØË¶ñÂåñ„ÉªÂ†±Âëä]

    B --> B1[„Éé„Ç§„Ç∫Èô§Âéª]
    B --> B2[„Ç≥„É≥„Éà„É©„Çπ„ÉàË™øÊï¥]
    B --> B3[‰∫åÂÄ§Âåñ]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style G fill:#fff9c4</div>

---

<h2>3.2 ÁîªÂÉèÂâçÂá¶ÁêÜ</h2>

<h3>„Éé„Ç§„Ç∫Èô§Âéª</h3>

<strong>„Ç≥„Éº„Éâ‰æã1: ÂêÑÁ®Æ„Éé„Ç§„Ç∫Èô§Âéª„Éï„Ç£„É´„Çø„ÅÆÊØîËºÉ</strong>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
import cv2
from skimage import filters, io
from scipy import ndimage

<h1>„Çµ„É≥„Éó„É´SEMÁîªÂÉèÁîüÊàêÔºàÁ≤íÂ≠ê„ÇíÊ®°Êì¨Ôºâ</h1>
def generate_synthetic_sem(size=512, num_particles=30):
    """ÂêàÊàêSEMÁîªÂÉè„ÅÆÁîüÊàê"""
    image = np.zeros((size, size), dtype=np.float32)

    # „É©„É≥„ÉÄ„É†„Å™Á≤íÂ≠êÈÖçÁΩÆ
    np.random.seed(42)
    for _ in range(num_particles):
        x = np.random.randint(50, size - 50)
        y = np.random.randint(50, size - 50)
        radius = np.random.randint(15, 35)

        # ÂÜÜÂΩ¢Á≤íÂ≠ê
        Y, X = np.ogrid[:size, :size]
        mask = (X - x)<strong>2 + (Y - y)</strong>2 <= radius**2
        image[mask] = 200

    # „Ç¨„Ç¶„Ç∑„Ç¢„É≥„Éé„Ç§„Ç∫ËøΩÂä†
    noise = np.random.normal(0, 25, image.shape)
    noisy_image = np.clip(image + noise, 0, 255).astype(np.uint8)

    return noisy_image

<h1>ÁîªÂÉèÁîüÊàê</h1>
noisy_image = generate_synthetic_sem()

<h1>ÂêÑÁ®Æ„Éé„Ç§„Ç∫Èô§Âéª„Éï„Ç£„É´„Çø</h1>
gaussian_blur = cv2.GaussianBlur(noisy_image, (5, 5), 1.0)
median_filter = cv2.medianBlur(noisy_image, 5)
bilateral_filter = cv2.bilateralFilter(noisy_image, 9, 75, 75)
nlm_filter = cv2.fastNlMeansDenoising(noisy_image, None, 10, 7, 21)

<h1>ÂèØË¶ñÂåñ</h1>
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

axes[0, 0].imshow(noisy_image, cmap='gray')
axes[0, 0].set_title('Noisy SEM Image')
axes[0, 0].axis('off')

axes[0, 1].imshow(gaussian_blur, cmap='gray')
axes[0, 1].set_title('Gaussian Blur')
axes[0, 1].axis('off')

axes[0, 2].imshow(median_filter, cmap='gray')
axes[0, 2].set_title('Median Filter')
axes[0, 2].axis('off')

axes[1, 0].imshow(bilateral_filter, cmap='gray')
axes[1, 0].set_title('Bilateral Filter')
axes[1, 0].axis('off')

axes[1, 1].imshow(nlm_filter, cmap='gray')
axes[1, 1].set_title('Non-Local Means')
axes[1, 1].axis('off')

<h1>„Éé„Ç§„Ç∫„É¨„Éô„É´ÊØîËºÉ</h1>
axes[1, 2].bar(['Original', 'Gaussian', 'Median', 'Bilateral', 'NLM'],
               [np.std(noisy_image),
                np.std(gaussian_blur),
                np.std(median_filter),
                np.std(bilateral_filter),
                np.std(nlm_filter)])
axes[1, 2].set_ylabel('Noise Level (std)')
axes[1, 2].set_title('Denoising Performance')
axes[1, 2].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print("=== „Éé„Ç§„Ç∫„É¨„Éô„É´ÔºàÊ®ôÊ∫ñÂÅèÂ∑ÆÔºâ ===")
print(f"ÂÖÉÁîªÂÉè: {np.std(noisy_image):.2f}")
print(f"Gaussian: {np.std(gaussian_blur):.2f}")
print(f"Median: {np.std(median_filter):.2f}")
print(f"Bilateral: {np.std(bilateral_filter):.2f}")
print(f"NLM: {np.std(nlm_filter):.2f}")</code></pre>

<strong>„Éï„Ç£„É´„Çø„ÅÆ‰Ωø„ÅÑÂàÜ„Åë</strong>:
- <strong>Gaussian</strong>: È´òÈÄü„ÄÅ„Ç®„ÉÉ„Ç∏„ÅåÊªë„Çâ„Åã ‚Üí ‰∏ÄËà¨ÁöÑ„Å™ÂâçÂá¶ÁêÜ
- <strong>Median</strong>: „Ç®„ÉÉ„Ç∏‰øùÊåÅ„ÄÅ„ÇΩ„É´„ÉàÔºÜ„Éö„ÉÉ„Éë„Éº„Éé„Ç§„Ç∫„Å´Âº∑„ÅÑ
- <strong>Bilateral</strong>: „Ç®„ÉÉ„Ç∏‰øùÊåÅ„ÅåÂÑ™ÁßÄ„ÄÅË®àÁÆó„Ç≥„Çπ„Éà‰∏≠
- <strong>Non-Local Means</strong>: ÊúÄÈ´òÂìÅË≥™„ÄÅË®àÁÆó„Ç≥„Çπ„ÉàÂ§ß

<h3>„Ç≥„É≥„Éà„É©„Çπ„ÉàË™øÊï¥</h3>

<strong>„Ç≥„Éº„Éâ‰æã2: „Éí„Çπ„Éà„Ç∞„É©„É†ÂùáÁ≠âÂåñ„Å®CLAHE</strong>

<pre><code class="language-python">from skimage import exposure

<h1>„Ç≥„É≥„Éà„É©„Çπ„ÉàË™øÊï¥</h1>
hist_eq = exposure.equalize_hist(noisy_image)
clahe = exposure.equalize_adapthist(noisy_image, clip_limit=0.03)

<h1>„Éí„Çπ„Éà„Ç∞„É©„É†Ë®àÁÆó</h1>
hist_original = np.histogram(noisy_image, bins=256, range=(0, 256))[0]
hist_eq_vals = np.histogram(
    (hist_eq * 255).astype(np.uint8), bins=256, range=(0, 256))[0]
hist_clahe_vals = np.histogram(
    (clahe * 255).astype(np.uint8), bins=256, range=(0, 256))[0]

<h1>ÂèØË¶ñÂåñ</h1>
fig = plt.figure(figsize=(16, 10))

<h1>ÁîªÂÉè</h1>
ax1 = plt.subplot(2, 3, 1)
ax1.imshow(noisy_image, cmap='gray')
ax1.set_title('Original')
ax1.axis('off')

ax2 = plt.subplot(2, 3, 2)
ax2.imshow(hist_eq, cmap='gray')
ax2.set_title('Histogram Equalization')
ax2.axis('off')

ax3 = plt.subplot(2, 3, 3)
ax3.imshow(clahe, cmap='gray')
ax3.set_title('CLAHE (Adaptive)')
ax3.axis('off')

<h1>„Éí„Çπ„Éà„Ç∞„É©„É†</h1>
ax4 = plt.subplot(2, 3, 4)
ax4.hist(noisy_image.ravel(), bins=256, range=(0, 256), alpha=0.7)
ax4.set_xlabel('Pixel Value')
ax4.set_ylabel('Frequency')
ax4.set_title('Original Histogram')
ax4.grid(True, alpha=0.3)

ax5 = plt.subplot(2, 3, 5)
ax5.hist((hist_eq * 255).astype(np.uint8).ravel(),
         bins=256, range=(0, 256), alpha=0.7, color='orange')
ax5.set_xlabel('Pixel Value')
ax5.set_ylabel('Frequency')
ax5.set_title('Histogram Eq. Histogram')
ax5.grid(True, alpha=0.3)

ax6 = plt.subplot(2, 3, 6)
ax6.hist((clahe * 255).astype(np.uint8).ravel(),
         bins=256, range=(0, 256), alpha=0.7, color='green')
ax6.set_xlabel('Pixel Value')
ax6.set_ylabel('Frequency')
ax6.set_title('CLAHE Histogram')
ax6.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== „Ç≥„É≥„Éà„É©„Çπ„ÉàÊåáÊ®ô ===")
print(f"ÂÖÉÁîªÂÉè„ÅÆ„Ç≥„É≥„Éà„É©„Çπ„Éà: {noisy_image.max() - noisy_image.min()}")
print(f"Histogram Eq.: {(hist_eq * 255).max() - (hist_eq * 255).min():.1f}")
print(f"CLAHE: {(clahe * 255).max() - (clahe * 255).min():.1f}")</code></pre>

<strong>CLAHEÔºàContrast Limited Adaptive Histogram EqualizationÔºâ„ÅÆÂà©ÁÇπ</strong>:
- Â±ÄÊâÄÁöÑ„Å™„Ç≥„É≥„Éà„É©„Çπ„ÉàÂêë‰∏ä
- ÈÅéÂâ∞„Å™Âº∑Ë™ø„ÇíÊäëÂà∂Ôºàclip_limit„Éë„É©„É°„Éº„ÇøÔºâ
- SEMÁîªÂÉè„ÅÆÊöóÈÉ®„ÉªÊòéÈÉ®‰∏°Êñπ„ÅßË©≥Á¥∞„ÅåË¶ã„Åà„Çã

---

<h2>3.3 Á≤íÂ≠êÊ§úÂá∫ÔºàWatershedÊ≥ïÔºâ</h2>

<h3>‰∫åÂÄ§Âåñ„Å®Ë∑ùÈõ¢Â§âÊèõ</h3>

<strong>„Ç≥„Éº„Éâ‰æã3: OtsuÊ≥ï„Å´„Çà„ÇãËá™Âãï‰∫åÂÄ§Âåñ</strong>

<pre><code class="language-python">from skimage import morphology, measure
from scipy.ndimage import distance_transform_edt

<h1>„Éé„Ç§„Ç∫Èô§ÂéªÂæå„ÅÆÁîªÂÉè„Çí‰ΩøÁî®</h1>
denoised = cv2.fastNlMeansDenoising(noisy_image, None, 10, 7, 21)

<h1>OtsuÊ≥ï„Å´„Çà„Çã‰∫åÂÄ§Âåñ</h1>
threshold = filters.threshold_otsu(denoised)
binary = denoised > threshold

<h1>„É¢„É´„Éï„Ç©„É≠„Ç∏„ÉºÊºîÁÆóÔºàÂ∞è„Åï„Å™„Éé„Ç§„Ç∫Èô§ÂéªÔºâ</h1>
binary_cleaned = morphology.remove_small_objects(binary, min_size=50)
binary_cleaned = morphology.remove_small_holes(binary_cleaned, area_threshold=50)

<h1>Ë∑ùÈõ¢Â§âÊèõ</h1>
distance = distance_transform_edt(binary_cleaned)

<h1>ÂèØË¶ñÂåñ</h1>
fig, axes = plt.subplots(2, 2, figsize=(12, 12))

axes[0, 0].imshow(denoised, cmap='gray')
axes[0, 0].set_title('Denoised Image')
axes[0, 0].axis('off')

axes[0, 1].imshow(binary, cmap='gray')
axes[0, 1].set_title(f'Binary (Otsu threshold={threshold:.1f})')
axes[0, 1].axis('off')

axes[1, 0].imshow(binary_cleaned, cmap='gray')
axes[1, 0].set_title('After Morphology')
axes[1, 0].axis('off')

axes[1, 1].imshow(distance, cmap='jet')
axes[1, 1].set_title('Distance Transform')
axes[1, 1].axis('off')
axes[1, 1].colorbar = plt.colorbar(axes[1, 1].imshow(distance, cmap='jet'),
                                   ax=axes[1, 1])

plt.tight_layout()
plt.show()

print(f"=== ‰∫åÂÄ§ÂåñÁµêÊûú ===")
print(f"OtsuÈñæÂÄ§: {threshold:.1f}")
print(f"ÁôΩ„Éî„ÇØ„Çª„É´Ââ≤Âêà: {binary_cleaned.sum() / binary_cleaned.size * 100:.1f}%")</code></pre>

<h3>WatershedÊ≥ï„Å´„Çà„ÇãÁ≤íÂ≠êÂàÜÈõ¢</h3>

<strong>„Ç≥„Éº„Éâ‰æã4: Watershed „Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥</strong>

<pre><code class="language-python">from skimage.feature import peak_local_max
from skimage.segmentation import watershed

<h1>Â±ÄÊâÄÊúÄÂ§ßÂÄ§Ê§úÂá∫ÔºàÁ≤íÂ≠ê‰∏≠ÂøÉ„ÅÆÊé®ÂÆöÔºâ</h1>
local_max = peak_local_max(
    distance,
    min_distance=20,
    threshold_abs=5,
    labels=binary_cleaned
)

<h1>„Éû„Éº„Ç´„Éº‰ΩúÊàê</h1>
markers = np.zeros_like(distance, dtype=int)
markers[tuple(local_max.T)] = np.arange(1, len(local_max) + 1)

<h1>WatershedÂÆüË°å</h1>
labels = watershed(-distance, markers, mask=binary_cleaned)

<h1>ÂèØË¶ñÂåñ</h1>
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

<h1>„Éû„Éº„Ç´„ÉºË°®Á§∫</h1>
axes[0].imshow(denoised, cmap='gray')
axes[0].plot(local_max[:, 1], local_max[:, 0], 'r+',
             markersize=12, markeredgewidth=2)
axes[0].set_title(f'Detected Centers ({len(local_max)} particles)')
axes[0].axis('off')

<h1>Watershed„É©„Éô„É´</h1>
axes[1].imshow(labels, cmap='nipy_spectral')
axes[1].set_title('Watershed Segmentation')
axes[1].axis('off')

<h1>Ëº™ÈÉ≠Èáç„Å≠Âêà„Çè„Åõ</h1>
overlay = denoised.copy()
overlay_rgb = cv2.cvtColor(overlay, cv2.COLOR_GRAY2RGB)
for region in measure.regionprops(labels):
    minr, minc, maxr, maxc = region.bbox
    cv2.rectangle(overlay_rgb, (minc, minr), (maxc, maxr),
                  (255, 0, 0), 2)

axes[2].imshow(overlay_rgb)
axes[2].set_title('Detected Particles')
axes[2].axis('off')

plt.tight_layout()
plt.show()

print(f"=== WatershedÁµêÊûú ===")
print(f"Ê§úÂá∫„Åï„Çå„ÅüÁ≤íÂ≠êÊï∞: {len(local_max)}")
print(f"„É©„Éô„É´Êï∞: {labels.max()}")</code></pre>

---

<h2>3.4 Á≤íÂæÑÂàÜÂ∏ÉËß£Êûê</h2>

<h3>Á≤íÂ≠êÁâπÂæ¥Èáè„ÅÆÊäΩÂá∫</h3>

<strong>„Ç≥„Éº„Éâ‰æã5: Á≤íÂæÑ„ÉªÂΩ¢Áä∂„Éë„É©„É°„Éº„Çø„ÅÆË®àÁÆó</strong>

<pre><code class="language-python"><h1>ÂêÑÁ≤íÂ≠ê„ÅÆÁâπÂæ¥ÈáèÊäΩÂá∫</h1>
particle_data = []

for region in measure.regionprops(labels):
    # Èù¢Á©ç„Åã„ÇâÂÜÜÁõ∏ÂΩìÁõ¥ÂæÑ„ÇíË®àÁÆó
    area = region.area
    equivalent_diameter = np.sqrt(4 * area / np.pi)

    # „Ç¢„Çπ„Éö„ÇØ„ÉàÊØî
    major_axis = region.major_axis_length
    minor_axis = region.minor_axis_length
    aspect_ratio = major_axis / (minor_axis + 1e-10)

    # ÂÜÜÂΩ¢Â∫¶Ôºà4œÄ√óÈù¢Á©ç/Âë®Âõ≤Èï∑^2Ôºâ
    perimeter = region.perimeter
    circularity = 4 * np.pi * area / (perimeter ** 2 + 1e-10)

    particle_data.append({
        'label': region.label,
        'area': area,
        'diameter': equivalent_diameter,
        'aspect_ratio': aspect_ratio,
        'circularity': circularity,
        'centroid': region.centroid
    })

<h1>DataFrame„Å´Â§âÊèõ</h1>
import pandas as pd
df_particles = pd.DataFrame(particle_data)

print("=== Á≤íÂ≠êÁâπÂæ¥ÈáèÁµ±Ë®à ===")
print(df_particles[['diameter', 'aspect_ratio', 'circularity']].describe())

<h1>Á≤íÂæÑÂàÜÂ∏É„Éó„É≠„ÉÉ„Éà</h1>
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

<h1>„Éí„Çπ„Éà„Ç∞„É©„É†</h1>
axes[0, 0].hist(df_particles['diameter'], bins=20, alpha=0.7,
                edgecolor='black')
axes[0, 0].set_xlabel('Diameter (pixels)')
axes[0, 0].set_ylabel('Frequency')
axes[0, 0].set_title('Particle Size Distribution')
axes[0, 0].axvline(df_particles['diameter'].mean(), color='red',
                   linestyle='--', label=f'Mean: {df_particles["diameter"].mean():.1f}')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3, axis='y')

<h1>Á¥ØÁ©çÂàÜÂ∏É</h1>
sorted_diameters = np.sort(df_particles['diameter'])
cumulative = np.arange(1, len(sorted_diameters) + 1) / len(sorted_diameters) * 100
axes[0, 1].plot(sorted_diameters, cumulative, linewidth=2)
axes[0, 1].set_xlabel('Diameter (pixels)')
axes[0, 1].set_ylabel('Cumulative Percentage (%)')
axes[0, 1].set_title('Cumulative Size Distribution')
axes[0, 1].axhline(50, color='red', linestyle='--',
                   label=f'D50: {np.median(df_particles["diameter"]):.1f}')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

<h1>Êï£Â∏ÉÂõ≥ÔºàÁõ¥ÂæÑ vs „Ç¢„Çπ„Éö„ÇØ„ÉàÊØîÔºâ</h1>
axes[1, 0].scatter(df_particles['diameter'],
                   df_particles['aspect_ratio'],
                   alpha=0.6, s=50)
axes[1, 0].set_xlabel('Diameter (pixels)')
axes[1, 0].set_ylabel('Aspect Ratio')
axes[1, 0].set_title('Diameter vs Aspect Ratio')
axes[1, 0].grid(True, alpha=0.3)

<h1>Êï£Â∏ÉÂõ≥ÔºàÁõ¥ÂæÑ vs ÂÜÜÂΩ¢Â∫¶Ôºâ</h1>
axes[1, 1].scatter(df_particles['diameter'],
                   df_particles['circularity'],
                   alpha=0.6, s=50, color='green')
axes[1, 1].set_xlabel('Diameter (pixels)')
axes[1, 1].set_ylabel('Circularity')
axes[1, 1].set_title('Diameter vs Circularity')
axes[1, 1].axhline(0.8, color='red', linestyle='--',
                   label='Spherical threshold')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

<h1>Á≤íÂæÑÁµ±Ë®à</h1>
print("\n=== Á≤íÂæÑÁµ±Ë®à ===")
print(f"Âπ≥ÂùáÁõ¥ÂæÑ: {df_particles['diameter'].mean():.2f} pixels")
print(f"‰∏≠Â§ÆÂÄ§(D50): {df_particles['diameter'].median():.2f} pixels")
print(f"Ê®ôÊ∫ñÂÅèÂ∑Æ: {df_particles['diameter'].std():.2f} pixels")
print(f"ÊúÄÂ∞èÁõ¥ÂæÑ: {df_particles['diameter'].min():.2f} pixels")
print(f"ÊúÄÂ§ßÁõ¥ÂæÑ: {df_particles['diameter'].max():.2f} pixels")</code></pre>

<h3>Á≤íÂæÑÂàÜÂ∏É„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞ÔºàÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏ÉÔºâ</h3>

<strong>„Ç≥„Éº„Éâ‰æã6: ÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞</strong>

<pre><code class="language-python">from scipy.stats import lognorm

<h1>ÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É„ÅÆ„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞</h1>
diameters = df_particles['diameter'].values
shape, loc, scale = lognorm.fit(diameters, floc=0)

<h1>„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞ÁµêÊûú</h1>
x = np.linspace(diameters.min(), diameters.max(), 200)
pdf_fitted = lognorm.pdf(x, shape, loc, scale)

<h1>ÂèØË¶ñÂåñ</h1>
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.hist(diameters, bins=20, density=True, alpha=0.6,
         label='Observed', edgecolor='black')
plt.plot(x, pdf_fitted, 'r-', linewidth=2,
         label=f'Log-normal fit (œÉ={shape:.2f})')
plt.xlabel('Diameter (pixels)')
plt.ylabel('Probability Density')
plt.title('Particle Size Distribution Fitting')
plt.legend()
plt.grid(True, alpha=0.3)

<h1>Q-Q„Éó„É≠„ÉÉ„ÉàÔºàÈÅ©ÂêàÂ∫¶Á¢∫Ë™çÔºâ</h1>
plt.subplot(1, 2, 2)
theoretical_quantiles = lognorm.ppf(np.linspace(0.01, 0.99, 100),
                                    shape, loc, scale)
observed_quantiles = np.percentile(diameters, np.linspace(1, 99, 100))
plt.scatter(theoretical_quantiles, observed_quantiles, alpha=0.6)
plt.plot([diameters.min(), diameters.max()],
         [diameters.min(), diameters.max()],
         'r--', linewidth=2, label='Perfect fit')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Observed Quantiles')
plt.title('Q-Q Plot (Log-normal)')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=== ÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É„Éë„É©„É°„Éº„Çø ===")
print(f"Shape (œÉ): {shape:.3f}")
print(f"Scale (median): {scale:.2f} pixels")</code></pre>

---

<h2>3.5 Ê∑±Â±§Â≠¶Áøí„Å´„Çà„ÇãÁîªÂÉèÂàÜÈ°û</h2>

<h3>Ëª¢ÁßªÂ≠¶ÁøíÔºàVGG16Ôºâ„Å´„Çà„ÇãÊùêÊñôÁîªÂÉèÂàÜÈ°û</h3>

<strong>„Ç≥„Éº„Éâ‰æã7: CNN„Å´„Çà„ÇãÊùêÊñôÁõ∏ÂàÜÈ°û</strong>

<pre><code class="language-python"><h1>TensorFlow/Keras„ÅÆ„Ç§„É≥„Éù„Éº„Éà</h1>
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.applications import VGG16
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("TensorFlow not available. Skipping this example.")

if TENSORFLOW_AVAILABLE:
    # „Çµ„É≥„Éó„É´ÁîªÂÉè„Éá„Éº„ÇøÁîüÊàêÔºà3„ÇØ„É©„ÇπÂàÜÈ°ûÔºâ
    def generate_material_images(num_samples=100, img_size=128):
        """
        ÊùêÊñôÁîªÂÉè„ÅÆ„Çµ„É≥„Éó„É´ÁîüÊàê
        „ÇØ„É©„Çπ0: ÁêÉÂΩ¢Á≤íÂ≠ê
        „ÇØ„É©„Çπ1: Ê£íÁä∂Á≤íÂ≠ê
        „ÇØ„É©„Çπ2: ‰∏çÂÆöÂΩ¢Á≤íÂ≠ê
        """
        images = []
        labels = []

        for class_id in range(3):
            for _ in range(num_samples):
                img = np.zeros((img_size, img_size), dtype=np.uint8)

                if class_id == 0:  # ÁêÉÂΩ¢
                    num_particles = np.random.randint(5, 15)
                    for _ in range(num_particles):
                        x = np.random.randint(20, img_size - 20)
                        y = np.random.randint(20, img_size - 20)
                        r = np.random.randint(8, 15)
                        cv2.circle(img, (x, y), r, 200, -1)

                elif class_id == 1:  # Ê£íÁä∂
                    num_rods = np.random.randint(3, 8)
                    for _ in range(num_rods):
                        x1 = np.random.randint(10, img_size - 10)
                        y1 = np.random.randint(10, img_size - 10)
                        length = np.random.randint(30, 60)
                        angle = np.random.rand() * 2 * np.pi
                        x2 = int(x1 + length * np.cos(angle))
                        y2 = int(y1 + length * np.sin(angle))
                        cv2.line(img, (x1, y1), (x2, y2), 200, 3)

                else:  # ‰∏çÂÆöÂΩ¢
                    num_shapes = np.random.randint(5, 12)
                    for _ in range(num_shapes):
                        pts = np.random.randint(10, img_size - 10,
                                                size=(6, 2))
                        cv2.fillPoly(img, [pts], 200)

                # „Éé„Ç§„Ç∫ËøΩÂä†
                noise = np.random.normal(0, 20, img.shape)
                img = np.clip(img + noise, 0, 255).astype(np.uint8)

                # RGBÂ§âÊèõ
                img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
                images.append(img_rgb)
                labels.append(class_id)

        return np.array(images), np.array(labels)

    # „Éá„Éº„ÇøÁîüÊàê
    X_data, y_data = generate_material_images(num_samples=150)

    # Ë®ìÁ∑¥„Éª„ÉÜ„Çπ„Éà„Éá„Éº„ÇøÂàÜÂâ≤
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X_data, y_data, test_size=0.2, random_state=42
    )

    # „Éá„Éº„ÇøÊ≠£Ë¶èÂåñ
    X_train = X_train.astype('float32') / 255.0
    X_test = X_test.astype('float32') / 255.0

    # VGG16„É¢„Éá„É´„ÅÆË™≠„ÅøËæº„ÅøÔºàImageNetÈáç„ÅøÔºâ
    base_model = VGG16(
        weights='imagenet',
        include_top=False,
        input_shape=(128, 128, 3)
    )

    # „Éô„Éº„Çπ„É¢„Éá„É´„ÅÆÈáç„Åø„ÇíÂõ∫ÂÆö
    base_model.trainable = False

    # Êñ∞„Åó„ÅÑÂàÜÈ°ûÂ±§„ÇíËøΩÂä†
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    predictions = Dense(3, activation='softmax')(x)

    model = Model(inputs=base_model.input, outputs=predictions)

    # „É¢„Éá„É´„Ç≥„É≥„Éë„Ç§„É´
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # „É¢„Éá„É´Ë®ìÁ∑¥
    history = model.fit(
        X_train, y_train,
        validation_split=0.2,
        epochs=10,
        batch_size=16,
        verbose=0
    )

    # „ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÅßË©ï‰æ°
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)

    print("=== CNNÂàÜÈ°ûÁµêÊûú ===")
    print(f"„ÉÜ„Çπ„ÉàÁ≤æÂ∫¶: {test_acc * 100:.2f}%")

    # Â≠¶ÁøíÊõ≤Á∑ö„Éó„É≠„ÉÉ„Éà
    plt.figure(figsize=(14, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Ê∑∑ÂêåË°åÂàó
    from sklearn.metrics import confusion_matrix, classification_report
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)

    cm = confusion_matrix(y_test, y_pred_classes)

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.colorbar()
    tick_marks = np.arange(3)
    plt.xticks(tick_marks, ['Spherical', 'Rod', 'Irregular'])
    plt.yticks(tick_marks, ['Spherical', 'Rod', 'Irregular'])

    # Êï∞ÂÄ§Ë°®Á§∫
    for i in range(3):
        for j in range(3):
            plt.text(j, i, cm[i, j], ha='center', va='center',
                    color='white' if cm[i, j] > cm.max() / 2 else 'black')

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    plt.show()

    print("\n=== ÂàÜÈ°û„É¨„Éù„Éº„Éà ===")
    print(classification_report(
        y_test, y_pred_classes,
        target_names=['Spherical', 'Rod', 'Irregular']
    ))</code></pre>

---

<h2>3.6 Áµ±ÂêàÁîªÂÉèËß£Êûê„Éë„Ç§„Éó„É©„Ç§„É≥</h2>

<h3>Ëá™ÂãïËß£Êûê„Ç∑„Çπ„ÉÜ„É†„ÅÆÊßãÁØâ</h3>

<strong>„Ç≥„Éº„Éâ‰æã8: ÁîªÂÉèËß£Êûê„Éë„Ç§„Éó„É©„Ç§„É≥„ÇØ„É©„Çπ</strong>

<pre><code class="language-python">from dataclasses import dataclass
from typing import List, Dict
import json

@dataclass
class ParticleAnalysisResult:
    """Á≤íÂ≠êËß£ÊûêÁµêÊûú"""
    num_particles: int
    mean_diameter: float
    std_diameter: float
    mean_circularity: float
    particle_data: List[Dict]

class SEMImageAnalyzer:
    """SEMÁîªÂÉèËá™ÂãïËß£Êûê„Ç∑„Çπ„ÉÜ„É†"""

    def __init__(self, img_size=(512, 512)):
        self.img_size = img_size
        self.image = None
        self.binary = None
        self.labels = None
        self.particles = []

    def load_image(self, image: np.ndarray):
        """ÁîªÂÉèË™≠„ÅøËæº„Åø"""
        if len(image.shape) == 3:
            self.image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            self.image = image

        # „É™„Çµ„Ç§„Ç∫
        self.image = cv2.resize(self.image, self.img_size)

    def preprocess(self, denoise_strength=10):
        """ÂâçÂá¶ÁêÜÔºà„Éé„Ç§„Ç∫Èô§Âéª„Éª„Ç≥„É≥„Éà„É©„Çπ„ÉàË™øÊï¥Ôºâ"""
        # „Éé„Ç§„Ç∫Èô§Âéª
        denoised = cv2.fastNlMeansDenoising(
            self.image, None, denoise_strength, 7, 21
        )

        # CLAHE
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(denoised)

        self.image = enhanced

    def segment_particles(self, min_size=50):
        """Á≤íÂ≠ê„Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥"""
        # Otsu‰∫åÂÄ§Âåñ
        threshold = filters.threshold_otsu(self.image)
        binary = self.image > threshold

        # „É¢„É´„Éï„Ç©„É≠„Ç∏„Éº
        binary_cleaned = morphology.remove_small_objects(
            binary, min_size=min_size
        )
        binary_cleaned = morphology.remove_small_holes(
            binary_cleaned, area_threshold=min_size
        )

        # Ë∑ùÈõ¢Â§âÊèõ
        distance = distance_transform_edt(binary_cleaned)

        # Watershed
        local_max = peak_local_max(
            distance,
            min_distance=20,
            threshold_abs=5,
            labels=binary_cleaned
        )

        markers = np.zeros_like(distance, dtype=int)
        markers[tuple(local_max.T)] = np.arange(1, len(local_max) + 1)

        self.labels = watershed(-distance, markers, mask=binary_cleaned)
        self.binary = binary_cleaned

    def extract_features(self):
        """ÁâπÂæ¥ÈáèÊäΩÂá∫"""
        self.particles = []

        for region in measure.regionprops(self.labels):
            area = region.area
            diameter = np.sqrt(4 * area / np.pi)
            aspect_ratio = region.major_axis_length / \
                          (region.minor_axis_length + 1e-10)
            circularity = 4 * np.pi * area / \
                         (region.perimeter ** 2 + 1e-10)

            self.particles.append({
                'label': region.label,
                'area': area,
                'diameter': diameter,
                'aspect_ratio': aspect_ratio,
                'circularity': circularity,
                'centroid': region.centroid
            })

    def get_results(self) -> ParticleAnalysisResult:
        """ÁµêÊûúÂèñÂæó"""
        df = pd.DataFrame(self.particles)

        return ParticleAnalysisResult(
            num_particles=len(self.particles),
            mean_diameter=df['diameter'].mean(),
            std_diameter=df['diameter'].std(),
            mean_circularity=df['circularity'].mean(),
            particle_data=self.particles
        )

    def visualize(self):
        """ÁµêÊûúÂèØË¶ñÂåñ"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 14))

        # ÂÖÉÁîªÂÉè
        axes[0, 0].imshow(self.image, cmap='gray')
        axes[0, 0].set_title('Preprocessed Image')
        axes[0, 0].axis('off')

        # ‰∫åÂÄ§Âåñ
        axes[0, 1].imshow(self.binary, cmap='gray')
        axes[0, 1].set_title('Binary Segmentation')
        axes[0, 1].axis('off')

        # Watershed„É©„Éô„É´
        axes[1, 0].imshow(self.labels, cmap='nipy_spectral')
        axes[1, 0].set_title(f'Particles ({len(self.particles)})')
        axes[1, 0].axis('off')

        # Á≤íÂæÑÂàÜÂ∏É
        df = pd.DataFrame(self.particles)
        axes[1, 1].hist(df['diameter'], bins=20, alpha=0.7,
                       edgecolor='black')
        axes[1, 1].set_xlabel('Diameter (pixels)')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('Particle Size Distribution')
        axes[1, 1].grid(True, alpha=0.3, axis='y')

        plt.tight_layout()
        plt.show()

    def save_results(self, filename='analysis_results.json'):
        """ÁµêÊûú„ÇíJSON‰øùÂ≠ò"""
        results = self.get_results()
        output = {
            'num_particles': results.num_particles,
            'mean_diameter': results.mean_diameter,
            'std_diameter': results.std_diameter,
            'mean_circularity': results.mean_circularity,
            'particles': results.particle_data
        }

        with open(filename, 'w') as f:
            json.dump(output, f, indent=2)

        print(f"Results saved to {filename}")

<h1>‰ΩøÁî®‰æã</h1>
analyzer = SEMImageAnalyzer()
analyzer.load_image(noisy_image)
analyzer.preprocess(denoise_strength=10)
analyzer.segment_particles(min_size=50)
analyzer.extract_features()

results = analyzer.get_results()
print("=== Ëß£ÊûêÁµêÊûú ===")
print(f"Ê§úÂá∫Á≤íÂ≠êÊï∞: {results.num_particles}")
print(f"Âπ≥ÂùáÁõ¥ÂæÑ: {results.mean_diameter:.2f} ¬± {results.std_diameter:.2f} pixels")
print(f"Âπ≥ÂùáÂÜÜÂΩ¢Â∫¶: {results.mean_circularity:.3f}")

analyzer.visualize()
analyzer.save_results('sem_analysis.json')</code></pre>

---

<h2>3.7 Êú¨Á´†„ÅÆ„Åæ„Å®„ÇÅ</h2>

<h3>Â≠¶„Çì„Å†„Åì„Å®</h3>

1. <strong>ÁîªÂÉèÂâçÂá¶ÁêÜ</strong>
   - „Éé„Ç§„Ç∫Èô§ÂéªÔºàGaussian„ÄÅMedian„ÄÅBilateral„ÄÅNLMÔºâ
   - „Ç≥„É≥„Éà„É©„Çπ„ÉàË™øÊï¥ÔºàHistogram Equalization„ÄÅCLAHEÔºâ
   - ‰∫åÂÄ§ÂåñÔºàOtsuÊ≥ïÔºâ

2. <strong>Á≤íÂ≠êÊ§úÂá∫</strong>
   - WatershedÊ≥ï„Å´„Çà„Çã„Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥
   - Ë∑ùÈõ¢Â§âÊèõ„Å®Â±ÄÊâÄÊúÄÂ§ßÂÄ§Ê§úÂá∫
   - „É¢„É´„Éï„Ç©„É≠„Ç∏„ÉºÊºîÁÆó

3. <strong>ÂÆöÈáèËß£Êûê</strong>
   - Á≤íÂæÑÂàÜÂ∏ÉÔºà„Éí„Çπ„Éà„Ç∞„É©„É†„ÄÅÁ¥ØÁ©çÂàÜÂ∏ÉÔºâ
   - ÂΩ¢Áä∂„Éë„É©„É°„Éº„ÇøÔºàÂÜÜÂΩ¢Â∫¶„ÄÅ„Ç¢„Çπ„Éö„ÇØ„ÉàÊØîÔºâ
   - ÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞

4. <strong>Ê∑±Â±§Â≠¶Áøí</strong>
   - Ëª¢ÁßªÂ≠¶ÁøíÔºàVGG16Ôºâ
   - ÊùêÊñôÁîªÂÉèÂàÜÈ°û
   - Ê∑∑ÂêåË°åÂàó„Å´„Çà„ÇãÊÄßËÉΩË©ï‰æ°

<h3>ÈáçË¶Å„Å™„Éù„Ç§„É≥„Éà</h3>

- ‚úÖ ÂâçÂá¶ÁêÜ„ÅÆË≥™„Åå„Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥Á≤æÂ∫¶„ÇíÊ±∫ÂÆö„Åô„Çã
- ‚úÖ WatershedÊ≥ï„ÅØ„Éë„É©„É°„Éº„ÇøË™øÊï¥„ÅåÈáçË¶ÅÔºàmin_distance„ÄÅthreshold_absÔºâ
- ‚úÖ Á≤íÂæÑÂàÜÂ∏É„ÅØÁµ±Ë®à„É¢„Éá„É´ÔºàÂØæÊï∞Ê≠£Ë¶èÔºâ„Åß„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞ÂèØËÉΩ
- ‚úÖ Ëª¢ÁßªÂ≠¶Áøí„Å´„Çà„ÇäÂ∞ëÈáè„Éá„Éº„Çø„Åß„ÇÇÈ´òÁ≤æÂ∫¶ÂàÜÈ°û„ÅåÂÆüÁèæ

<h3>Ê¨°„ÅÆÁ´†„Å∏</h3>

Á¨¨4Á´†„Åß„ÅØ„ÄÅÊôÇÁ≥ªÂàó„Éá„Éº„Çø„Å®Áµ±ÂêàËß£Êûê„ÇíÂ≠¶„Å≥„Åæ„ÅôÔºö
- Ê∏©Â∫¶„ÉªÂúßÂäõ„Çª„É≥„Çµ„Éº„Éá„Éº„Çø„ÅÆÂâçÂá¶ÁêÜ
- ÁßªÂãïÁ™ìËß£Êûê
- Áï∞Â∏∏Ê§úÁü•
- PCA„Å´„Çà„ÇãÊ¨°ÂÖÉÂâäÊ∏õ
- sklearn Pipeline„Å´„Çà„ÇãËá™ÂãïÂåñ

<strong>[Á¨¨4Á´†ÔºöÊôÇÁ≥ªÂàó„Éá„Éº„Çø„Å®Áµ±ÂêàËß£Êûê ‚Üí](./chapter-4.md)</strong>

---

<h2>ÊºîÁøíÂïèÈ°å</h2>

<h3>ÂïèÈ°å1ÔºàÈõ£ÊòìÂ∫¶ÔºöeasyÔºâ</h3>

Ê¨°„ÅÆÊñáÁ´†„ÅÆÊ≠£Ë™§„ÇíÂà§ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

1. Bilateral Filter„ÅØ„Ç®„ÉÉ„Ç∏„ÇíGaussian Filter„Çà„Çä„ÇÇ‰øùÊåÅ„Åß„Åç„Çã
2. CLAHE„ÅØÁîªÂÉèÂÖ®‰Ωì„Å´Âêå‰∏Ä„ÅÆ„Éí„Çπ„Éà„Ç∞„É©„É†ÂùáÁ≠âÂåñ„ÇíÈÅ©Áî®„Åô„Çã
3. WatershedÊ≥ï„Åß„ÅØË∑ùÈõ¢Â§âÊèõ„ÅÆÂ±ÄÊâÄÊúÄÂ§ßÂÄ§„ÇíÁ≤íÂ≠ê‰∏≠ÂøÉ„Å®„Åø„Å™„Åô

<details>
<summary>„Éí„É≥„Éà</summary>

1. Bilateral Filter„ÅÆÂãï‰ΩúÂéüÁêÜÔºàÁ©∫ÈñìË∑ùÈõ¢„Å®ËºùÂ∫¶Â∑Æ„ÅÆ‰∏°Êñπ„ÇíËÄÉÊÖÆÔºâ
2. CLAHE„ÅÆ"Adaptive"„ÅÆÊÑèÂë≥
3. Watershed„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅÆÊµÅ„ÇåÔºàË∑ùÈõ¢Â§âÊèõ‚Üí„Éû„Éº„Ç´„Éº‚ÜíwatershedÔºâ

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>

<strong>Ëß£Á≠î</strong>:
1. <strong>Ê≠£</strong> - Bilateral Filter„ÅØËºùÂ∫¶Â∑Æ„ÇÇËÄÉÊÖÆ„Åô„Çã„Åü„ÇÅ„ÄÅ„Ç®„ÉÉ„Ç∏‰ªòËøë„Åß„ÅØÂπ≥ÊªëÂåñ„ÅåÊäëÂà∂„Åï„Çå„Çã
2. <strong>Ë™§</strong> - CLAHE„ÅØÁîªÂÉè„ÇíÂ∞èÈ†òÂüüÔºà„Çø„Ç§„É´Ôºâ„Å´ÂàÜÂâ≤„Åó„ÄÅÂêÑÈ†òÂüü„ÅßÈÅ©ÂøúÁöÑ„Å´„Éí„Çπ„Éà„Ç∞„É©„É†ÂùáÁ≠âÂåñ„ÇíË°å„ÅÜ
3. <strong>Ê≠£</strong> - Ë∑ùÈõ¢Â§âÊèõ„ÅÆÂ±ÄÊâÄÊúÄÂ§ßÂÄ§„ÅåÁ≤íÂ≠ê‰∏≠ÂøÉ„Å´ÂØæÂøú„Åó„ÄÅWatershed„ÅÆ„Éû„Éº„Ç´„Éº„Å®„Åó„Å¶‰ΩøÁî®„Åï„Çå„Çã

<strong>Ëß£Ë™¨</strong>:
ÁîªÂÉèÂâçÂá¶ÁêÜ„Åß„ÅØ„ÄÅÂá¶ÁêÜ„ÅÆÁõÆÁöÑÔºà„Éé„Ç§„Ç∫Èô§Âéª vs „Ç®„ÉÉ„Ç∏‰øùÊåÅÔºâ„Å´Âøú„Åò„Å¶ÈÅ©Âàá„Å™ÊâãÊ≥ï„ÇíÈÅ∏Êäû„Åô„Çã„Åì„Å®„ÅåÈáçË¶Å„Åß„Åô„ÄÇWatershed„ÅØË∑ùÈõ¢Â§âÊèõ„ÇíÊ¥ªÁî®„Åô„Çã„Åì„Å®„Åß„ÄÅÊé•Ëß¶„Åó„Å¶„ÅÑ„ÇãÁ≤íÂ≠ê„ÇÇÂàÜÈõ¢„Åß„Åç„ÇãÂº∑Âäõ„Å™ÊâãÊ≥ï„Åß„Åô„ÄÇ

</details>

---

<h3>ÂïèÈ°å2ÔºàÈõ£ÊòìÂ∫¶ÔºömediumÔºâ</h3>

‰ª•‰∏ã„ÅÆSEMÁîªÂÉè„Éá„Éº„Çø„Å´ÂØæ„Åó„Å¶„ÄÅÁ≤íÂ≠êÊ§úÂá∫„Å®Á≤íÂæÑÂàÜÂ∏ÉËß£Êûê„ÇíÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

<pre><code class="language-python">import numpy as np

<h1>„Çµ„É≥„Éó„É´SEMÁîªÂÉèÁîüÊàê</h1>
def generate_sample_sem():
    np.random.seed(100)
    img = np.zeros((512, 512), dtype=np.uint8)

    for _ in range(40):
        x = np.random.randint(30, 482)
        y = np.random.randint(30, 482)
        r = np.random.randint(10, 25)
        cv2.circle(img, (x, y), r, 200, -1)

    noise = np.random.normal(0, 30, img.shape)
    return np.clip(img + noise, 0, 255).astype(np.uint8)

sample_image = generate_sample_sem()</code></pre>

<strong>Ë¶ÅÊ±Ç‰∫ãÈ†Ö</strong>:
1. Non-Local Means„Åß„Éé„Ç§„Ç∫Èô§Âéª
2. OtsuÊ≥ï„Åß‰∫åÂÄ§Âåñ
3. WatershedÊ≥ï„ÅßÁ≤íÂ≠êÊ§úÂá∫
4. Á≤íÂæÑÂàÜÂ∏É„Çí„Éí„Çπ„Éà„Ç∞„É©„É†„Åß„Éó„É≠„ÉÉ„Éà
5. Âπ≥ÂùáÁ≤íÂæÑ„ÉªÊ®ôÊ∫ñÂÅèÂ∑Æ„ÇíÂá∫Âäõ

<details>
<summary>„Éí„É≥„Éà</summary>

<strong>Âá¶ÁêÜ„Éï„É≠„Éº</strong>:
1. <code>cv2.fastNlMeansDenoising</code>„Åß„Éé„Ç§„Ç∫Èô§Âéª
2. <code>filters.threshold_otsu</code>„ÅßÈñæÂÄ§Ë®àÁÆó‚Üí‰∫åÂÄ§Âåñ
3. <code>distance_transform_edt</code> + <code>peak_local_max</code> + <code>watershed</code>
4. <code>measure.regionprops</code>„ÅßÁ≤íÂ≠êÁâπÂæ¥ÈáèÊäΩÂá∫
5. <code>matplotlib.pyplot.hist</code>„ÅßÂèØË¶ñÂåñ

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>

<pre><code class="language-python">import numpy as np
import cv2
import matplotlib.pyplot as plt
from skimage import filters, morphology, measure
from skimage.feature import peak_local_max
from skimage.segmentation import watershed
from scipy.ndimage import distance_transform_edt

<h1>„Çµ„É≥„Éó„É´ÁîªÂÉèÁîüÊàê</h1>
def generate_sample_sem():
    np.random.seed(100)
    img = np.zeros((512, 512), dtype=np.uint8)

    for _ in range(40):
        x = np.random.randint(30, 482)
        y = np.random.randint(30, 482)
        r = np.random.randint(10, 25)
        cv2.circle(img, (x, y), r, 200, -1)

    noise = np.random.normal(0, 30, img.shape)
    return np.clip(img + noise, 0, 255).astype(np.uint8)

sample_image = generate_sample_sem()

<h1>„Çπ„ÉÜ„ÉÉ„Éó1: „Éé„Ç§„Ç∫Èô§Âéª</h1>
denoised = cv2.fastNlMeansDenoising(sample_image, None, 10, 7, 21)

<h1>„Çπ„ÉÜ„ÉÉ„Éó2: Otsu‰∫åÂÄ§Âåñ</h1>
threshold = filters.threshold_otsu(denoised)
binary = denoised > threshold
binary = morphology.remove_small_objects(binary, min_size=30)

<h1>„Çπ„ÉÜ„ÉÉ„Éó3: Watershed</h1>
distance = distance_transform_edt(binary)
local_max = peak_local_max(distance, min_distance=15,
                           threshold_abs=3, labels=binary)
markers = np.zeros_like(distance, dtype=int)
markers[tuple(local_max.T)] = np.arange(1, len(local_max) + 1)
labels = watershed(-distance, markers, mask=binary)

<h1>„Çπ„ÉÜ„ÉÉ„Éó4: Á≤íÂæÑË®àÁÆó</h1>
diameters = []
for region in measure.regionprops(labels):
    area = region.area
    diameter = np.sqrt(4 * area / np.pi)
    diameters.append(diameter)

diameters = np.array(diameters)

<h1>„Çπ„ÉÜ„ÉÉ„Éó5: Áµ±Ë®à„ÉªÂèØË¶ñÂåñ</h1>
print("=== Á≤íÂæÑÁµ±Ë®à ===")
print(f"Ê§úÂá∫Á≤íÂ≠êÊï∞: {len(diameters)}")
print(f"Âπ≥ÂùáÁ≤íÂæÑ: {diameters.mean():.2f} pixels")
print(f"Ê®ôÊ∫ñÂÅèÂ∑Æ: {diameters.std():.2f} pixels")

fig, axes = plt.subplots(2, 2, figsize=(14, 14))

axes[0, 0].imshow(sample_image, cmap='gray')
axes[0, 0].set_title('Original Image')
axes[0, 0].axis('off')

axes[0, 1].imshow(binary, cmap='gray')
axes[0, 1].set_title(f'Binary (Otsu={threshold:.1f})')
axes[0, 1].axis('off')

axes[1, 0].imshow(labels, cmap='nipy_spectral')
axes[1, 0].set_title(f'Detected Particles ({len(diameters)})')
axes[1, 0].axis('off')

axes[1, 1].hist(diameters, bins=15, alpha=0.7, edgecolor='black')
axes[1, 1].axvline(diameters.mean(), color='red', linestyle='--',
                  label=f'Mean: {diameters.mean():.1f}')
axes[1, 1].set_xlabel('Diameter (pixels)')
axes[1, 1].set_ylabel('Frequency')
axes[1, 1].set_title('Particle Size Distribution')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()</code></pre>

<strong>Âá∫Âäõ‰æã</strong>:
<pre><code>=== Á≤íÂæÑÁµ±Ë®à ===
Ê§úÂá∫Á≤íÂ≠êÊï∞: 38
Âπ≥ÂùáÁ≤íÂæÑ: 30.45 pixels
Ê®ôÊ∫ñÂÅèÂ∑Æ: 8.23 pixels</code></pre>

<strong>Ëß£Ë™¨</strong>:
„Åì„ÅÆ‰æã„Åß„ÅØ„ÄÅWatershed„ÅÆmin_distance=15„Å´„Çà„Çä„ÄÅËøëÊé•Á≤íÂ≠ê„ÅÆÈÅéÊ§úÂá∫„ÇíÊäëÂà∂„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÁ≤íÂæÑ„ÅÆ„Å∞„Çâ„Å§„ÅçÔºàÊ®ôÊ∫ñÂÅèÂ∑ÆÔºâ„ÅØÂêàÊàêÈÅéÁ®ã„Å´Ëµ∑Âõ†„Åó„Åæ„Åô„ÄÇÂÆü„Éá„Éº„Çø„Åß„ÅØ„ÄÅÊ∏¨ÂÆöÊù°‰ª∂„ÇÑÊùêÊñô„ÅÆ‰∏çÂùá‰∏ÄÊÄß„ÇíÂèçÊò†„Åó„Åæ„Åô„ÄÇ

</details>

---

<h3>ÂïèÈ°å3ÔºàÈõ£ÊòìÂ∫¶ÔºöhardÔºâ</h3>

Ë§áÊï∞„ÅÆSEMÁîªÂÉè„ÇíËá™ÂãïÂá¶ÁêÜ„Åó„ÄÅÁ≤íÂæÑÂàÜÂ∏É„ÅÆÁµ±Ë®àÊØîËºÉ„ÇíË°å„ÅÜ„Ç∑„Çπ„ÉÜ„É†„ÇíÊßãÁØâ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

<strong>ËÉåÊôØ</strong>:
Áï∞„Å™„ÇãÂêàÊàêÊù°‰ª∂„Åß‰ΩúË£Ω„Åï„Çå„ÅüÊùêÊñôA„ÄÅB„ÄÅC„ÅÆ„Çµ„É≥„Éó„É´„Å´„Å§„ÅÑ„Å¶„ÄÅÂêÑ10Êûö„ÅÆSEMÁîªÂÉè„ÅåÊíÆÂΩ±„Åï„Çå„Åæ„Åó„Åü„ÄÇÂêÑ„Çµ„É≥„Éó„É´„ÅÆÁ≤íÂæÑÂàÜÂ∏É„ÇíËá™ÂãïËß£Êûê„Åó„ÄÅÁµ±Ë®àÁöÑ„Å´ÊØîËºÉ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ

<strong>Ë™≤È°å</strong>:
1. „Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Å´„Çà„Çä30Êûö„ÅÆÁîªÂÉè„ÇíËá™ÂãïËß£Êûê
2. „Çµ„É≥„Éó„É´„Åî„Å®„ÅÆÁ≤íÂæÑÂàÜÂ∏É„ÇíÂèØË¶ñÂåñ
3. ÂàÜÊï£ÂàÜÊûêÔºàANOVAÔºâ„Å´„Çà„ÇãÁµ±Ë®àÁöÑÊúâÊÑèÂ∑ÆÊ§úÂÆö
4. ÁµêÊûú„ÇíPDF„É¨„Éù„Éº„Éà„Å®„Åó„Å¶Âá∫Âäõ

<strong>Âà∂Á¥ÑÊù°‰ª∂</strong>:
- ÂêÑÁîªÂÉè„ÅÆÊ∏¨ÂÆöÊù°‰ª∂ÔºàÂÄçÁéá„ÄÅÈú≤ÂÖâÔºâ„ÅåÁï∞„Å™„ÇãÂèØËÉΩÊÄß
- ‰∏ÄÈÉ®ÁîªÂÉè„ÅØ„Ç≥„É≥„Éà„É©„Çπ„Éà‰∏çËâØ
- Âá¶ÁêÜÊôÇÈñìÔºö5Áßí‰ª•ÂÜÖ/ÁîªÂÉè

<details>
<summary>„Éí„É≥„Éà</summary>

<strong>Ë®≠Ë®àÊñπÈáù</strong>:
1. <code>SEMImageAnalyzer</code>„ÇØ„É©„Çπ„ÇíÊã°Âºµ
2. ÈÅ©ÂøúÁöÑÂâçÂá¶ÁêÜÔºà„Éí„Çπ„Éà„Ç∞„É©„É†Ëß£Êûê„Åß„Ç≥„É≥„Éà„É©„Çπ„ÉàÂà§ÂÆöÔºâ
3. ÁµêÊûú„ÇíÊßãÈÄ†Âåñ„Åó„Å¶‰øùÂ≠òÔºàJSON/CSVÔºâ
4. <code>scipy.stats.f_oneway</code>„ÅßANOVA
5. <code>matplotlib.backends.backend_pdf</code>„ÅßPDFÁîüÊàê

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>

<strong>Ëß£Á≠î„ÅÆÊ¶ÇË¶Å</strong>:
„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„ÄÅÁµ±Ë®àËß£Êûê„ÄÅ„É¨„Éù„Éº„ÉàÁîüÊàê„ÇíÂê´„ÇÄÁµ±Âêà„Ç∑„Çπ„ÉÜ„É†„ÇíÊßãÁØâ„Åó„Åæ„Åô„ÄÇ

<strong>ÂÆüË£Ö„Ç≥„Éº„Éâ</strong>:

<pre><code class="language-python">from scipy.stats import f_oneway
from matplotlib.backends.backend_pdf import PdfPages

class BatchSEMAnalyzer:
    """„Éê„ÉÉ„ÉÅSEMÁîªÂÉèËß£Êûê„Ç∑„Çπ„ÉÜ„É†"""

    def __init__(self):
        self.results = {}

    def adaptive_preprocess(self, image):
        """ÈÅ©ÂøúÁöÑÂâçÂá¶ÁêÜ"""
        # „Ç≥„É≥„Éà„É©„Çπ„ÉàË©ï‰æ°
        contrast = image.max() - image.min()

        if contrast < 100:  # ‰Ωé„Ç≥„É≥„Éà„É©„Çπ„Éà
            # CLAHEÂº∑Âåñ
            clahe = cv2.createCLAHE(clipLimit=3.0,
                                    tileGridSize=(8, 8))
            image = clahe.apply(image)

        # „Éé„Ç§„Ç∫Èô§ÂéªÔºàÈÅ©ÂøúÁöÑÂº∑Â∫¶Ôºâ
        noise_std = np.std(np.diff(image, axis=0))
        h = 10 if noise_std < 20 else 15

        denoised = cv2.fastNlMeansDenoising(image, None, h, 7, 21)
        return denoised

    def analyze_single(self, image, sample_id):
        """Âçò‰∏ÄÁîªÂÉèËß£Êûê"""
        # ÂâçÂá¶ÁêÜ
        preprocessed = self.adaptive_preprocess(image)

        # Otsu‰∫åÂÄ§Âåñ
        threshold = filters.threshold_otsu(preprocessed)
        binary = preprocessed > threshold
        binary = morphology.remove_small_objects(binary, min_size=30)

        # Watershed
        distance = distance_transform_edt(binary)
        local_max = peak_local_max(distance, min_distance=15,
                                   labels=binary)
        markers = np.zeros_like(distance, dtype=int)
        markers[tuple(local_max.T)] = np.arange(1, len(local_max) + 1)
        labels = watershed(-distance, markers, mask=binary)

        # Á≤íÂæÑÊäΩÂá∫
        diameters = []
        for region in measure.regionprops(labels):
            area = region.area
            diameter = np.sqrt(4 * area / np.pi)
            diameters.append(diameter)

        return np.array(diameters)

    def batch_analyze(self, image_dict):
        """
        „Éê„ÉÉ„ÉÅËß£Êûê

        Parameters:
        -----------
        image_dict : dict
            {'sample_A': [img1, img2, ...], 'sample_B': [...]}
        """
        for sample_id, images in image_dict.items():
            all_diameters = []

            for img in images:
                diameters = self.analyze_single(img, sample_id)
                all_diameters.extend(diameters)

            self.results[sample_id] = np.array(all_diameters)

    def statistical_comparison(self):
        """Áµ±Ë®àÁöÑÊØîËºÉÔºàANOVAÔºâ"""
        groups = list(self.results.values())
        f_stat, p_value = f_oneway(*groups)

        print("=== ÂàÜÊï£ÂàÜÊûêÔºàANOVAÔºâ===")
        print(f"FÁµ±Ë®àÈáè: {f_stat:.3f}")
        print(f"pÂÄ§: {p_value:.4f}")

        if p_value < 0.05:
            print("ÁµêË´ñ: „Çµ„É≥„Éó„É´Èñì„Å´ÊúâÊÑèÂ∑Æ„ÅÇ„ÇäÔºàp < 0.05Ôºâ")
        else:
            print("ÁµêË´ñ: „Çµ„É≥„Éó„É´Èñì„Å´ÊúâÊÑèÂ∑Æ„Å™„ÅóÔºàp ‚â• 0.05Ôºâ")

        return f_stat, p_value

    def generate_report(self, filename='sem_report.pdf'):
        """PDF„É¨„Éù„Éº„ÉàÁîüÊàê"""
        with PdfPages(filename) as pdf:
            # „Éö„Éº„Ç∏1: Á≤íÂæÑÂàÜÂ∏ÉÊØîËºÉ
            fig, axes = plt.subplots(2, 2, figsize=(11, 8.5))

            for i, (sample_id, diameters) in enumerate(self.results.items()):
                ax = axes.ravel()[i]
                ax.hist(diameters, bins=20, alpha=0.7, edgecolor='black')
                ax.axvline(diameters.mean(), color='red',
                          linestyle='--',
                          label=f'Mean: {diameters.mean():.1f}')
                ax.set_xlabel('Diameter (pixels)')
                ax.set_ylabel('Frequency')
                ax.set_title(f'{sample_id} (n={len(diameters)})')
                ax.legend()
                ax.grid(True, alpha=0.3, axis='y')

            # Áµ±Ë®à„Çµ„Éû„É™„Éº
            ax = axes.ravel()[3]
            ax.axis('off')
            summary_text = "=== Statistical Summary ===\n\n"
            for sample_id, diameters in self.results.items():
                summary_text += f"{sample_id}:\n"
                summary_text += f"  Mean: {diameters.mean():.2f}\n"
                summary_text += f"  Std: {diameters.std():.2f}\n"
                summary_text += f"  n: {len(diameters)}\n\n"

            ax.text(0.1, 0.5, summary_text, fontsize=12,
                   verticalalignment='center', family='monospace')

            plt.tight_layout()
            pdf.savefig(fig)
            plt.close()

            # „Éö„Éº„Ç∏2: Box plotÊØîËºÉ
            fig, ax = plt.subplots(figsize=(11, 8.5))
            data = [self.results[key] for key in self.results.keys()]
            ax.boxplot(data, labels=list(self.results.keys()))
            ax.set_ylabel('Diameter (pixels)')
            ax.set_title('Particle Size Distribution Comparison')
            ax.grid(True, alpha=0.3, axis='y')

            pdf.savefig(fig)
            plt.close()

        print(f"Report saved to {filename}")

<h1>„Éá„É¢ÂÆüË°å</h1>
if __name__ == "__main__":
    # „Çµ„É≥„Éó„É´„Éá„Éº„ÇøÁîüÊàê
    np.random.seed(42)

    image_dict = {}
    for sample_id, mean_size in [('Sample_A', 25),
                                   ('Sample_B', 35),
                                   ('Sample_C', 30)]:
        images = []
        for _ in range(10):
            img = np.zeros((512, 512), dtype=np.uint8)
            num_particles = np.random.randint(30, 50)

            for _ in range(num_particles):
                x = np.random.randint(30, 482)
                y = np.random.randint(30, 482)
                r = int(np.random.normal(mean_size, 5))
                r = max(10, min(40, r))
                cv2.circle(img, (x, y), r, 200, -1)

            noise = np.random.normal(0, 25, img.shape)
            img = np.clip(img + noise, 0, 255).astype(np.uint8)
            images.append(img)

        image_dict[sample_id] = images

    # „Éê„ÉÉ„ÉÅËß£Êûê
    analyzer = BatchSEMAnalyzer()
    analyzer.batch_analyze(image_dict)

    # Áµ±Ë®àÊØîËºÉ
    analyzer.statistical_comparison()

    # „É¨„Éù„Éº„ÉàÁîüÊàê
    analyzer.generate_report('sem_comparison_report.pdf')

    print("\n=== „Çµ„É≥„Éó„É´Âà•Áµ±Ë®à ===")
    for sample_id, diameters in analyzer.results.items():
        print(f"{sample_id}:")
        print(f"  Á≤íÂ≠êÊï∞: {len(diameters)}")
        print(f"  Âπ≥Âùá: {diameters.mean():.2f} ¬± {diameters.std():.2f}")</code></pre>

<strong>ÁµêÊûú‰æã</strong>:
<pre><code>=== ÂàÜÊï£ÂàÜÊûêÔºàANOVAÔºâ===
FÁµ±Ë®àÈáè: 124.567
pÂÄ§: 0.0001
ÁµêË´ñ: „Çµ„É≥„Éó„É´Èñì„Å´ÊúâÊÑèÂ∑Æ„ÅÇ„ÇäÔºàp < 0.05Ôºâ

=== „Çµ„É≥„Éó„É´Âà•Áµ±Ë®à ===
Sample_A:
  Á≤íÂ≠êÊï∞: 423
  Âπ≥Âùá: 25.12 ¬± 4.89
Sample_B:
  Á≤íÂ≠êÊï∞: 398
  Âπ≥Âùá: 35.34 ¬± 5.23
Sample_C:
  Á≤íÂ≠êÊï∞: 415
  Âπ≥Âùá: 30.05 ¬± 4.76

Report saved to sem_comparison_report.pdf</code></pre>

<strong>Ë©≥Á¥∞„Å™Ëß£Ë™¨</strong>:
1. <strong>ÈÅ©ÂøúÁöÑÂâçÂá¶ÁêÜ</strong>: ÂêÑÁîªÂÉè„ÅÆ„Ç≥„É≥„Éà„É©„Çπ„Éà„Å®„Éé„Ç§„Ç∫„É¨„Éô„É´„ÇíË©ï‰æ°„Åó„ÄÅ„Éë„É©„É°„Éº„ÇøËá™ÂãïË™øÊï¥
2. <strong>Áµ±Ë®àÊ§úÂÆö</strong>: ANOVA„Å´„Çà„Çä3Áæ§Èñì„ÅÆÁ≤íÂæÑÂ∑Æ„ÇíÂÆöÈáèË©ï‰æ°
3. <strong>PDFÂá∫Âäõ</strong>: Ë§áÊï∞„Éö„Éº„Ç∏„ÅÆ„É¨„Éù„Éº„ÉàËá™ÂãïÁîüÊàêÔºàË´ñÊñá„ÉªÂ†±ÂëäÊõ∏„Å´Áõ¥Êé•‰ΩøÁî®ÂèØËÉΩÔºâ

<strong>ËøΩÂä†„ÅÆÊ§úË®é‰∫ãÈ†Ö</strong>:
- Tukey HSDÊ§úÂÆö„Å´„Çà„ÇãÂ§öÈáçÊØîËºÉÔºà„Å©„ÅÆ„Éö„Ç¢Èñì„Å´ÊúâÊÑèÂ∑Æ„Åå„ÅÇ„Çã„ÅãÔºâ
- Á≤íÂæÑÂàÜÂ∏É„ÅÆÂΩ¢Áä∂ÊØîËºÉÔºàÊ≠™Â∫¶„ÄÅÂ∞ñÂ∫¶Ôºâ
- Ê©üÊ¢∞Â≠¶Áøí„Å´„Çà„ÇãÁîªÂÉèÂìÅË≥™Ë©ï‰æ°Ôºà‰∏çËâØÁîªÂÉè„ÅÆËá™ÂãïÈô§Â§ñÔºâ

</details>

---

<h2>ÂèÇËÄÉÊñáÁåÆ</h2>

1. Bradski, G., & Kaehler, A. (2008). "Learning OpenCV: Computer Vision with the OpenCV Library." O'Reilly Media. ISBN: 978-0596516130

2. van der Walt, S. et al. (2014). "scikit-image: image processing in Python." *PeerJ*, 2, e453. DOI: [10.7717/peerj.453](https://doi.org/10.7717/peerj.453)

3. Beucher, S., & Meyer, F. (1993). "The morphological approach to segmentation: the watershed transformation." *Mathematical Morphology in Image Processing*, 433-481.

4. Simonyan, K., & Zisserman, A. (2015). "Very Deep Convolutional Networks for Large-Scale Image Recognition." *ICLR 2015*. arXiv: [1409.1556](https://arxiv.org/abs/1409.1556)

5. OpenCV Documentation: Image Processing. URL: [https://docs.opencv.org/4.x/d2/d96/tutorial_py_table_of_contents_imgproc.html](https://docs.opencv.org/4.x/d2/d96/tutorial_py_table_of_contents_imgproc.html)

---

<h2>„Éä„Éì„Ç≤„Éº„Ç∑„Éß„É≥</h2>

<h3>Ââç„ÅÆÁ´†</h3>
<strong>[Á¨¨2Á´†:„Çπ„Éö„ÇØ„Éà„É´„Éá„Éº„ÇøËß£Êûê ‚Üê](./chapter-2.md)</strong>

<h3>Ê¨°„ÅÆÁ´†</h3>
<strong>[Á¨¨4Á´†ÔºöÊôÇÁ≥ªÂàó„Éá„Éº„Çø„Å®Áµ±ÂêàËß£Êûê ‚Üí](./chapter-4.md)</strong>

<h3>„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°</h3>
<strong>[‚Üê „Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã](./index.md)</strong>

---

<h2>ËëóËÄÖÊÉÖÂ†±</h2>

<strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team
<strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ
<strong>‰ΩúÊàêÊó•</strong>: 2025-10-17
<strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0

<strong>Êõ¥Êñ∞Â±•Ê≠¥</strong>:
- 2025-10-17: v1.0 ÂàùÁâàÂÖ¨Èñã

<strong>„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ</strong>:
- GitHub Issues: [„É™„Éù„Ç∏„Éà„É™URL]/issues
- Email: yusuke.hashimoto.b8@tohoku.ac.jp

<strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0

---

<strong>Ê¨°„ÅÆÁ´†„ÅßÂ≠¶Áøí„ÇíÁ∂ö„Åë„Åæ„Åó„Çá„ÅÜÔºÅ</strong>
<div class="navigation">
    <a href="chapter-2.html" class="nav-button">‚Üê Á¨¨2Á´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter-4.html" class="nav-button">Á¨¨4Á´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-17</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>
