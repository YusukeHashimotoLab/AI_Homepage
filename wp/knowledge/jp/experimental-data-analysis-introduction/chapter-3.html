<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第3章：画像データ解析 - AI Terakoya</title>
    <style>
        :root {
            --color-primary-900: #1a252f;
            --color-primary-700: #2c3e50;
            --color-primary-500: #34495e;
            --color-primary-300: #4a5f7a;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-bg: #ffffff;
            --color-bg-secondary: #f8f9fa;
            --color-text: #2c3e50;
            --color-text-light: #6c757d;
            --color-border: #e9ecef;
            --color-code-bg: #f8f9fa;
            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;
            --font-base: 1rem;
            --font-sm: 0.875rem;
            --font-lg: 1.125rem;
            --font-xl: 1.5rem;
            --font-2xl: 2rem;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Noto Sans JP", sans-serif;
            line-height: 1.7;
            color: var(--color-text);
            background: var(--color-bg);
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            text-align: center;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }

        .header-content {
            max-width: 800px;
            margin: 0 auto;
        }

        header h1 {
            font-size: var(--font-2xl);
            margin-bottom: var(--spacing-sm);
            font-weight: 700;
        }

        .subtitle {
            font-size: var(--font-lg);
            opacity: 0.95;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            justify-content: center;
            font-size: var(--font-sm);
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: var(--spacing-xl) var(--spacing-md);
        }

        h2 {
            font-size: var(--font-xl);
            color: var(--color-accent);
            margin: var(--spacing-lg) 0 var(--spacing-md) 0;
            padding-bottom: var(--spacing-xs);
            border-bottom: 2px solid var(--color-accent-light);
        }

        h3 {
            font-size: var(--font-lg);
            color: var(--color-primary-700);
            margin: var(--spacing-md) 0 var(--spacing-sm) 0;
        }

        h4 {
            font-size: var(--font-base);
            color: var(--color-primary-500);
            margin: var(--spacing-sm) 0;
        }

        p {
            margin-bottom: var(--spacing-md);
            line-height: 1.8;
        }

        ul, ol {
            margin: var(--spacing-md) 0;
            padding-left: var(--spacing-lg);
        }

        li {
            margin-bottom: var(--spacing-xs);
        }

        code {
            background: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
            color: var(--color-accent);
        }

        pre {
            background: var(--color-code-bg);
            border-left: 4px solid var(--color-accent);
            padding: var(--spacing-md);
            overflow-x: auto;
            border-radius: 4px;
            margin: var(--spacing-md) 0;
        }

        pre code {
            background: none;
            padding: 0;
            color: var(--color-text);
        }

        .mermaid {
            background: white;
            padding: var(--spacing-md);
            border-radius: 8px;
            margin: var(--spacing-md) 0;
            text-align: center;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: var(--spacing-md) 0;
            background: white;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        th, td {
            padding: var(--spacing-sm);
            text-align: left;
            border-bottom: 1px solid var(--color-border);
        }

        th {
            background: var(--color-accent);
            color: white;
            font-weight: 600;
        }

        tr:hover {
            background: var(--color-bg-secondary);
        }

        details {
            background: var(--color-bg-secondary);
            padding: var(--spacing-md);
            border-radius: 8px;
            margin: var(--spacing-md) 0;
            border: 1px solid var(--color-border);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-accent);
            padding: var(--spacing-sm);
            margin: calc(-1 * var(--spacing-md));
            margin-bottom: var(--spacing-md);
            background: white;
            border-radius: 8px 8px 0 0;
        }

        summary:hover {
            background: var(--color-bg-secondary);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 1px solid var(--color-border);
        }

        .btn {
            display: inline-block;
            padding: var(--spacing-sm) var(--spacing-md);
            border-radius: 6px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s;
            min-height: 44px;
            display: flex;
            align-items: center;
        }

        .btn-primary {
            background: var(--color-accent);
            color: white;
        }

        .btn-secondary {
            background: var(--color-primary-500);
            color: white;
        }

        @media (hover: hover) and (pointer: fine) {
            .btn-primary:hover {
                background: var(--color-accent-light);
                transform: translateY(-2px);
                box-shadow: 0 4px 8px rgba(123, 44, 191, 0.3);
            }

            .btn-secondary:hover {
                background: var(--color-primary-700);
                transform: translateY(-2px);
                box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
            }
        }

        footer {
            background: var(--color-primary-900);
            color: white;
            padding: var(--spacing-lg);
            text-align: center;
            margin-top: var(--spacing-xl);
        }

        footer a {
            color: var(--color-accent-light);
            text-decoration: none;
        }

        @media (max-width: 768px) {
            header h1 {
                font-size: var(--font-xl);
            }

            .subtitle {
                font-size: var(--font-base);
            }

            .meta {
                font-size: 0.8rem;
            }

            .navigation {
                flex-direction: column;
            }
        }
    </style>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第3章：画像データ解析</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">📖 読了時間: 30-35分</span>
                <span class="meta-item">📊 難易度: 中級</span>
                <span class="meta-item">💻 コード例: 13</span>
                <span class="meta-item">📝 演習問題: 3</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p>
<h1>第3章：画像データ解析</h1>
</p>
<p>
<strong>SEM・TEM画像の自動解析 - 粒子検出から深層学習まで</strong>
</p>
<p>
<h2>学習目標</h2>
</p>
<p>
この章を読むことで、以下を習得できます：
</p>
<p>
<ul><li>✅ SEM・TEM画像の前処理（ノイズ除去、コントラスト調整）を実行できる</li>
<li>✅ Watershed法による粒子検出を実装できる</li>
<li>✅ 粒径分布、形状パラメータ（円形度、アスペクト比）を定量できる</li>
<li>✅ CNNによる材料画像分類（転移学習）を実行できる</li>
<li>✅ OpenCV・scikit-imageを使った画像解析パイプラインを構築できる</li>
</p>
<p>
<strong>読了時間</strong>: 30-35分
<strong>コード例</strong>: 13個
<strong>演習問題</strong>: 3問
</p>
<p>
---
</p>
<p>
<h2>3.1 画像データの特徴と前処理戦略</h2>
</p>
<p>
<h3>SEM・TEM画像の特徴</h3>
</p>
<p>
電子顕微鏡画像は材料のナノ〜マイクロスケール構造を可視化する強力なツールです。
</p>
<p>
<table>
<thead>
<tr>
<th>測定技術</th>
<th>空間分解能</th>
<th>典型的な視野</th>
<th>主な情報</th>
<th>画像特性</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SEM</strong></td>
<td>数nm～数μm</td>
<td>10μm～1mm</td>
<td>表面形態、組織</td>
<td>深い被写界深度、影効果</td>
</tr>
<tr>
<td><strong>TEM</strong></td>
<td>原子レベル</td>
<td>数十nm～数μm</td>
<td>内部構造、結晶性</td>
<td>高コントラスト、回折像</td>
</tr>
<tr>
<td><strong>STEM</strong></td>
<td>サブnm</td>
<td>数十nm～数百nm</td>
<td>原子配列、元素分布</td>
<td>原子分解能</td>
</tr>
</tbody>
</table></p>
<p>
<h3>画像解析の典型的ワークフロー</h3>
</p>
<p>
<pre><code class="language-mermaid">flowchart TD
    A[画像取得] --> B[前処理]
    B --> C[セグメンテーション]
    C --> D[特徴抽出]
    D --> E[定量解析]
    E --> F[統計処理]
    F --> G[可視化・報告]
</p>
<p>
    B --> B1[ノイズ除去]
    B --> B2[コントラスト調整]
    B --> B3[二値化]
</p>
<p>
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style G fill:#fff9c4
</code></pre>
</p>
<p>
---
</p>
<p>
<h2>3.2 画像前処理</h2>
</p>
<p>
<h3>ノイズ除去</h3>
</p>
<p>
<strong>コード例1: 各種ノイズ除去フィルタの比較</strong>
</p>
<p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
import cv2
from skimage import filters, io
from scipy import ndimage
</p>
<p>
<h1>サンプルSEM画像生成（粒子を模擬）</h1>
def generate_synthetic_sem(size=512, num_particles=30):
    """合成SEM画像の生成"""
    image = np.zeros((size, size), dtype=np.float32)
</p>
<p>
    # ランダムな粒子配置
    np.random.seed(42)
    for _ in range(num_particles):
        x = np.random.randint(50, size - 50)
        y = np.random.randint(50, size - 50)
        radius = np.random.randint(15, 35)
</p>
<p>
        # 円形粒子
        Y, X = np.ogrid[:size, :size]
        mask = (X - x)<strong>2 + (Y - y)</strong>2 <= radius**2
        image[mask] = 200
</p>
<p>
    # ガウシアンノイズ追加
    noise = np.random.normal(0, 25, image.shape)
    noisy_image = np.clip(image + noise, 0, 255).astype(np.uint8)
</p>
<p>
    return noisy_image
</p>
<p>
<h1>画像生成</h1>
noisy_image = generate_synthetic_sem()
</p>
<p>
<h1>各種ノイズ除去フィルタ</h1>
gaussian_blur = cv2.GaussianBlur(noisy_image, (5, 5), 1.0)
median_filter = cv2.medianBlur(noisy_image, 5)
bilateral_filter = cv2.bilateralFilter(noisy_image, 9, 75, 75)
nlm_filter = cv2.fastNlMeansDenoising(noisy_image, None, 10, 7, 21)
</p>
<p>
<h1>可視化</h1>
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
</p>
<p>
axes[0, 0].imshow(noisy_image, cmap='gray')
axes[0, 0].set_title('Noisy SEM Image')
axes[0, 0].axis('off')
</p>
<p>
axes[0, 1].imshow(gaussian_blur, cmap='gray')
axes[0, 1].set_title('Gaussian Blur')
axes[0, 1].axis('off')
</p>
<p>
axes[0, 2].imshow(median_filter, cmap='gray')
axes[0, 2].set_title('Median Filter')
axes[0, 2].axis('off')
</p>
<p>
axes[1, 0].imshow(bilateral_filter, cmap='gray')
axes[1, 0].set_title('Bilateral Filter')
axes[1, 0].axis('off')
</p>
<p>
axes[1, 1].imshow(nlm_filter, cmap='gray')
axes[1, 1].set_title('Non-Local Means')
axes[1, 1].axis('off')
</p>
<p>
<h1>ノイズレベル比較</h1>
axes[1, 2].bar(['Original', 'Gaussian', 'Median', 'Bilateral', 'NLM'],
               [np.std(noisy_image),
                np.std(gaussian_blur),
                np.std(median_filter),
                np.std(bilateral_filter),
                np.std(nlm_filter)])
axes[1, 2].set_ylabel('Noise Level (std)')
axes[1, 2].set_title('Denoising Performance')
axes[1, 2].grid(True, alpha=0.3, axis='y')
</p>
<p>
plt.tight_layout()
plt.show()
</p>
<p>
print("=== ノイズレベル（標準偏差） ===")
print(f"元画像: {np.std(noisy_image):.2f}")
print(f"Gaussian: {np.std(gaussian_blur):.2f}")
print(f"Median: {np.std(median_filter):.2f}")
print(f"Bilateral: {np.std(bilateral_filter):.2f}")
print(f"NLM: {np.std(nlm_filter):.2f}")
</code></pre>
</p>
<p>
<strong>フィルタの使い分け</strong>:
<li><strong>Gaussian</strong>: 高速、エッジが滑らか → 一般的な前処理</li>
<li><strong>Median</strong>: エッジ保持、ソルト＆ペッパーノイズに強い</li>
<li><strong>Bilateral</strong>: エッジ保持が優秀、計算コスト中</li>
<li><strong>Non-Local Means</strong>: 最高品質、計算コスト大</li>
</p>
<p>
<h3>コントラスト調整</h3>
</p>
<p>
<strong>コード例2: ヒストグラム均等化とCLAHE</strong>
</p>
<p>
<pre><code class="language-python">from skimage import exposure
</p>
<p>
<h1>コントラスト調整</h1>
hist_eq = exposure.equalize_hist(noisy_image)
clahe = exposure.equalize_adapthist(noisy_image, clip_limit=0.03)
</p>
<p>
<h1>ヒストグラム計算</h1>
hist_original = np.histogram(noisy_image, bins=256, range=(0, 256))[0]
hist_eq_vals = np.histogram(
    (hist_eq * 255).astype(np.uint8), bins=256, range=(0, 256))[0]
hist_clahe_vals = np.histogram(
    (clahe * 255).astype(np.uint8), bins=256, range=(0, 256))[0]
</p>
<p>
<h1>可視化</h1>
fig = plt.figure(figsize=(16, 10))
</p>
<p>
<h1>画像</h1>
ax1 = plt.subplot(2, 3, 1)
ax1.imshow(noisy_image, cmap='gray')
ax1.set_title('Original')
ax1.axis('off')
</p>
<p>
ax2 = plt.subplot(2, 3, 2)
ax2.imshow(hist_eq, cmap='gray')
ax2.set_title('Histogram Equalization')
ax2.axis('off')
</p>
<p>
ax3 = plt.subplot(2, 3, 3)
ax3.imshow(clahe, cmap='gray')
ax3.set_title('CLAHE (Adaptive)')
ax3.axis('off')
</p>
<p>
<h1>ヒストグラム</h1>
ax4 = plt.subplot(2, 3, 4)
ax4.hist(noisy_image.ravel(), bins=256, range=(0, 256), alpha=0.7)
ax4.set_xlabel('Pixel Value')
ax4.set_ylabel('Frequency')
ax4.set_title('Original Histogram')
ax4.grid(True, alpha=0.3)
</p>
<p>
ax5 = plt.subplot(2, 3, 5)
ax5.hist((hist_eq * 255).astype(np.uint8).ravel(),
         bins=256, range=(0, 256), alpha=0.7, color='orange')
ax5.set_xlabel('Pixel Value')
ax5.set_ylabel('Frequency')
ax5.set_title('Histogram Eq. Histogram')
ax5.grid(True, alpha=0.3)
</p>
<p>
ax6 = plt.subplot(2, 3, 6)
ax6.hist((clahe * 255).astype(np.uint8).ravel(),
         bins=256, range=(0, 256), alpha=0.7, color='green')
ax6.set_xlabel('Pixel Value')
ax6.set_ylabel('Frequency')
ax6.set_title('CLAHE Histogram')
ax6.grid(True, alpha=0.3)
</p>
<p>
plt.tight_layout()
plt.show()
</p>
<p>
print("=== コントラスト指標 ===")
print(f"元画像のコントラスト: {noisy_image.max() - noisy_image.min()}")
print(f"Histogram Eq.: {(hist_eq <em> 255).max() - (hist_eq </em> 255).min():.1f}")
print(f"CLAHE: {(clahe <em> 255).max() - (clahe </em> 255).min():.1f}")
</code></pre>
</p>
<p>
<strong>CLAHE（Contrast Limited Adaptive Histogram Equalization）の利点</strong>:
<li>局所的なコントラスト向上</li>
<li>過剰な強調を抑制（clip_limitパラメータ）</li>
<li>SEM画像の暗部・明部両方で詳細が見える</li>
</p>
<p>
---
</p>
<p>
<h2>3.3 粒子検出（Watershed法）</h2>
</p>
<p>
<h3>二値化と距離変換</h3>
</p>
<p>
<strong>コード例3: Otsu法による自動二値化</strong>
</p>
<p>
<pre><code class="language-python">from skimage import morphology, measure
from scipy.ndimage import distance_transform_edt
</p>
<p>
<h1>ノイズ除去後の画像を使用</h1>
denoised = cv2.fastNlMeansDenoising(noisy_image, None, 10, 7, 21)
</p>
<p>
<h1>Otsu法による二値化</h1>
threshold = filters.threshold_otsu(denoised)
binary = denoised > threshold
</p>
<p>
<h1>モルフォロジー演算（小さなノイズ除去）</h1>
binary_cleaned = morphology.remove_small_objects(binary, min_size=50)
binary_cleaned = morphology.remove_small_holes(binary_cleaned, area_threshold=50)
</p>
<p>
<h1>距離変換</h1>
distance = distance_transform_edt(binary_cleaned)
</p>
<p>
<h1>可視化</h1>
fig, axes = plt.subplots(2, 2, figsize=(12, 12))
</p>
<p>
axes[0, 0].imshow(denoised, cmap='gray')
axes[0, 0].set_title('Denoised Image')
axes[0, 0].axis('off')
</p>
<p>
axes[0, 1].imshow(binary, cmap='gray')
axes[0, 1].set_title(f'Binary (Otsu threshold={threshold:.1f})')
axes[0, 1].axis('off')
</p>
<p>
axes[1, 0].imshow(binary_cleaned, cmap='gray')
axes[1, 0].set_title('After Morphology')
axes[1, 0].axis('off')
</p>
<p>
axes[1, 1].imshow(distance, cmap='jet')
axes[1, 1].set_title('Distance Transform')
axes[1, 1].axis('off')
axes[1, 1].colorbar = plt.colorbar(axes[1, 1].imshow(distance, cmap='jet'),
                                   ax=axes[1, 1])
</p>
<p>
plt.tight_layout()
plt.show()
</p>
<p>
print(f"=== 二値化結果 ===")
print(f"Otsu閾値: {threshold:.1f}")
print(f"白ピクセル割合: {binary_cleaned.sum() / binary_cleaned.size * 100:.1f}%")
</code></pre>
</p>
<p>
<h3>Watershed法による粒子分離</h3>
</p>
<p>
<strong>コード例4: Watershed セグメンテーション</strong>
</p>
<p>
<pre><code class="language-python">from skimage.feature import peak_local_max
from skimage.segmentation import watershed
</p>
<p>
<h1>局所最大値検出（粒子中心の推定）</h1>
local_max = peak_local_max(
    distance,
    min_distance=20,
    threshold_abs=5,
    labels=binary_cleaned
)
</p>
<p>
<h1>マーカー作成</h1>
markers = np.zeros_like(distance, dtype=int)
markers[tuple(local_max.T)] = np.arange(1, len(local_max) + 1)
</p>
<p>
<h1>Watershed実行</h1>
labels = watershed(-distance, markers, mask=binary_cleaned)
</p>
<p>
<h1>可視化</h1>
fig, axes = plt.subplots(1, 3, figsize=(18, 6))
</p>
<p>
<h1>マーカー表示</h1>
axes[0].imshow(denoised, cmap='gray')
axes[0].plot(local_max[:, 1], local_max[:, 0], 'r+',
             markersize=12, markeredgewidth=2)
axes[0].set_title(f'Detected Centers ({len(local_max)} particles)')
axes[0].axis('off')
</p>
<p>
<h1>Watershedラベル</h1>
axes[1].imshow(labels, cmap='nipy_spectral')
axes[1].set_title('Watershed Segmentation')
axes[1].axis('off')
</p>
<p>
<h1>輪郭重ね合わせ</h1>
overlay = denoised.copy()
overlay_rgb = cv2.cvtColor(overlay, cv2.COLOR_GRAY2RGB)
for region in measure.regionprops(labels):
    minr, minc, maxr, maxc = region.bbox
    cv2.rectangle(overlay_rgb, (minc, minr), (maxc, maxr),
                  (255, 0, 0), 2)
</p>
<p>
axes[2].imshow(overlay_rgb)
axes[2].set_title('Detected Particles')
axes[2].axis('off')
</p>
<p>
plt.tight_layout()
plt.show()
</p>
<p>
print(f"=== Watershed結果 ===")
print(f"検出された粒子数: {len(local_max)}")
print(f"ラベル数: {labels.max()}")
</code></pre>
</p>
<p>
---
</p>
<p>
<h2>3.4 粒径分布解析</h2>
</p>
<p>
<h3>粒子特徴量の抽出</h3>
</p>
<p>
<strong>コード例5: 粒径・形状パラメータの計算</strong>
</p>
<p>
<pre><code class="language-python"># 各粒子の特徴量抽出
particle_data = []
</p>
<p>
for region in measure.regionprops(labels):
    # 面積から円相当直径を計算
    area = region.area
    equivalent_diameter = np.sqrt(4 * area / np.pi)
</p>
<p>
    # アスペクト比
    major_axis = region.major_axis_length
    minor_axis = region.minor_axis_length
    aspect_ratio = major_axis / (minor_axis + 1e-10)
</p>
<p>
    # 円形度（4π×面積/周囲長^2）
    perimeter = region.perimeter
    circularity = 4 <em> np.pi </em> area / (perimeter ** 2 + 1e-10)
</p>
<p>
    particle_data.append({
        'label': region.label,
        'area': area,
        'diameter': equivalent_diameter,
        'aspect_ratio': aspect_ratio,
        'circularity': circularity,
        'centroid': region.centroid
    })
</p>
<p>
<h1>DataFrameに変換</h1>
import pandas as pd
df_particles = pd.DataFrame(particle_data)
</p>
<p>
print("=== 粒子特徴量統計 ===")
print(df_particles[['diameter', 'aspect_ratio', 'circularity']].describe())
</p>
<p>
<h1>粒径分布プロット</h1>
fig, axes = plt.subplots(2, 2, figsize=(14, 12))
</p>
<p>
<h1>ヒストグラム</h1>
axes[0, 0].hist(df_particles['diameter'], bins=20, alpha=0.7,
                edgecolor='black')
axes[0, 0].set_xlabel('Diameter (pixels)')
axes[0, 0].set_ylabel('Frequency')
axes[0, 0].set_title('Particle Size Distribution')
axes[0, 0].axvline(df_particles['diameter'].mean(), color='red',
                   linestyle='--', label=f'Mean: {df_particles["diameter"].mean():.1f}')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3, axis='y')
</p>
<p>
<h1>累積分布</h1>
sorted_diameters = np.sort(df_particles['diameter'])
cumulative = np.arange(1, len(sorted_diameters) + 1) / len(sorted_diameters) * 100
axes[0, 1].plot(sorted_diameters, cumulative, linewidth=2)
axes[0, 1].set_xlabel('Diameter (pixels)')
axes[0, 1].set_ylabel('Cumulative Percentage (%)')
axes[0, 1].set_title('Cumulative Size Distribution')
axes[0, 1].axhline(50, color='red', linestyle='--',
                   label=f'D50: {np.median(df_particles["diameter"]):.1f}')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)
</p>
<p>
<h1>散布図（直径 vs アスペクト比）</h1>
axes[1, 0].scatter(df_particles['diameter'],
                   df_particles['aspect_ratio'],
                   alpha=0.6, s=50)
axes[1, 0].set_xlabel('Diameter (pixels)')
axes[1, 0].set_ylabel('Aspect Ratio')
axes[1, 0].set_title('Diameter vs Aspect Ratio')
axes[1, 0].grid(True, alpha=0.3)
</p>
<p>
<h1>散布図（直径 vs 円形度）</h1>
axes[1, 1].scatter(df_particles['diameter'],
                   df_particles['circularity'],
                   alpha=0.6, s=50, color='green')
axes[1, 1].set_xlabel('Diameter (pixels)')
axes[1, 1].set_ylabel('Circularity')
axes[1, 1].set_title('Diameter vs Circularity')
axes[1, 1].axhline(0.8, color='red', linestyle='--',
                   label='Spherical threshold')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)
</p>
<p>
plt.tight_layout()
plt.show()
</p>
<p>
<h1>粒径統計</h1>
print("\n=== 粒径統計 ===")
print(f"平均直径: {df_particles['diameter'].mean():.2f} pixels")
print(f"中央値(D50): {df_particles['diameter'].median():.2f} pixels")
print(f"標準偏差: {df_particles['diameter'].std():.2f} pixels")
print(f"最小直径: {df_particles['diameter'].min():.2f} pixels")
print(f"最大直径: {df_particles['diameter'].max():.2f} pixels")
</code></pre>
</p>
<p>
<h3>粒径分布フィッティング（対数正規分布）</h3>
</p>
<p>
<strong>コード例6: 対数正規分布フィッティング</strong>
</p>
<p>
<pre><code class="language-python">from scipy.stats import lognorm
</p>
<p>
<h1>対数正規分布のフィッティング</h1>
diameters = df_particles['diameter'].values
shape, loc, scale = lognorm.fit(diameters, floc=0)
</p>
<p>
<h1>フィッティング結果</h1>
x = np.linspace(diameters.min(), diameters.max(), 200)
pdf_fitted = lognorm.pdf(x, shape, loc, scale)
</p>
<p>
<h1>可視化</h1>
plt.figure(figsize=(12, 6))
</p>
<p>
plt.subplot(1, 2, 1)
plt.hist(diameters, bins=20, density=True, alpha=0.6,
         label='Observed', edgecolor='black')
plt.plot(x, pdf_fitted, 'r-', linewidth=2,
         label=f'Log-normal fit (σ={shape:.2f})')
plt.xlabel('Diameter (pixels)')
plt.ylabel('Probability Density')
plt.title('Particle Size Distribution Fitting')
plt.legend()
plt.grid(True, alpha=0.3)
</p>
<p>
<h1>Q-Qプロット（適合度確認）</h1>
plt.subplot(1, 2, 2)
theoretical_quantiles = lognorm.ppf(np.linspace(0.01, 0.99, 100),
                                    shape, loc, scale)
observed_quantiles = np.percentile(diameters, np.linspace(1, 99, 100))
plt.scatter(theoretical_quantiles, observed_quantiles, alpha=0.6)
plt.plot([diameters.min(), diameters.max()],
         [diameters.min(), diameters.max()],
         'r--', linewidth=2, label='Perfect fit')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Observed Quantiles')
plt.title('Q-Q Plot (Log-normal)')
plt.legend()
plt.grid(True, alpha=0.3)
</p>
<p>
plt.tight_layout()
plt.show()
</p>
<p>
print("=== 対数正規分布パラメータ ===")
print(f"Shape (σ): {shape:.3f}")
print(f"Scale (median): {scale:.2f} pixels")
</code></pre>
</p>
<p>
---
</p>
<p>
<h2>3.5 深層学習による画像分類</h2>
</p>
<p>
<h3>転移学習（VGG16）による材料画像分類</h3>
</p>
<p>
<strong>コード例7: CNNによる材料相分類</strong>
</p>
<p>
<pre><code class="language-python"># TensorFlow/Kerasのインポート
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.applications import VGG16
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("TensorFlow not available. Skipping this example.")
</p>
<p>
if TENSORFLOW_AVAILABLE:
    # サンプル画像データ生成（3クラス分類）
    def generate_material_images(num_samples=100, img_size=128):
        """
        材料画像のサンプル生成
        クラス0: 球形粒子
        クラス1: 棒状粒子
        クラス2: 不定形粒子
        """
        images = []
        labels = []
</p>
<p>
        for class_id in range(3):
            for _ in range(num_samples):
                img = np.zeros((img_size, img_size), dtype=np.uint8)
</p>
<p>
                if class_id == 0:  # 球形
                    num_particles = np.random.randint(5, 15)
                    for _ in range(num_particles):
                        x = np.random.randint(20, img_size - 20)
                        y = np.random.randint(20, img_size - 20)
                        r = np.random.randint(8, 15)
                        cv2.circle(img, (x, y), r, 200, -1)
</p>
<p>
                elif class_id == 1:  # 棒状
                    num_rods = np.random.randint(3, 8)
                    for _ in range(num_rods):
                        x1 = np.random.randint(10, img_size - 10)
                        y1 = np.random.randint(10, img_size - 10)
                        length = np.random.randint(30, 60)
                        angle = np.random.rand() <em> 2 </em> np.pi
                        x2 = int(x1 + length * np.cos(angle))
                        y2 = int(y1 + length * np.sin(angle))
                        cv2.line(img, (x1, y1), (x2, y2), 200, 3)
</p>
<p>
                else:  # 不定形
                    num_shapes = np.random.randint(5, 12)
                    for _ in range(num_shapes):
                        pts = np.random.randint(10, img_size - 10,
                                                size=(6, 2))
                        cv2.fillPoly(img, [pts], 200)
</p>
<p>
                # ノイズ追加
                noise = np.random.normal(0, 20, img.shape)
                img = np.clip(img + noise, 0, 255).astype(np.uint8)
</p>
<p>
                # RGB変換
                img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
                images.append(img_rgb)
                labels.append(class_id)
</p>
<p>
        return np.array(images), np.array(labels)
</p>
<p>
    # データ生成
    X_data, y_data = generate_material_images(num_samples=150)
</p>
<p>
    # 訓練・テストデータ分割
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X_data, y_data, test_size=0.2, random_state=42
    )
</p>
<p>
    # データ正規化
    X_train = X_train.astype('float32') / 255.0
    X_test = X_test.astype('float32') / 255.0
</p>
<p>
    # VGG16モデルの読み込み（ImageNet重み）
    base_model = VGG16(
        weights='imagenet',
        include_top=False,
        input_shape=(128, 128, 3)
    )
</p>
<p>
    # ベースモデルの重みを固定
    base_model.trainable = False
</p>
<p>
    # 新しい分類層を追加
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    predictions = Dense(3, activation='softmax')(x)
</p>
<p>
    model = Model(inputs=base_model.input, outputs=predictions)
</p>
<p>
    # モデルコンパイル
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
</p>
<p>
    # モデル訓練
    history = model.fit(
        X_train, y_train,
        validation_split=0.2,
        epochs=10,
        batch_size=16,
        verbose=0
    )
</p>
<p>
    # テストデータで評価
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
</p>
<p>
    print("=== CNN分類結果 ===")
    print(f"テスト精度: {test_acc * 100:.2f}%")
</p>
<p>
    # 学習曲線プロット
    plt.figure(figsize=(14, 5))
</p>
<p>
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
</p>
<p>
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)
</p>
<p>
    plt.tight_layout()
    plt.show()
</p>
<p>
    # 混同行列
    from sklearn.metrics import confusion_matrix, classification_report
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
</p>
<p>
    cm = confusion_matrix(y_test, y_pred_classes)
</p>
<p>
    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.colorbar()
    tick_marks = np.arange(3)
    plt.xticks(tick_marks, ['Spherical', 'Rod', 'Irregular'])
    plt.yticks(tick_marks, ['Spherical', 'Rod', 'Irregular'])
</p>
<p>
    # 数値表示
    for i in range(3):
        for j in range(3):
            plt.text(j, i, cm[i, j], ha='center', va='center',
                    color='white' if cm[i, j] > cm.max() / 2 else 'black')
</p>
<p>
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    plt.show()
</p>
<p>
    print("\n=== 分類レポート ===")
    print(classification_report(
        y_test, y_pred_classes,
        target_names=['Spherical', 'Rod', 'Irregular']
    ))
</code></pre>
</p>
<p>
---
</p>
<p>
<h2>3.6 統合画像解析パイプライン</h2>
</p>
<p>
<h3>自動解析システムの構築</h3>
</p>
<p>
<strong>コード例8: 画像解析パイプラインクラス</strong>
</p>
<p>
<pre><code class="language-python">from dataclasses import dataclass
from typing import List, Dict
import json
</p>
<p>
@dataclass
class ParticleAnalysisResult:
    """粒子解析結果"""
    num_particles: int
    mean_diameter: float
    std_diameter: float
    mean_circularity: float
    particle_data: List[Dict]
</p>
<p>
class SEMImageAnalyzer:
    """SEM画像自動解析システム"""
</p>
<p>
    def __init__(self, img_size=(512, 512)):
        self.img_size = img_size
        self.image = None
        self.binary = None
        self.labels = None
        self.particles = []
</p>
<p>
    def load_image(self, image: np.ndarray):
        """画像読み込み"""
        if len(image.shape) == 3:
            self.image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            self.image = image
</p>
<p>
        # リサイズ
        self.image = cv2.resize(self.image, self.img_size)
</p>
<p>
    def preprocess(self, denoise_strength=10):
        """前処理（ノイズ除去・コントラスト調整）"""
        # ノイズ除去
        denoised = cv2.fastNlMeansDenoising(
            self.image, None, denoise_strength, 7, 21
        )
</p>
<p>
        # CLAHE
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(denoised)
</p>
<p>
        self.image = enhanced
</p>
<p>
    def segment_particles(self, min_size=50):
        """粒子セグメンテーション"""
        # Otsu二値化
        threshold = filters.threshold_otsu(self.image)
        binary = self.image > threshold
</p>
<p>
        # モルフォロジー
        binary_cleaned = morphology.remove_small_objects(
            binary, min_size=min_size
        )
        binary_cleaned = morphology.remove_small_holes(
            binary_cleaned, area_threshold=min_size
        )
</p>
<p>
        # 距離変換
        distance = distance_transform_edt(binary_cleaned)
</p>
<p>
        # Watershed
        local_max = peak_local_max(
            distance,
            min_distance=20,
            threshold_abs=5,
            labels=binary_cleaned
        )
</p>
<p>
        markers = np.zeros_like(distance, dtype=int)
        markers[tuple(local_max.T)] = np.arange(1, len(local_max) + 1)
</p>
<p>
        self.labels = watershed(-distance, markers, mask=binary_cleaned)
        self.binary = binary_cleaned
</p>
<p>
    def extract_features(self):
        """特徴量抽出"""
        self.particles = []
</p>
<p>
        for region in measure.regionprops(self.labels):
            area = region.area
            diameter = np.sqrt(4 * area / np.pi)
            aspect_ratio = region.major_axis_length / \
                          (region.minor_axis_length + 1e-10)
            circularity = 4 <em> np.pi </em> area / \
                         (region.perimeter ** 2 + 1e-10)
</p>
<p>
            self.particles.append({
                'label': region.label,
                'area': area,
                'diameter': diameter,
                'aspect_ratio': aspect_ratio,
                'circularity': circularity,
                'centroid': region.centroid
            })
</p>
<p>
    def get_results(self) -> ParticleAnalysisResult:
        """結果取得"""
        df = pd.DataFrame(self.particles)
</p>
<p>
        return ParticleAnalysisResult(
            num_particles=len(self.particles),
            mean_diameter=df['diameter'].mean(),
            std_diameter=df['diameter'].std(),
            mean_circularity=df['circularity'].mean(),
            particle_data=self.particles
        )
</p>
<p>
    def visualize(self):
        """結果可視化"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 14))
</p>
<p>
        # 元画像
        axes[0, 0].imshow(self.image, cmap='gray')
        axes[0, 0].set_title('Preprocessed Image')
        axes[0, 0].axis('off')
</p>
<p>
        # 二値化
        axes[0, 1].imshow(self.binary, cmap='gray')
        axes[0, 1].set_title('Binary Segmentation')
        axes[0, 1].axis('off')
</p>
<p>
        # Watershedラベル
        axes[1, 0].imshow(self.labels, cmap='nipy_spectral')
        axes[1, 0].set_title(f'Particles ({len(self.particles)})')
        axes[1, 0].axis('off')
</p>
<p>
        # 粒径分布
        df = pd.DataFrame(self.particles)
        axes[1, 1].hist(df['diameter'], bins=20, alpha=0.7,
                       edgecolor='black')
        axes[1, 1].set_xlabel('Diameter (pixels)')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('Particle Size Distribution')
        axes[1, 1].grid(True, alpha=0.3, axis='y')
</p>
<p>
        plt.tight_layout()
        plt.show()
</p>
<p>
    def save_results(self, filename='analysis_results.json'):
        """結果をJSON保存"""
        results = self.get_results()
        output = {
            'num_particles': results.num_particles,
            'mean_diameter': results.mean_diameter,
            'std_diameter': results.std_diameter,
            'mean_circularity': results.mean_circularity,
            'particles': results.particle_data
        }
</p>
<p>
        with open(filename, 'w') as f:
            json.dump(output, f, indent=2)
</p>
<p>
        print(f"Results saved to {filename}")
</p>
<p>
<h1>使用例</h1>
analyzer = SEMImageAnalyzer()
analyzer.load_image(noisy_image)
analyzer.preprocess(denoise_strength=10)
analyzer.segment_particles(min_size=50)
analyzer.extract_features()
</p>
<p>
results = analyzer.get_results()
print("=== 解析結果 ===")
print(f"検出粒子数: {results.num_particles}")
print(f"平均直径: {results.mean_diameter:.2f} ± {results.std_diameter:.2f} pixels")
print(f"平均円形度: {results.mean_circularity:.3f}")
</p>
<p>
analyzer.visualize()
analyzer.save_results('sem_analysis.json')
</code></pre>
</p>
<p>
---
</p>
<p>
<h2>3.7 本章のまとめ</h2>
</p>
<p>
<h3>学んだこと</h3>
</p>
<p>
1. <strong>画像前処理</strong>
   - ノイズ除去（Gaussian、Median、Bilateral、NLM）
   - コントラスト調整（Histogram Equalization、CLAHE）
   - 二値化（Otsu法）
</p>
<p>
2. <strong>粒子検出</strong>
   - Watershed法によるセグメンテーション
   - 距離変換と局所最大値検出
   - モルフォロジー演算
</p>
<p>
3. <strong>定量解析</strong>
   - 粒径分布（ヒストグラム、累積分布）
   - 形状パラメータ（円形度、アスペクト比）
   - 対数正規分布フィッティング
</p>
<p>
4. <strong>深層学習</strong>
   - 転移学習（VGG16）
   - 材料画像分類
   - 混同行列による性能評価
</p>
<p>
<h3>重要なポイント</h3>
</p>
<p>
<li>✅ 前処理の質がセグメンテーション精度を決定する</li>
<li>✅ Watershed法はパラメータ調整が重要（min_distance、threshold_abs）</li>
<li>✅ 粒径分布は統計モデル（対数正規）でフィッティング可能</li>
<li>✅ 転移学習により少量データでも高精度分類が実現</li>
</p>
<p>
<h3>次の章へ</h3>
</p>
<p>
第4章では、時系列データと統合解析を学びます：
<li>温度・圧力センサーデータの前処理</li>
<li>移動窓解析</li>
<li>異常検知</li>
<li>PCAによる次元削減</li>
<li>sklearn Pipelineによる自動化</li>
</p>
<p>
<strong><a href="./chapter-4.md">第4章：時系列データと統合解析 →</a></strong>
</p>
<p>
---
</p>
<p>
<h2>演習問題</h2>
</p>
<p>
<h3>問題1（難易度：easy）</h3>
</p>
<p>
次の文章の正誤を判定してください。
</p>
<p>
1. Bilateral FilterはエッジをGaussian Filterよりも保持できる
2. CLAHEは画像全体に同一のヒストグラム均等化を適用する
3. Watershed法では距離変換の局所最大値を粒子中心とみなす
</p>
<p>
<details>
<summary>ヒント</summary>
</p>
<p>
1. Bilateral Filterの動作原理（空間距離と輝度差の両方を考慮）
2. CLAHEの"Adaptive"の意味
3. Watershedアルゴリズムの流れ（距離変換→マーカー→watershed）
</p>
<p>
</details>
</p>
<p>
<details>
<summary>解答例</summary>
</p>
<p>
<strong>解答</strong>:
1. <strong>正</strong> - Bilateral Filterは輝度差も考慮するため、エッジ付近では平滑化が抑制される
2. <strong>誤</strong> - CLAHEは画像を小領域（タイル）に分割し、各領域で適応的にヒストグラム均等化を行う
3. <strong>正</strong> - 距離変換の局所最大値が粒子中心に対応し、Watershedのマーカーとして使用される
</p>
<p>
<strong>解説</strong>:
画像前処理では、処理の目的（ノイズ除去 vs エッジ保持）に応じて適切な手法を選択することが重要です。Watershedは距離変換を活用することで、接触している粒子も分離できる強力な手法です。
</p>
<p>
</details>
</p>
<p>
---
</p>
<p>
<h3>問題2（難易度：medium）</h3>
</p>
<p>
以下のSEM画像データに対して、粒子検出と粒径分布解析を実行してください。
</p>
<p>
<pre><code class="language-python">import numpy as np
</p>
<p>
<h1>サンプルSEM画像生成</h1>
def generate_sample_sem():
    np.random.seed(100)
    img = np.zeros((512, 512), dtype=np.uint8)
</p>
<p>
    for _ in range(40):
        x = np.random.randint(30, 482)
        y = np.random.randint(30, 482)
        r = np.random.randint(10, 25)
        cv2.circle(img, (x, y), r, 200, -1)
</p>
<p>
    noise = np.random.normal(0, 30, img.shape)
    return np.clip(img + noise, 0, 255).astype(np.uint8)
</p>
<p>
sample_image = generate_sample_sem()
</code></pre>
</p>
<p>
<strong>要求事項</strong>:
1. Non-Local Meansでノイズ除去
2. Otsu法で二値化
3. Watershed法で粒子検出
4. 粒径分布をヒストグラムでプロット
5. 平均粒径・標準偏差を出力
</p>
<p>
<details>
<summary>ヒント</summary>
</p>
<p>
<strong>処理フロー</strong>:
1. <code>cv2.fastNlMeansDenoising</code>でノイズ除去
2. <code>filters.threshold_otsu</code>で閾値計算→二値化
3. <code>distance_transform_edt</code> + <code>peak_local_max</code> + <code>watershed</code>
4. <code>measure.regionprops</code>で粒子特徴量抽出
5. <code>matplotlib.pyplot.hist</code>で可視化
</p>
<p>
</details>
</p>
<p>
<details>
<summary>解答例</summary>
</p>
<p>
<pre><code class="language-python">import numpy as np
import cv2
import matplotlib.pyplot as plt
from skimage import filters, morphology, measure
from skimage.feature import peak_local_max
from skimage.segmentation import watershed
from scipy.ndimage import distance_transform_edt
</p>
<p>
<h1>サンプル画像生成</h1>
def generate_sample_sem():
    np.random.seed(100)
    img = np.zeros((512, 512), dtype=np.uint8)
</p>
<p>
    for _ in range(40):
        x = np.random.randint(30, 482)
        y = np.random.randint(30, 482)
        r = np.random.randint(10, 25)
        cv2.circle(img, (x, y), r, 200, -1)
</p>
<p>
    noise = np.random.normal(0, 30, img.shape)
    return np.clip(img + noise, 0, 255).astype(np.uint8)
</p>
<p>
sample_image = generate_sample_sem()
</p>
<p>
<h1>ステップ1: ノイズ除去</h1>
denoised = cv2.fastNlMeansDenoising(sample_image, None, 10, 7, 21)
</p>
<p>
<h1>ステップ2: Otsu二値化</h1>
threshold = filters.threshold_otsu(denoised)
binary = denoised > threshold
binary = morphology.remove_small_objects(binary, min_size=30)
</p>
<p>
<h1>ステップ3: Watershed</h1>
distance = distance_transform_edt(binary)
local_max = peak_local_max(distance, min_distance=15,
                           threshold_abs=3, labels=binary)
markers = np.zeros_like(distance, dtype=int)
markers[tuple(local_max.T)] = np.arange(1, len(local_max) + 1)
labels = watershed(-distance, markers, mask=binary)
</p>
<p>
<h1>ステップ4: 粒径計算</h1>
diameters = []
for region in measure.regionprops(labels):
    area = region.area
    diameter = np.sqrt(4 * area / np.pi)
    diameters.append(diameter)
</p>
<p>
diameters = np.array(diameters)
</p>
<p>
<h1>ステップ5: 統計・可視化</h1>
print("=== 粒径統計 ===")
print(f"検出粒子数: {len(diameters)}")
print(f"平均粒径: {diameters.mean():.2f} pixels")
print(f"標準偏差: {diameters.std():.2f} pixels")
</p>
<p>
fig, axes = plt.subplots(2, 2, figsize=(14, 14))
</p>
<p>
axes[0, 0].imshow(sample_image, cmap='gray')
axes[0, 0].set_title('Original Image')
axes[0, 0].axis('off')
</p>
<p>
axes[0, 1].imshow(binary, cmap='gray')
axes[0, 1].set_title(f'Binary (Otsu={threshold:.1f})')
axes[0, 1].axis('off')
</p>
<p>
axes[1, 0].imshow(labels, cmap='nipy_spectral')
axes[1, 0].set_title(f'Detected Particles ({len(diameters)})')
axes[1, 0].axis('off')
</p>
<p>
axes[1, 1].hist(diameters, bins=15, alpha=0.7, edgecolor='black')
axes[1, 1].axvline(diameters.mean(), color='red', linestyle='--',
                  label=f'Mean: {diameters.mean():.1f}')
axes[1, 1].set_xlabel('Diameter (pixels)')
axes[1, 1].set_ylabel('Frequency')
axes[1, 1].set_title('Particle Size Distribution')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3, axis='y')
</p>
<p>
plt.tight_layout()
plt.show()
</code></pre>
</p>
<p>
<strong>出力例</strong>:
``<code>
=== 粒径統計 ===
検出粒子数: 38
平均粒径: 30.45 pixels
標準偏差: 8.23 pixels
</code>`<code>
</p>
<p>
<strong>解説</strong>:
この例では、Watershedのmin_distance=15により、近接粒子の過検出を抑制しています。粒径のばらつき（標準偏差）は合成過程に起因します。実データでは、測定条件や材料の不均一性を反映します。
</p>
<p>
</details>
</p>
<p>
---
</p>
<p>
<h3>問題3（難易度：hard）</h3>
</p>
<p>
複数のSEM画像を自動処理し、粒径分布の統計比較を行うシステムを構築してください。
</p>
<p>
<strong>背景</strong>:
異なる合成条件で作製された材料A、B、Cのサンプルについて、各10枚のSEM画像が撮影されました。各サンプルの粒径分布を自動解析し、統計的に比較する必要があります。
</p>
<p>
<strong>課題</strong>:
1. バッチ処理により30枚の画像を自動解析
2. サンプルごとの粒径分布を可視化
3. 分散分析（ANOVA）による統計的有意差検定
4. 結果をPDFレポートとして出力
</p>
<p>
<strong>制約条件</strong>:
<li>各画像の測定条件（倍率、露光）が異なる可能性</li>
<li>一部画像はコントラスト不良</li>
<li>処理時間：5秒以内/画像</li>
</p>
<p>
<details>
<summary>ヒント</summary>
</p>
<p>
<strong>設計方針</strong>:
1. </code>SEMImageAnalyzer<code>クラスを拡張
2. 適応的前処理（ヒストグラム解析でコントラスト判定）
3. 結果を構造化して保存（JSON/CSV）
4. </code>scipy.stats.f_oneway<code>でANOVA
5. </code>matplotlib.backends.backend_pdf<code>でPDF生成
</p>
<p>
</details>
</p>
<p>
<details>
<summary>解答例</summary>
</p>
<p>
<strong>解答の概要</strong>:
バッチ処理、統計解析、レポート生成を含む統合システムを構築します。
</p>
<p>
<strong>実装コード</strong>:
</p>
<p>
<pre><code class="language-python">from scipy.stats import f_oneway
from matplotlib.backends.backend_pdf import PdfPages
</p>
<p>
class BatchSEMAnalyzer:
    """バッチSEM画像解析システム"""
</p>
<p>
    def __init__(self):
        self.results = {}
</p>
<p>
    def adaptive_preprocess(self, image):
        """適応的前処理"""
        # コントラスト評価
        contrast = image.max() - image.min()
</p>
<p>
        if contrast < 100:  # 低コントラスト
            # CLAHE強化
            clahe = cv2.createCLAHE(clipLimit=3.0,
                                    tileGridSize=(8, 8))
            image = clahe.apply(image)
</p>
<p>
        # ノイズ除去（適応的強度）
        noise_std = np.std(np.diff(image, axis=0))
        h = 10 if noise_std < 20 else 15
</p>
<p>
        denoised = cv2.fastNlMeansDenoising(image, None, h, 7, 21)
        return denoised
</p>
<p>
    def analyze_single(self, image, sample_id):
        """単一画像解析"""
        # 前処理
        preprocessed = self.adaptive_preprocess(image)
</p>
<p>
        # Otsu二値化
        threshold = filters.threshold_otsu(preprocessed)
        binary = preprocessed > threshold
        binary = morphology.remove_small_objects(binary, min_size=30)
</p>
<p>
        # Watershed
        distance = distance_transform_edt(binary)
        local_max = peak_local_max(distance, min_distance=15,
                                   labels=binary)
        markers = np.zeros_like(distance, dtype=int)
        markers[tuple(local_max.T)] = np.arange(1, len(local_max) + 1)
        labels = watershed(-distance, markers, mask=binary)
</p>
<p>
        # 粒径抽出
        diameters = []
        for region in measure.regionprops(labels):
            area = region.area
            diameter = np.sqrt(4 * area / np.pi)
            diameters.append(diameter)
</p>
<p>
        return np.array(diameters)
</p>
<p>
    def batch_analyze(self, image_dict):
        """
        バッチ解析
</p>
<p>
        Parameters:
        -----------
        image_dict : dict
            {'sample_A': [img1, img2, ...], 'sample_B': [...]}
        """
        for sample_id, images in image_dict.items():
            all_diameters = []
</p>
<p>
            for img in images:
                diameters = self.analyze_single(img, sample_id)
                all_diameters.extend(diameters)
</p>
<p>
            self.results[sample_id] = np.array(all_diameters)
</p>
<p>
    def statistical_comparison(self):
        """統計的比較（ANOVA）"""
        groups = list(self.results.values())
        f_stat, p_value = f_oneway(*groups)
</p>
<p>
        print("=== 分散分析（ANOVA）===")
        print(f"F統計量: {f_stat:.3f}")
        print(f"p値: {p_value:.4f}")
</p>
<p>
        if p_value < 0.05:
            print("結論: サンプル間に有意差あり（p < 0.05）")
        else:
            print("結論: サンプル間に有意差なし（p ≥ 0.05）")
</p>
<p>
        return f_stat, p_value
</p>
<p>
    def generate_report(self, filename='sem_report.pdf'):
        """PDFレポート生成"""
        with PdfPages(filename) as pdf:
            # ページ1: 粒径分布比較
            fig, axes = plt.subplots(2, 2, figsize=(11, 8.5))
</p>
<p>
            for i, (sample_id, diameters) in enumerate(self.results.items()):
                ax = axes.ravel()[i]
                ax.hist(diameters, bins=20, alpha=0.7, edgecolor='black')
                ax.axvline(diameters.mean(), color='red',
                          linestyle='--',
                          label=f'Mean: {diameters.mean():.1f}')
                ax.set_xlabel('Diameter (pixels)')
                ax.set_ylabel('Frequency')
                ax.set_title(f'{sample_id} (n={len(diameters)})')
                ax.legend()
                ax.grid(True, alpha=0.3, axis='y')
</p>
<p>
            # 統計サマリー
            ax = axes.ravel()[3]
            ax.axis('off')
            summary_text = "=== Statistical Summary ===\n\n"
            for sample_id, diameters in self.results.items():
                summary_text += f"{sample_id}:\n"
                summary_text += f"  Mean: {diameters.mean():.2f}\n"
                summary_text += f"  Std: {diameters.std():.2f}\n"
                summary_text += f"  n: {len(diameters)}\n\n"
</p>
<p>
            ax.text(0.1, 0.5, summary_text, fontsize=12,
                   verticalalignment='center', family='monospace')
</p>
<p>
            plt.tight_layout()
            pdf.savefig(fig)
            plt.close()
</p>
<p>
            # ページ2: Box plot比較
            fig, ax = plt.subplots(figsize=(11, 8.5))
            data = [self.results[key] for key in self.results.keys()]
            ax.boxplot(data, labels=list(self.results.keys()))
            ax.set_ylabel('Diameter (pixels)')
            ax.set_title('Particle Size Distribution Comparison')
            ax.grid(True, alpha=0.3, axis='y')
</p>
<p>
            pdf.savefig(fig)
            plt.close()
</p>
<p>
        print(f"Report saved to {filename}")
</p>
<p>
<h1>デモ実行</h1>
if __name__ == "__main__":
    # サンプルデータ生成
    np.random.seed(42)
</p>
<p>
    image_dict = {}
    for sample_id, mean_size in [('Sample_A', 25),
                                   ('Sample_B', 35),
                                   ('Sample_C', 30)]:
        images = []
        for _ in range(10):
            img = np.zeros((512, 512), dtype=np.uint8)
            num_particles = np.random.randint(30, 50)
</p>
<p>
            for _ in range(num_particles):
                x = np.random.randint(30, 482)
                y = np.random.randint(30, 482)
                r = int(np.random.normal(mean_size, 5))
                r = max(10, min(40, r))
                cv2.circle(img, (x, y), r, 200, -1)
</p>
<p>
            noise = np.random.normal(0, 25, img.shape)
            img = np.clip(img + noise, 0, 255).astype(np.uint8)
            images.append(img)
</p>
<p>
        image_dict[sample_id] = images
</p>
<p>
    # バッチ解析
    analyzer = BatchSEMAnalyzer()
    analyzer.batch_analyze(image_dict)
</p>
<p>
    # 統計比較
    analyzer.statistical_comparison()
</p>
<p>
    # レポート生成
    analyzer.generate_report('sem_comparison_report.pdf')
</p>
<p>
    print("\n=== サンプル別統計 ===")
    for sample_id, diameters in analyzer.results.items():
        print(f"{sample_id}:")
        print(f"  粒子数: {len(diameters)}")
        print(f"  平均: {diameters.mean():.2f} ± {diameters.std():.2f}")
</code></pre>
</p>
<p>
<strong>結果例</strong>:
</code>`<code>
=== 分散分析（ANOVA）===
F統計量: 124.567
p値: 0.0001
結論: サンプル間に有意差あり（p < 0.05）
</p>
<p>
=== サンプル別統計 ===
Sample_A:
  粒子数: 423
  平均: 25.12 ± 4.89
Sample_B:
  粒子数: 398
  平均: 35.34 ± 5.23
Sample_C:
  粒子数: 415
  平均: 30.05 ± 4.76
</p>
<p>
Report saved to sem_comparison_report.pdf
</code>``
</p>
<p>
<strong>詳細な解説</strong>:
1. <strong>適応的前処理</strong>: 各画像のコントラストとノイズレベルを評価し、パラメータ自動調整
2. <strong>統計検定</strong>: ANOVAにより3群間の粒径差を定量評価
3. <strong>PDF出力</strong>: 複数ページのレポート自動生成（論文・報告書に直接使用可能）
</p>
<p>
<strong>追加の検討事項</strong>:
<li>Tukey HSD検定による多重比較（どのペア間に有意差があるか）</li>
<li>粒径分布の形状比較（歪度、尖度）</li>
<li>機械学習による画像品質評価（不良画像の自動除外）</li>
</p>
<p>
</details>
</p>
<p>
---
</p>
<p>
<h2>参考文献</h2>
</p>
<p>
1. Bradski, G., & Kaehler, A. (2008). "Learning OpenCV: Computer Vision with the OpenCV Library." O'Reilly Media. ISBN: 978-0596516130
</p>
<p>
2. van der Walt, S. et al. (2014). "scikit-image: image processing in Python." <em>PeerJ</em>, 2, e453. DOI: <a href="https://doi.org/10.7717/peerj.453">10.7717/peerj.453</a>
</p>
<p>
3. Beucher, S., & Meyer, F. (1993). "The morphological approach to segmentation: the watershed transformation." <em>Mathematical Morphology in Image Processing</em>, 433-481.
</p>
<p>
4. Simonyan, K., & Zisserman, A. (2015). "Very Deep Convolutional Networks for Large-Scale Image Recognition." <em>ICLR 2015</em>. arXiv: <a href="https://arxiv.org/abs/1409.1556">1409.1556</a>
</p>
<p>
5. OpenCV Documentation: Image Processing. URL: <a href="https://docs.opencv.org/4.x/d2/d96/tutorial_py_table_of_contents_imgproc.html">https://docs.opencv.org/4.x/d2/d96/tutorial_py_table_of_contents_imgproc.html</a>
</p>
<p>
---
</p>
<p>
<h2>ナビゲーション</h2>
</p>
<p>
<h3>前の章</h3>
<strong><a href="./chapter-2.md">第2章:スペクトルデータ解析 ←</a></strong>
</p>
<p>
<h3>次の章</h3>
<strong><a href="./chapter-4.md">第4章：時系列データと統合解析 →</a></strong>
</p>
<p>
<h3>シリーズ目次</h3>
<strong><a href="./index.md">← シリーズ目次に戻る</a></strong>
</p>
<p>
---
</p>
<p>
<h2>著者情報</h2>
</p>
<p>
<strong>作成者</strong>: AI Terakoya Content Team
<strong>監修</strong>: Dr. Yusuke Hashimoto（東北大学）
<strong>作成日</strong>: 2025-10-17
<strong>バージョン</strong>: 1.0
</p>
<p>
<strong>更新履歴</strong>:
<li>2025-10-17: v1.0 初版公開</li>
</p>
<p>
<strong>フィードバック</strong>:
<li>GitHub Issues: [リポジトリURL]/issues</li>
<li>Email: yusuke.hashimoto.b8@tohoku.ac.jp</li></ul>
</p>
<p>
<strong>ライセンス</strong>: Creative Commons BY 4.0
</p>
<p>
---
</p>
<p>
<strong>次の章で学習を続けましょう！</strong>

</p>

        <div class="navigation">
            <div>
                <a href="chapter-2.html" class="btn btn-secondary">← 前へ</a>
                <a href="index.html" class="btn btn-secondary">目次へ</a>
            </div>
            <div>
                <a href="chapter-4.html" class="btn btn-primary">次へ →</a>
            </div>
        </div>
    </main>

    <footer>
        <p>&copy; 2025 AI Terakoya - Tohoku University. All rights reserved.</p>
        <p><a href="https://ai.tohoku.ac.jp">AI Terakoya Home</a></p>
    </footer>
</body>
</html>