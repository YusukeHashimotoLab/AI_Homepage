<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="機械学習ポテンシャルの理論と実践の基盤">
    <title>第2章：MLP基礎 - 概念、手法、エコシステム - MI Knowledge Hub</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 0.9rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        tbody {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: #666;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Details/Summary (for exercises) */
        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            padding: 0.5rem;
        }

        summary:hover {
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }

        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }

            table {
                font-size: 0.9rem;
            }
        }
    </style>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="container">
            <h1>第2章：MLP基礎 - 概念、手法、エコシステム</h1>
            <div class="meta">
                <span>📖 読了時間: 不明</span>
                <span>📊 レベル: beginner-intermediate</span>
            </div>
        </div>
    </header>

    <main class="container">
        <h1 id="2mlp-">第2章：MLP基礎 - 概念、手法、エコシステム</h1>
<h2 id="_1">学習目標</h2>
<p>この章を読むことで、以下を習得できます：<br />
- MLPの正確な定義と関連分野（量子化学、機械学習、分子動力学）との関係を理解する<br />
- MLP研究で頻出する15の専門用語を説明できる<br />
- MLPワークフローの5ステップ（データ収集、記述子設計、モデル訓練、検証、シミュレーション）を理解する<br />
- 3種類の記述子（対称性関数、SOAP、グラフ）の特徴と適用場面を比較できる</p>
<hr />
<h2 id="21-mlp">2.1 MLPとは何か：正確な定義</h2>
<p>第1章では、MLPが「DFT精度と経験的力場の速度を兼ね備える」革新的な技術であることを学びました。ここでは、より正確に定義します。</p>
<h3 id="_2">定義</h3>
<p><strong>機械学習ポテンシャル（Machine Learning Potential, MLP）</strong>とは：</p>
<blockquote>
<p>量子力学計算（主にDFT）から得られた原子配置とエネルギー・力のデータセットを用いて、機械学習モデルを訓練することで、任意の原子配置のポテンシャルエネルギーと力を高速かつ高精度に予測する手法。</p>
</blockquote>
<p><strong>数式で表現すると</strong>：</p>
<pre class="codehilite"><code>訓練データ: D = {(R₁, E₁, F₁), (R₂, E₂, F₂), ..., (Rₙ, Eₙ, Fₙ)}

R: 原子配置（3N次元ベクトル、N=原子数）
E: エネルギー（スカラー）
F: 力（3N次元ベクトル）

目的: 関数 f_MLP を学習
  E_pred = f_MLP(R)
  F_pred = -∇_R f_MLP(R)

制約:
  - |E_pred - E_DFT| &lt; 数 meV（ミリ電子ボルト）
  - |F_pred - F_DFT| &lt; 数十 meV/Å
  - 計算時間: DFTの 10⁴-10⁶ 倍高速
</code></pre>

<h3 id="mlp3">MLPの3つの本質的要素</h3>
<p><strong>1. データ駆動型（Data-Driven）</strong><br />
- DFT計算から得られた大量のデータ（数千〜数万配置）が必要<br />
- データの質と量がモデルの性能を決定<br />
- 従来の経験的力場のような「手動パラメータ調整」は不要</p>
<p><strong>2. 高次元関数近似（High-Dimensional Function Approximation）</strong><br />
- ポテンシャルエネルギー曲面（Potential Energy Surface, PES）は超高次元<br />
  - 例: 100原子系 → 300次元空間<br />
- ニューラルネットワークがこの複雑な関数を効率的に学習</p>
<p><strong>3. 物理的制約の組み込み（Physical Constraints）</strong><br />
- エネルギー保存則<br />
- 並進・回転不変性（系全体を動かしてもエネルギー不変）<br />
- 原子の交換対称性（同じ元素の原子を入れ替えてもエネルギー不変）<br />
- 力とエネルギーの整合性（F = -∇E）</p>
<h3 id="_3">関連分野との位置づけ</h3>
<pre class="codehilite"><code class="language-mermaid">graph TD
    QC[量子化学&lt;br&gt;Quantum Chemistry] --&gt;|参照データ生成| MLP[機械学習ポテンシャル&lt;br&gt;MLP]
    ML[機械学習&lt;br&gt;Machine Learning] --&gt;|モデル・アルゴリズム| MLP
    MD[分子動力学&lt;br&gt;Molecular Dynamics] --&gt;|シミュレーション手法| MLP

    MLP --&gt; APP1[材料設計&lt;br&gt;Materials Design]
    MLP --&gt; APP2[触媒開発&lt;br&gt;Catalysis]
    MLP --&gt; APP3[薬物設計&lt;br&gt;Drug Design]
    MLP --&gt; APP4[表面科学&lt;br&gt;Surface Science]

    style MLP fill:#a8d5ff
    style APP1 fill:#d4f1d4
    style APP2 fill:#d4f1d4
    style APP3 fill:#d4f1d4
    style APP4 fill:#d4f1d4
</code></pre>

<p><strong>量子化学からの貢献</strong>:<br />
- DFT、量子化学計算手法（CCSD(T)など）<br />
- ポテンシャルエネルギー曲面の概念<br />
- 化学的洞察（結合、反応機構）</p>
<p><strong>機械学習からの貢献</strong>:<br />
- ニューラルネットワーク（NN）、グラフニューラルネットワーク（GNN）<br />
- 最適化アルゴリズム（Adam、SGDなど）<br />
- 正則化、過学習対策</p>
<p><strong>分子動力学からの貢献</strong>:<br />
- MD統合法（Verlet法など）<br />
- アンサンブル理論（NVT, NPT）<br />
- 統計力学的解析手法</p>
<hr />
<h2 id="22-mlp15">2.2 MLP用語集：15の重要概念</h2>
<p>MLP研究で頻繁に登場する専門用語を簡潔に解説します。</p>
<table>
<thead>
<tr>
<th>用語</th>
<th>英語</th>
<th>説明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ポテンシャルエネルギー曲面</strong></td>
<td>Potential Energy Surface (PES)</td>
<td>原子配置とエネルギーの関係を表す超高次元の関数。化学反応の「地形図」に相当。</td>
</tr>
<tr>
<td><strong>記述子</strong></td>
<td>Descriptor</td>
<td>原子配置の特徴を数値ベクトルで表現したもの。対称性関数、SOAP、グラフ表現など。</td>
</tr>
<tr>
<td><strong>対称性関数</strong></td>
<td>Symmetry Function</td>
<td>Behler-Parrinelloが提案した記述子。原子周辺の動径・角度分布を記述。</td>
</tr>
<tr>
<td><strong>メッセージパッシング</strong></td>
<td>Message Passing</td>
<td>グラフニューラルネットワークで、隣接原子間で情報を伝播する操作。</td>
</tr>
<tr>
<td><strong>等変性</strong></td>
<td>Equivariance</td>
<td>入力の変換（回転など）に対して、出力も対応して変換される性質。E(3)等変性が重要。</td>
</tr>
<tr>
<td><strong>不変性</strong></td>
<td>Invariance</td>
<td>入力の変換（回転など）に対して、出力が変化しない性質。エネルギーは回転不変。</td>
</tr>
<tr>
<td><strong>カットオフ半径</strong></td>
<td>Cutoff Radius</td>
<td>原子間相互作用を考慮する最大距離。典型的には5-10Å。計算コストと精度のバランス。</td>
</tr>
<tr>
<td><strong>MAE</strong></td>
<td>Mean Absolute Error</td>
<td>平均絶対誤差。MLPの精度評価指標。エネルギーではmeV/atom、力ではmeV/Å単位。</td>
</tr>
<tr>
<td><strong>Active Learning</strong></td>
<td>Active Learning</td>
<td>モデルが不確実な配置を自動選択し、DFT計算を追加してデータセットを効率的に拡張する手法。</td>
</tr>
<tr>
<td><strong>データ効率</strong></td>
<td>Data Efficiency</td>
<td>少ないトレーニングデータで高精度を達成する能力。最新手法（MACE）は数千配置で十分。</td>
</tr>
<tr>
<td><strong>汎化性能</strong></td>
<td>Generalization</td>
<td>訓練データに含まれない配置でも正確に予測できる能力。過学習の回避が重要。</td>
</tr>
<tr>
<td><strong>アンサンブル</strong></td>
<td>Ensemble</td>
<td>複数の独立したMLPモデルを訓練し、予測の平均と不確実性を評価する手法。</td>
</tr>
<tr>
<td><strong>転移学習</strong></td>
<td>Transfer Learning</td>
<td>ある系で訓練したモデルを、関連する別の系に適用する手法。計算コスト削減。</td>
</tr>
<tr>
<td><strong>多体相互作用</strong></td>
<td>Many-Body Interaction</td>
<td>3つ以上の原子が関与する相互作用。化学結合の記述に不可欠。</td>
</tr>
<tr>
<td><strong>E(3)等変性</strong></td>
<td>E(3) Equivariance</td>
<td>3次元ユークリッド群（並進・回転・反転）に対する等変性。NequIP、MACEの核心技術。</td>
</tr>
</tbody>
</table>
<h3 id="5">重要度の高い用語トップ5</h3>
<p><strong>初学者が最初に理解すべき用語</strong>：</p>
<ol>
<li><strong>ポテンシャルエネルギー曲面（PES）</strong>: MLPが学習する対象</li>
<li><strong>記述子（Descriptor）</strong>: 原子配置の数値表現</li>
<li><strong>不変性と等変性</strong>: 物理法則を満たすための数学的性質</li>
<li><strong>MAE</strong>: モデル精度の定量評価</li>
<li><strong>Active Learning</strong>: 効率的なデータ収集戦略</li>
</ol>
<hr />
<h2 id="23-mlp">2.3 MLPへの入力：原子配置データの種類</h2>
<p>MLPの訓練には多様な原子配置が必要です。どのようなデータが使われるのでしょうか？</p>
<h3 id="_4">主要な入力データタイプ</h3>
<p><strong>1. 平衡構造周辺の配置（Equilibrium Structures）</strong><br />
- <strong>説明</strong>: 最安定構造（エネルギー最小点）とその近傍<br />
- <strong>用途</strong>: 安定構造の性質、振動スペクトル<br />
- <strong>生成方法</strong>: DFT構造最適化 + 小さな変位<br />
- <strong>データ量</strong>: 数百〜数千配置<br />
- <strong>例</strong>: 結晶構造、分子の最適化構造</p>
<p><strong>2. 分子動力学トラジェクトリ（MD Trajectories）</strong><br />
- <strong>説明</strong>: ab initio MD（AIMD）で得られる時系列配置<br />
- <strong>用途</strong>: 動的な挙動、高温での性質<br />
- <strong>生成方法</strong>: 短時間（数十ps）のAIMDを様々な温度で実行<br />
- <strong>データ量</strong>: 数千〜数万配置<br />
- <strong>例</strong>: 液体、融解、拡散過程</p>
<p><strong>3. 反応経路（Reaction Pathways）</strong><br />
- <strong>説明</strong>: 反応物から生成物への遷移状態を含む経路<br />
- <strong>生成方法</strong>: NEB（Nudged Elastic Band）法、String法<br />
- <strong>用途</strong>: 触媒反応、化学反応機構<br />
- <strong>データ量</strong>: 数百〜数千配置（複数経路を含む）<br />
- <strong>例</strong>: CO₂還元反応、水素生成反応</p>
<p><strong>4. ランダムサンプリング（Random Sampling）</strong><br />
- <strong>説明</strong>: 配置空間をランダムに探索<br />
- <strong>生成方法</strong>: 既存構造に大きな変位を加える、モンテカルロサンプリング<br />
- <strong>用途</strong>: 汎化性能の向上、未知領域のカバー<br />
- <strong>データ量</strong>: 数千〜数万配置<br />
- <strong>注意</strong>: 高エネルギー領域が含まれるため、計算が不安定になる場合あり</p>
<p><strong>5. 欠陥・界面構造（Defects and Interfaces）</strong><br />
- <strong>説明</strong>: 結晶欠陥、表面、粒界、ナノ粒子<br />
- <strong>用途</strong>: 材料の破壊、触媒活性サイト<br />
- <strong>生成方法</strong>: 系統的に欠陥を導入してDFT計算<br />
- <strong>データ量</strong>: 数百〜数千配置<br />
- <strong>例</strong>: 空孔、転位、表面吸着サイト</p>
<h3 id="_5">データタイプの組み合わせ例</h3>
<p><strong>典型的なデータセット構成（Cu触媒CO₂還元反応の場合）</strong>:</p>
<table>
<thead>
<tr>
<th>データタイプ</th>
<th>配置数</th>
<th>割合</th>
<th>目的</th>
</tr>
</thead>
<tbody>
<tr>
<td>平衡構造（Cu表面 + 吸着種）</td>
<td>500</td>
<td>10%</td>
<td>基本構造</td>
</tr>
<tr>
<td>AIMD（300K, 500K, 700K）</td>
<td>3,000</td>
<td>60%</td>
<td>熱揺らぎ</td>
</tr>
<tr>
<td>反応経路（5つの経路）</td>
<td>500</td>
<td>10%</td>
<td>反応機構</td>
</tr>
<tr>
<td>ランダムサンプリング</td>
<td>500</td>
<td>10%</td>
<td>汎化性</td>
</tr>
<tr>
<td>表面欠陥</td>
<td>500</td>
<td>10%</td>
<td>実触媒の不均一性</td>
</tr>
<tr>
<td><strong>合計</strong></td>
<td><strong>5,000</strong></td>
<td><strong>100%</strong></td>
<td></td>
</tr>
</tbody>
</table>
<hr />
<h2 id="24-mlp">2.4 MLPエコシステム：全体像の理解</h2>
<p>MLPは単独で機能するのではなく、データ生成、訓練、シミュレーション、解析という一連のエコシステムの中で動作します。</p>
<pre class="codehilite"><code class="language-mermaid">graph TB
    subgraph &quot;Phase 1: データ生成&quot;
        DFT[DFT計算&lt;br&gt;VASP, Quantum ESPRESSO] --&gt; DATA[データセット&lt;br&gt;R, E, F]
    end

    subgraph &quot;Phase 2: モデル訓練&quot;
        DATA --&gt; DESC[記述子生成&lt;br&gt;Symmetry Func, SOAP, Graph]
        DESC --&gt; TRAIN[NN訓練&lt;br&gt;PyTorch, JAX]
        TRAIN --&gt; VALID{精度検証&lt;br&gt;MAE &lt; 閾値?}
        VALID --&gt;|No| AL[Active Learning&lt;br&gt;配置追加選択]
        AL --&gt; DFT
        VALID --&gt;|Yes| MODEL[訓練済みMLP]
    end

    subgraph &quot;Phase 3: シミュレーション&quot;
        MODEL --&gt; MD[MLP-MD&lt;br&gt;LAMMPS, ASE]
        MD --&gt; TRAJ[トラジェクトリ&lt;br&gt;配置・エネルギー時系列]
    end

    subgraph &quot;Phase 4: 解析&quot;
        TRAJ --&gt; ANA1[構造解析&lt;br&gt;RDF, 配位数]
        TRAJ --&gt; ANA2[動的性質&lt;br&gt;拡散係数, 反応速度]
        TRAJ --&gt; ANA3[熱力学&lt;br&gt;自由エネルギー]
        ANA1 --&gt; INSIGHT[科学的洞察]
        ANA2 --&gt; INSIGHT
        ANA3 --&gt; INSIGHT
    end

    INSIGHT -.-&gt;|新しい仮説| DFT

    style MODEL fill:#ffeb99
    style TRAJ fill:#d4f1d4
    style INSIGHT fill:#ffb3b3
</code></pre>

<h3 id="_6">エコシステムの各フェーズ詳細</h3>
<p><strong>Phase 1: データ生成（DFT計算）</strong><br />
- <strong>ツール</strong>: VASP, Quantum ESPRESSO, CP2K, GPAW<br />
- <strong>計算時間</strong>: スーパーコンピュータで数日〜数週間<br />
- <strong>出力</strong>: 原子配置、エネルギー、力、応力テンソル</p>
<p><strong>Phase 2: モデル訓練</strong><br />
- <strong>ツール</strong>: SchNetPack, NequIP, MACE, DeePMD-kit<br />
- <strong>計算資源</strong>: GPU 1台〜数台<br />
- <strong>訓練時間</strong>: 数時間〜数日<br />
- <strong>出力</strong>: 訓練済みMLPモデル（.pthファイルなど）</p>
<p><strong>Phase 3: シミュレーション</strong><br />
- <strong>ツール</strong>: LAMMPS, ASE, i-PI<br />
- <strong>計算資源</strong>: GPU 1台〜数十台<br />
- <strong>シミュレーション時間</strong>: ナノ秒〜マイクロ秒<br />
- <strong>出力</strong>: トラジェクトリファイル（.xyz, .lammpstrj）</p>
<p><strong>Phase 4: 解析</strong><br />
- <strong>ツール</strong>: Python（NumPy, MDAnalysis, MDTraj）, OVITO, VMD<br />
- <strong>解析内容</strong>:<br />
  - 構造: 動径分布関数（RDF）、配位数、クラスター解析<br />
  - 動的: 拡散係数、反応速度定数、滞在時間<br />
  - 熱力学: 自由エネルギー、エントロピー</p>
<hr />
<h2 id="25-mlp5">2.5 MLPワークフロー：5つのステップ</h2>
<p>MLPを用いた研究プロジェクトは、以下の5ステップで進行します。</p>
<h3 id="step-1-data-collection">Step 1: データ収集（Data Collection）</h3>
<p><strong>目的</strong>: MLPの訓練に必要な高品質なDFTデータを生成</p>
<p><strong>具体的な作業</strong>:<br />
1. <strong>系の定義</strong>: 対象とする化学系、サイズ、組成を決定<br />
2. <strong>サンプリング戦略</strong>: どのような配置を計算するか計画<br />
   - 平衡構造周辺<br />
   - AIMD（複数温度）<br />
   - 反応経路<br />
   - ランダムサンプリング<br />
3. <strong>DFT計算設定</strong>:<br />
   - 汎関数（PBE, HSE06など）<br />
   - 基底関数（平面波、局在軌道）<br />
   - カットオフエネルギー、k点メッシュ<br />
4. <strong>並列計算実行</strong>: スーパーコンピュータで数千配置を計算</p>
<p><strong>成功の鍵</strong>:<br />
- <strong>多様性</strong>: 様々な配置を含める（単調なデータは汎化性能を下げる）<br />
- <strong>バランス</strong>: 低エネルギー領域と高エネルギー領域のバランス<br />
- <strong>品質管理</strong>: SCF収束、力の収束を確認</p>
<p><strong>典型的なコスト</strong>: 5,000配置 → スーパーコンピュータで3-7日</p>
<h3 id="step-2-descriptor-design">Step 2: 記述子設計（Descriptor Design）</h3>
<p><strong>目的</strong>: 原子配置を機械学習可能な数値ベクトルに変換</p>
<p><strong>主要な記述子タイプ</strong>（詳細は2.6で説明）:<br />
- <strong>対称性関数（Symmetry Functions）</strong>: 手動設計、Behler-Parrinello<br />
- <strong>SOAP（Smooth Overlap of Atomic Positions）</strong>: 数学的に洗練された表現<br />
- <strong>グラフニューラルネットワーク（GNN）</strong>: 自動学習、SchNet, DimeNet</p>
<p><strong>記述子の選択基準</strong>:<br />
- <strong>データ効率</strong>: 少ないデータで高精度を達成できるか<br />
- <strong>計算コスト</strong>: 推論（予測）時の計算時間<br />
- <strong>物理的解釈性</strong>: 化学的洞察が得られるか</p>
<p><strong>ハイパーパラメータ</strong>:<br />
- カットオフ半径（5-10Å）<br />
- 動径基底関数の数（10-50）<br />
- 角度分解能</p>
<h3 id="step-3-model-training">Step 3: モデル訓練（Model Training）</h3>
<p><strong>目的</strong>: ニューラルネットワークを最適化して、PESを学習</p>
<p><strong>訓練プロセス</strong>:<br />
1. <strong>データ分割</strong>: 訓練（80%）、検証（10%）、テスト（10%）<br />
2. <strong>損失関数の定義</strong>:<br />
   ```<br />
   Loss = w_E × MSE(E_pred, E_true) + w_F × MSE(F_pred, F_true)</p>
<p>w_E: エネルギー重み（典型的に1）<br />
   w_F: 力の重み（典型的に100-1000、単位換算のため）<br />
   ```<br />
3. <strong>最適化</strong>: Adam, SGDなどで数千〜数万エポック<br />
4. <strong>正則化</strong>: 過学習を防ぐためのL2正則化、ドロップアウト</p>
<p><strong>ハイパーパラメータ調整</strong>:<br />
- 学習率（learning rate）: 10⁻³ 〜 10⁻⁵<br />
- バッチサイズ: 32-256<br />
- ネットワーク深さ: 3-6層<br />
- 隠れ層のサイズ: 64-512ノード</p>
<p><strong>計算資源</strong>: GPU 1台で数時間〜2日</p>
<h3 id="step-4-validation">Step 4: 精度検証（Validation）</h3>
<p><strong>目的</strong>: モデルが十分な精度と汎化性能を持つか評価</p>
<p><strong>定量的指標</strong>:<br />
| 指標 | 目標値 | 説明 |<br />
|------|--------|------|<br />
| エネルギーMAE | &lt; 1-5 meV/atom | 平均絶対誤差（テストセット） |<br />
| 力のMAE | &lt; 50-150 meV/Å | 原子に働く力の誤差 |<br />
| 応力のMAE | &lt; 0.1 GPa | 固体材料の場合 |<br />
| R²（決定係数） | &gt; 0.99 | エネルギーの相関 |</p>
<p><strong>定性的検証</strong>:<br />
- <strong>外挿テスト</strong>: 訓練データの範囲外の配置で精度を確認<br />
- <strong>物理量の再現</strong>: 格子定数、弾性定数、振動スペクトルがDFTと一致するか<br />
- <strong>短時間MDテスト</strong>: 10-100 psのMDを実行し、エネルギー保存を確認</p>
<p><strong>不合格の場合の対処</strong>:<br />
- データ追加（Active Learning）<br />
- ハイパーパラメータ調整<br />
- より強力なモデル（SchNet → NequIP）への変更</p>
<h3 id="step-5-production-simulation">Step 5: 生産的シミュレーション（Production Simulation）</h3>
<p><strong>目的</strong>: 訓練したMLPで科学的に意義のあるシミュレーションを実行</p>
<p><strong>典型的なMLP-MDシミュレーション設定</strong>:</p>
<pre class="codehilite"><code>系のサイズ: 10³-10⁴ 原子
温度: 300-1000 K（目的に応じて）
圧力: 1気圧 or 等体積
時間ステップ: 0.5-1.0 fs
総シミュレーション時間: 1-100 ns
アンサンブル: NVT（正準）, NPT（等温等圧）
</code></pre>

<p><strong>実行時間の見積もり</strong>:<br />
- 1,000原子、1 ns シミュレーション → GPU 1台で1-3日<br />
- 並列化により更に高速化可能</p>
<p><strong>注意点</strong>:<br />
- <strong>エネルギードリフト</strong>: 長時間シミュレーションでエネルギーが単調増加/減少しないか監視<br />
- <strong>未学習領域</strong>: 訓練データにない配置に遭遇した場合、精度が低下する可能性<br />
- <strong>アンサンブル不確実性</strong>: 複数の独立したMLPモデルで予測のばらつきを評価</p>
<hr />
<h2 id="26">2.6 記述子の種類：原子配置の数値化</h2>
<p>MLPの性能は、記述子の設計に大きく依存します。ここでは主要な3つのアプローチを比較します。</p>
<h3 id="1-symmetry-functions">1. 対称性関数（Symmetry Functions）</h3>
<p><strong>提案</strong>: Behler &amp; Parrinello (2007)</p>
<p><strong>基本アイデア</strong>:<br />
- 各原子 i の周辺環境を、動径（距離）と角度の関数で表現<br />
- 回転・並進不変性を満たすように設計</p>
<p><strong>動径対称性関数（Radial Symmetry Functions）</strong>:</p>
<pre class="codehilite"><code>G_i^rad = Σ_j exp(-η(r_ij - R_s)²) × f_c(r_ij)

r_ij: 原子 i-j 間の距離
η: ガウス関数の幅を決めるパラメータ
R_s: 中心距離（複数の値を使う）
f_c: カットオフ関数（遠方の原子の影響を滑らかに減衰）
</code></pre>

<p><strong>角度対称性関数（Angular Symmetry Functions）</strong>:</p>
<pre class="codehilite"><code>G_i^ang = 2^(1-ζ) Σ_(j,k≠i) (1 + λcosθ_ijk)^ζ ×
          exp(-η(r_ij² + r_ik² + r_jk²)) ×
          f_c(r_ij) × f_c(r_ik) × f_c(r_jk)

θ_ijk: 原子 j-i-k がなす角度
ζ, λ: 角度分解能を制御するパラメータ
</code></pre>

<p><strong>利点</strong>:<br />
- 物理的に解釈しやすい（動径分布関数、角度分布に対応）<br />
- 回転・並進不変性が保証される<br />
- 実装が比較的簡単</p>
<p><strong>欠点</strong>:<br />
- <strong>手動設計</strong>: η, R_s, ζ, λ などのパラメータを手動で選ぶ必要がある<br />
- <strong>高次元</strong>: 50-100次元の記述子が必要（計算コスト増）<br />
- <strong>データ効率が低い</strong>: 数万配置が必要な場合がある</p>
<p><strong>適用例</strong>: 水、シリコン、金属表面</p>
<h3 id="2-soapsmooth-overlap-of-atomic-positions">2. SOAP（Smooth Overlap of Atomic Positions）</h3>
<p><strong>提案</strong>: Bartók et al. (2013)</p>
<p><strong>基本アイデア</strong>:<br />
- 原子周辺の電子密度を「ガウス密度」で近似<br />
- この密度分布の「重なり」を計算して記述子とする<br />
- 数学的に厳密な回転不変性</p>
<p><strong>数学的定義（簡略版）</strong>:</p>
<pre class="codehilite"><code>ρ_i(r) = Σ_j exp(-α|r - r_j|²)  （原子 i 周辺のガウス密度）

SOAP_i = 積分[ρ_i(r) × ρ_i(r') × kernel(r, r')]

kernel: 動径・角度基底関数
</code></pre>

<p>実際には、<strong>球面調和関数展開</strong>を用いて効率的に計算されます。</p>
<p><strong>利点</strong>:<br />
- 数学的に洗練された表現<br />
- 対称性関数より少ないパラメータ<br />
- カーネル法（Gaussian Process Regression）と相性が良い</p>
<p><strong>欠点</strong>:<br />
- 計算コストがやや高い<br />
- ニューラルネットワークとの組み合わせはSchNetより複雑</p>
<p><strong>適用例</strong>: 結晶材料、ナノクラスター、複雑な合金</p>
<h3 id="3-graph-neural-networks">3. グラフニューラルネットワーク（Graph Neural Networks）</h3>
<p><strong>代表的手法</strong>: SchNet (2017), DimeNet (2020), PaiNN (2021)</p>
<p><strong>基本アイデア</strong>:<br />
- 分子を<strong>グラフ</strong>として表現<br />
  - ノード（頂点）= 原子<br />
  - エッジ（辺）= 原子間の相互作用<br />
- <strong>メッセージパッシング</strong>: 隣接原子間で情報を伝播<br />
- 記述子を手動で設計せず、<strong>ニューラルネットワークが自動学習</strong></p>
<p><strong>SchNetのアーキテクチャ（概念図）</strong>:</p>
<pre class="codehilite"><code>初期状態:
  原子 i の特徴ベクトル h_i^(0) = Embedding(Z_i)  （Z_i: 原子番号）

メッセージパッシング（L層繰り返し）:
  for l = 1 to L:
    m_ij = NN_filter(r_ij) × h_j^(l-1)  （距離に応じたフィルタ）
    h_i^(l) = h_i^(l-1) + Σ_j m_ij      （メッセージの集約）
    h_i^(l) = NN_update(h_i^(l))        （非線形変換）

エネルギー予測:
  E_i = NN_output(h_i^(L))              （原子ごとのエネルギー）
  E_total = Σ_i E_i
</code></pre>

<p><strong>利点</strong>:<br />
- <strong>エンドツーエンド学習</strong>: 記述子設計が不要<br />
- <strong>柔軟性</strong>: 様々な系に適用可能<br />
- <strong>データ効率</strong>: 対称性関数より少ないデータで高精度</p>
<p><strong>欠点</strong>:<br />
- ブラックボックス性（解釈しにくい）<br />
- 訓練に計算資源が必要</p>
<p><strong>進化形: DimeNet（角度情報を追加）</strong>:</p>
<pre class="codehilite"><code>DimeNet = SchNet + 結合角 θ_ijk の明示的な考慮

メッセージに角度情報を埋め込む:
  m_ij = NN(r_ij, {θ_ijk}_k)
</code></pre>

<p><strong>最新: E(3)等変GNN（NequIP, MACE）</strong>:<br />
- 回転<strong>不変性</strong>ではなく<strong>等変性</strong>を実装<br />
- ベクトル・テンソル場をメッセージとして伝播<br />
- データ効率が劇的に向上（数千配置で十分）</p>
<p><strong>適用例</strong>: 有機分子、触媒反応、複雑な生体分子</p>
<h3 id="_7">記述子比較表</h3>
<table>
<thead>
<tr>
<th>項目</th>
<th>対称性関数</th>
<th>SOAP</th>
<th>GNN（SchNet系）</th>
<th>E(3)等変GNN</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>設計方針</strong></td>
<td>手動</td>
<td>数学的定式化</td>
<td>自動学習</td>
<td>自動学習+物理法則</td>
</tr>
<tr>
<td><strong>不変性</strong></td>
<td>回転・並進</td>
<td>回転・並進</td>
<td>回転・並進</td>
<td>E(3)等変</td>
</tr>
<tr>
<td><strong>次元数</strong></td>
<td>50-100</td>
<td>30-50</td>
<td>学習可能</td>
<td>学習可能</td>
</tr>
<tr>
<td><strong>データ効率</strong></td>
<td>低</td>
<td>中</td>
<td>中</td>
<td><strong>高</strong></td>
</tr>
<tr>
<td><strong>精度</strong></td>
<td>中</td>
<td>高</td>
<td>高</td>
<td><strong>最高</strong></td>
</tr>
<tr>
<td><strong>計算コスト</strong></td>
<td>低</td>
<td>中</td>
<td>中</td>
<td>中-高</td>
</tr>
<tr>
<td><strong>実装難易度</strong></td>
<td>低</td>
<td>中</td>
<td>中</td>
<td>高</td>
</tr>
<tr>
<td><strong>解釈性</strong></td>
<td>高</td>
<td>中</td>
<td>低</td>
<td>低</td>
</tr>
</tbody>
</table>
<p><strong>選択のガイドライン</strong>:<br />
- <strong>初学者・小規模系</strong>: 対称性関数（理解しやすい）<br />
- <strong>複雑な結晶・合金</strong>: SOAP（カーネル法と組み合わせ）<br />
- <strong>有機分子・触媒</strong>: GNN（SchNet, DimeNet）<br />
- <strong>最先端・データ不足</strong>: E(3)等変GNN（NequIP, MACE）</p>
<hr />
<h2 id="27-mlp">2.7 主要MLPアーキテクチャの比較</h2>
<p>ここまで学んだ記述子を用いた代表的なMLP手法を比較します。</p>
<table>
<thead>
<tr>
<th>手法</th>
<th>年</th>
<th>記述子</th>
<th>特徴</th>
<th>データ効率</th>
<th>精度</th>
<th>実装</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Behler-Parrinello NN</strong></td>
<td>2007</td>
<td>対称性関数</td>
<td>原子ごとのNN、シンプル</td>
<td>低（数万配置）</td>
<td>中</td>
<td>n2p2, AMP</td>
</tr>
<tr>
<td><strong>GAP（SOAP + GP）</strong></td>
<td>2010</td>
<td>SOAP</td>
<td>ガウス過程回帰、不確実性定量化</td>
<td>中（数千配置）</td>
<td>高</td>
<td>QUIP</td>
</tr>
<tr>
<td><strong>ANI</strong></td>
<td>2017</td>
<td>対称性関数</td>
<td>有機分子特化、大規模データセット</td>
<td>中</td>
<td>高</td>
<td>TorchANI</td>
</tr>
<tr>
<td><strong>SchNet</strong></td>
<td>2017</td>
<td>GNN（自動学習）</td>
<td>連続フィルタ畳み込み、エンドツーエンド</td>
<td>中（5千-1万）</td>
<td>高</td>
<td>SchNetPack</td>
</tr>
<tr>
<td><strong>DimeNet</strong></td>
<td>2020</td>
<td>GNN（角度考慮）</td>
<td>方向性メッセージパッシング</td>
<td>中（5千-1万）</td>
<td>高</td>
<td>PyG</td>
</tr>
<tr>
<td><strong>NequIP</strong></td>
<td>2021</td>
<td>E(3)等変GNN</td>
<td>テンソル場メッセージパッシング</td>
<td><strong>高（数千）</strong></td>
<td><strong>最高</strong></td>
<td>NequIP</td>
</tr>
<tr>
<td><strong>MACE</strong></td>
<td>2022</td>
<td>E(3)等変 + ACE</td>
<td>高次多体項、最高データ効率</td>
<td><strong>最高（数千）</strong></td>
<td><strong>最高</strong></td>
<td>MACE</td>
</tr>
</tbody>
</table>
<h3 id="_8">時系列での進化</h3>
<pre class="codehilite"><code class="language-mermaid">timeline
    title MLP手法の進化（2007-2024）
    2007 : Behler-Parrinello NN
         : 対称性関数 + フィードフォワードNN
    2010 : GAP（SOAP + GP）
         : 数学的に厳密な記述子
    2017 : SchNet, ANI
         : グラフNN、エンドツーエンド学習
    2020 : DimeNet, PaiNN
         : 角度情報、方向性メッセージ
    2021 : NequIP
         : E(3)等変性の実装
    2022 : MACE
         : 高次多体項 + データ効率最適化
    2024 : 大規模事前訓練モデル
         : ORB, GNoME（数億配置で訓練）
</code></pre>

<h3 id="_9">精度とデータ効率のトレードオフ</h3>
<p><strong>典型的な性能（100原子の分子系での目安）</strong>:</p>
<table>
<thead>
<tr>
<th>手法</th>
<th>訓練データ数</th>
<th>エネルギーMAE</th>
<th>力のMAE</th>
<th>訓練時間（GPU 1台）</th>
</tr>
</thead>
<tbody>
<tr>
<td>Behler-Parrinello</td>
<td>30,000</td>
<td>3-5 meV/atom</td>
<td>80-120 meV/Å</td>
<td>6-12時間</td>
</tr>
<tr>
<td>GAP</td>
<td>10,000</td>
<td>1-2 meV/atom</td>
<td>40-60 meV/Å</td>
<td>12-24時間（CPU）</td>
</tr>
<tr>
<td>SchNet</td>
<td>8,000</td>
<td>1-3 meV/atom</td>
<td>50-80 meV/Å</td>
<td>4-8時間</td>
</tr>
<tr>
<td>DimeNet</td>
<td>8,000</td>
<td>0.8-2 meV/atom</td>
<td>40-60 meV/Å</td>
<td>8-16時間</td>
</tr>
<tr>
<td>NequIP</td>
<td>4,000</td>
<td>0.5-1 meV/atom</td>
<td>30-50 meV/Å</td>
<td>12-24時間</td>
</tr>
<tr>
<td>MACE</td>
<td>3,000</td>
<td><strong>0.3-0.8 meV/atom</strong></td>
<td><strong>20-40 meV/Å</strong></td>
<td>16-32時間</td>
</tr>
</tbody>
</table>
<p><strong>重要な観察</strong>:<br />
- データ効率は<strong>10倍向上</strong>（30,000 → 3,000配置）<br />
- 精度も<strong>10倍向上</strong>（5 meV → 0.5 meV/atom）<br />
- 訓練時間は大きく変わらない（数時間〜1日）</p>
<hr />
<h2 id="28-active-learning">2.8 コラム：Active Learningによる効率的なデータ収集</h2>
<p>通常のMLP訓練では、あらかじめ大量のDFTデータを準備します。しかし、<strong>Active Learning</strong>を使えば、必要最小限のデータで高精度を達成できます。</p>
<h3 id="active-learning">Active Learningのワークフロー</h3>
<pre class="codehilite"><code class="language-mermaid">graph LR
    A[初期データ&lt;br&gt;数百配置] --&gt; B[MLP訓練&lt;br&gt;v1.0]
    B --&gt; C[探索的MD&lt;br&gt;v1.0で実行]
    C --&gt; D{不確実性&lt;br&gt;評価}
    D --&gt;|高い配置を発見| E[DFT計算&lt;br&gt;追加]
    E --&gt; F[データセット&lt;br&gt;更新]
    F --&gt; G[MLP再訓練&lt;br&gt;v1.1]
    G --&gt; H{精度&lt;br&gt;十分?}
    H --&gt;|No| C
    H --&gt;|Yes| I[最終MLP&lt;br&gt;生産使用]

    style D fill:#ffffcc
    style H fill:#ccffcc
</code></pre>

<h3 id="_10">不確実性の評価方法</h3>
<p><strong>アンサンブル法</strong>:</p>
<pre class="codehilite"><code class="language-python"># 独立に訓練した5つのMLPモデル
models = [MLP_1, MLP_2, MLP_3, MLP_4, MLP_5]

# ある配置 R についてエネルギーを予測
energies = [model.predict(R) for model in models]

# 平均と標準偏差
E_mean = mean(energies)
E_std = std(energies)  # 不確実性の指標

# 閾値以上なら追加DFT計算
if E_std &gt; threshold:
    E_DFT = run_DFT(R)
    add_to_dataset(R, E_DFT)
</code></pre>

<p><strong>利点</strong>:<br />
- データ収集効率が<strong>3-5倍向上</strong><br />
- 重要な配置（遷移状態、欠陥など）を自動発見<br />
- 人間の直感に頼らない客観的なサンプリング</p>
<p><strong>成功例</strong>:<br />
- Siの相転移: 初期500配置 → Active Learningで+1,500配置 → 合計2,000配置でDFT精度達成（通常は10,000配置必要）<br />
- Cu触媒CO₂還元: 反応中間体を自動発見、DFT計算コストを60%削減</p>
<hr />
<h2 id="29">2.9 本章のまとめ</h2>
<h3 id="_11">学んだこと</h3>
<ol>
<li>
<p><strong>MLPの正確な定義</strong><br />
   - データ駆動型の高次元関数近似<br />
   - 物理的制約（不変性、等変性、力とエネルギーの整合性）の重要性<br />
   - 量子化学、機械学習、分子動力学の融合技術</p>
</li>
<li>
<p><strong>15の重要用語</strong><br />
   - PES、記述子、対称性関数、メッセージパッシング、等変性、不変性<br />
   - カットオフ半径、MAE、Active Learning、データ効率、汎化性能<br />
   - アンサンブル、転移学習、多体相互作用、E(3)等変性</p>
</li>
<li>
<p><strong>MLPへの入力データタイプ</strong><br />
   - 平衡構造、MD トラジェクトリ、反応経路、ランダムサンプリング、欠陥構造<br />
   - データセット構成のバランス（低/高エネルギー領域、多様性）</p>
</li>
<li>
<p><strong>MLPエコシステム</strong><br />
   - 4つのフェーズ: データ生成 → モデル訓練 → シミュレーション → 解析<br />
   - 各フェーズの代表的ツール（VASP, SchNetPack, LAMMPS, MDAnalysis）</p>
</li>
<li>
<p><strong>5ステップワークフロー</strong><br />
   - Step 1: データ収集（DFT計算、サンプリング戦略）<br />
   - Step 2: 記述子設計（対称性関数、SOAP、GNN）<br />
   - Step 3: モデル訓練（損失関数、最適化、ハイパーパラメータ）<br />
   - Step 4: 精度検証（MAE、外挿テスト、物理量再現）<br />
   - Step 5: 生産的シミュレーション（MLP-MD、長時間スケール）</p>
</li>
<li>
<p><strong>3種類の記述子</strong><br />
   - <strong>対称性関数</strong>: 手動設計、物理的解釈性高、データ効率低<br />
   - <strong>SOAP</strong>: 数学的厳密性、カーネル法と相性良、中程度のデータ効率<br />
   - <strong>GNN</strong>: 自動学習、エンドツーエンド、高データ効率（特にE(3)等変型）</p>
</li>
<li>
<p><strong>主要MLPアーキテクチャ</strong><br />
   - 2007年 Behler-Parrinello → 2022年 MACE: データ効率10倍向上、精度10倍向上<br />
   - 最新手法（NequIP, MACE）は数千配置でDFT精度達成</p>
</li>
</ol>
<h3 id="_12">重要なポイント</h3>
<ul>
<li>MLPの性能は<strong>記述子の選択</strong>と<strong>データの質・量</strong>に大きく依存</li>
<li><strong>E(3)等変性</strong>を持つ最新手法（NequIP, MACE）がデータ効率と精度で最高</li>
<li><strong>Active Learning</strong>により、DFT計算コストを50-70%削減可能</li>
<li>MLPは単独技術ではなく、DFT、MD、機械学習の<strong>エコシステム</strong>の一部</li>
</ul>
<h3 id="_13">次の章へ</h3>
<p>第3章では、実際に<strong>SchNetを使ったMLP訓練</strong>を体験します：<br />
- Pythonコードによる実装<br />
- 小規模データセット（MD17）での訓練<br />
- 精度評価とハイパーパラメータ調整<br />
- トラブルシューティング</p>
<p>さらに、第4章では<strong>NequIP/MACEの高度な技術</strong>と<strong>実際の研究応用例</strong>を学びます。</p>
<hr />
<h2 id="_14">演習問題</h2>
<h3 id="1easy">問題1（難易度：easy）</h3>
<p>次の3つの記述子（対称性関数、SOAP、グラフニューラルネットワーク）について、「手動設計の必要性」「データ効率」「物理的解釈性」の3つの観点で比較表を作成してください。</p>
<details>
<summary>ヒント</summary>

各記述子の特徴を思い出しましょう：
- 対称性関数: パラメータ（η, R_s, ζ, λ）を手動で選ぶ必要がある
- SOAP: 数学的に定義されているが、一部パラメータ調整が必要
- GNN: ニューラルネットワークが自動で学習

</details>

<details>
<summary>解答例</summary>

| 記述子 | 手動設計の必要性 | データ効率 | 物理的解釈性 |
|--------|----------------|-----------|------------|
| **対称性関数** | **高**<br>- η, R_s, ζ, λ を手動選択<br>- 系ごとに最適化が必要 | **低**<br>- 数万配置が必要<br>- 高次元（50-100次元） | **高**<br>- 動径/角度分布に対応<br>- 化学的に解釈可能 |
| **SOAP** | **中**<br>- 基底関数の数など最小限のパラメータ<br>- 数学的に定義済み | **中**<br>- 数千〜1万配置<br>- 30-50次元 | **中**<br>- 電子密度の重なりとして解釈<br>- やや抽象的 |
| **GNN（SchNet系）** | **低**<br>- エンドツーエンド学習<br>- 記述子が自動生成される | **中-高**<br>- 5千〜1万配置<br>- 学習可能な次元 | **低**<br>- ブラックボックス<br>- 可視化が困難 |
| **E(3)等変GNN<br>（NequIP, MACE）** | **低**<br>- 完全自動学習<br>- 物理法則を自動的に組み込む | **高**<br>- 数千配置で十分<br>- 最高のデータ効率 | **低**<br>- テンソル場の伝播<br>- 高度な数学的解釈が必要 |

**結論**:
- 初学者・解釈性重視 → 対称性関数
- バランス型 → SOAP
- 最高性能・データ不足 → E(3)等変GNN
- 汎用性・実装容易 → GNN（SchNet）

</details>

<h3 id="2medium">問題2（難易度：medium）</h3>
<p>あなたは銅触媒表面でのメタノール酸化反応（CH₃OH → HCHO + H₂）の研究を始めます。MLPの5ステップワークフロー（データ収集、記述子設計、訓練、検証、シミュレーション）に沿って、具体的な作業計画を立ててください。特に、Step 1（データ収集）でどのような原子配置を何個程度用意すべきか、理由とともに説明してください。</p>
<details>
<summary>ヒント</summary>

以下を考慮しましょう：
- 触媒反応には、安定構造だけでなく遷移状態も含まれる
- 温度効果（熱揺らぎ）も重要
- Cu表面の構造（テラス、ステップ、欠陥）
- 吸着種の多様性（CH₃OH, CH₃O, CH₂O, CHO, Hなど）

</details>

<details>
<summary>解答例</summary>

**Step 1: データ収集（DFT計算）**

| データタイプ | 配置数 | 理由 |
|------------|--------|------|
| **平衡構造** | 300 | Cu(111)表面の基本構造、各吸着種（CH₃OH, CH₃O, CH₂O, CHO, H, OH）の安定吸着サイト（top, bridge, hollow） |
| **AIMD（300K, 500K）** | 2,000 | 実験温度（300K）と高温（500K）での熱揺らぎ。分子の回転、表面拡散を含む動的挙動 |
| **反応経路** | 800 | 4つの主要反応経路（CH₃OH → CH₃O、CH₃O → CH₂O、CH₂O → CHO、CHO → CO + H）をNEB法で計算。各経路20点 × 4経路 × 10条件 |
| **表面欠陥** | 400 | ステップエッジ、キンク、空孔（実際の触媒は完全な表面ではない） |
| **高エネルギー配置** | 500 | ランダムサンプリングで汎化性能向上。分子解離状態、多吸着状態 |
| **合計** | **4,000** | |

**DFT計算設定**:
- 汎関数: PBE + D3（分散力補正、メタノールの吸着に重要）
- カットオフ: 500 eV
- k点: 4×4×1（表面スラブ）
- スラブサイズ: 4×4 Cu(111)表面（64 Cu原子）+ 真空層15Å
- 計算時間: スーパーコンピュータで約5日（並列化）

**Step 2: 記述子設計**
- **選択**: SchNet（実装が容易、化学反応の記述に実績あり）
- カットオフ半径: 6Å（Cu-Cu 最近接距離の2倍以上）
- 理由: 反応には多体相互作用（Cu-C-O-H）が重要。GNNが自動で学習。

**Step 3: モデル訓練**
- フレームワーク: SchNetPack（PyTorch）
- 損失関数: w_E=1, w_F=100（力の学習を重視）
- 訓練/検証/テスト: 70%/15%/15%（2,800/600/600配置）
- 訓練時間: GPU 1台で約8時間（100エポック）

**Step 4: 精度検証**
- **目標**: エネルギーMAE < 2 meV/atom、力のMAE < 60 meV/Å
- **外挿テスト**:
  - 訓練データにない吸着サイト（4配位Cuサイト）で精度確認
  - 高温（700K）AIMDで精度低下をチェック
- **物理量再現**:
  - CH₃OHの吸着エネルギー（実験値: -0.4 eV程度）
  - 反応障壁（実験値と比較）
- **不合格時**: Active Learningで不確実性の高い配置を追加（+500-1,000配置）

**Step 5: 生産的シミュレーション**
- **系**: 8×8 Cu(111)表面（256 Cu原子）+ 10 CH₃OH分子
- **条件**: 500K、大気圧相当
- **時間**: 10 ns（実験の反応時間スケールに到達）
- **期待される観察**:
  - CH₃OH の脱水素反応イベント（10-50回）
  - 反応速度定数の統計的評価
  - 律速段階の特定
- **計算時間**: GPU 1台で約3日

**期待される成果**:
- メタノール酸化の反応機構解明
- 律速段階の特定
- 触媒設計（Cu合金化など）への指針

</details>

<h3 id="3hard">問題3（難易度：hard）</h3>
<p>Active Learningを使わない通常のMLP訓練と、Active Learningを使う場合で、Cu触媒CO₂還元反応のプロジェクト全体（データ収集からシミュレーションまで）のコストと時間がどのように変わるか、定量的に比較してください。スーパーコンピュータの計算時間とGPU時間の両方を考慮してください。</p>
<details>
<summary>ヒント</summary>

- 通常の訓練: 最初に大量データ（例: 15,000配置）を一度に計算
- Active Learning: 少量データ（例: 500配置）から開始し、3-4回の反復で追加（各回+500-1,000配置）
- DFT計算: 1配置あたり1 node-hour（スパコンの1ノードで1時間）
- MLP訓練: 1回あたりGPU 1台で8時間
- 各反復でMLP-MDによる探索が必要（GPU 1台で1日）

</details>

<details>
<summary>解答例</summary>

### シナリオA: 通常のMLP訓練（Active Learningなし）

**Phase 1: 大量データ収集**
1. 初期サンプリング計画: 2週間（人間の作業）
2. DFT計算: 15,000配置 × 1 node-hour = **15,000 node-hour**
   - 並列化（100ノード）: 150時間 = **約6日**
3. データ前処理: 1日

**Phase 2: MLP訓練**
1. SchNet訓練: GPU 1台で12時間
2. 精度検証: 半日
3. 精度不足のリスク: 30%の確率で再訓練が必要
   - 失敗時: データ追加（+5,000配置、2日）+ 再訓練（12時間）

**Phase 3: シミュレーション**
1. 1 μs MLP-MD: GPU 1台で3日

**合計（成功時）**:
- スパコン: 15,000 node-hour
- GPU: 3.5日
- 実時間: 約2週間（並列実行）

**合計（失敗時、30%の確率）**:
- スパコン: 20,000 node-hour
- GPU: 4日
- 実時間: 約3週間

**期待値**:
- スパコン: 0.7×15,000 + 0.3×20,000 = **16,500 node-hour**
- GPU: 0.7×3.5 + 0.3×4 = **3.65日**
- 実時間: 約**2.5週間**

---

### シナリオB: Active Learning使用

**Phase 1: 初期小規模データ収集**
1. 初期サンプリング: 3日
2. DFT計算: 500配置 × 1 node-hour = **500 node-hour**（半日）
3. データ前処理: 半日

**Phase 2: 反復サイクル（3回）**

**反復1**:
1. MLP訓練v1.0: GPU 1台で4時間（データ少ないため高速）
2. 探索的MLP-MD: GPU 1台で1日
3. 不確実性評価: 半日
4. 追加DFT計算: 800配置 × 1 node-hour = **800 node-hour**（1日）

**反復2**:
1. MLP訓練v1.1（累計1,300配置）: GPU 1台で6時間
2. 探索的MLP-MD: GPU 1台で1日
3. 不確実性評価: 半日
4. 追加DFT計算: 600配置 × 1 node-hour = **600 node-hour**（1日）

**反復3**:
1. MLP訓練v1.2（累計1,900配置）: GPU 1台で8時間
2. 探索的MLP-MD: GPU 1台で1日
3. 精度検証: 合格（MAE < 閾値）

**Phase 3: 生産的シミュレーション**
1. 最終MLP訓練v2.0（累計2,000配置）: GPU 1台で10時間
2. 1 μs MLP-MD: GPU 1台で3日

**合計**:
- スパコン: 500 + 800 + 600 = **1,900 node-hour**
- GPU: 0.17 + 1 + 0.25 + 1 + 0.33 + 1 + 0.42 + 3 = **7.17日**
- 実時間: 約**2週間**（並列実行が少ないため）

---

### 比較表

| 項目 | 通常訓練 | Active Learning | 削減率 |
|------|---------|----------------|--------|
| **スパコン計算時間** | 16,500 node-hour | 1,900 node-hour | **88%削減** |
| **GPU計算時間** | 3.65日 | 7.17日 | -96%（増加） |
| **実時間** | 2.5週間 | 2週間 | 20%削減 |
| **総データ数** | 15,000配置 | 2,000配置 | 87%削減 |
| **成功の確実性** | 70% | 95% | 高い |

---

### コスト換算（仮定：スパコン1 node-hour = $1, GPU 1時間 = $1）

| 項目 | 通常訓練 | Active Learning | 差額 |
|------|---------|----------------|------|
| スパコン費用 | $16,500 | $1,900 | **-$14,600** |
| GPU費用 | $88 | $172 | +$84 |
| **合計** | **$16,588** | **$2,072** | **-$14,516（87%削減）** |

---

### 結論

**Active Learningの利点**:
1. **スパコン計算時間が88%削減** → 最大のコスト削減
2. **データ収集効率が7.5倍向上**（15,000 → 2,000配置）
3. **成功の確実性が向上**（70% → 95%）
   - 理由: 重要な配置（遷移状態、欠陥）を自動発見
4. **総コストが87%削減**

**Active Learningの欠点**:
1. **GPU時間は約2倍** → しかしGPUはスパコンより安価
2. **人間の介入が必要** → 反復ごとに結果確認
3. **実時間はほぼ同じ** → 並列実行度が低い

**推奨**:
- **スパコン計算資源が限られている場合**: Active Learning必須
- **大規模プロジェクト**: Active Learningで初期モデルを作成後、生産シミュレーションで大規模展開
- **探索的研究**: Active Learningで効率的に配置空間を探索

**実際の研究例**:
- Nature Materials (2023)の論文では、Active Learningにより**DFT計算コストを65%削減**
- Phys. Rev. Lett. (2022)では、反応中間体を自動発見し、**研究期間を4ヶ月短縮**

</details>

<hr />
<h2 id="_15">参考文献</h2>
<ol>
<li>
<p>Behler, J., &amp; Parrinello, M. (2007). "Generalized neural-network representation of high-dimensional potential-energy surfaces." <em>Physical Review Letters</em>, 98(14), 146401.<br />
   DOI: <a href="https://doi.org/10.1103/PhysRevLett.98.146401">10.1103/PhysRevLett.98.146401</a></p>
</li>
<li>
<p>Bartók, A. P., et al. (2013). "On representing chemical environments." <em>Physical Review B</em>, 87(18), 184115.<br />
   DOI: <a href="https://doi.org/10.1103/PhysRevB.87.184115">10.1103/PhysRevB.87.184115</a></p>
</li>
<li>
<p>Schütt, K. T., et al. (2017). "SchNet: A continuous-filter convolutional neural network for modeling quantum interactions." <em>Advances in Neural Information Processing Systems</em>, 30.<br />
   arXiv: <a href="https://arxiv.org/abs/1706.08566">1706.08566</a></p>
</li>
<li>
<p>Klicpera, J., et al. (2020). "Directional message passing for molecular graphs." <em>International Conference on Learning Representations (ICLR)</em>.<br />
   arXiv: <a href="https://arxiv.org/abs/2003.03123">2003.03123</a></p>
</li>
<li>
<p>Batzner, S., et al. (2022). "E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials." <em>Nature Communications</em>, 13(1), 2453.<br />
   DOI: <a href="https://doi.org/10.1038/s41467-022-29939-5">10.1038/s41467-022-29939-5</a></p>
</li>
<li>
<p>Batatia, I., et al. (2022). "MACE: Higher order equivariant message passing neural networks for fast and accurate force fields." <em>Advances in Neural Information Processing Systems</em>, 35.<br />
   arXiv: <a href="https://arxiv.org/abs/2206.07697">2206.07697</a></p>
</li>
<li>
<p>Smith, J. S., et al. (2017). "ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost." <em>Chemical Science</em>, 8(4), 3192-3203.<br />
   DOI: <a href="https://doi.org/10.1039/C6SC05720A">10.1039/C6SC05720A</a></p>
</li>
<li>
<p>Zhang, L., et al. (2018). "End-to-end symmetry preserving inter-atomic potential energy model for finite and extended systems." <em>Advances in Neural Information Processing Systems</em>, 31.<br />
   arXiv: <a href="https://arxiv.org/abs/1805.09003">1805.09003</a></p>
</li>
<li>
<p>Schütt, K. T., et al. (2019). "Unifying machine learning and quantum chemistry with a deep neural network for molecular wavefunctions." <em>Nature Communications</em>, 10(1), 5024.<br />
   DOI: <a href="https://doi.org/10.1038/s41467-019-12875-2">10.1038/s41467-019-12875-2</a></p>
</li>
<li>
<p>Musaelian, A., et al. (2023). "Learning local equivariant representations for large-scale atomistic dynamics." <em>Nature Communications</em>, 14(1), 579.<br />
    DOI: <a href="https://doi.org/10.1038/s41467-023-36329-y">10.1038/s41467-023-36329-y</a></p>
</li>
</ol>
<hr />
<h2 id="_16">著者情報</h2>
<p><strong>作成者</strong>: MI Knowledge Hub Content Team<br />
<strong>監修</strong>: Dr. Yusuke Hashimoto（東北大学）<br />
<strong>作成日</strong>: 2025-10-17<br />
<strong>バージョン</strong>: 1.0（Chapter 2 initial version）<br />
<strong>シリーズ</strong>: MLP入門シリーズ</p>
<p><strong>更新履歴</strong>:<br />
- 2025-10-17: v1.0 第2章初版作成<br />
  - MLPの正確な定義と関連分野との位置づけ<br />
  - 15の重要用語集（簡潔な表形式）<br />
  - 主要な入力データタイプ5種類<br />
  - MLPエコシステム図（Mermaid）<br />
  - 5ステップワークフロー（各ステップ詳細）<br />
  - 記述子の種類（対称性関数、SOAP、GNN）と比較表<br />
  - 主要MLPアーキテクチャ比較（7手法）<br />
  - Active Learningコラム<br />
  - 学習目標4つ、演習問題3問（easy, medium, hard）<br />
  - 参考文献10件</p>
<p><strong>ライセンス</strong>: Creative Commons BY-NC-SA 4.0</p>

        <div class="nav-buttons">
            <a href="index.html" class="nav-button">← シリーズ目次に戻る</a>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 MI Knowledge Hub - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
