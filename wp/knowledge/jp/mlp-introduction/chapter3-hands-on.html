<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="環境構築から訓練、MLP-MDまで">
    <title>第3章：Pythonで体験するMLP - SchNetPackハンズオン - MI Knowledge Hub</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Hiragino Sans", "Hiragino Kaku Gothic ProN", Meiryo, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 0.9rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        tbody {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: #666;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Details/Summary (for exercises) */
        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            padding: 0.5rem;
        }

        summary:hover {
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }

        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }

            table {
                font-size: 0.9rem;
            }
        }
    </style>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="container">
            <h1>第3章：Pythonで体験するMLP - SchNetPackハンズオン</h1>
            <div class="meta">
                <span>📖 読了時間: 不明</span>
                <span>📊 レベル: beginner-intermediate</span>
            </div>
        </div>
    </header>

    <main class="container">
        <h1 id="3pythonmlp-schnetpack">第3章：Pythonで体験するMLP - SchNetPackハンズオン</h1>
<h2 id="_1">学習目標</h2>
<p>この章を読むことで、以下を習得できます：<br />
- Python環境でSchNetPackをインストールし、環境をセットアップできる<br />
- 小規模データセット（MD17のアスピリン分子）を用いてMLPモデルを訓練できる<br />
- 訓練済みモデルの精度を評価し、エネルギー・力の予測誤差を確認できる<br />
- MLP-MDシミュレーションを実行し、トラジェクトリを解析できる<br />
- よくあるエラーと対処法を理解する</p>
<hr />
<h2 id="31">3.1 環境構築：必要なツールのインストール</h2>
<p>MLPを実践するには、Python環境とSchNetPackのセットアップが必要です。</p>
<h3 id="_2">必要なソフトウェア</h3>
<table>
<thead>
<tr>
<th>ツール</th>
<th>バージョン</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Python</strong></td>
<td>3.9-3.11</td>
<td>基盤言語</td>
</tr>
<tr>
<td><strong>PyTorch</strong></td>
<td>2.0+</td>
<td>ディープラーニングフレームワーク</td>
</tr>
<tr>
<td><strong>SchNetPack</strong></td>
<td>2.0+</td>
<td>MLP訓練・推論</td>
</tr>
<tr>
<td><strong>ASE</strong></td>
<td>3.22+</td>
<td>原子構造操作、MD実行</td>
</tr>
<tr>
<td><strong>NumPy/Matplotlib</strong></td>
<td>最新版</td>
<td>データ解析・可視化</td>
</tr>
</tbody>
</table>
<h3 id="_3">インストール手順</h3>
<p><strong>ステップ1: Conda環境の作成</strong></p>
<pre class="codehilite"><code class="language-bash"># 新しいConda環境を作成（Python 3.10）
conda create -n mlp-tutorial python=3.10 -y
conda activate mlp-tutorial
</code></pre>

<p><strong>ステップ2: PyTorchのインストール</strong></p>
<pre class="codehilite"><code class="language-bash"># CPU版（ローカルマシン、軽量）
conda install pytorch cpuonly -c pytorch

# GPU版（CUDAが利用可能な場合）
conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia
</code></pre>

<p><strong>ステップ3: SchNetPackとASEのインストール</strong></p>
<pre class="codehilite"><code class="language-bash"># SchNetPack（pip推奨）
pip install schnetpack

# ASE（原子シミュレーション環境）
pip install ase

# 可視化ツール
pip install matplotlib seaborn
</code></pre>

<p><strong>ステップ4: 動作確認</strong></p>
<pre class="codehilite"><code class="language-python"># Example 1: 環境確認スクリプト（5行）
import torch
import schnetpack as spk
print(f&quot;PyTorch: {torch.__version__}&quot;)
print(f&quot;SchNetPack: {spk.__version__}&quot;)
print(f&quot;GPU available: {torch.cuda.is_available()}&quot;)
</code></pre>

<p><strong>期待される出力</strong>:</p>
<pre class="codehilite"><code>PyTorch: 2.1.0
SchNetPack: 2.0.3
GPU available: False  # CPUの場合
</code></pre>

<hr />
<h2 id="32-md17">3.2 データ準備：MD17データセットの取得</h2>
<p>SchNetPackは、小規模分子のベンチマークデータセット<strong>MD17</strong>を内蔵しています。</p>
<h3 id="md17">MD17データセットとは</h3>
<ul>
<li><strong>内容</strong>: DFT計算による分子動力学トラジェクトリ</li>
<li><strong>対象分子</strong>: アスピリン、ベンゼン、エタノールなど10種類</li>
<li><strong>データ数</strong>: 各分子約10万配置</li>
<li><strong>精度</strong>: PBE/def2-SVP レベル（DFT）</li>
<li><strong>用途</strong>: MLP手法のベンチマーク</li>
</ul>
<h3 id="_4">データのダウンロードと読み込み</h3>
<p><strong>Example 2: MD17データセットのロード（10行）</strong></p>
<pre class="codehilite"><code class="language-python">from schnetpack.datasets import MD17
from schnetpack.data import AtomsDataModule

# アスピリン分子のデータセット（約10万配置）をダウンロード
dataset = MD17(
    datapath='./data',
    molecule='aspirin',
    download=True
)

print(f&quot;Total samples: {len(dataset)}&quot;)
print(f&quot;Properties: {dataset.available_properties}&quot;)
print(f&quot;First sample: {dataset[0]}&quot;)
</code></pre>

<p><strong>出力</strong>:</p>
<pre class="codehilite"><code>Total samples: 211762
Properties: ['energy', 'forces']
First sample: {'_atomic_numbers': tensor([...]), 'energy': tensor(-1234.5), 'forces': tensor([...])}
</code></pre>

<h3 id="_5">データの分割</h3>
<p><strong>Example 3: 訓練/検証/テストセットの分割（10行）</strong></p>
<pre class="codehilite"><code class="language-python"># データを訓練:検証:テスト = 70%:15%:15%に分割
data_module = AtomsDataModule(
    datapath='./data',
    dataset=dataset,
    batch_size=32,
    num_train=100000,      # 訓練データ数
    num_val=10000,          # 検証データ数
    num_test=10000,         # テストデータ数
    split_file='split.npz', # 分割情報を保存
)
data_module.prepare_data()
data_module.setup()
</code></pre>

<p><strong>説明</strong>:<br />
- <code>batch_size=32</code>: 32配置ずつまとめて処理（メモリ効率）<br />
- <code>num_train=100000</code>: 大量データで汎化性能向上<br />
- <code>split_file</code>: 分割をファイルに保存（再現性確保）</p>
<hr />
<h2 id="33-schnetpack">3.3 SchNetPackでのモデル訓練</h2>
<p>SchNetモデルを訓練し、エネルギーと力を学習します。</p>
<h3 id="schnet">SchNetアーキテクチャの設定</h3>
<p><strong>Example 4: SchNetモデルの定義（15行）</strong></p>
<pre class="codehilite"><code class="language-python">import schnetpack.transform as trn
from schnetpack.representation import SchNet
from schnetpack.model import AtomisticModel
from schnetpack.task import ModelOutput

# 1. SchNet表現層（原子配置→特徴ベクトル）
representation = SchNet(
    n_atom_basis=128,      # 原子特徴ベクトルの次元
    n_interactions=6,      # メッセージパッシング層の数
    cutoff=5.0,            # カットオフ半径（Å）
    n_filters=128          # フィルタ数
)

# 2. 出力層（エネルギー予測）
output = ModelOutput(
    name='energy',
    loss_fn=torch.nn.MSELoss(),
    metrics={'MAE': spk.metrics.MeanAbsoluteError()}
)
</code></pre>

<p><strong>パラメータ解説</strong>:<br />
- <code>n_atom_basis=128</code>: 各原子の特徴ベクトルが128次元（典型的な値）<br />
- <code>n_interactions=6</code>: 6層のメッセージパッシング（深いほど長距離相互作用を捉える）<br />
- <code>cutoff=5.0Å</code>: この距離以上の原子間相互作用を無視（計算効率）</p>
<h3 id="_6">訓練の実行</h3>
<p><strong>Example 5: 訓練ループの設定（15行）</strong></p>
<pre class="codehilite"><code class="language-python">import pytorch_lightning as pl
from schnetpack.task import AtomisticTask

# 訓練タスクの定義
task = AtomisticTask(
    model=AtomisticModel(representation, [output]),
    outputs=[output],
    optimizer_cls=torch.optim.AdamW,
    optimizer_args={'lr': 1e-4}  # 学習率
)

# Trainerの設定
trainer = pl.Trainer(
    max_epochs=50,               # 最大50エポック
    accelerator='cpu',           # CPU使用（GPU: 'gpu'）
    devices=1,
    default_root_dir='./training'
)

# 訓練開始
trainer.fit(task, datamodule=data_module)
</code></pre>

<p><strong>訓練時間の目安</strong>:<br />
- CPU（4コア）: 約2-3時間（10万配置）<br />
- GPU（RTX 3090）: 約15-20分</p>
<h3 id="_7">訓練の進捗確認</h3>
<p><strong>Example 6: TensorBoardでの可視化（10行）</strong></p>
<pre class="codehilite"><code class="language-python"># TensorBoardの起動（別ターミナル）
# tensorboard --logdir=./training/lightning_logs

# Pythonからのログ確認
import pandas as pd

metrics = pd.read_csv('./training/lightning_logs/version_0/metrics.csv')
print(metrics[['epoch', 'train_loss', 'val_loss']].tail(10))
</code></pre>

<p><strong>期待される出力</strong>:</p>
<pre class="codehilite"><code>   epoch  train_loss  val_loss
40    40      0.0023    0.0031
41    41      0.0022    0.0030
42    42      0.0021    0.0029
...
</code></pre>

<p><strong>観察ポイント</strong>:<br />
- <code>train_loss</code>と<code>val_loss</code>がともに減少 → 正常に学習中<br />
- <code>val_loss</code>が増加し始めたら <strong>過学習</strong>の兆候 → Early Stoppingを検討</p>
<hr />
<h2 id="34">3.4 精度検証：エネルギーと力の予測精度</h2>
<p>訓練したモデルがDFT精度を達成しているか評価します。</p>
<h3 id="_8">テストセットでの評価</h3>
<p><strong>Example 7: テストセット評価（12行）</strong></p>
<pre class="codehilite"><code class="language-python"># テストセットで評価
test_results = trainer.test(task, datamodule=data_module)

# 結果の表示
print(f&quot;Energy MAE: {test_results[0]['test_energy_MAE']:.4f} eV&quot;)
print(f&quot;Energy RMSE: {test_results[0]['test_energy_RMSE']:.4f} eV&quot;)

# 力の評価（別途計算が必要）
from schnetpack.metrics import MeanAbsoluteError
force_mae = MeanAbsoluteError(target='forces')
# ... 力の評価コード
</code></pre>

<p><strong>良好な精度の目安</strong>（アスピリン分子、21原子）:<br />
- <strong>エネルギーMAE</strong>: &lt; 1 kcal/mol（&lt; 0.043 eV）<br />
- <strong>力のMAE</strong>: &lt; 1 kcal/mol/Å（&lt; 0.043 eV/Å）</p>
<h3 id="_9">予測値と真値の相関プロット</h3>
<p><strong>Example 8: 予測精度の可視化（15行）</strong></p>
<pre class="codehilite"><code class="language-python">import matplotlib.pyplot as plt
import numpy as np

# テストデータで予測
model = task.model
predictions, targets = [], []

for batch in data_module.test_dataloader():
    pred = model(batch)['energy'].detach().numpy()
    true = batch['energy'].numpy()
    predictions.extend(pred)
    targets.extend(true)

# 散布図プロット
plt.scatter(targets, predictions, alpha=0.5, s=1)
plt.plot([min(targets), max(targets)], [min(targets), max(targets)], 'r--')
plt.xlabel('DFT Energy (eV)')
plt.ylabel('MLP Predicted Energy (eV)')
plt.title('Energy Prediction Accuracy')
plt.show()
</code></pre>

<p><strong>理想的な結果</strong>:<br />
- 点が赤い対角線（y=x）上に密集<br />
- R² &gt; 0.99（決定係数）</p>
<hr />
<h2 id="35-mlp-md">3.5 MLP-MDシミュレーション：分子動力学の実行</h2>
<p>訓練したMLPを使って、DFTより10⁴倍高速なMDシミュレーションを実行します。</p>
<h3 id="asemlp-md">ASEでのMLP-MD設定</h3>
<p><strong>Example 9: MLP-MD計算の準備（10行）</strong></p>
<pre class="codehilite"><code class="language-python">from ase import units
from ase.md.velocitydistribution import MaxwellBoltzmannDistribution
from ase.md.verlet import VelocityVerlet
import schnetpack.interfaces.ase_interface as spk_ase

# MLPをASE Calculatorとしてラップ
calculator = spk_ase.SpkCalculator(
    model_file='./training/best_model.ckpt',
    device='cpu'
)

# 初期構造の準備（MD17の最初の配置）
atoms = dataset.get_atoms(0)
atoms.calc = calculator
</code></pre>

<h3 id="_10">初期速度の設定と平衡化</h3>
<p><strong>Example 10: 温度初期化（10行）</strong></p>
<pre class="codehilite"><code class="language-python"># 300Kでの速度分布を設定
temperature = 300  # K
MaxwellBoltzmannDistribution(atoms, temperature_K=temperature)

# 運動量をゼロに（系全体の並進を除去）
from ase.md.velocitydistribution import Stationary
Stationary(atoms)

print(f&quot;Initial kinetic energy: {atoms.get_kinetic_energy():.3f} eV&quot;)
print(f&quot;Initial potential energy: {atoms.get_potential_energy():.3f} eV&quot;)
</code></pre>

<h3 id="md">MDシミュレーションの実行</h3>
<p><strong>Example 11: MD実行とトラジェクトリ保存（12行）</strong></p>
<pre class="codehilite"><code class="language-python">from ase.io.trajectory import Trajectory

# MDシミュレータの設定
timestep = 0.5 * units.fs  # 0.5フェムト秒
dyn = VelocityVerlet(atoms, timestep=timestep)

# トラジェクトリファイル出力
traj = Trajectory('aspirin_md.traj', 'w', atoms)
dyn.attach(traj.write, interval=10)  # 10ステップごとに保存

# 10,000ステップ（5ピコ秒）のMD実行
dyn.run(10000)
print(&quot;MD simulation completed!&quot;)
</code></pre>

<p><strong>計算時間の目安</strong>:<br />
- CPU（4コア）: 約5分（10,000ステップ）<br />
- DFTなら: 約1週間（10,000ステップ）<br />
- <strong>10⁴倍の高速化達成！</strong></p>
<h3 id="_11">トラジェクトリの解析</h3>
<p><strong>Example 12: エネルギー保存とRDF計算（15行）</strong></p>
<pre class="codehilite"><code class="language-python">from ase.io import read
import numpy as np

# トラジェクトリの読み込み
traj_data = read('aspirin_md.traj', index=':')

# エネルギー保存の確認
energies = [a.get_total_energy() for a in traj_data]
plt.plot(energies)
plt.xlabel('Time step')
plt.ylabel('Total Energy (eV)')
plt.title('Energy Conservation Check')
plt.show()

# エネルギードリフト（単調増加/減少）の計算
drift = (energies[-1] - energies[0]) / len(energies)
print(f&quot;Energy drift: {drift:.6f} eV/step&quot;)
</code></pre>

<p><strong>良好なシミュレーションの指標</strong>:<br />
- エネルギードリフト: &lt; 0.001 eV/step<br />
- 全エネルギーが時間とともに振動（保存則）</p>
<hr />
<h2 id="36">3.6 物性計算：振動スペクトルと拡散係数</h2>
<p>MLP-MDから物理的な物性値を計算します。</p>
<h3 id="_12">振動スペクトル（パワースペクトル）</h3>
<p><strong>Example 13: 振動スペクトル計算（15行）</strong></p>
<pre class="codehilite"><code class="language-python">from scipy.fft import fft, fftfreq

# 1つの原子の速度時系列を抽出
atom_idx = 0  # 最初の原子
velocities = np.array([a.get_velocities()[atom_idx] for a in traj_data])

# x方向速度のフーリエ変換
vx = velocities[:, 0]
freq = fftfreq(len(vx), d=timestep)
spectrum = np.abs(fft(vx))**2

# 正の周波数のみプロット
mask = freq &gt; 0
plt.plot(freq[mask] * 1e15 / (2 * np.pi), spectrum[mask])  # Hz → THz変換
plt.xlabel('Frequency (THz)')
plt.ylabel('Power Spectrum')
plt.title('Vibrational Spectrum')
plt.xlim(0, 100)
plt.show()
</code></pre>

<p><strong>解釈</strong>:<br />
- ピークが分子の振動モードに対応<br />
- DFTで計算した振動スペクトルと比較することで精度検証</p>
<h3 id="msd">平均二乗変位（MSD）と拡散係数</h3>
<p><strong>Example 14: MSD計算（15行）</strong></p>
<pre class="codehilite"><code class="language-python">def calculate_msd(traj, atom_idx=0):
    &quot;&quot;&quot;平均二乗変位を計算&quot;&quot;&quot;
    positions = np.array([a.positions[atom_idx] for a in traj])
    msd = np.zeros(len(positions))

    for t in range(len(positions)):
        displacement = positions[t:] - positions[:-t or None]
        msd[t] = np.mean(np.sum(displacement**2, axis=1))

    return msd

# MSD計算とプロット
msd = calculate_msd(traj_data)
time_ps = np.arange(len(msd)) * timestep / units.fs * 1e-3  # ピコ秒

plt.plot(time_ps, msd)
plt.xlabel('Time (ps)')
plt.ylabel('MSD (Ų)')
plt.title('Mean Square Displacement')
plt.show()
</code></pre>

<p><strong>拡散係数の計算</strong>:</p>
<pre class="codehilite"><code class="language-python"># MSDの線形領域から拡散係数を計算（Einstein関係式）
# D = lim_{t→∞} MSD(t) / (6t)
linear_region = slice(100, 500)
fit = np.polyfit(time_ps[linear_region], msd[linear_region], deg=1)
D = fit[0] / 6  # Ų/ps → cm²/s変換が必要
print(f&quot;Diffusion coefficient: {D:.6f} Ų/ps&quot;)
</code></pre>

<hr />
<h2 id="37-active-learning">3.7 Active Learning：効率的なデータ追加</h2>
<p>モデルが不確実な配置を自動検出し、DFT計算を追加します。</p>
<h3 id="_13">アンサンブル不確実性の評価</h3>
<p><strong>Example 15: 予測の不確実性（15行）</strong></p>
<pre class="codehilite"><code class="language-python"># 複数の独立したモデルを訓練（省略：Example 5を3回実行）
models = [model1, model2, model3]  # 3つの独立モデル

def predict_with_uncertainty(atoms, models):
    &quot;&quot;&quot;アンサンブル予測と不確実性&quot;&quot;&quot;
    predictions = []
    for model in models:
        atoms.calc = spk_ase.SpkCalculator(model_file=model, device='cpu')
        predictions.append(atoms.get_potential_energy())

    mean = np.mean(predictions)
    std = np.std(predictions)
    return mean, std

# MDトラジェクトリの各配置で不確実性評価
uncertainties = []
for atoms in traj_data[::100]:  # 100フレームごと
    _, std = predict_with_uncertainty(atoms, models)
    uncertainties.append(std)

# 不確実性が高い配置を特定
threshold = np.percentile(uncertainties, 95)
high_uncertainty_frames = np.where(np.array(uncertainties) &gt; threshold)[0]
print(f&quot;High uncertainty frames: {high_uncertainty_frames}&quot;)
</code></pre>

<p><strong>次のステップ</strong>:<br />
- 不確実性の高い配置をDFT計算に追加<br />
- データセットを更新してモデル再訓練<br />
- 精度向上を確認</p>
<hr />
<h2 id="38">3.8 トラブルシューティング：よくあるエラーと対処法</h2>
<p>実践でよく遭遇する問題と解決策を紹介します。</p>
<table>
<thead>
<tr>
<th>エラー</th>
<th>原因</th>
<th>対処法</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Out of Memory (OOM)</strong></td>
<td>バッチサイズが大きすぎる</td>
<td><code>batch_size</code>を32→16→8と減らす</td>
</tr>
<tr>
<td><strong>Loss becomes NaN</strong></td>
<td>学習率が高すぎる</td>
<td><code>lr=1e-4</code>→<code>1e-5</code>に下げる</td>
</tr>
<tr>
<td><strong>Energy drift in MD</strong></td>
<td>タイムステップが大きすぎる</td>
<td><code>timestep=0.5fs</code>→<code>0.25fs</code>に減らす</td>
</tr>
<tr>
<td><strong>Poor generalization</strong></td>
<td>訓練データが偏っている</td>
<td>Active Learningでデータ多様化</td>
</tr>
<tr>
<td><strong>CUDA error</strong></td>
<td>GPU互換性の問題</td>
<td>PyTorchとCUDAバージョン確認</td>
</tr>
</tbody>
</table>
<h3 id="_14">デバッグのベストプラクティス</h3>
<pre class="codehilite"><code class="language-python"># 1. 小規模データでテスト
data_module.num_train = 1000  # 1,000配置でクイックテスト

# 2. 1バッチでのオーバーフィッティング確認
trainer = pl.Trainer(max_epochs=100, overfit_batches=1)
# 訓練誤差が0に近づけば、モデルに学習能力あり

# 3. グラディエントのクリッピング
task = AtomisticTask(..., gradient_clip_val=1.0)  # 勾配爆発防止
</code></pre>

<hr />
<h2 id="39">3.9 本章のまとめ</h2>
<h3 id="_15">学んだこと</h3>
<ol>
<li>
<p><strong>環境構築</strong><br />
   - Conda環境、PyTorch、SchNetPackのインストール<br />
   - GPU/CPU環境の選択</p>
</li>
<li>
<p><strong>データ準備</strong><br />
   - MD17データセットのダウンロードと読み込み<br />
   - 訓練/検証/テストセットへの分割</p>
</li>
<li>
<p><strong>モデル訓練</strong><br />
   - SchNetアーキテクチャの設定（6層、128次元）<br />
   - 50エポックの訓練（CPU: 2-3時間）<br />
   - TensorBoardでの進捗確認</p>
</li>
<li>
<p><strong>精度検証</strong><br />
   - エネルギーMAE &lt; 1 kcal/mol達成を確認<br />
   - 予測値vs真値の相関プロット<br />
   - R² &gt; 0.99の高精度</p>
</li>
<li>
<p><strong>MLP-MD実行</strong><br />
   - ASE Calculatorとしての統合<br />
   - 10,000ステップ（5ピコ秒）のMD実行<br />
   - DFTより10⁴倍高速化を体験</p>
</li>
<li>
<p><strong>物性計算</strong><br />
   - 振動スペクトル（フーリエ変換）<br />
   - 拡散係数（平均二乗変位から計算）</p>
</li>
<li>
<p><strong>Active Learning</strong><br />
   - アンサンブル不確実性による配置選択<br />
   - データ追加の自動化戦略</p>
</li>
</ol>
<h3 id="_16">重要なポイント</h3>
<ul>
<li><strong>SchNetPackは実装が容易</strong>: 数十行のコードでMLP訓練が可能</li>
<li><strong>小規模データ（10万配置）で実用精度達成</strong>: MD17は優れたベンチマーク</li>
<li><strong>MLP-MDは実用的</strong>: DFTの10⁴倍高速、個人のPCで実行可能</li>
<li><strong>Active Learningで効率化</strong>: 重要な配置を自動発見、データ収集コスト削減</li>
</ul>
<h3 id="_17">次の章へ</h3>
<p>第4章では、最新のMLP手法（NequIP、MACE）と実際の研究応用例を学びます：<br />
- E(3)等変グラフニューラルネットワークの理論<br />
- データ効率の劇的向上（10万→3,000配置）<br />
- 触媒反応、バッテリー材料への応用事例<br />
- 大規模シミュレーション（100万原子）の実現</p>
<hr />
<h2 id="_18">演習問題</h2>
<h3 id="1easy">問題1（難易度：easy）</h3>
<p>Example 4のSchNet設定で、<code>n_interactions</code>（メッセージパッシング層の数）を3, 6, 9に変えて訓練し、テストMAEがどのように変化するか予測してください。</p>
<details>
<summary>ヒント</summary>

層が深いほど、長距離の原子間相互作用を捉えられます。しかし、深すぎると過学習のリスクも。

</details>

<details>
<summary>解答例</summary>

**予測される結果**:

| `n_interactions` | テストMAE予測 | 訓練時間 | 特徴 |
|-----------------|-------------|---------|------|
| **3** | 0.8-1.2 kcal/mol | 1時間 | 浅いため長距離相互作用を捉えきれない |
| **6** | 0.5-0.8 kcal/mol | 2-3時間 | バランスが良い（推奨） |
| **9** | 0.6-1.0 kcal/mol | 4-5時間 | 過学習リスク、訓練データ不足なら精度低下 |

**実験方法**:

<pre class="codehilite"><code class="language-python">for n in [3, 6, 9]:
    representation = SchNet(n_interactions=n, ...)
    task = AtomisticTask(...)
    trainer.fit(task, datamodule=data_module)
    results = trainer.test(task, datamodule=data_module)
    print(f&quot;n={n}: MAE={results[0]['test_energy_MAE']:.4f} eV&quot;)
</code></pre>



**結論**: 小分子（アスピリン21原子）では`n_interactions=6`が最適。大規模系（100原子以上）では9-12層が有効な場合もある。

</details>

<h3 id="2medium">問題2（難易度：medium）</h3>
<p>Example 11のMLP-MDで、エネルギードリフトが許容範囲を超えた場合（例: 0.01 eV/step）、どのような対処法が考えられますか？3つ挙げてください。</p>
<details>
<summary>ヒント</summary>

タイムステップ、訓練精度、MDアルゴリズムの3つの観点から考えましょう。

</details>

<details>
<summary>解答例</summary>

**対処法1: タイムステップを小さくする**

<pre class="codehilite"><code class="language-python">timestep = 0.25 * units.fs  # 0.5fs → 0.25fsに半減
dyn = VelocityVerlet(atoms, timestep=timestep)
</code></pre>


- **理由**: 小さいタイムステップは数値積分の誤差を減らす
- **デメリット**: 2倍の計算時間

**対処法2: モデル訓練精度を向上**

<pre class="codehilite"><code class="language-python"># より多くのデータで訓練
data_module.num_train = 200000  # 10万→20万配置に増加

# または力の損失関数の重みを増やす
task = AtomisticTask(..., loss_weights={'energy': 1.0, 'forces': 1000})
</code></pre>


- **理由**: 力の予測精度が低いとMDが不安定
- **目標**: 力のMAE < 0.05 eV/Å

**対処法3: Langevin動力学に変更（熱浴結合）**

<pre class="codehilite"><code class="language-python">from ase.md.langevin import Langevin
dyn = Langevin(atoms, timestep=0.5*units.fs,
               temperature_K=300, friction=0.01)
</code></pre>


- **理由**: 熱浴がエネルギードリフトを吸収
- **注意**: 厳密な微小正準アンサンブル（NVE）ではなくなる

**優先順位**: 対処法2（精度向上）→ 対処法1（タイムステップ）→ 対処法3（Langevin）

</details>

<hr />
<h2 id="_19">参考文献</h2>
<ol>
<li>
<p>Schütt, K. T., et al. (2019). "SchNetPack: A Deep Learning Toolbox For Atomistic Systems." <em>Journal of Chemical Theory and Computation</em>, 15(1), 448-455.<br />
   DOI: <a href="https://doi.org/10.1021/acs.jctc.8b00908">10.1021/acs.jctc.8b00908</a></p>
</li>
<li>
<p>Chmiela, S., et al. (2017). "Machine learning of accurate energy-conserving molecular force fields." <em>Science Advances</em>, 3(5), e1603015.<br />
   DOI: <a href="https://doi.org/10.1126/sciadv.1603015">10.1126/sciadv.1603015</a></p>
</li>
<li>
<p>Larsen, A. H., et al. (2017). "The atomic simulation environment—a Python library for working with atoms." <em>Journal of Physics: Condensed Matter</em>, 29(27), 273002.<br />
   DOI: <a href="https://doi.org/10.1088/1361-648X/aa680e">10.1088/1361-648X/aa680e</a></p>
</li>
<li>
<p>Paszke, A., et al. (2019). "PyTorch: An imperative style, high-performance deep learning library." <em>Advances in Neural Information Processing Systems</em>, 32.<br />
   arXiv: <a href="https://arxiv.org/abs/1912.01703">1912.01703</a></p>
</li>
<li>
<p>Zhang, L., et al. (2020). "Active learning of uniformly accurate interatomic potentials for materials simulation." <em>Physical Review Materials</em>, 3(2), 023804.<br />
   DOI: <a href="https://doi.org/10.1103/PhysRevMaterials.3.023804">10.1103/PhysRevMaterials.3.023804</a></p>
</li>
<li>
<p>Schütt, K. T., et al. (2017). "Quantum-chemical insights from deep tensor neural networks." <em>Nature Communications</em>, 8(1), 13890.<br />
   DOI: <a href="https://doi.org/10.1038/ncomms13890">10.1038/ncomms13890</a></p>
</li>
</ol>
<hr />
<h2 id="_20">著者情報</h2>
<p><strong>作成者</strong>: MI Knowledge Hub Content Team<br />
<strong>監修</strong>: Dr. Yusuke Hashimoto（東北大学）<br />
<strong>作成日</strong>: 2025-10-17<br />
<strong>バージョン</strong>: 1.0（Chapter 3 initial version）<br />
<strong>シリーズ</strong>: MLP入門シリーズ</p>
<p><strong>更新履歴</strong>:<br />
- 2025-10-17: v1.0 第3章初版作成<br />
  - Python環境構築（Conda, PyTorch, SchNetPack）<br />
  - MD17データセット準備と分割<br />
  - SchNetモデル訓練（15コード例）<br />
  - MLP-MD実行と解析（トラジェクトリ、振動スペクトル、MSD）<br />
  - Active Learning不確実性評価<br />
  - トラブルシューティング表（5項目）<br />
  - 演習問題2問（easy, medium）<br />
  - 参考文献6件</p>
<p><strong>ライセンス</strong>: Creative Commons BY-NC-SA 4.0</p>

        <div class="nav-buttons">
            <a href="index.html" class="nav-button">← シリーズ目次に戻る</a>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 MI Knowledge Hub - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
