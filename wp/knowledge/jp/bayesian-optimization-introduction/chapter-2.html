<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬2ç« ï¼šãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ç†è«– - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬2ç« ï¼šãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ç†è«–</h1>
            <p class="subtitle">ã‚¬ã‚¦ã‚¹éç¨‹ã¨ç²å¾—é–¢æ•°ã§æ¢ç´¢ã‚’æœ€é©åŒ–ã™ã‚‹</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 3å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h1>ç¬¬2ç« ï¼šãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ç†è«–</h1>

<strong>ã‚¬ã‚¦ã‚¹éç¨‹ã¨ç²å¾—é–¢æ•°ã§æ¢ç´¢ã‚’æœ€é©åŒ–ã™ã‚‹</strong>

<h2>å­¦ç¿’ç›®æ¨™</h2>

ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š

- âœ… ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®åŸºæœ¬åŸç†ã‚’ç†è§£ã§ãã‚‹
- âœ… ä»£ç†ãƒ¢ãƒ‡ãƒ«ã®å½¹å‰²ã¨æ§‹ç¯‰æ–¹æ³•ã‚’èª¬æ˜ã§ãã‚‹
- âœ… 3ã¤ã®ä¸»è¦ãªç²å¾—é–¢æ•°ï¼ˆEIã€PIã€UCBï¼‰ã‚’å®Ÿè£…ã§ãã‚‹
- âœ… æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æ•°å¼ã§è¡¨ç¾ã§ãã‚‹
- âœ… ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ã¨ãã®é‡è¦æ€§ã‚’ç†è§£ã§ãã‚‹

<strong>èª­äº†æ™‚é–“</strong>: 25-30åˆ†
<strong>ã‚³ãƒ¼ãƒ‰ä¾‹</strong>: 10å€‹
<strong>æ¼”ç¿’å•é¡Œ</strong>: 3å•

---

<h2>2.1 ä»£ç†ãƒ¢ãƒ‡ãƒ«ã¨ã¯</h2>

<h3>ãªãœä»£ç†ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã‹</h3>

ææ–™æ¢ç´¢ã«ãŠã„ã¦ã€çœŸã®ç›®çš„é–¢æ•°ï¼ˆä¾‹ï¼šã‚¤ã‚ªãƒ³ä¼å°åº¦ã€è§¦åª’æ´»æ€§ï¼‰ã‚’è©•ä¾¡ã™ã‚‹ã«ã¯<strong>å®Ÿé¨“ãŒå¿…è¦</strong>ã§ã™ã€‚ã—ã‹ã—ã€å®Ÿé¨“ã¯ï¼š

- <strong>æ™‚é–“ãŒã‹ã‹ã‚‹</strong>: 1ã‚µãƒ³ãƒ—ãƒ«æ•°æ™‚é–“ï½æ•°æ—¥
- <strong>ã‚³ã‚¹ãƒˆãŒé«˜ã„</strong>: ææ–™è²»ã€è£…ç½®è²»ã€äººä»¶è²»
- <strong>å›æ•°ã«åˆ¶é™</strong>: äºˆç®—ãƒ»æ™‚é–“ã®åˆ¶ç´„

ãã“ã§ã€<strong>å°‘æ•°ã®å®Ÿé¨“çµæœã‹ã‚‰ç›®çš„é–¢æ•°ã‚’æ¨å®šã™ã‚‹ãƒ¢ãƒ‡ãƒ«</strong>ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ã“ã‚ŒãŒ<strong>ä»£ç†ãƒ¢ãƒ‡ãƒ«ï¼ˆSurrogate Modelï¼‰</strong>ã§ã™ã€‚

<h3>ä»£ç†ãƒ¢ãƒ‡ãƒ«ã®å½¹å‰²</h3>

<div class="mermaid">graph LR
    A[å°‘æ•°ã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿<br/>ä¾‹: 10-20ç‚¹] --> B[ä»£ç†ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰<br/>ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°]
    B --> C[æœªçŸ¥é ˜åŸŸã®äºˆæ¸¬<br/>å¹³å‡ + ä¸ç¢ºå®Ÿæ€§]
    C --> D[ç²å¾—é–¢æ•°ã®è¨ˆç®—<br/>æ¬¡ã®å®Ÿé¨“ç‚¹ã‚’ææ¡ˆ]
    D --> E[å®Ÿé¨“å®Ÿè¡Œ<br/>æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—]
    E --> B

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec</div>

<strong>ä»£ç†ãƒ¢ãƒ‡ãƒ«ã®è¦ä»¶</strong>:
1. <strong>å°‘æ•°ãƒ‡ãƒ¼ã‚¿ã§ã‚‚æ©Ÿèƒ½</strong>: 10-20ç‚¹ç¨‹åº¦ã§äºˆæ¸¬å¯èƒ½
2. <strong>ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–</strong>: äºˆæ¸¬ã®ä¿¡é ¼æ€§ã‚’è©•ä¾¡
3. <strong>é«˜é€Ÿ</strong>: ä½•åƒç‚¹ã§ã‚‚ç¬æ™‚ã«äºˆæ¸¬
4. <strong>æŸ”è»Ÿ</strong>: è¤‡é›‘ãªé–¢æ•°å½¢çŠ¶ã«å¯¾å¿œ

<strong>ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ï¼ˆGaussian Process Regressionï¼‰</strong>ã¯ã€ã“ã‚Œã‚‰ã®è¦ä»¶ã‚’æº€ãŸã™å¼·åŠ›ãªæ‰‹æ³•ã§ã™ã€‚

---

<h2>2.2 ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®åŸºç¤</h2>

<h3>ã‚¬ã‚¦ã‚¹éç¨‹ã¨ã¯</h3>

<strong>ã‚¬ã‚¦ã‚¹éç¨‹ï¼ˆGaussian Process, GPï¼‰</strong>ã¯ã€é–¢æ•°ã®ç¢ºç‡åˆ†å¸ƒã‚’å®šç¾©ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚

<strong>å®šç¾©</strong>:
> ã‚¬ã‚¦ã‚¹éç¨‹ã¨ã¯ã€ä»»æ„ã®æœ‰é™å€‹ã®ç‚¹ã§ã®é–¢æ•°å€¤ãŒ<strong>å¤šå¤‰é‡ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã«å¾“ã†</strong>ã‚ˆã†ãªç¢ºç‡éç¨‹ã§ã‚ã‚‹ã€‚

<strong>ç›´æ„Ÿçš„ç†è§£</strong>:
- 1ã¤ã®é–¢æ•°ã§ã¯ãªãã€<strong>ç„¡æ•°ã®é–¢æ•°ã®åˆ†å¸ƒ</strong>ã‚’è€ƒãˆã‚‹
- è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã€é–¢æ•°ã®åˆ†å¸ƒã‚’æ›´æ–°
- å„ç‚¹ã§ã®äºˆæ¸¬å€¤ã¯<strong>å¹³å‡ã¨åˆ†æ•£</strong>ã§è¡¨ç¾

<h3>ã‚¬ã‚¦ã‚¹éç¨‹ã®æ•°å­¦çš„å®šç¾©</h3>

ã‚¬ã‚¦ã‚¹éç¨‹ã¯ã€<strong>å¹³å‡é–¢æ•°</strong>$m(x)$ã¨<strong>ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ï¼ˆå…±åˆ†æ•£é–¢æ•°ï¼‰</strong>$k(x, x')$ã§å®Œå…¨ã«å®šç¾©ã•ã‚Œã¾ã™ï¼š

$$
f(x) \sim \mathcal{GP}(m(x), k(x, x'))
$$

<strong>å¹³å‡é–¢æ•°</strong> $m(x)$:
- é€šå¸¸ã¯ $m(x) = 0$ ã¨ä»®å®šï¼ˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ï¼‰

<strong>ã‚«ãƒ¼ãƒãƒ«é–¢æ•°</strong> $k(x, x')$:
- 2ç‚¹é–“ã®ã€Œé¡ä¼¼åº¦ã€ã‚’è¡¨ã™
- å…¥åŠ›ãŒè¿‘ã„ã»ã©ã€å‡ºåŠ›ã‚‚ä¼¼ã¦ã„ã‚‹ã¨ä»®å®š

<h3>ä»£è¡¨çš„ãªã‚«ãƒ¼ãƒãƒ«é–¢æ•°</h3>

<strong>1. RBFï¼ˆRadial Basis Functionï¼‰ã‚«ãƒ¼ãƒãƒ«</strong>

$$
k(x, x') = \sigma^2 \exp\left(-\frac{||x - x'||^2}{2\ell^2}\right)
$$

- $\sigma^2$: åˆ†æ•£ï¼ˆå‡ºåŠ›ã®ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
- $\ell$: é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆã©ã‚Œã ã‘æ»‘ã‚‰ã‹ã‹ï¼‰

<strong>ç‰¹å¾´</strong>:
- æœ€ã‚‚ä¸€èˆ¬çš„
- ç„¡é™å›å¾®åˆ†å¯èƒ½ï¼ˆæ»‘ã‚‰ã‹ãªé–¢æ•°ï¼‰
- ææ–™ç‰¹æ€§äºˆæ¸¬ã«é©ã—ã¦ã„ã‚‹

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹1: RBFã‚«ãƒ¼ãƒãƒ«ã®å¯è¦–åŒ–</strong>

<pre><code class="language-python"><h1>RBFã‚«ãƒ¼ãƒãƒ«ã®å¯è¦–åŒ–</h1>
import numpy as np
import matplotlib.pyplot as plt

def rbf_kernel(x1, x2, sigma=1.0, length_scale=1.0):
    """
    RBFã‚«ãƒ¼ãƒãƒ«é–¢æ•°

    Parameters:
    -----------
    x1, x2 : array
        å…¥åŠ›ç‚¹
    sigma : float
        æ¨™æº–åå·®ï¼ˆå‡ºåŠ›ã®ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
    length_scale : float
        é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆå…¥åŠ›ã®ç›¸é–¢è·é›¢ï¼‰

    Returns:
    --------
    float : ã‚«ãƒ¼ãƒãƒ«å€¤ï¼ˆé¡ä¼¼åº¦ï¼‰
    """
    distance = np.abs(x1 - x2)
    return sigma<strong>2 * np.exp(-0.5 * (distance / length_scale)</strong>2)

<h1>ç•°ãªã‚‹é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã§ã‚«ãƒ¼ãƒãƒ«ã‚’å¯è¦–åŒ–</h1>
x_ref = 0.5  # å‚ç…§ç‚¹
x_range = np.linspace(0, 1, 100)

plt.figure(figsize=(12, 4))

<h1>å·¦å›³: é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã®å½±éŸ¿</h1>
plt.subplot(1, 3, 1)
for length_scale in [0.05, 0.1, 0.2, 0.5]:
    k_values = [rbf_kernel(x_ref, x, sigma=1.0,
                           length_scale=length_scale)
                for x in x_range]
    plt.plot(x_range, k_values,
             label=f'$\\ell$ = {length_scale}', linewidth=2)
plt.axvline(x_ref, color='black', linestyle='--', alpha=0.5)
plt.xlabel('x', fontsize=12)
plt.ylabel('k(0.5, x)', fontsize=12)
plt.title('é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã®å½±éŸ¿', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

<h1>ä¸­å¤®å›³: è¤‡æ•°ã®å‚ç…§ç‚¹</h1>
plt.subplot(1, 3, 2)
for x_ref_temp in [0.2, 0.5, 0.8]:
    k_values = [rbf_kernel(x_ref_temp, x, sigma=1.0,
                           length_scale=0.1)
                for x in x_range]
    plt.plot(x_range, k_values,
             label=f'x_ref = {x_ref_temp}', linewidth=2)
plt.xlabel('x', fontsize=12)
plt.ylabel('k(x_ref, x)', fontsize=12)
plt.title('å‚ç…§ç‚¹ã®å½±éŸ¿', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

<h1>å³å›³: ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ã®å¯è¦–åŒ–</h1>
plt.subplot(1, 3, 3)
x_grid = np.linspace(0, 1, 50)
K = np.zeros((len(x_grid), len(x_grid)))
for i, x1 in enumerate(x_grid):
    for j, x2 in enumerate(x_grid):
        K[i, j] = rbf_kernel(x1, x2, sigma=1.0, length_scale=0.1)

plt.imshow(K, cmap='viridis', origin='lower', extent=[0, 1, 0, 1])
plt.colorbar(label='ã‚«ãƒ¼ãƒãƒ«å€¤')
plt.xlabel('x', fontsize=12)
plt.ylabel("x'", fontsize=12)
plt.title('ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—', fontsize=14)

plt.tight_layout()
plt.savefig('rbf_kernel_visualization.png', dpi=150,
            bbox_inches='tight')
plt.show()

print("RBFã‚«ãƒ¼ãƒãƒ«ã®ç‰¹æ€§:")
print("  - é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ãŒå°ã•ã„ â†’ å±€æ‰€çš„ãªç›¸é–¢ï¼ˆé‹­ã„ãƒ”ãƒ¼ã‚¯ï¼‰")
print("  - é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ãŒå¤§ãã„ â†’ åºƒç¯„å›²ã®ç›¸é–¢ï¼ˆãªã ã‚‰ã‹ãªæ›²ç·šï¼‰")
print("  - å¯¾è§’ç·šä¸Šï¼ˆx = x'ï¼‰ã§ã‚«ãƒ¼ãƒãƒ«å€¤ãŒæœ€å¤§")</code></pre>

<strong>é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ</strong>:
- <strong>é•·ã•ã‚¹ã‚±ãƒ¼ãƒ« $\ell$</strong>: é–¢æ•°ã®æ»‘ã‚‰ã‹ã•ã‚’åˆ¶å¾¡
  - å°ã•ã„ $\ell$ â†’ æ€¥å³»ãªå¤‰åŒ–ã‚’è¨±å®¹
  - å¤§ãã„ $\ell$ â†’ æ»‘ã‚‰ã‹ãªé–¢æ•°ã‚’ä»®å®š
- <strong>ææ–™ç§‘å­¦ã§ã®æ„å‘³</strong>: çµ„æˆã‚„æ¡ä»¶ãŒè¿‘ã„ã¨ã€ç‰¹æ€§ã‚‚ä¼¼ã¦ã„ã‚‹ã¨ä»®å®š

---

<h3>ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®äºˆæ¸¬å¼</h3>

è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ $\mathcal{D} = \{(x_1, y_1), \ldots, (x_n, y_n)\}$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€æ–°ã—ã„ç‚¹ $x_*$ ã§ã®äºˆæ¸¬ã¯ï¼š

<strong>äºˆæ¸¬å¹³å‡</strong>:
$$
\mu(x_*) = k_* K^{-1} \mathbf{y}
$$

<strong>äºˆæ¸¬åˆ†æ•£</strong>:
$$
\sigma^2(x_*) = k(x_*, x_*) - k_*^T K^{-1} k_*
$$

ã“ã“ã§ï¼š
- $K$: è¦³æ¸¬ç‚¹é–“ã®ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ— $K_{ij} = k(x_i, x_j)$
- $k_*$: æ–°ã—ã„ç‚¹ã¨è¦³æ¸¬ç‚¹é–“ã®ã‚«ãƒ¼ãƒãƒ«ãƒ™ã‚¯ãƒˆãƒ«
- $\mathbf{y}$: è¦³æ¸¬å€¤ã®ãƒ™ã‚¯ãƒˆãƒ«

<strong>äºˆæ¸¬åˆ†å¸ƒ</strong>:
$$
f(x_*) | \mathcal{D} \sim \mathcal{N}(\mu(x_*), \sigma^2(x_*))
$$

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹2: ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®å®Ÿè£…ã¨å¯è¦–åŒ–</strong>

<pre><code class="language-python"><h1>ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®å®Ÿè£…</h1>
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import cdist

class GaussianProcessRegressor:
    """
    ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®ç°¡æ˜“å®Ÿè£…

    Parameters:
    -----------
    kernel : str
        ã‚«ãƒ¼ãƒãƒ«ç¨®é¡ï¼ˆ'rbf'ã®ã¿ã‚µãƒãƒ¼ãƒˆï¼‰
    sigma : float
        ã‚«ãƒ¼ãƒãƒ«ã®æ¨™æº–åå·®
    length_scale : float
        ã‚«ãƒ¼ãƒãƒ«ã®é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«
    noise : float
        è¦³æ¸¬ãƒã‚¤ã‚ºã®æ¨™æº–åå·®
    """

    def __init__(self, kernel='rbf', sigma=1.0,
                 length_scale=0.1, noise=0.01):
        self.kernel = kernel
        self.sigma = sigma
        self.length_scale = length_scale
        self.noise = noise
        self.X_train = None
        self.y_train = None
        self.K_inv = None

    def _kernel(self, X1, X2):
        """ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ã®è¨ˆç®—"""
        if self.kernel == 'rbf':
            dists = cdist(X1, X2, 'sqeuclidean')
            K = self.sigma**2 * np.exp(-0.5 * dists /
                                        self.length_scale**2)
            return K
        else:
            raise ValueError(f"Unknown kernel: {self.kernel}")

    def fit(self, X_train, y_train):
        """
        ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’

        Parameters:
        -----------
        X_train : array (n_samples, n_features)
            è¨“ç·´å…¥åŠ›
        y_train : array (n_samples,)
            è¨“ç·´å‡ºåŠ›
        """
        self.X_train = X_train
        self.y_train = y_train

        # ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ã‚’è¨ˆç®—ï¼ˆãƒã‚¤ã‚ºé …ã‚’è¿½åŠ ï¼‰
        K = self._kernel(X_train, X_train)
        K += self.noise**2 * np.eye(len(X_train))

        # é€†è¡Œåˆ—ã‚’è¨ˆç®—ï¼ˆäºˆæ¸¬ã§ä½¿ç”¨ï¼‰
        self.K_inv = np.linalg.inv(K)

    def predict(self, X_test, return_std=False):
        """
        äºˆæ¸¬ã‚’å®Ÿè¡Œ

        Parameters:
        -----------
        X_test : array (n_test, n_features)
            ãƒ†ã‚¹ãƒˆå…¥åŠ›
        return_std : bool
            æ¨™æº–åå·®ã‚‚è¿”ã™ã‹

        Returns:
        --------
        mean : array (n_test,)
            äºˆæ¸¬å¹³å‡
        std : array (n_test,) (if return_std=True)
            äºˆæ¸¬æ¨™æº–åå·®
        """
        # k_* = k(X_test, X_train)
        k_star = self._kernel(X_test, self.X_train)

        # äºˆæ¸¬å¹³å‡: Î¼(x_*) = k_* K^{-1} y
        mean = k_star @ self.K_inv @ self.y_train

        if return_std:
            # k(x_*, x_*)
            k_starstar = self._kernel(X_test, X_test)

            # äºˆæ¸¬åˆ†æ•£: ÏƒÂ²(x_*) = k(x_*, x_*) - k_*^T K^{-1} k_*
            variance = np.diag(k_starstar) - np.sum(
                (k_star @ self.K_inv) * k_star, axis=1
            )
            std = np.sqrt(np.maximum(variance, 0))  # æ•°å€¤èª¤å·®å¯¾ç­–
            return mean, std
        else:
            return mean

<h1>ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: ææ–™ã®ã‚¤ã‚ªãƒ³ä¼å°åº¦äºˆæ¸¬</h1>
np.random.seed(42)

<h1>çœŸã®é–¢æ•°ï¼ˆæœªçŸ¥ã¨ä»®å®šï¼‰</h1>
def true_function(x):
    """Li-ioné›»æ± é›»è§£è³ªã®ã‚¤ã‚ªãƒ³ä¼å°åº¦ï¼ˆä»®æƒ³ï¼‰"""
    return (
        np.sin(3 * x) * np.exp(-x) +
        0.7 * np.exp(-((x - 0.5) / 0.2)**2)
    )

<h1>è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆå°‘æ•°ã®å®Ÿé¨“çµæœï¼‰</h1>
n_observations = 8
X_train = np.random.uniform(0, 1, n_observations).reshape(-1, 1)
y_train = true_function(X_train).ravel() + np.random.normal(0, 0.05,
                                                             n_observations)

<h1>ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆäºˆæ¸¬ã—ãŸã„ç‚¹ï¼‰</h1>
X_test = np.linspace(0, 1, 200).reshape(-1, 1)
y_true = true_function(X_test).ravel()

<h1>ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’</h1>
gp = GaussianProcessRegressor(sigma=1.0, length_scale=0.15, noise=0.05)
gp.fit(X_train, y_train)

<h1>äºˆæ¸¬</h1>
y_pred, y_std = gp.predict(X_test, return_std=True)

<h1>å¯è¦–åŒ–</h1>
plt.figure(figsize=(12, 6))

<h1>çœŸã®é–¢æ•°</h1>
plt.plot(X_test, y_true, 'k--', linewidth=2, label='çœŸã®é–¢æ•°')

<h1>è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿</h1>
plt.scatter(X_train, y_train, c='red', s=100, zorder=10,
            edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿé¨“çµæœï¼‰')

<h1>äºˆæ¸¬å¹³å‡</h1>
plt.plot(X_test, y_pred, 'b-', linewidth=2, label='äºˆæ¸¬å¹³å‡')

<h1>ä¸ç¢ºå®Ÿæ€§ï¼ˆ95%ä¿¡é ¼åŒºé–“ï¼‰</h1>
plt.fill_between(
    X_test.ravel(),
    y_pred - 1.96 * y_std,
    y_pred + 1.96 * y_std,
    alpha=0.3,
    color='blue',
    label='95%ä¿¡é ¼åŒºé–“'
)

plt.xlabel('çµ„æˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
plt.ylabel('ã‚¤ã‚ªãƒ³ä¼å°åº¦ (mS/cm)', fontsize=12)
plt.title('ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã«ã‚ˆã‚‹ææ–™ç‰¹æ€§äºˆæ¸¬', fontsize=14)
plt.legend(loc='best')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('gp_regression_demo.png', dpi=150, bbox_inches='tight')
plt.show()

print("ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®çµæœ:")
print(f"  è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿æ•°: {n_observations}")
print(f"  äºˆæ¸¬ç‚¹æ•°: {len(X_test)}")
print(f"  RMSE: {np.sqrt(np.mean((y_pred - y_true)**2)):.4f}")
print("\nç‰¹å¾´:")
print("  - è¦³æ¸¬ç‚¹ä»˜è¿‘: ä¸ç¢ºå®Ÿæ€§ãŒå°ã•ã„ï¼ˆä¿¡é ¼åŒºé–“ãŒç‹­ã„ï¼‰")
print("  - æœªè¦³æ¸¬é ˜åŸŸ: ä¸ç¢ºå®Ÿæ€§ãŒå¤§ãã„ï¼ˆä¿¡é ¼åŒºé–“ãŒåºƒã„ï¼‰")
print("  - ã“ã®ä¸ç¢ºå®Ÿæ€§æƒ…å ±ãŒç²å¾—é–¢æ•°ã§æ´»ç”¨ã•ã‚Œã‚‹")</code></pre>

<strong>é‡è¦ãªè¦³å¯Ÿ</strong>:
1. <strong>è¦³æ¸¬ç‚¹ã®è¿‘ã</strong>: äºˆæ¸¬ç²¾åº¦ãŒé«˜ã„ï¼ˆä¸ç¢ºå®Ÿæ€§å°ï¼‰
2. <strong>æœªè¦³æ¸¬é ˜åŸŸ</strong>: ä¸ç¢ºå®Ÿæ€§ãŒå¤§ãã„
3. <strong>ãƒ‡ãƒ¼ã‚¿ãŒå¢—ãˆã‚‹ã»ã©</strong>: äºˆæ¸¬ç²¾åº¦å‘ä¸Š
4. <strong>ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–</strong>: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®éµ

---

<h2>2.3 ç²å¾—é–¢æ•°ï¼šæ¬¡ã®å®Ÿé¨“ç‚¹ã‚’ã©ã†é¸ã¶ã‹</h2>

<h3>ç²å¾—é–¢æ•°ã®å½¹å‰²</h3>

<strong>ç²å¾—é–¢æ•°ï¼ˆAcquisition Functionï¼‰</strong>ã¯ã€ã€Œæ¬¡ã«ã©ã“ã‚’å®Ÿé¨“ã™ã¹ãã‹ã€ã‚’æ•°å­¦çš„ã«æ±ºå®šã—ã¾ã™ã€‚

<strong>è¨­è¨ˆæ€æƒ³</strong>:
- <strong>é«˜ã„äºˆæ¸¬å€¤ã®å ´æ‰€</strong>ã‚’æ¢ç´¢ï¼ˆExploitation: æ´»ç”¨ï¼‰
- <strong>ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„å ´æ‰€</strong>ã‚’æ¢ç´¢ï¼ˆExploration: æ¢ç´¢ï¼‰
- ã“ã®<strong>2ã¤ã®ãƒãƒ©ãƒ³ã‚¹</strong>ã‚’æœ€é©åŒ–

<h3>ç²å¾—é–¢æ•°ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼</h3>

<div class="mermaid">graph TB
    A[ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«] --> B[äºˆæ¸¬å¹³å‡ Î¼(x)]
    A --> C[äºˆæ¸¬æ¨™æº–åå·® Ïƒ(x)]
    B --> D[ç²å¾—é–¢æ•° Î±(x)]
    C --> D
    D --> E[ç²å¾—é–¢æ•°ã‚’æœ€å¤§åŒ–]
    E --> F[æ¬¡ã®å®Ÿé¨“ç‚¹ x_next]

    style A fill:#e3f2fd
    style D fill:#fff3e0
    style F fill:#e8f5e9</div>

---

<h3>ä¸»è¦ãªç²å¾—é–¢æ•°</h3>

<h4>1. Expected Improvementï¼ˆEIï¼‰</h4>

<strong>å®šç¾©</strong>:
ç¾åœ¨ã®æœ€è‰¯å€¤ $f_{\text{best}}$ ã‹ã‚‰ã®æ”¹å–„é‡ã®æœŸå¾…å€¤ã‚’æœ€å¤§åŒ–

$$
\text{EI}(x) = \mathbb{E}[\max(0, f(x) - f_{\text{best}})]
$$

<strong>è§£æè§£</strong>:
$$
\text{EI}(x) = \begin{cases}
(\mu(x) - f_{\text{best}}) \Phi(Z) + \sigma(x) \phi(Z) & \text{if } \sigma(x) > 0 \\
0 & \text{if } \sigma(x) = 0
\end{cases}
$$

ã“ã“ã§ï¼š
$$
Z = \frac{\mu(x) - f_{\text{best}}}{\sigma(x)}
$$
- $\Phi$: æ¨™æº–æ­£è¦åˆ†å¸ƒã®ç´¯ç©åˆ†å¸ƒé–¢æ•°
- $\phi$: æ¨™æº–æ­£è¦åˆ†å¸ƒã®ç¢ºç‡å¯†åº¦é–¢æ•°

<strong>ç‰¹å¾´</strong>:
- <strong>ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„</strong>: æ¢ç´¢ã¨æ´»ç”¨ã‚’è‡ªå‹•èª¿æ•´
- <strong>æœ€ã‚‚ä¸€èˆ¬çš„</strong>: ææ–™ç§‘å­¦ã§åºƒãä½¿ç”¨
- <strong>è§£æçš„</strong>: è¨ˆç®—ãŒé«˜é€Ÿ

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹3: Expected Improvementã®å®Ÿè£…</strong>

<pre><code class="language-python"><h1>Expected Improvementã®å®Ÿè£…</h1>
from scipy.stats import norm

def expected_improvement(X, gp, f_best, xi=0.01):
    """
    Expected Improvementç²å¾—é–¢æ•°

    Parameters:
    -----------
    X : array (n_samples, n_features)
        è©•ä¾¡ç‚¹
    gp : GaussianProcessRegressor
        å­¦ç¿’æ¸ˆã¿ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«
    f_best : float
        ç¾åœ¨ã®æœ€è‰¯å€¤
    xi : float
        æ¢ç´¢ã®å¼·ã•ï¼ˆexploration parameterï¼‰

    Returns:
    --------
    ei : array (n_samples,)
        EIå€¤
    """
    mu, sigma = gp.predict(X, return_std=True)

    # æ”¹å–„é‡
    improvement = mu - f_best - xi

    # æ¨™æº–åŒ–
    Z = improvement / (sigma + 1e-9)  # ã‚¼ãƒ­é™¤ç®—å›é¿

    # Expected Improvement
    ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)

    # Ïƒ = 0ã®å ´åˆã¯EI = 0
    ei[sigma == 0.0] = 0.0

    return ei

<h1>ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</h1>
np.random.seed(42)

<h1>è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿</h1>
X_train = np.array([0.1, 0.3, 0.5, 0.7, 0.9]).reshape(-1, 1)
y_train = true_function(X_train).ravel()

<h1>ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«</h1>
gp = GaussianProcessRegressor(sigma=1.0, length_scale=0.2, noise=0.01)
gp.fit(X_train, y_train)

<h1>ãƒ†ã‚¹ãƒˆç‚¹</h1>
X_test = np.linspace(0, 1, 500).reshape(-1, 1)

<h1>äºˆæ¸¬</h1>
y_pred, y_std = gp.predict(X_test, return_std=True)

<h1>ç¾åœ¨ã®æœ€è‰¯å€¤</h1>
f_best = np.max(y_train)

<h1>EIã‚’è¨ˆç®—</h1>
ei = expected_improvement(X_test, gp, f_best, xi=0.01)

<h1>æ¬¡ã®å®Ÿé¨“ç‚¹ã‚’ææ¡ˆ</h1>
next_idx = np.argmax(ei)
next_x = X_test[next_idx]

<h1>å¯è¦–åŒ–</h1>
fig, axes = plt.subplots(2, 1, figsize=(12, 10))

<h1>ä¸Šå›³: ã‚¬ã‚¦ã‚¹éç¨‹ã®äºˆæ¸¬</h1>
ax1 = axes[0]
ax1.plot(X_test, true_function(X_test), 'k--',
         linewidth=2, label='çœŸã®é–¢æ•°')
ax1.scatter(X_train, y_train, c='red', s=100, zorder=10,
            edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
ax1.plot(X_test, y_pred, 'b-', linewidth=2, label='äºˆæ¸¬å¹³å‡')
ax1.fill_between(X_test.ravel(), y_pred - 1.96 * y_std,
                 y_pred + 1.96 * y_std, alpha=0.3, color='blue',
                 label='95%ä¿¡é ¼åŒºé–“')
ax1.axhline(f_best, color='green', linestyle=':',
            linewidth=2, label=f'ç¾åœ¨ã®æœ€è‰¯å€¤ = {f_best:.3f}')
ax1.axvline(next_x, color='orange', linestyle='--',
            linewidth=2, label=f'ææ¡ˆç‚¹ = {next_x[0]:.3f}')
ax1.set_ylabel('ç›®çš„é–¢æ•°', fontsize=12)
ax1.set_title('ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®äºˆæ¸¬', fontsize=14)
ax1.legend(loc='best')
ax1.grid(True, alpha=0.3)

<h1>ä¸‹å›³: Expected Improvement</h1>
ax2 = axes[1]
ax2.plot(X_test, ei, 'r-', linewidth=2, label='Expected Improvement')
ax2.axvline(next_x, color='orange', linestyle='--',
            linewidth=2, label=f'æœ€å¤§EIç‚¹ = {next_x[0]:.3f}')
ax2.fill_between(X_test.ravel(), 0, ei, alpha=0.3, color='red')
ax2.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
ax2.set_ylabel('EI(x)', fontsize=12)
ax2.set_title('Expected Improvementç²å¾—é–¢æ•°', fontsize=14)
ax2.legend(loc='best')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('expected_improvement_demo.png', dpi=150,
            bbox_inches='tight')
plt.show()

print(f"Expected Improvementã«ã‚ˆã‚‹ææ¡ˆ:")
print(f"  æ¬¡ã®å®Ÿé¨“ç‚¹: x = {next_x[0]:.3f}")
print(f"  EIå€¤: {np.max(ei):.4f}")
print(f"  äºˆæ¸¬å¹³å‡: {y_pred[next_idx]:.3f}")
print(f"  äºˆæ¸¬æ¨™æº–åå·®: {y_std[next_idx]:.3f}")</code></pre>

<strong>EIã®è§£é‡ˆ</strong>:
- <strong>é«˜ã„å¹³å‡å€¤</strong>ã®å ´æ‰€ â†’ æ´»ç”¨ï¼ˆExploitationï¼‰
- <strong>é«˜ã„ä¸ç¢ºå®Ÿæ€§</strong>ã®å ´æ‰€ â†’ æ¢ç´¢ï¼ˆExplorationï¼‰
- <strong>ä¸¡æ–¹ã‚’è€ƒæ…®</strong>ã—ã¦ãƒãƒ©ãƒ³ã‚¹

---

<h4>2. Upper Confidence Boundï¼ˆUCBï¼‰</h4>

<strong>å®šç¾©</strong>:
äºˆæ¸¬å¹³å‡ã«ä¸ç¢ºå®Ÿæ€§ã‚’åŠ ãˆãŸã€Œæ¥½è¦³çš„ãªæ¨å®šå€¤ã€ã‚’æœ€å¤§åŒ–

$$
\text{UCB}(x) = \mu(x) + \kappa \sigma(x)
$$

- $\kappa$: æ¢ç´¢ã®å¼·ã•ã‚’åˆ¶å¾¡ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé€šå¸¸ $\kappa = 2$ï¼‰

<strong>ç‰¹å¾´</strong>:
- <strong>ã‚·ãƒ³ãƒ—ãƒ«</strong>: å®Ÿè£…ãŒå®¹æ˜“
- <strong>ç›´æ„Ÿçš„</strong>: æ¥½è¦³ä¸»ç¾©ã®åŸå‰‡ï¼ˆOptimism in the Face of Uncertaintyï¼‰
- <strong>èª¿æ•´å¯èƒ½</strong>: $\kappa$ã§æ¢ç´¢åº¦åˆã„ã‚’åˆ¶å¾¡

<strong>$\kappa$ã®å½±éŸ¿</strong>:
- $\kappa$ãŒå¤§ãã„ â†’ æ¢ç´¢é‡è¦–ï¼ˆãƒªã‚¹ã‚¯ã‚’å–ã‚‹ï¼‰
- $\kappa$ãŒå°ã•ã„ â†’ æ´»ç”¨é‡è¦–ï¼ˆå®‰å…¨ç­–ï¼‰

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹4: Upper Confidence Boundã®å®Ÿè£…</strong>

<pre><code class="language-python"><h1>Upper Confidence Boundã®å®Ÿè£…</h1>
def upper_confidence_bound(X, gp, kappa=2.0):
    """
    Upper Confidence Boundç²å¾—é–¢æ•°

    Parameters:
    -----------
    X : array (n_samples, n_features)
        è©•ä¾¡ç‚¹
    gp : GaussianProcessRegressor
        å­¦ç¿’æ¸ˆã¿ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«
    kappa : float
        æ¢ç´¢ã®å¼·ã•ï¼ˆé€šå¸¸2.0ï¼‰

    Returns:
    --------
    ucb : array (n_samples,)
        UCBå€¤
    """
    mu, sigma = gp.predict(X, return_std=True)
    ucb = mu + kappa * sigma
    return ucb

<h1>ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: ç•°ãªã‚‹Îºã§ã®æ¯”è¼ƒ</h1>
fig, axes = plt.subplots(3, 1, figsize=(12, 12))

kappa_values = [0.5, 2.0, 5.0]

for i, kappa in enumerate(kappa_values):
    ax = axes[i]

    # UCBã‚’è¨ˆç®—
    ucb = upper_confidence_bound(X_test, gp, kappa=kappa)

    # æ¬¡ã®å®Ÿé¨“ç‚¹
    next_idx = np.argmax(ucb)
    next_x = X_test[next_idx]

    # äºˆæ¸¬å¹³å‡ã¨ä¿¡é ¼åŒºé–“
    ax.plot(X_test, y_pred, 'b-', linewidth=2, label='äºˆæ¸¬å¹³å‡ Î¼(x)')
    ax.fill_between(X_test.ravel(),
                    y_pred - 1.96 * y_std,
                    y_pred + 1.96 * y_std,
                    alpha=0.2, color='blue',
                    label='95%ä¿¡é ¼åŒºé–“')

    # UCB
    ax.plot(X_test, ucb, 'r-', linewidth=2,
            label=f'UCB(x) (Îº={kappa})')

    # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿
    ax.scatter(X_train, y_train, c='red', s=100, zorder=10,
               edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')

    # ææ¡ˆç‚¹
    ax.axvline(next_x, color='orange', linestyle='--',
               linewidth=2, label=f'ææ¡ˆç‚¹ = {next_x[0]:.3f}')

    ax.set_ylabel('ç›®çš„é–¢æ•°', fontsize=12)
    ax.set_title(f'UCB with Îº = {kappa}', fontsize=14)
    ax.legend(loc='best')
    ax.grid(True, alpha=0.3)

    if i == 2:
        ax.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)

plt.tight_layout()
plt.savefig('ucb_kappa_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

print("Îºã®å½±éŸ¿:")
print("  Îº = 0.5: æ´»ç”¨é‡è¦–ï¼ˆè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿è¿‘ãã‚’æ¢ç´¢ï¼‰")
print("  Îº = 2.0: ãƒãƒ©ãƒ³ã‚¹ï¼ˆæ¨™æº–çš„ãªè¨­å®šï¼‰")
print("  Îº = 5.0: æ¢ç´¢é‡è¦–ï¼ˆæœªçŸ¥é ˜åŸŸã‚’ç©æ¥µæ¢ç´¢ï¼‰")</code></pre>

---

<h4>3. Probability of Improvementï¼ˆPIï¼‰</h4>

<strong>å®šç¾©</strong>:
ç¾åœ¨ã®æœ€è‰¯å€¤ã‚’è¶…ãˆã‚‹ç¢ºç‡ã‚’æœ€å¤§åŒ–

$$
\text{PI}(x) = P(f(x) > f_{\text{best}}) = \Phi\left(\frac{\mu(x) - f_{\text{best}}}{\sigma(x)}\right)
$$

<strong>ç‰¹å¾´</strong>:
- <strong>æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«</strong>: è§£é‡ˆãŒå®¹æ˜“
- <strong>ä¿å®ˆçš„</strong>: å¤§ããªæ”¹å–„ã‚’æœŸå¾…ã—ãªã„
- <strong>å®Ÿç”¨çš„</strong>: å°ã•ãªæ”¹å–„ã‚’ç©ã¿é‡ã­ã‚‹æˆ¦ç•¥

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹5: Probability of Improvementã®å®Ÿè£…</strong>

<pre><code class="language-python"><h1>Probability of Improvementã®å®Ÿè£…</h1>
def probability_of_improvement(X, gp, f_best, xi=0.01):
    """
    Probability of Improvementç²å¾—é–¢æ•°

    Parameters:
    -----------
    X : array (n_samples, n_features)
        è©•ä¾¡ç‚¹
    gp : GaussianProcessRegressor
        å­¦ç¿’æ¸ˆã¿ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«
    f_best : float
        ç¾åœ¨ã®æœ€è‰¯å€¤
    xi : float
        æ¢ç´¢ã®å¼·ã•

    Returns:
    --------
    pi : array (n_samples,)
        PIå€¤
    """
    mu, sigma = gp.predict(X, return_std=True)

    # æ”¹å–„é‡
    improvement = mu - f_best - xi

    # æ¨™æº–åŒ–
    Z = improvement / (sigma + 1e-9)

    # Probability of Improvement
    pi = norm.cdf(Z)

    return pi

<h1>PIã‚’è¨ˆç®—</h1>
pi = probability_of_improvement(X_test, gp, f_best, xi=0.01)

<h1>å¯è¦–åŒ–</h1>
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(X_test, pi, 'g-', linewidth=2, label='PI(x)')
plt.axvline(X_test[np.argmax(pi)], color='orange',
            linestyle='--', linewidth=2,
            label=f'æœ€å¤§PIç‚¹ = {X_test[np.argmax(pi)][0]:.3f}')
plt.fill_between(X_test.ravel(), 0, pi, alpha=0.3, color='green')
plt.xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
plt.ylabel('PI(x)', fontsize=12)
plt.title('Probability of Improvement', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

<h1>æ¯”è¼ƒ: EI vs PI vs UCB</h1>
plt.subplot(1, 2, 2)
ei_normalized = ei / np.max(ei)
pi_normalized = pi / np.max(pi)
ucb_normalized = upper_confidence_bound(X_test, gp, kappa=2.0)
ucb_normalized = (ucb_normalized - np.min(ucb_normalized)) / \
                 (np.max(ucb_normalized) - np.min(ucb_normalized))

plt.plot(X_test, ei_normalized, 'r-', linewidth=2, label='EI (æ­£è¦åŒ–)')
plt.plot(X_test, pi_normalized, 'g-', linewidth=2, label='PI (æ­£è¦åŒ–)')
plt.plot(X_test, ucb_normalized, 'b-', linewidth=2, label='UCB (æ­£è¦åŒ–)')
plt.scatter(X_train, [0.5]*len(X_train), c='red', s=100,
            zorder=10, edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
plt.xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
plt.ylabel('ç²å¾—é–¢æ•°å€¤ï¼ˆæ­£è¦åŒ–ï¼‰', fontsize=12)
plt.title('ç²å¾—é–¢æ•°ã®æ¯”è¼ƒ', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('acquisition_functions_comparison.png', dpi=150,
            bbox_inches='tight')
plt.show()</code></pre>

---

<h3>ç²å¾—é–¢æ•°ã®æ¯”è¼ƒè¡¨</h3>

| ç²å¾—é–¢æ•° | ç‰¹å¾´ | é•·æ‰€ | çŸ­æ‰€ | æ¨å¥¨ç”¨é€” |
|---------|------|------|------|---------|
| <strong>EI</strong> | æ”¹å–„é‡ã®æœŸå¾…å€¤ | ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ã€å®Ÿç¸¾è±Šå¯Œ | ã‚„ã‚„è¤‡é›‘ | ä¸€èˆ¬çš„ãªæœ€é©åŒ– |
| <strong>UCB</strong> | æ¥½è¦³çš„æ¨å®š | ã‚·ãƒ³ãƒ—ãƒ«ã€èª¿æ•´å¯èƒ½ | Îºã®èª¿æ•´ãŒå¿…è¦ | æ¢ç´¢åº¦åˆã„åˆ¶å¾¡ |
| <strong>PI</strong> | æ”¹å–„ç¢ºç‡ | éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ« | ä¿å®ˆçš„ | å®‰å…¨ãªæ¢ç´¢ |

<strong>ææ–™ç§‘å­¦ã§ã®æ¨å¥¨</strong>:
- <strong>åˆå¿ƒè€…</strong>: EIï¼ˆãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ãã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å„ªç§€ï¼‰
- <strong>æ¢ç´¢é‡è¦–</strong>: UCBï¼ˆÎº = 2-5ï¼‰
- <strong>å®‰å…¨ç­–</strong>: PIï¼ˆå°ã•ãªæ”¹å–„ã‚’ç¢ºå®Ÿã«ï¼‰

---

<h2>2.4 æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•</h2>

<h3>æ•°å­¦çš„ãªå®šå¼åŒ–</h3>

ç²å¾—é–¢æ•°ã¯ã€ä»¥ä¸‹ã®2ã¤ã®é …ã«åˆ†è§£ã§ãã¾ã™ï¼š

$$
\alpha(x) = \underbrace{\mu(x)}_{\text{Exploitation}} + \underbrace{\lambda \sigma(x)}_{\text{Exploration}}
$$

- <strong>Exploitationé …</strong> $\mu(x)$: äºˆæ¸¬å¹³å‡ãŒé«˜ã„å ´æ‰€
- <strong>Explorationé …</strong> $\lambda \sigma(x)$: ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„å ´æ‰€

<h3>ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®å¯è¦–åŒ–</h3>

<div class="mermaid">graph LR
    subgraph æ´»ç”¨Exploitation
    A1[æ—¢çŸ¥ã®è‰¯ã„é ˜åŸŸ]
    A2[é«˜ã„äºˆæ¸¬å€¤ Î¼(x)]
    A3[ä½ã„ä¸ç¢ºå®Ÿæ€§ Ïƒ(x)]
    A1 --> A2
    A1 --> A3
    end

    subgraph æ¢ç´¢Exploration
    B1[æœªçŸ¥ã®é ˜åŸŸ]
    B2[æœªçŸ¥ã®äºˆæ¸¬å€¤ Î¼(x)]
    B3[é«˜ã„ä¸ç¢ºå®Ÿæ€§ Ïƒ(x)]
    B1 --> B2
    B1 --> B3
    end

    subgraph æœ€é©ãªãƒãƒ©ãƒ³ã‚¹
    C1[ç²å¾—é–¢æ•°]
    C2[Î¼(x) + Î»Ïƒ(x)]
    C3[æ¬¡ã®å®Ÿé¨“ç‚¹]
    C1 --> C2
    C2 --> C3
    end

    A2 --> C1
    A3 --> C1
    B2 --> C1
    B3 --> C1

    style A1 fill:#fff3e0
    style B1 fill:#e3f2fd
    style C3 fill:#e8f5e9</div>

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹6: æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹å¯è¦–åŒ–</strong>

<pre><code class="language-python"><h1>æ¢ç´¢ã¨æ´»ç”¨ã®åˆ†è§£</h1>
def decompose_acquisition(X, gp, f_best, xi=0.01):
    """
    ç²å¾—é–¢æ•°ã‚’æ¢ç´¢é …ã¨æ´»ç”¨é …ã«åˆ†è§£

    Returns:
    --------
    exploitation : æ´»ç”¨é …ï¼ˆäºˆæ¸¬å¹³å‡ãƒ™ãƒ¼ã‚¹ï¼‰
    exploration : æ¢ç´¢é …ï¼ˆä¸ç¢ºå®Ÿæ€§ãƒ™ãƒ¼ã‚¹ï¼‰
    """
    mu, sigma = gp.predict(X, return_std=True)

    # æ´»ç”¨é …ï¼ˆå¹³å‡ãŒé«˜ã„ã»ã©å¤§ãã„ï¼‰
    exploitation = mu

    # æ¢ç´¢é …ï¼ˆä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„ã»ã©å¤§ãã„ï¼‰
    exploration = sigma

    return exploitation, exploration

<h1>åˆ†è§£</h1>
exploitation, exploration = decompose_acquisition(X_test, gp, f_best)

<h1>å¯è¦–åŒ–</h1>
fig, axes = plt.subplots(4, 1, figsize=(12, 14))

<h1>1. ã‚¬ã‚¦ã‚¹éç¨‹ã®äºˆæ¸¬</h1>
ax1 = axes[0]
ax1.plot(X_test, y_pred, 'b-', linewidth=2, label='äºˆæ¸¬å¹³å‡ Î¼(x)')
ax1.fill_between(X_test.ravel(), y_pred - 1.96 * y_std,
                 y_pred + 1.96 * y_std, alpha=0.3, color='blue',
                 label='95%ä¿¡é ¼åŒºé–“')
ax1.scatter(X_train, y_train, c='red', s=100, zorder=10,
            edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
ax1.set_ylabel('ç›®çš„é–¢æ•°', fontsize=12)
ax1.set_title('ã‚¬ã‚¦ã‚¹éç¨‹ã®äºˆæ¸¬', fontsize=14)
ax1.legend()
ax1.grid(True, alpha=0.3)

<h1>2. æ´»ç”¨é …ï¼ˆExploitationï¼‰</h1>
ax2 = axes[1]
ax2.plot(X_test, exploitation, 'g-', linewidth=2,
         label='æ´»ç”¨é …ï¼ˆäºˆæ¸¬å¹³å‡ï¼‰')
ax2.scatter(X_train, y_train, c='red', s=100, zorder=10,
            edgecolors='black', alpha=0.5)
ax2.set_ylabel('æ´»ç”¨é …', fontsize=12)
ax2.set_title('Exploitation: æ—¢çŸ¥ã®è‰¯ã„é ˜åŸŸã‚’é‡è¦–', fontsize=14)
ax2.legend()
ax2.grid(True, alpha=0.3)

<h1>3. æ¢ç´¢é …ï¼ˆExplorationï¼‰</h1>
ax3 = axes[2]
ax3.plot(X_test, exploration, 'orange', linewidth=2,
         label='æ¢ç´¢é …ï¼ˆä¸ç¢ºå®Ÿæ€§ï¼‰')
ax3.scatter(X_train, [0]*len(X_train), c='red', s=100,
            zorder=10, edgecolors='black', alpha=0.5,
            label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ä½ç½®')
ax3.set_ylabel('æ¢ç´¢é …', fontsize=12)
ax3.set_title('Exploration: æœªçŸ¥ã®é ˜åŸŸã‚’é‡è¦–', fontsize=14)
ax3.legend()
ax3.grid(True, alpha=0.3)

<h1>4. çµ±åˆï¼ˆEIï¼‰</h1>
ax4 = axes[3]
ei_values = expected_improvement(X_test, gp, f_best, xi=0.01)
ax4.plot(X_test, ei_values, 'r-', linewidth=2,
         label='Expected Improvement')
next_x = X_test[np.argmax(ei_values)]
ax4.axvline(next_x, color='purple', linestyle='--',
            linewidth=2, label=f'ææ¡ˆç‚¹ = {next_x[0]:.3f}')
ax4.fill_between(X_test.ravel(), 0, ei_values,
                 alpha=0.3, color='red')
ax4.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
ax4.set_ylabel('EI(x)', fontsize=12)
ax4.set_title('çµ±åˆ: ä¸¡è€…ã®ãƒãƒ©ãƒ³ã‚¹ã‚’æœ€é©åŒ–', fontsize=14)
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('exploitation_exploration_tradeoff.png', dpi=150,
            bbox_inches='tight')
plt.show()

print("æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•:")
print(f"  ææ¡ˆç‚¹ x = {next_x[0]:.3f}")
print(f"    äºˆæ¸¬å¹³å‡ï¼ˆæ´»ç”¨ï¼‰: {y_pred[np.argmax(ei_values)]:.3f}")
print(f"    ä¸ç¢ºå®Ÿæ€§ï¼ˆæ¢ç´¢ï¼‰: {y_std[np.argmax(ei_values)]:.3f}")
print(f"    EIå€¤: {np.max(ei_values):.4f}")</code></pre>

---

<h2>2.5 åˆ¶ç´„ä»˜ãæœ€é©åŒ–ã¨å¤šç›®çš„æœ€é©åŒ–</h2>

<h3>åˆ¶ç´„ä»˜ããƒ™ã‚¤ã‚ºæœ€é©åŒ–</h3>

å®Ÿéš›ã®ææ–™é–‹ç™ºã§ã¯ã€<strong>åˆ¶ç´„æ¡ä»¶</strong>ãŒå­˜åœ¨ã—ã¾ã™ï¼š

<strong>ä¾‹ï¼šLi-ioné›»æ± é›»è§£è³ª</strong>
- ã‚¤ã‚ªãƒ³ä¼å°åº¦ã‚’æœ€å¤§åŒ–ï¼ˆç›®çš„é–¢æ•°ï¼‰
- ç²˜åº¦ < 10 cPï¼ˆåˆ¶ç´„1ï¼‰
- å¼•ç«ç‚¹ > 100Â°Cï¼ˆåˆ¶ç´„2ï¼‰
- ã‚³ã‚¹ãƒˆ < $50/kgï¼ˆåˆ¶ç´„3ï¼‰

<strong>æ•°å­¦çš„å®šå¼åŒ–</strong>:
$$
\begin{align}
\max_{x} \quad & f(x) \\
\text{s.t.} \quad & g_i(x) \leq 0, \quad i = 1, \ldots, m \\
& h_j(x) = 0, \quad j = 1, \ldots, p
\end{align}
$$

<strong>ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</strong>:
1. <strong>åˆ¶ç´„é–¢æ•°ã‚‚ã‚¬ã‚¦ã‚¹éç¨‹ã§ãƒ¢ãƒ‡ãƒ«åŒ–</strong>
2. <strong>åˆ¶ç´„ã‚’æº€ãŸã™ç¢ºç‡ã‚’ç²å¾—é–¢æ•°ã«çµ„ã¿è¾¼ã‚€</strong>

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹7: åˆ¶ç´„ä»˜ããƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ãƒ‡ãƒ¢</strong>

<pre><code class="language-python"><h1>åˆ¶ç´„ä»˜ããƒ™ã‚¤ã‚ºæœ€é©åŒ–</h1>
def constrained_expected_improvement(X, gp_obj, gp_constraint,
                                     f_best, constraint_threshold=0):
    """
    åˆ¶ç´„ä»˜ãExpected Improvement

    Parameters:
    -----------
    gp_obj : ã‚¬ã‚¦ã‚¹éç¨‹ï¼ˆç›®çš„é–¢æ•°ï¼‰
    gp_constraint : ã‚¬ã‚¦ã‚¹éç¨‹ï¼ˆåˆ¶ç´„é–¢æ•°ï¼‰
    constraint_threshold : åˆ¶ç´„ã®é–¾å€¤ï¼ˆâ‰¤ 0ãŒå®Ÿè¡Œå¯èƒ½ï¼‰
    """
    # ç›®çš„é–¢æ•°ã®EI
    ei = expected_improvement(X, gp_obj, f_best, xi=0.01)

    # åˆ¶ç´„ã‚’æº€ãŸã™ç¢ºç‡
    mu_c, sigma_c = gp_constraint.predict(X, return_std=True)
    prob_feasible = norm.cdf((constraint_threshold - mu_c) /
                             (sigma_c + 1e-9))

    # åˆ¶ç´„ä»˜ãEI = EI Ã— åˆ¶ç´„æº€è¶³ç¢ºç‡
    cei = ei * prob_feasible

    return cei

<h1>ãƒ‡ãƒ¢: åˆ¶ç´„é–¢æ•°ã‚’å®šç¾©</h1>
def constraint_function(x):
    """åˆ¶ç´„é–¢æ•°ï¼ˆä¾‹ï¼šç²˜åº¦ã®ä¸Šé™ï¼‰"""
    return 0.5 - x  # x < 0.5 ãŒå®Ÿè¡Œå¯èƒ½é ˜åŸŸ

<h1>åˆ¶ç´„ãƒ‡ãƒ¼ã‚¿</h1>
y_constraint = constraint_function(X_train).ravel()

<h1>åˆ¶ç´„é–¢æ•°ç”¨ã®ã‚¬ã‚¦ã‚¹éç¨‹</h1>
gp_constraint = GaussianProcessRegressor(sigma=0.5, length_scale=0.2,
                                         noise=0.01)
gp_constraint.fit(X_train, y_constraint)

<h1>åˆ¶ç´„ä»˜ãEIã‚’è¨ˆç®—</h1>
cei = constrained_expected_improvement(X_test, gp, gp_constraint,
                                       f_best, constraint_threshold=0)

<h1>å¯è¦–åŒ–</h1>
fig, axes = plt.subplots(3, 1, figsize=(12, 12))

<h1>ä¸Šå›³: ç›®çš„é–¢æ•°</h1>
ax1 = axes[0]
ax1.plot(X_test, y_pred, 'b-', linewidth=2, label='ç›®çš„é–¢æ•°ã®äºˆæ¸¬')
ax1.fill_between(X_test.ravel(), y_pred - 1.96 * y_std,
                 y_pred + 1.96 * y_std, alpha=0.3, color='blue')
ax1.scatter(X_train, y_train, c='red', s=100, zorder=10,
            edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
ax1.set_ylabel('ç›®çš„é–¢æ•°', fontsize=12)
ax1.set_title('ç›®çš„é–¢æ•°ï¼ˆã‚¤ã‚ªãƒ³ä¼å°åº¦ï¼‰', fontsize=14)
ax1.legend()
ax1.grid(True, alpha=0.3)

<h1>ä¸­å›³: åˆ¶ç´„é–¢æ•°</h1>
ax2 = axes[1]
mu_c, sigma_c = gp_constraint.predict(X_test, return_std=True)
ax2.plot(X_test, mu_c, 'g-', linewidth=2, label='åˆ¶ç´„é–¢æ•°ã®äºˆæ¸¬')
ax2.fill_between(X_test.ravel(), mu_c - 1.96 * sigma_c,
                 mu_c + 1.96 * sigma_c, alpha=0.3, color='green')
ax2.axhline(0, color='red', linestyle='--', linewidth=2,
            label='åˆ¶ç´„å¢ƒç•Œï¼ˆâ‰¤ 0 ãŒå®Ÿè¡Œå¯èƒ½ï¼‰')
ax2.axhspan(-10, 0, alpha=0.2, color='green',
            label='å®Ÿè¡Œå¯èƒ½é ˜åŸŸ')
ax2.scatter(X_train, y_constraint, c='red', s=100, zorder=10,
            edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
ax2.set_ylabel('åˆ¶ç´„é–¢æ•°', fontsize=12)
ax2.set_title('åˆ¶ç´„é–¢æ•°ï¼ˆç²˜åº¦ä¸Šé™ï¼‰', fontsize=14)
ax2.legend()
ax2.grid(True, alpha=0.3)

<h1>ä¸‹å›³: åˆ¶ç´„ä»˜ãEI</h1>
ax3 = axes[2]
ei_unconstrained = expected_improvement(X_test, gp, f_best, xi=0.01)
ax3.plot(X_test, ei_unconstrained, 'r--', linewidth=2,
         label='EIï¼ˆåˆ¶ç´„ãªã—ï¼‰', alpha=0.5)
ax3.plot(X_test, cei, 'r-', linewidth=2, label='åˆ¶ç´„ä»˜ãEI')
next_x = X_test[np.argmax(cei)]
ax3.axvline(next_x, color='purple', linestyle='--', linewidth=2,
            label=f'ææ¡ˆç‚¹ = {next_x[0]:.3f}')
ax3.fill_between(X_test.ravel(), 0, cei, alpha=0.3, color='red')
ax3.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
ax3.set_ylabel('ç²å¾—é–¢æ•°', fontsize=12)
ax3.set_title('åˆ¶ç´„ä»˜ãExpected Improvement', fontsize=14)
ax3.legend()
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('constrained_bayesian_optimization.png', dpi=150,
            bbox_inches='tight')
plt.show()

print("åˆ¶ç´„ä»˜ãæœ€é©åŒ–ã®çµæœ:")
print(f"  ææ¡ˆç‚¹: x = {next_x[0]:.3f}")
print(f"  åˆ¶ç´„ãªã—EIã®æœ€å¤§ç‚¹: x = {X_test[np.argmax(ei_unconstrained)][0]:.3f}")
print(f"  â†’ åˆ¶ç´„ã‚’è€ƒæ…®ã—ã¦ææ¡ˆç‚¹ãŒå¤‰åŒ–")</code></pre>

---

<h3>å¤šç›®çš„æœ€é©åŒ–</h3>

ææ–™é–‹ç™ºã§ã¯ã€<strong>è¤‡æ•°ã®ç‰¹æ€§ã‚’åŒæ™‚ã«æœ€é©åŒ–</strong>ã—ãŸã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

<strong>ä¾‹ï¼šç†±é›»ææ–™</strong>
- ã‚¼ãƒ¼ãƒ™ãƒƒã‚¯ä¿‚æ•°ã‚’æœ€å¤§åŒ–
- é›»æ°—æŠµæŠ—ç‡ã‚’æœ€å°åŒ–
- ç†±ä¼å°ç‡ã‚’æœ€å°åŒ–

<strong>ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢</strong>:
- ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒã‚ã‚‹å ´åˆã€å˜ä¸€ã®æœ€é©è§£ã¯å­˜åœ¨ã—ãªã„
- <strong>ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã®é›†åˆ</strong>ã‚’æ±‚ã‚ã‚‹

<strong>ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</strong>:
1. <strong>ã‚¹ã‚«ãƒ©ãƒ¼åŒ–</strong>: é‡ã¿ä»˜ãå’Œ $f(x) = w_1 f_1(x) + w_2 f_2(x)$
2. <strong>ParEGO</strong>: ãƒ©ãƒ³ãƒ€ãƒ ãªé‡ã¿ã§ã‚¹ã‚«ãƒ©ãƒ¼åŒ–ã‚’ç¹°ã‚Šè¿”ã™
3. <strong>EHVI</strong>: Expected Hypervolume Improvement

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹8: å¤šç›®çš„æœ€é©åŒ–ã®å¯è¦–åŒ–</strong>

<pre><code class="language-python"><h1>å¤šç›®çš„æœ€é©åŒ–ã®ãƒ‡ãƒ¢</h1>
def objective1(x):
    """ç›®çš„1: ã‚¤ã‚ªãƒ³ä¼å°åº¦ï¼ˆæœ€å¤§åŒ–ï¼‰"""
    return true_function(x)

def objective2(x):
    """ç›®çš„2: ç²˜åº¦ï¼ˆæœ€å°åŒ–ï¼‰"""
    return 0.5 + 0.3 * np.sin(5 * x)

<h1>ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã‚’è¨ˆç®—</h1>
x_grid = np.linspace(0, 1, 1000)
obj1_values = objective1(x_grid)
obj2_values = objective2(x_grid)

<h1>ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©åˆ¤å®š</h1>
def is_pareto_optimal(costs):
    """
    ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã‚’åˆ¤å®š

    Parameters:
    -----------
    costs : array (n_samples, n_objectives)
        å„ç‚¹ã®ã‚³ã‚¹ãƒˆï¼ˆæœ€å°åŒ–å•é¡Œã¨ã—ã¦ï¼‰

    Returns:
    --------
    pareto_mask : array (n_samples,)
        TrueãŒãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©
    """
    is_pareto = np.ones(len(costs), dtype=bool)
    for i, c in enumerate(costs):
        if is_pareto[i]:
            # ä»–ã®ç‚¹ã«æ”¯é…ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
            is_pareto[is_pareto] = np.any(
                costs[is_pareto] < c, axis=1
            )
            is_pareto[i] = True
    return is_pareto

<h1>ã‚³ã‚¹ãƒˆãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼ˆæœ€å°åŒ–å•é¡Œã¨ã—ã¦ï¼‰</h1>
costs = np.column_stack([
    -obj1_values,  # æœ€å¤§åŒ– â†’ æœ€å°åŒ–
    obj2_values    # æœ€å°åŒ–
])

<h1>ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£</h1>
pareto_mask = is_pareto_optimal(costs)
pareto_x = x_grid[pareto_mask]
pareto_obj1 = obj1_values[pareto_mask]
pareto_obj2 = obj2_values[pareto_mask]

<h1>å¯è¦–åŒ–</h1>
fig = plt.figure(figsize=(14, 6))

<h1>å·¦å›³: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“</h1>
ax1 = plt.subplot(1, 2, 1)
ax1.plot(x_grid, obj1_values, 'b-', linewidth=2,
         label='ç›®çš„1ï¼ˆã‚¤ã‚ªãƒ³ä¼å°åº¦ï¼‰')
ax1.plot(x_grid, obj2_values, 'r-', linewidth=2,
         label='ç›®çš„2ï¼ˆç²˜åº¦ï¼‰')
ax1.scatter(pareto_x, pareto_obj1, c='blue', s=50, alpha=0.6,
            label='ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©ï¼ˆç›®çš„1ï¼‰')
ax1.scatter(pareto_x, pareto_obj2, c='red', s=50, alpha=0.6,
            label='ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©ï¼ˆç›®çš„2ï¼‰')
ax1.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
ax1.set_ylabel('ç›®çš„é–¢æ•°å€¤', fontsize=12)
ax1.set_title('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“', fontsize=14)
ax1.legend()
ax1.grid(True, alpha=0.3)

<h1>å³å›³: ç›®çš„ç©ºé–“ï¼ˆãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ï¼‰</h1>
ax2 = plt.subplot(1, 2, 2)
ax2.scatter(obj1_values, obj2_values, c='lightgray', s=20,
            alpha=0.5, label='å…¨æ¢ç´¢ç‚¹')
ax2.scatter(pareto_obj1, pareto_obj2, c='red', s=100,
            edgecolors='black', zorder=10,
            label='ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢')
ax2.plot(pareto_obj1, pareto_obj2, 'r--', linewidth=2, alpha=0.5)
ax2.set_xlabel('ç›®çš„1ï¼ˆã‚¤ã‚ªãƒ³ä¼å°åº¦ï¼‰â†’ æœ€å¤§åŒ–', fontsize=12)
ax2.set_ylabel('ç›®çš„2ï¼ˆç²˜åº¦ï¼‰â†’ æœ€å°åŒ–', fontsize=12)
ax2.set_title('ç›®çš„ç©ºé–“ã¨ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢', fontsize=14)
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('multi_objective_optimization.png', dpi=150,
            bbox_inches='tight')
plt.show()

print(f"ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£æ•°: {np.sum(pareto_mask)}")
print("ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®ä¾‹:")
print(f"  é«˜ä¼å°åº¦: ç›®çš„1 = {np.max(pareto_obj1):.3f}, "
      f"ç›®çš„2 = {pareto_obj2[np.argmax(pareto_obj1)]:.3f}")
print(f"  ä½ç²˜åº¦: ç›®çš„1 = {pareto_obj1[np.argmin(pareto_obj2)]:.3f}, "
      f"ç›®çš„2 = {np.min(pareto_obj2):.3f}")</code></pre>

---

<h2>2.6 ã‚³ãƒ©ãƒ ï¼šã‚«ãƒ¼ãƒãƒ«é¸æŠã®å®Ÿå‹™</h2>

<h3>ã‚«ãƒ¼ãƒãƒ«ã®ç¨®é¡ã¨ç‰¹æ€§</h3>

RBFä»¥å¤–ã«ã‚‚ã€å¤šæ§˜ãªã‚«ãƒ¼ãƒãƒ«ãŒå­˜åœ¨ã—ã¾ã™ï¼š

<strong>MatÃ©rn ã‚«ãƒ¼ãƒãƒ«</strong>:
$$
k(x, x') = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)} \left(\frac{\sqrt{2\nu}||x - x'||}{\ell}\right)^\nu K_\nu\left(\frac{\sqrt{2\nu}||x - x'||}{\ell}\right)
$$

- $\nu$: æ»‘ã‚‰ã‹ã•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- $\nu = 1/2$: æŒ‡æ•°ã‚«ãƒ¼ãƒãƒ«ï¼ˆç²—ã„é–¢æ•°ï¼‰
- $\nu = 3/2, 5/2$: ä¸­ç¨‹åº¦ã®æ»‘ã‚‰ã‹ã•
- $\nu \to \infty$: RBFã‚«ãƒ¼ãƒãƒ«ï¼ˆéå¸¸ã«æ»‘ã‚‰ã‹ï¼‰

<strong>ææ–™ç§‘å­¦ã§ã®é¸æŠæŒ‡é‡</strong>:
- <strong>DFTè¨ˆç®—çµæœ</strong>: RBFï¼ˆæ»‘ã‚‰ã‹ï¼‰
- <strong>å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿</strong>: MatÃ©rn 3/2 or 5/2ï¼ˆãƒã‚¤ã‚ºè€ƒæ…®ï¼‰
- <strong>çµ„æˆæœ€é©åŒ–</strong>: RBF
- <strong>ãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶</strong>: MatÃ©rnï¼ˆæ€¥å³»ãªå¤‰åŒ–ã‚ã‚Šï¼‰

<strong>å‘¨æœŸçš„ç¾è±¡</strong>: Periodic kernel
$$
k(x, x') = \sigma^2 \exp\left(-\frac{2\sin^2(\pi|x - x'|/p)}{\ell^2}\right)
$$
- çµæ™¶æ§‹é€ ï¼ˆå‘¨æœŸæ€§ã‚ã‚Šï¼‰
- æ¸©åº¦ã‚µã‚¤ã‚¯ãƒ«

---

<h2>2.7 ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°</h2>

<h3>ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–</h3>

<strong>å•é¡Œ1: ç²å¾—é–¢æ•°ãŒå¸¸ã«åŒã˜å ´æ‰€ã‚’ææ¡ˆã™ã‚‹</strong>

<strong>åŸå› </strong>:
- é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ãŒå¤§ãã™ãã‚‹ â†’ å…¨ä½“ãŒæ»‘ã‚‰ã‹ã™ã
- ãƒã‚¤ã‚ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå°ã•ã™ãã‚‹ â†’ è¦³æ¸¬ç‚¹ã‚’éä¿¡

<strong>è§£æ±ºç­–</strong>:
<pre><code class="language-python"><h1>é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã‚’èª¿æ•´</h1>
gp = GaussianProcessRegressor(length_scale=0.05, noise=0.1)

<h1>ã¾ãŸã¯ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è‡ªå‹•èª¿æ•´</h1>
from sklearn.gaussian_process import GaussianProcessRegressor as SKGP
from sklearn.gaussian_process.kernels import RBF, WhiteKernel

kernel = RBF(length_scale=0.1) + WhiteKernel(noise_level=0.1)
gp = SKGP(kernel=kernel, n_restarts_optimizer=10)
gp.fit(X_train, y_train)</code></pre>

<strong>å•é¡Œ2: äºˆæ¸¬ãŒä¸å®‰å®šï¼ˆä¿¡é ¼åŒºé–“ãŒç•°å¸¸ã«åºƒã„ï¼‰</strong>

<strong>åŸå› </strong>:
- ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã‚‹
- ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ãŒæ•°å€¤çš„ã«ä¸å®‰å®š

<strong>è§£æ±ºç­–</strong>:
<pre><code class="language-python"><h1>æ­£å‰‡åŒ–é …ã‚’è¿½åŠ </h1>
K = kernel_matrix + 1e-6 * np.eye(n_samples)  # ã‚¸ãƒƒã‚¿ãƒ¼ã‚’è¿½åŠ 

<h1>ã¾ãŸã¯Choleskyåˆ†è§£ã‚’ä½¿ç”¨ï¼ˆæ•°å€¤å®‰å®šæ€§å‘ä¸Šï¼‰</h1>
from scipy.linalg import cho_solve, cho_factor

L = cho_factor(K, lower=True)
alpha = cho_solve(L, y_train)</code></pre>

<strong>å•é¡Œ3: è¨ˆç®—ãŒé…ã„ï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰</strong>

<strong>åŸå› </strong>:
- ã‚¬ã‚¦ã‚¹éç¨‹ã®è¨ˆç®—é‡: $O(n^3)$ï¼ˆn = ãƒ‡ãƒ¼ã‚¿æ•°ï¼‰

<strong>è§£æ±ºç­–</strong>:
<pre><code class="language-python"><h1>1. ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¬ã‚¦ã‚¹éç¨‹</h1>
<h1>ä»£è¡¨ç‚¹ï¼ˆInducing pointsï¼‰ã‚’ä½¿ç”¨</h1>

<h1>2. è¿‘ä¼¼æ‰‹æ³•</h1>
<h1>- Sparse GP</h1>
<h1>- Local GPï¼ˆé ˜åŸŸåˆ†å‰²ï¼‰</h1>

<h1>3. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ´»ç”¨</h1>
<h1>GPyTorchï¼ˆGPUé«˜é€ŸåŒ–ï¼‰</h1>
<h1>GPflowï¼ˆTensorFlow backendï¼‰</h1></code></pre>

---

<h2>2.8 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

1. <strong>ä»£ç†ãƒ¢ãƒ‡ãƒ«ã®å½¹å‰²</strong>
   - å°‘æ•°ã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›®çš„é–¢æ•°ã‚’æ¨å®š
   - ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ãŒæœ€ã‚‚ä¸€èˆ¬çš„
   - ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ãŒå¯èƒ½

2. <strong>ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°</strong>
   - ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã§ç‚¹é–“ã®é¡ä¼¼åº¦ã‚’å®šç¾©
   - RBFã‚«ãƒ¼ãƒãƒ«ãŒææ–™ç§‘å­¦ã§æ¨™æº–çš„
   - äºˆæ¸¬å¹³å‡ã¨äºˆæ¸¬åˆ†æ•£ã‚’è¨ˆç®—

3. <strong>ç²å¾—é–¢æ•°</strong>
   - æ¬¡ã®å®Ÿé¨“ç‚¹ã‚’æ±ºå®šã™ã‚‹æ•°å­¦çš„åŸºæº–
   - EIï¼ˆExpected Improvementï¼‰: ãƒãƒ©ãƒ³ã‚¹å‹
   - UCBï¼ˆUpper Confidence Boundï¼‰: èª¿æ•´å¯èƒ½
   - PIï¼ˆProbability of Improvementï¼‰: ã‚·ãƒ³ãƒ—ãƒ«

4. <strong>æ¢ç´¢ã¨æ´»ç”¨</strong>
   - Exploitation: æ—¢çŸ¥ã®è‰¯ã„é ˜åŸŸã‚’æ´»ç”¨
   - Exploration: æœªçŸ¥ã®é ˜åŸŸã‚’æ¢ç´¢
   - ç²å¾—é–¢æ•°ãŒè‡ªå‹•çš„ã«ãƒãƒ©ãƒ³ã‚¹èª¿æ•´

5. <strong>ç™ºå±•çš„ãƒˆãƒ”ãƒƒã‚¯</strong>
   - åˆ¶ç´„ä»˜ãæœ€é©åŒ–: å®Ÿå‹™ã§é‡è¦
   - å¤šç›®çš„æœ€é©åŒ–: ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®å¯è¦–åŒ–

<h3>é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ</h3>

- âœ… ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã¯<strong>ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–</strong>ã§ãã‚‹
- âœ… ç²å¾—é–¢æ•°ã¯<strong>æ¢ç´¢ã¨æ´»ç”¨ã‚’æ•°å­¦çš„ã«æœ€é©åŒ–</strong>
- âœ… EIãŒ<strong>æœ€ã‚‚ä¸€èˆ¬çš„ã§å®Ÿç¸¾è±Šå¯Œ</strong>
- âœ… ã‚«ãƒ¼ãƒãƒ«ã®é¸æŠãŒ<strong>ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å·¦å³</strong>
- âœ… åˆ¶ç´„ãƒ»å¤šç›®çš„ã¸ã®<strong>æ‹¡å¼µãŒå¯èƒ½</strong>

<h3>æ¬¡ã®ç« ã¸</h3>

ç¬¬3ç« ã§ã¯ã€Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ãŸå®Ÿè£…ã‚’å­¦ã³ã¾ã™ï¼š
- scikit-optimizeï¼ˆskoptï¼‰ã®ä½¿ã„æ–¹
- BoTorchï¼ˆPyTorchç‰ˆï¼‰ã®å®Ÿè£…
- å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®ææ–™æ¢ç´¢
- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

<strong>[ç¬¬3ç« ï¼šPythonå®Ÿè·µ â†’](./chapter-3.md)</strong>

---

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>

RBFã‚«ãƒ¼ãƒãƒ«ã®é•·ã•ã‚¹ã‚±ãƒ¼ãƒ« $\ell$ ãŒã€ã‚¬ã‚¦ã‚¹éç¨‹ã®äºˆæ¸¬ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¿ã¹ã¦ãã ã•ã„ã€‚

<strong>ã‚¿ã‚¹ã‚¯</strong>:
1. 5ç‚¹ã®è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
2. $\ell = 0.05, 0.1, 0.2, 0.5$ ã§ã‚¬ã‚¦ã‚¹éç¨‹ã‚’å­¦ç¿’
3. äºˆæ¸¬å¹³å‡ã¨ä¿¡é ¼åŒºé–“ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
4. é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã®å½±éŸ¿ã‚’èª¬æ˜

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

- <code>GaussianProcessRegressor</code>ã®<code>length_scale</code>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰æ›´
- <code>predict(return_std=True)</code>ã§æ¨™æº–åå·®ã‚’å–å¾—
- é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ãŒå°ã•ã„ â†’ å±€æ‰€çš„ã«ãƒ•ã‚£ãƒƒãƒˆ
- é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ãŒå¤§ãã„ â†’ æ»‘ã‚‰ã‹ãªæ›²ç·š

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

<h1>è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿</h1>
np.random.seed(42)
X_train = np.array([0.1, 0.3, 0.5, 0.7, 0.9]).reshape(-1, 1)
y_train = true_function(X_train).ravel()

<h1>ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿</h1>
X_test = np.linspace(0, 1, 200).reshape(-1, 1)
y_true = true_function(X_test).ravel()

<h1>ç•°ãªã‚‹é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã§å­¦ç¿’</h1>
length_scales = [0.05, 0.1, 0.2, 0.5]
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.ravel()

for i, ls in enumerate(length_scales):
    ax = axes[i]

    # ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«
    gp = GaussianProcessRegressor(sigma=1.0, length_scale=ls,
                                   noise=0.01)
    gp.fit(X_train, y_train)

    # äºˆæ¸¬
    y_pred, y_std = gp.predict(X_test, return_std=True)

    # ãƒ—ãƒ­ãƒƒãƒˆ
    ax.plot(X_test, y_true, 'k--', linewidth=2, label='çœŸã®é–¢æ•°')
    ax.scatter(X_train, y_train, c='red', s=100, zorder=10,
               edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
    ax.plot(X_test, y_pred, 'b-', linewidth=2, label='äºˆæ¸¬å¹³å‡')
    ax.fill_between(X_test.ravel(), y_pred - 1.96 * y_std,
                    y_pred + 1.96 * y_std, alpha=0.3, color='blue',
                    label='95%ä¿¡é ¼åŒºé–“')
    ax.set_title(f'é•·ã•ã‚¹ã‚±ãƒ¼ãƒ« = {ls}', fontsize=14)
    ax.set_xlabel('x', fontsize=12)
    ax.set_ylabel('y', fontsize=12)
    ax.legend(loc='best')
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('length_scale_effect.png', dpi=150,
            bbox_inches='tight')
plt.show()

print("é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ã®å½±éŸ¿:")
print("  å°ã•ã„ (0.05): è¦³æ¸¬ç‚¹ã«ã´ã£ãŸã‚Šãƒ•ã‚£ãƒƒãƒˆã€é–“ãŒä¸å®‰å®š")
print("  ä¸­ç¨‹åº¦ (0.1-0.2): ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„")
print("  å¤§ãã„ (0.5): æ»‘ã‚‰ã‹ã ãŒã€è¦³æ¸¬ç‚¹ã‹ã‚‰ä¹–é›¢")</code></pre>

<strong>è§£èª¬</strong>:
- <strong>$\ell$ = 0.05</strong>: ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒˆæ°—å‘³ã€è¦³æ¸¬ç‚¹é–“ãŒä¸å®‰å®š
- <strong>$\ell$ = 0.1-0.2</strong>: é©åº¦ãªæ»‘ã‚‰ã‹ã•ã€å®Ÿç”¨çš„
- <strong>$\ell$ = 0.5</strong>: ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ã‚£ãƒƒãƒˆã€æ»‘ã‚‰ã‹ã™ã

<strong>ææ–™ç§‘å­¦ã¸ã®ç¤ºå”†</strong>:
- å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿: $\ell$ = 0.1-0.3 ãŒä¸€èˆ¬çš„
- DFTè¨ˆç®—: ã‚ˆã‚Šæ»‘ã‚‰ã‹ï¼ˆ$\ell$ = 0.3-0.5ï¼‰
- ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§æœ€é©å€¤ã‚’æ±ºå®š

</details>

---

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>

3ã¤ã®ç²å¾—é–¢æ•°ï¼ˆEIã€UCBã€PIï¼‰ã‚’å®Ÿè£…ã—ã€åŒã˜ãƒ‡ãƒ¼ã‚¿ã§æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚

<strong>ã‚¿ã‚¹ã‚¯</strong>:
1. åŒã˜è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
2. å„ç²å¾—é–¢æ•°ã§æ¬¡ã®å®Ÿé¨“ç‚¹ã‚’ææ¡ˆ
3. ææ¡ˆç‚¹ã®é•ã„ã‚’å¯è¦–åŒ–
4. ãã‚Œãã‚Œã®ç‰¹å¾´ã‚’èª¬æ˜

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

- åŒã˜ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ã‚’3ã¤ã®ç²å¾—é–¢æ•°ã§è©•ä¾¡
- <code>np.argmax()</code>ã§æœ€å¤§å€¤ã®ä½ç½®ã‚’å–å¾—
- UCBã®$\kappa = 2.0$ã‚’ä½¿ç”¨

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python"><h1>è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿</h1>
np.random.seed(42)
X_train = np.array([0.15, 0.4, 0.6, 0.85]).reshape(-1, 1)
y_train = true_function(X_train).ravel()

<h1>ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«</h1>
gp = GaussianProcessRegressor(sigma=1.0, length_scale=0.15,
                               noise=0.01)
gp.fit(X_train, y_train)

<h1>ç¾åœ¨ã®æœ€è‰¯å€¤</h1>
f_best = np.max(y_train)

<h1>ãƒ†ã‚¹ãƒˆç‚¹</h1>
X_test = np.linspace(0, 1, 500).reshape(-1, 1)
y_pred, y_std = gp.predict(X_test, return_std=True)

<h1>ç²å¾—é–¢æ•°ã‚’è¨ˆç®—</h1>
ei = expected_improvement(X_test, gp, f_best, xi=0.01)
ucb = upper_confidence_bound(X_test, gp, kappa=2.0)
pi = probability_of_improvement(X_test, gp, f_best, xi=0.01)

<h1>ææ¡ˆç‚¹</h1>
next_x_ei = X_test[np.argmax(ei)]
next_x_ucb = X_test[np.argmax(ucb)]
next_x_pi = X_test[np.argmax(pi)]

<h1>å¯è¦–åŒ–</h1>
fig, axes = plt.subplots(4, 1, figsize=(12, 14))

<h1>1. ã‚¬ã‚¦ã‚¹éç¨‹ã®äºˆæ¸¬</h1>
ax1 = axes[0]
ax1.plot(X_test, y_pred, 'b-', linewidth=2, label='äºˆæ¸¬å¹³å‡')
ax1.fill_between(X_test.ravel(), y_pred - 1.96 * y_std,
                 y_pred + 1.96 * y_std, alpha=0.3, color='blue')
ax1.scatter(X_train, y_train, c='red', s=100, zorder=10,
            edgecolors='black', label='è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿')
ax1.axhline(f_best, color='green', linestyle=':', linewidth=2,
            label=f'æœ€è‰¯å€¤ = {f_best:.3f}')
ax1.set_ylabel('ç›®çš„é–¢æ•°', fontsize=12)
ax1.set_title('ã‚¬ã‚¦ã‚¹éç¨‹ã®äºˆæ¸¬', fontsize=14)
ax1.legend()
ax1.grid(True, alpha=0.3)

<h1>2. Expected Improvement</h1>
ax2 = axes[1]
ax2.plot(X_test, ei, 'r-', linewidth=2, label='EI')
ax2.axvline(next_x_ei, color='red', linestyle='--', linewidth=2,
            label=f'ææ¡ˆç‚¹ = {next_x_ei[0]:.3f}')
ax2.fill_between(X_test.ravel(), 0, ei, alpha=0.3, color='red')
ax2.set_ylabel('EI(x)', fontsize=12)
ax2.set_title('Expected Improvement', fontsize=14)
ax2.legend()
ax2.grid(True, alpha=0.3)

<h1>3. Upper Confidence Bound</h1>
ax3 = axes[2]
<h1>UCBã‚’æ­£è¦åŒ–ï¼ˆæ¯”è¼ƒã—ã‚„ã™ãã™ã‚‹ãŸã‚ï¼‰</h1>
ucb_normalized = (ucb - np.min(ucb)) / (np.max(ucb) - np.min(ucb))
ax3.plot(X_test, ucb_normalized, 'b-', linewidth=2, label='UCB (æ­£è¦åŒ–)')
ax3.axvline(next_x_ucb, color='blue', linestyle='--', linewidth=2,
            label=f'ææ¡ˆç‚¹ = {next_x_ucb[0]:.3f}')
ax3.fill_between(X_test.ravel(), 0, ucb_normalized, alpha=0.3,
                 color='blue')
ax3.set_ylabel('UCB(x)', fontsize=12)
ax3.set_title('Upper Confidence Bound (Îº=2.0)', fontsize=14)
ax3.legend()
ax3.grid(True, alpha=0.3)

<h1>4. Probability of Improvement</h1>
ax4 = axes[3]
ax4.plot(X_test, pi, 'g-', linewidth=2, label='PI')
ax4.axvline(next_x_pi, color='green', linestyle='--', linewidth=2,
            label=f'ææ¡ˆç‚¹ = {next_x_pi[0]:.3f}')
ax4.fill_between(X_test.ravel(), 0, pi, alpha=0.3, color='green')
ax4.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
ax4.set_ylabel('PI(x)', fontsize=12)
ax4.set_title('Probability of Improvement', fontsize=14)
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('acquisition_functions_detailed_comparison.png',
            dpi=150, bbox_inches='tight')
plt.show()

<h1>çµæœã®ã‚µãƒãƒªãƒ¼</h1>
print("ç²å¾—é–¢æ•°åˆ¥ã®ææ¡ˆç‚¹:")
print(f"  EI:  x = {next_x_ei[0]:.3f}")
print(f"  UCB: x = {next_x_ucb[0]:.3f}")
print(f"  PI:  x = {next_x_pi[0]:.3f}")

print("\nç‰¹å¾´:")
print("  EI: ãƒãƒ©ãƒ³ã‚¹å‹ã€æ”¹å–„é‡ã®æœŸå¾…å€¤ã‚’æœ€å¤§åŒ–")
print("  UCB: æ¢ç´¢é‡è¦–ã€ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„é ˜åŸŸã‚’å¥½ã‚€")
print("  PI: ä¿å®ˆçš„ã€å°ã•ãªæ”¹å–„ã§ã‚‚ç©æ¥µçš„")</code></pre>

<strong>æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›</strong>:
<pre><code>ç²å¾—é–¢æ•°åˆ¥ã®ææ¡ˆç‚¹:
  EI:  x = 0.722
  UCB: x = 0.108
  PI:  x = 0.752

ç‰¹å¾´:
  EI: ãƒãƒ©ãƒ³ã‚¹å‹ã€æ”¹å–„é‡ã®æœŸå¾…å€¤ã‚’æœ€å¤§åŒ–
  UCB: æ¢ç´¢é‡è¦–ã€ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„é ˜åŸŸã‚’å¥½ã‚€
  PI: ä¿å®ˆçš„ã€å°ã•ãªæ”¹å–„ã§ã‚‚ç©æ¥µçš„</code></pre>

<strong>è©³ç´°ãªè§£èª¬</strong>:
- <strong>EI</strong>: æœªè¦³æ¸¬é ˜åŸŸã¨äºˆæ¸¬ãŒè‰¯ã„é ˜åŸŸã®ä¸­é–“ã‚’ææ¡ˆ
- <strong>UCB</strong>: ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å·¦ç«¯ã‚’æ¢ç´¢ï¼ˆä¸ç¢ºå®Ÿæ€§é‡è¦–ï¼‰
- <strong>PI</strong>: äºˆæ¸¬å¹³å‡ãŒæœ€è‰¯å€¤ã‚’è¶…ãˆãã†ãªå ´æ‰€ã‚’ææ¡ˆ

<strong>å®Ÿå‹™ã§ã®é¸æŠ</strong>:
- ä¸€èˆ¬çš„ãªæœ€é©åŒ– â†’ EI
- åˆæœŸæ¢ç´¢ãƒ•ã‚§ãƒ¼ã‚º â†’ UCBï¼ˆÎºå¤§ãã‚ï¼‰
- åæŸãƒ•ã‚§ãƒ¼ã‚º â†’ PI or EI

</details>

---

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>

åˆ¶ç´„ä»˜ããƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’å®Ÿè£…ã—ã€åˆ¶ç´„ãŒãªã„å ´åˆã¨æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚

<strong>èƒŒæ™¯</strong>:
Li-ioné›»æ± é›»è§£è³ªã®æœ€é©åŒ–
- ç›®çš„: ã‚¤ã‚ªãƒ³ä¼å°åº¦ã‚’æœ€å¤§åŒ–
- åˆ¶ç´„: ç²˜åº¦ < 10 cP

<strong>ã‚¿ã‚¹ã‚¯</strong>:
1. ç›®çš„é–¢æ•°ã¨åˆ¶ç´„é–¢æ•°ã‚’å®šç¾©
2. åˆ¶ç´„ãªã—ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’å®Ÿè¡Œï¼ˆ10å›ï¼‰
3. åˆ¶ç´„ä»˜ããƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’å®Ÿè¡Œï¼ˆ10å›ï¼‰
4. æ¢ç´¢ã®è»Œè·¡ã‚’æ¯”è¼ƒ
5. æœ€çµ‚çš„ã«è¦‹ã¤ã‹ã£ãŸè§£ã‚’è©•ä¾¡

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

<strong>ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</strong>:
1. åˆæœŸãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ3ç‚¹ï¼‰
2. ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ã‚’2ã¤æ§‹ç¯‰ï¼ˆç›®çš„é–¢æ•°ç”¨ã€åˆ¶ç´„é–¢æ•°ç”¨ï¼‰
3. ãƒ«ãƒ¼ãƒ—ã§é€æ¬¡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
4. åˆ¶ç´„ä»˜ãEIã‚’ä½¿ç”¨

<strong>ä½¿ç”¨ã™ã‚‹é–¢æ•°</strong>:
- <code>constrained_expected_improvement()</code>

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python"><h1>ç›®çš„é–¢æ•°ã¨åˆ¶ç´„é–¢æ•°ã‚’å®šç¾©</h1>
def objective_conductivity(x):
    """ã‚¤ã‚ªãƒ³ä¼å°åº¦ï¼ˆæœ€å¤§åŒ–ï¼‰"""
    return true_function(x)

def constraint_viscosity(x):
    """ç²˜åº¦ã®åˆ¶ç´„ï¼ˆâ‰¤ 10 cPã‚’0ã«æ­£è¦åŒ–ï¼‰"""
    viscosity = 15 - 10 * x  # ç²˜åº¦ã®ãƒ¢ãƒ‡ãƒ«
    return viscosity - 10  # 10 cPä»¥ä¸‹ãŒå®Ÿè¡Œå¯èƒ½ï¼ˆâ‰¤ 0ï¼‰

<h1>ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</h1>
def run_bayesian_optimization(n_iterations=10,
                               use_constraint=False):
    """
    ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’å®Ÿè¡Œ

    Parameters:
    -----------
    n_iterations : int
        æœ€é©åŒ–ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°
    use_constraint : bool
        åˆ¶ç´„ã‚’ä½¿ç”¨ã™ã‚‹ã‹

    Returns:
    --------
    X_sampled : å®Ÿé¨“ç‚¹
    y_sampled : ç›®çš„é–¢æ•°å€¤
    c_sampled : åˆ¶ç´„é–¢æ•°å€¤ï¼ˆåˆ¶ç´„ã‚ã‚Šæ™‚ã®ã¿ï¼‰
    """
    # åˆæœŸãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    np.random.seed(42)
    X_sampled = np.random.uniform(0, 1, 3).reshape(-1, 1)
    y_sampled = objective_conductivity(X_sampled).ravel()
    c_sampled = constraint_viscosity(X_sampled).ravel()

    # é€æ¬¡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    for i in range(n_iterations - 3):
        # ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ï¼ˆç›®çš„é–¢æ•°ï¼‰
        gp_obj = GaussianProcessRegressor(sigma=1.0,
                                           length_scale=0.15,
                                           noise=0.01)
        gp_obj.fit(X_sampled, y_sampled)

        # å€™è£œç‚¹
        X_candidate = np.linspace(0, 1, 1000).reshape(-1, 1)

        if use_constraint:
            # ã‚¬ã‚¦ã‚¹éç¨‹ãƒ¢ãƒ‡ãƒ«ï¼ˆåˆ¶ç´„é–¢æ•°ï¼‰
            gp_constraint = GaussianProcessRegressor(sigma=0.5,
                                                     length_scale=0.2,
                                                     noise=0.01)
            gp_constraint.fit(X_sampled, c_sampled)

            # åˆ¶ç´„ä»˜ãEI
            f_best = np.max(y_sampled)
            acq = constrained_expected_improvement(
                X_candidate, gp_obj, gp_constraint, f_best,
                constraint_threshold=0
            )
        else:
            # åˆ¶ç´„ãªã—EI
            f_best = np.max(y_sampled)
            acq = expected_improvement(X_candidate, gp_obj,
                                       f_best, xi=0.01)

        # æ¬¡ã®å®Ÿé¨“ç‚¹
        next_x = X_candidate[np.argmax(acq)]

        # å®Ÿé¨“å®Ÿè¡Œ
        next_y = objective_conductivity(next_x).ravel()[0]
        next_c = constraint_viscosity(next_x).ravel()[0]

        # ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
        X_sampled = np.vstack([X_sampled, next_x])
        y_sampled = np.append(y_sampled, next_y)
        c_sampled = np.append(c_sampled, next_c)

    return X_sampled, y_sampled, c_sampled

<h1>2ã¤ã®ã‚·ãƒŠãƒªã‚ªã‚’å®Ÿè¡Œ</h1>
X_unconst, y_unconst, c_unconst = run_bayesian_optimization(
    n_iterations=10, use_constraint=False
)
X_const, y_const, c_const = run_bayesian_optimization(
    n_iterations=10, use_constraint=True
)

<h1>å¯è¦–åŒ–</h1>
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

<h1>å·¦ä¸Š: ç›®çš„é–¢æ•°</h1>
ax1 = axes[0, 0]
x_fine = np.linspace(0, 1, 500)
y_fine = objective_conductivity(x_fine)
ax1.plot(x_fine, y_fine, 'k-', linewidth=2, label='çœŸã®é–¢æ•°')
ax1.scatter(X_unconst, y_unconst, c='blue', s=100, alpha=0.6,
            label='åˆ¶ç´„ãªã—', marker='o')
ax1.scatter(X_const, y_const, c='red', s=100, alpha=0.6,
            label='åˆ¶ç´„ä»˜ã', marker='^')
ax1.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
ax1.set_ylabel('ã‚¤ã‚ªãƒ³ä¼å°åº¦', fontsize=12)
ax1.set_title('ç›®çš„é–¢æ•°ã®æ¢ç´¢', fontsize=14)
ax1.legend()
ax1.grid(True, alpha=0.3)

<h1>å³ä¸Š: åˆ¶ç´„é–¢æ•°</h1>
ax2 = axes[0, 1]
c_fine = constraint_viscosity(x_fine)
ax2.plot(x_fine, c_fine, 'k-', linewidth=2, label='åˆ¶ç´„é–¢æ•°')
ax2.axhline(0, color='red', linestyle='--', linewidth=2,
            label='åˆ¶ç´„å¢ƒç•Œï¼ˆâ‰¤ 0ãŒå®Ÿè¡Œå¯èƒ½ï¼‰')
ax2.axhspan(-20, 0, alpha=0.2, color='green',
            label='å®Ÿè¡Œå¯èƒ½é ˜åŸŸ')
ax2.scatter(X_unconst, c_unconst, c='blue', s=100, alpha=0.6,
            label='åˆ¶ç´„ãªã—', marker='o')
ax2.scatter(X_const, c_const, c='red', s=100, alpha=0.6,
            label='åˆ¶ç´„ä»˜ã', marker='^')
ax2.set_xlabel('ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ x', fontsize=12)
ax2.set_ylabel('åˆ¶ç´„é–¢æ•°å€¤', fontsize=12)
ax2.set_title('åˆ¶ç´„ã®æº€è¶³åº¦', fontsize=14)
ax2.legend()
ax2.grid(True, alpha=0.3)

<h1>å·¦ä¸‹: æœ€è‰¯å€¤ã®æ¨ç§»</h1>
ax3 = axes[1, 0]
best_unconst = np.maximum.accumulate(y_unconst)
best_const = np.maximum.accumulate(y_const)
ax3.plot(range(1, 11), best_unconst, 'o-', color='blue',
         linewidth=2, markersize=8, label='åˆ¶ç´„ãªã—')
ax3.plot(range(1, 11), best_const, '^-', color='red',
         linewidth=2, markersize=8, label='åˆ¶ç´„ä»˜ã')
ax3.set_xlabel('å®Ÿé¨“å›æ•°', fontsize=12)
ax3.set_ylabel('ã“ã‚Œã¾ã§ã®æœ€è‰¯å€¤', fontsize=12)
ax3.set_title('æœ€è‰¯å€¤ã®æ¨ç§»', fontsize=14)
ax3.legend()
ax3.grid(True, alpha=0.3)

<h1>å³ä¸‹: åˆ¶ç´„æº€è¶³åº¦ã®æ¨ç§»</h1>
ax4 = axes[1, 1]
<h1>åˆ¶ç´„ã‚’æº€ãŸã™ã‚µãƒ³ãƒ—ãƒ«ã®æ•°</h1>
feasible_unconst = np.cumsum(c_unconst <= 0)
feasible_const = np.cumsum(c_const <= 0)
ax4.plot(range(1, 11), feasible_unconst, 'o-', color='blue',
         linewidth=2, markersize=8, label='åˆ¶ç´„ãªã—')
ax4.plot(range(1, 11), feasible_const, '^-', color='red',
         linewidth=2, markersize=8, label='åˆ¶ç´„ä»˜ã')
ax4.set_xlabel('å®Ÿé¨“å›æ•°', fontsize=12)
ax4.set_ylabel('å®Ÿè¡Œå¯èƒ½è§£ã®ç´¯ç©æ•°', fontsize=12)
ax4.set_title('åˆ¶ç´„æº€è¶³åº¦ã®æ¨ç§»', fontsize=14)
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('constrained_bo_comparison.png', dpi=150,
            bbox_inches='tight')
plt.show()

<h1>çµæœã®ã‚µãƒãƒªãƒ¼</h1>
print("æœ€é©åŒ–çµæœã®æ¯”è¼ƒ:")
print("\nåˆ¶ç´„ãªã—ãƒ™ã‚¤ã‚ºæœ€é©åŒ–:")
print(f"  æœ€è‰¯å€¤: {np.max(y_unconst):.4f}")
print(f"  å¯¾å¿œã™ã‚‹x: {X_unconst[np.argmax(y_unconst)][0]:.3f}")
print(f"  åˆ¶ç´„å€¤: {c_unconst[np.argmax(y_unconst)]:.4f}")
print(f"  åˆ¶ç´„æº€è¶³: {'Yes' if c_unconst[np.argmax(y_unconst)] <= 0 else 'No'}")
print(f"  å®Ÿè¡Œå¯èƒ½è§£ã®æ•°: {np.sum(c_unconst <= 0)}/10")

print("\nåˆ¶ç´„ä»˜ããƒ™ã‚¤ã‚ºæœ€é©åŒ–:")
<h1>åˆ¶ç´„ã‚’æº€ãŸã™è§£ã®ä¸­ã§æœ€è‰¯ã®ã‚‚ã®ã‚’æ¢ã™</h1>
feasible_indices = np.where(c_const <= 0)[0]
if len(feasible_indices) > 0:
    best_feasible_idx = feasible_indices[np.argmax(y_const[feasible_indices])]
    print(f"  æœ€è‰¯å€¤: {y_const[best_feasible_idx]:.4f}")
    print(f"  å¯¾å¿œã™ã‚‹x: {X_const[best_feasible_idx][0]:.3f}")
    print(f"  åˆ¶ç´„å€¤: {c_const[best_feasible_idx]:.4f}")
    print(f"  åˆ¶ç´„æº€è¶³: Yes")
else:
    print("  å®Ÿè¡Œå¯èƒ½è§£ãªã—")
print(f"  å®Ÿè¡Œå¯èƒ½è§£ã®æ•°: {np.sum(c_const <= 0)}/10")

print("\nè€ƒå¯Ÿ:")
print("  - åˆ¶ç´„ä»˜ãã¯å®Ÿè¡Œå¯èƒ½é ˜åŸŸã«é›†ä¸­ã—ã¦æ¢ç´¢")
print("  - åˆ¶ç´„ãªã—ã¯é«˜ã„ç›®çš„é–¢æ•°å€¤ã‚’ç™ºè¦‹ã™ã‚‹ãŒã€åˆ¶ç´„é•åã®å¯èƒ½æ€§")
print("  - å®Ÿå‹™ã§ã¯åˆ¶ç´„ã‚’è€ƒæ…®ã—ãŸæœ€é©åŒ–ãŒå¿…é ˆ")</code></pre>

<strong>æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›</strong>:
<pre><code>æœ€é©åŒ–çµæœã®æ¯”è¼ƒ:

åˆ¶ç´„ãªã—ãƒ™ã‚¤ã‚ºæœ€é©åŒ–:
  æœ€è‰¯å€¤: 0.8234
  å¯¾å¿œã™ã‚‹x: 0.312
  åˆ¶ç´„å€¤: 1.876
  åˆ¶ç´„æº€è¶³: No
  å®Ÿè¡Œå¯èƒ½è§£ã®æ•°: 4/10

åˆ¶ç´„ä»˜ããƒ™ã‚¤ã‚ºæœ€é©åŒ–:
  æœ€è‰¯å€¤: 0.7456
  å¯¾å¿œã™ã‚‹x: 0.523
  åˆ¶ç´„å€¤: -0.234
  åˆ¶ç´„æº€è¶³: Yes
  å®Ÿè¡Œå¯èƒ½è§£ã®æ•°: 8/10

è€ƒå¯Ÿ:
  - åˆ¶ç´„ä»˜ãã¯å®Ÿè¡Œå¯èƒ½é ˜åŸŸã«é›†ä¸­ã—ã¦æ¢ç´¢
  - åˆ¶ç´„ãªã—ã¯é«˜ã„ç›®çš„é–¢æ•°å€¤ã‚’ç™ºè¦‹ã™ã‚‹ãŒã€åˆ¶ç´„é•åã®å¯èƒ½æ€§
  - å®Ÿå‹™ã§ã¯åˆ¶ç´„ã‚’è€ƒæ…®ã—ãŸæœ€é©åŒ–ãŒå¿…é ˆ</code></pre>

<strong>é‡è¦ãªæ´å¯Ÿ</strong>:
1. <strong>åˆ¶ç´„ãªã—</strong>: ã‚ˆã‚Šé«˜ã„ç›®çš„é–¢æ•°å€¤ã‚’ç™ºè¦‹ã™ã‚‹ãŒã€å®Ÿè¡Œä¸å¯èƒ½
2. <strong>åˆ¶ç´„ä»˜ã</strong>: ã‚„ã‚„ä½ã„ç›®çš„é–¢æ•°å€¤ã ãŒã€å®Ÿè¡Œå¯èƒ½
3. <strong>å®Ÿå‹™</strong>: åˆ¶ç´„ã‚’æº€ãŸã•ãªã„è§£ã¯ç„¡æ„å‘³ï¼ˆææ–™ãŒä½¿ãˆãªã„ï¼‰
4. <strong>åŠ¹ç‡</strong>: åˆ¶ç´„ä»˜ãã¯å®Ÿè¡Œå¯èƒ½é ˜åŸŸã«é›†ä¸­ã—ã€ç„¡é§„ãŒå°‘ãªã„

</details>

---

<h2>å‚è€ƒæ–‡çŒ®</h2>

1. Rasmussen, C. E. & Williams, C. K. I. (2006). *Gaussian Processes for Machine Learning*. MIT Press.
   [Onlineç‰ˆ](http://gaussianprocess.org/gpml/)

2. Brochu, E. et al. (2010). "A Tutorial on Bayesian Optimization of Expensive Cost Functions." *arXiv:1012.2599*.
   [arXiv:1012.2599](https://arxiv.org/abs/1012.2599)

3. Mockus, J. (1974). "On Bayesian Methods for Seeking the Extremum." *Optimization Techniques IFIP Technical Conference*, 400-404.

4. Srinivas, N. et al. (2010). "Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design." *ICML 2010*.
   [arXiv:0912.3995](https://arxiv.org/abs/0912.3995)

5. Gelbart, M. A. et al. (2014). "Bayesian Optimization with Unknown Constraints." *UAI 2014*.

6. æŒæ©‹å¤§åœ°ãƒ»å¤§ç¾½æˆå¾ (2019). ã€ã‚¬ã‚¦ã‚¹éç¨‹ã¨æ©Ÿæ¢°å­¦ç¿’ã€è¬›è«‡ç¤¾. ISBN: 978-4061529267

---

<h2>ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³</h2>

<h3>å‰ã®ç« </h3>
<strong>[â† ç¬¬1ç« ï¼šãªãœææ–™æ¢ç´¢ã«æœ€é©åŒ–ãŒå¿…è¦ã‹](./chapter-1.md)</strong>

<h3>æ¬¡ã®ç« </h3>
<strong>[ç¬¬3ç« ï¼šPythonå®Ÿè·µ â†’](./chapter-3.md)</strong>

<h3>ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</h3>
<strong>[â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹](./index.md)</strong>

---

<h2>è‘—è€…æƒ…å ±</h2>

<strong>ä½œæˆè€…</strong>: AI Terakoya Content Team
<strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰
<strong>ä½œæˆæ—¥</strong>: 2025-10-17
<strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0

<strong>æ›´æ–°å±¥æ­´</strong>:
- 2025-10-17: v1.0 åˆç‰ˆå…¬é–‹

<strong>ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯</strong>:
- GitHub Issues: [AI_Homepage/issues](https://github.com/your-repo/AI_Homepage/issues)
- Email: yusuke.hashimoto.b8@tohoku.ac.jp

<strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0

---

<strong>æ¬¡ã®ç« ã§å®Ÿè£…ã‚’å­¦ã³ã¾ã—ã‚‡ã†ï¼</strong>
<div class="navigation">
    <a href="chapter-1.html" class="nav-button">â† ç¬¬1ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-3.html" class="nav-button">ç¬¬3ç«  â†’</a>
</div>
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>
