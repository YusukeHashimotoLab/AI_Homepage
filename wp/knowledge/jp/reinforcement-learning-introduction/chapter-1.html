<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Á¨¨1Á´†: „Å™„ÅúÊùêÊñôÁßëÂ≠¶„Å´Âº∑ÂåñÂ≠¶Áøí„Åã - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Á¨¨1Á´†: „Å™„ÅúÊùêÊñôÁßëÂ≠¶„Å´Âº∑ÂåñÂ≠¶Áøí„Åã</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 20-25ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ÂàùÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 6ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 3Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">

<h1>Á¨¨1Á´†: „Å™„ÅúÊùêÊñôÁßëÂ≠¶„Å´Âº∑ÂåñÂ≠¶Áøí„Åã</h1>

<h2>Â≠¶ÁøíÁõÆÊ®ô</h2>

„Åì„ÅÆÁ´†„Åß„ÅØ„ÄÅ‰ª•‰∏ã„ÇíÁøíÂæó„Åó„Åæ„ÅôÔºö

- ÊùêÊñôÊé¢Á¥¢„Å´„Åä„Åë„ÇãÂæìÊù•ÊâãÊ≥ï„ÅÆÈôêÁïå„Å®Âº∑ÂåñÂ≠¶Áøí„ÅÆÂΩπÂâ≤
- „Éû„É´„Ç≥„ÉïÊ±∫ÂÆöÈÅéÁ®ãÔºàMDPÔºâ„ÅÆÂü∫Êú¨Ê¶ÇÂøµ
- QÂ≠¶Áøí„Å®Deep Q-NetworkÔºàDQNÔºâ„ÅÆ‰ªïÁµÑ„Åø
- Á∞°Âçò„Å™ÊùêÊñôÊé¢Á¥¢„Çø„Çπ„ÇØ„Å∏„ÅÆÂÆüË£Ö

---

<h2>1.1 ÊùêÊñôÊé¢Á¥¢„ÅÆË™≤È°å„Å®Âº∑ÂåñÂ≠¶Áøí„ÅÆÂΩπÂâ≤</h2>

<h3>ÂæìÊù•„ÅÆÊùêÊñôÊé¢Á¥¢„ÅÆÈôêÁïå</h3>

Êñ∞ÊùêÊñôÈñãÁô∫„Å´„ÅØ„ÄÅËÜ®Â§ß„Å™Êé¢Á¥¢Á©∫ÈñìÔºàÁµÑÊàê„ÄÅÊßãÈÄ†„ÄÅ„Éó„É≠„Çª„ÇπÊù°‰ª∂Ôºâ„Åå„ÅÇ„Çä„Åæ„ÅôÔºö

- <strong>ÁµÑÊàêÊé¢Á¥¢</strong>: ÂÖÉÁ¥†Âë®ÊúüË°®„Åã„Çâ3ÂÖÉÁ¥†„ÇíÈÅ∏„Å∂„Å†„Åë„Åß$\binom{118}{3} \approx 267,000$ÈÄö„Çä
- <strong>ÊßãÈÄ†Êé¢Á¥¢</strong>: ÁµêÊô∂ÊßãÈÄ†„Å†„Åë„Åß230Á®Æ„ÅÆÁ©∫ÈñìÁæ§
- <strong>„Éó„É≠„Çª„ÇπÊé¢Á¥¢</strong>: Ê∏©Â∫¶„ÉªÂúßÂäõ„ÉªÊôÇÈñì„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ„ÅØÁÑ°Èôê

ÂæìÊù•„ÅÆ<strong>Ë©¶Ë°åÈåØË™§„Ç¢„Éó„É≠„Éº„ÉÅ</strong>„Åß„ÅØÔºö
- Á†îÁ©∂ËÄÖ„ÅÆÁµåÈ®ì„Å®Âãò„Å´‰æùÂ≠ò
- Ë©ï‰æ°„Å´ÊôÇÈñì„Å®„Ç≥„Çπ„ÉàÔºà1ÊùêÊñô„ÅÇ„Åü„ÇäÊï∞ÈÄ±Èñì„ÄúÊï∞„É∂ÊúàÔºâ
- Â±ÄÊâÄÊúÄÈÅ©Ëß£„Å´Èô•„Çä„ÇÑ„Åô„ÅÑ

<div class="mermaid">graph LR
    A[Á†îÁ©∂ËÄÖ] -->|ÁµåÈ®ì„ÉªÂãò| B[ÊùêÊñôÂÄôË£úÈÅ∏Êäû]
    B -->|ÂêàÊàê„ÉªË©ï‰æ°| C[ÁµêÊûú]
    C -->|Ëß£Èáà| A

    style A fill:#ffcccc
    style B fill:#ffcccc
    style C fill:#ffcccc</div>

<strong>ÂïèÈ°åÁÇπ</strong>:
1. <strong>ÂäπÁéá„ÅåÊÇ™„ÅÑ</strong>: Âêå„Åò„Çà„ÅÜ„Å™ÊùêÊñô„ÇíÁπ∞„ÇäËøî„ÅóË©¶„Åô
2. <strong>Êé¢Á¥¢„ÅåÁã≠„ÅÑ</strong>: Á†îÁ©∂ËÄÖ„ÅÆÁü•Ë≠òÁØÑÂõ≤„Å´ÈôêÂÆö
3. <strong>ÂÜçÁèæÊÄß„Åå‰Ωé„ÅÑ</strong>: ÊöóÈªôÁü•„Å´‰æùÂ≠ò

<h3>Âº∑ÂåñÂ≠¶Áøí„Å´„Çà„ÇãËß£Ê±∫Á≠ñ</h3>

Âº∑ÂåñÂ≠¶Áøí„ÅØ„ÄÅ<strong>Áí∞Â¢É„Å®„ÅÆÁõ∏‰∫í‰ΩúÁî®„ÇíÈÄö„Åò„Å¶ÊúÄÈÅ©„Å™Ë°åÂãï„ÇíÂ≠¶Áøí</strong>„Åô„ÇãÊû†ÁµÑ„Åø„Åß„ÅôÔºö

<div class="mermaid">graph LR
    A[„Ç®„Éº„Ç∏„Çß„É≥„Éà<br/>RL Algorithm] -->|Ë°åÂãï<br/>ÊùêÊñôÂÄôË£ú| B[Áí∞Â¢É<br/>ÂÆüÈ®ì/Ë®àÁÆó]
    B -->|Â†±ÈÖ¨<br/>ÁâπÊÄßË©ï‰æ°| A
    B -->|Áä∂ÊÖã<br/>ÁèæÂú®„ÅÆÁü•Ë¶ã| A

    style A fill:#e1f5ff
    style B fill:#ffe1cc</div>

<strong>Âº∑ÂåñÂ≠¶Áøí„ÅÆÂà©ÁÇπ</strong>:
1. <strong>Ëá™ÂãïÊúÄÈÅ©Âåñ</strong>: Ë©¶Ë°åÈåØË™§„ÇíËá™ÂãïÂåñ„Åó„ÄÅÂäπÁéáÁöÑ„Å™Êé¢Á¥¢Êà¶Áï•„ÇíÂ≠¶Áøí
2. <strong>Êé¢Á¥¢„Å®Ê¥ªÁî®„ÅÆ„Éê„É©„É≥„Çπ</strong>: Êú™Áü•È†òÂüü„ÅÆÊé¢Á¥¢„Å®Êó¢Áü•„ÅÆËâØ„ÅÑÈ†òÂüü„ÅÆÊ¥ªÁî®„ÇíË™øÊï¥
3. <strong>ÈÄêÊ¨°ÁöÑÊîπÂñÑ</strong>: ÂêÑË©ï‰æ°ÁµêÊûú„Åã„ÇâÂ≠¶Áøí„Åó„ÄÅÊ¨°„ÅÆÈÅ∏Êäû„ÇíÊîπÂñÑ
4. <strong>„ÇØ„É≠„Éº„Ç∫„Éâ„É´„Éº„Éó</strong>: ÂÆüÈ®ìË£ÖÁΩÆ„Å®Áµ±Âêà„Åó24ÊôÇÈñìÁ®ºÂÉçÂèØËÉΩ

<h3>ÊùêÊñôÁßëÂ≠¶„Åß„ÅÆÊàêÂäü‰∫ã‰æã</h3>

<strong>‰æã1: Li-ionÈõªÊ±†ÈõªËß£Ê∂≤„ÅÆÊúÄÈÅ©Âåñ</strong> (MIT, 2022)
- <strong>Ë™≤È°å</strong>: 5ÊàêÂàÜ„ÅÆÈÖçÂêàÊØîÁéá„ÇíÊúÄÈÅ©ÂåñÔºàÊé¢Á¥¢Á©∫Èñì > $10^6$Ôºâ
- <strong>ÊâãÊ≥ï</strong>: DQN„ÅßÈÄêÊ¨°ÁöÑ„Å´ÈÖçÂêà„ÇíÈÅ∏Êäû
- <strong>ÁµêÊûú</strong>: ÂæìÊù•ÊâãÊ≥ï„ÅÆ5ÂÄç„ÅÆÈÄüÂ∫¶„ÅßÊúÄÈÅ©Ëß£Áô∫Ë¶ã„ÄÅ„Ç§„Ç™„É≥‰ºùÂ∞éÂ∫¶30%Âêë‰∏ä

<strong>‰æã2: ÊúâÊ©üÂ§™ÈôΩÈõªÊ±†„Éâ„Éä„ÉºÊùêÊñô</strong> (TorontoÂ§ß, 2021)
- <strong>Ë™≤È°å</strong>: ÂàÜÂ≠êÊßãÈÄ†„ÅÆÊúÄÈÅ©ÂåñÔºà10^23ÈÄö„Çä„ÅÆÂÄôË£úÔºâ
- <strong>ÊâãÊ≥ï</strong>: Actor-Critic„ÅßÂàÜÂ≠êÁîüÊàê„Å®Ë©ï‰æ°„ÇíÁµ±Âêà
- <strong>ÁµêÊûú</strong>: ÂÖâÈõªÂ§âÊèõÂäπÁéá15%„ÅÆÊñ∞ÊùêÊñô„Çí3„É∂Êúà„ÅßÁô∫Ë¶ãÔºàÂæìÊù•„ÅØ2Âπ¥Ôºâ

---

<h2>1.2 „Éû„É´„Ç≥„ÉïÊ±∫ÂÆöÈÅéÁ®ãÔºàMDPÔºâ„ÅÆÂü∫Á§é</h2>

<h3>MDP„Å®„ÅØ</h3>

Âº∑ÂåñÂ≠¶Áøí„ÅÆÊï∞Â≠¶ÁöÑÂü∫Áõ§„ÅØ„ÄÅ<strong>„Éû„É´„Ç≥„ÉïÊ±∫ÂÆöÈÅéÁ®ã</strong>ÔºàMarkov Decision Process, MDPÔºâ„Åß„Åô„ÄÇMDP„ÅØ‰ª•‰∏ã„ÅÆ5„Å§ÁµÑ„ÅßÂÆöÁæ©„Åï„Çå„Åæ„ÅôÔºö

$$
\text{MDP} = (S, A, P, R, \gamma)
$$

- $S$: <strong>Áä∂ÊÖãÁ©∫Èñì</strong>Ôºà‰æã: ÁèæÂú®Ë©¶„Åó„ÅüÊùêÊñô„ÅÆÁâπÊÄßÔºâ
- $A$: <strong>Ë°åÂãïÁ©∫Èñì</strong>Ôºà‰æã: Ê¨°„Å´Ë©¶„ÅôÊùêÊñôÂÄôË£úÔºâ
- $P(s'|s, a)$: <strong>Áä∂ÊÖãÈÅ∑ÁßªÁ¢∫Áéá</strong>ÔºàË°åÂãï$a$„ÇíÂèñ„Å£„Åü„Å®„Åç„Å´Áä∂ÊÖã$s$„Åã„Çâ$s'$„Å∏ÈÅ∑Áßª„Åô„ÇãÁ¢∫ÁéáÔºâ
- $R(s, a, s')$: <strong>Â†±ÈÖ¨Èñ¢Êï∞</strong>ÔºàÁä∂ÊÖãÈÅ∑Áßª„ÅßÂæó„Çâ„Çå„ÇãÂ†±ÈÖ¨Ôºâ
- $\gamma \in [0, 1)$: <strong>Ââ≤ÂºïÁéá</strong>ÔºàÂ∞ÜÊù•„ÅÆÂ†±ÈÖ¨„ÅÆÈáçË¶ÅÂ∫¶Ôºâ

<h3>ÊùêÊñôÊé¢Á¥¢„Å∏„ÅÆ„Éû„ÉÉ„Éî„É≥„Ç∞</h3>

| MDPË¶ÅÁ¥† | ÊùêÊñôÊé¢Á¥¢„Åß„ÅÆÊÑèÂë≥ | ÂÖ∑‰Ωì‰æã |
|---------|-----------------|--------|
| Áä∂ÊÖã $s$ | ÁèæÂú®„ÅÆÁü•Ë¶ãÔºà„Åì„Çå„Åæ„Åß„ÅÆË©ï‰æ°ÁµêÊûúÔºâ | "ÊùêÊñôA: „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó2.1eV„ÄÅÊùêÊñôB: 2.5eV" |
| Ë°åÂãï $a$ | Ê¨°„Å´Ë©¶„ÅôÊùêÊñô | "Ti-Ni-OÁµÑÊàê„ÅÆÊùêÊñôC" |
| Â†±ÈÖ¨ $r$ | ÊùêÊñôÁâπÊÄß„ÅÆË©ï‰æ°ÂÄ§ | "ÊùêÊñôC„ÅÆ„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó2.8eVÔºàÁõÆÊ®ô3.0eV„Å´Ëøë„ÅÑÔºâ" |
| ÊñπÁ≠ñ $\pi$ | ÊùêÊñôÈÅ∏ÊäûÊà¶Áï• | "„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó„ÅåÁõÆÊ®ô„Å´Ëøë„ÅÑÂÖÉÁ¥†ÁµÑÊàê„ÇíÂÑ™ÂÖà" |

<h3>„Éû„É´„Ç≥„ÉïÊÄß</h3>

MDP„ÅÆÈáçË¶Å„Å™‰ªÆÂÆö„ÅØ<strong>„Éû„É´„Ç≥„ÉïÊÄß</strong>„Åß„ÅôÔºö

$$
P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \dots) = P(s_{t+1}|s_t, a_t)
$$

„Å§„Åæ„Çä„ÄÅ<strong>Ê¨°„ÅÆÁä∂ÊÖã„ÅØÁèæÂú®„ÅÆÁä∂ÊÖã„Å®Ë°åÂãï„ÅÆ„Åø„Å´‰æùÂ≠ò„Åó„ÄÅÈÅéÂéª„ÅÆÂ±•Ê≠¥„ÅØ‰∏çË¶Å</strong>„Åß„Åô„ÄÇ

ÊùêÊñôÊé¢Á¥¢„Åß„ÅØ„ÄÅÁèæÂú®„ÅÆË©ï‰æ°ÁµêÊûúÔºàÁä∂ÊÖãÔºâ„Å´Âü∫„Å•„ÅÑ„Å¶Ê¨°„ÅÆÊùêÊñôÔºàË°åÂãïÔºâ„ÇíÈÅ∏„Åπ„Å∞„ÄÅÈÅéÂéª„ÅÆÂÖ®Â±•Ê≠¥„ÇíË¶ö„Åà„ÇãÂøÖË¶Å„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ

<h3>ÊñπÁ≠ñ„Å®‰æ°ÂÄ§Èñ¢Êï∞</h3>

<strong>ÊñπÁ≠ñ</strong> $\pi(a|s)$: Áä∂ÊÖã$s$„ÅßË°åÂãï$a$„ÇíÈÅ∏„Å∂Á¢∫Áéá

<strong>Áä∂ÊÖã‰æ°ÂÄ§Èñ¢Êï∞</strong> $V^\pi(s)$: Áä∂ÊÖã$s$„Åã„ÇâÊñπÁ≠ñ$\pi$„Å´Âæì„Å£„Å¶Ë°åÂãï„Åó„Åü„Å®„Åç„ÅÆÊúüÂæÖÁ¥ØÁ©çÂ†±ÈÖ¨

$$
V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s \right]
$$

<strong>Ë°åÂãï‰æ°ÂÄ§Èñ¢Êï∞ÔºàQÈñ¢Êï∞Ôºâ</strong> $Q^\pi(s, a)$: Áä∂ÊÖã$s$„ÅßË°åÂãï$a$„ÇíÂèñ„Çä„ÄÅ„Åù„ÅÆÂæåÊñπÁ≠ñ$\pi$„Å´Âæì„Å£„Åü„Å®„Åç„ÅÆÊúüÂæÖÁ¥ØÁ©çÂ†±ÈÖ¨

$$
Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s, a_0 = a \right]
$$

<strong>ÊúÄÈÅ©ÊñπÁ≠ñ</strong> $\pi^*$: „Åô„Åπ„Å¶„ÅÆÁä∂ÊÖã„Åß‰æ°ÂÄ§Èñ¢Êï∞„ÇíÊúÄÂ§ßÂåñ„Åô„ÇãÊñπÁ≠ñ

$$
\pi^* = \arg\max_\pi V^\pi(s) \quad \forall s \in S
$$

---

<h2>1.3 QÂ≠¶ÁøíÔºàQ-LearningÔºâ</h2>

<h3>QÂ≠¶Áøí„ÅÆÂü∫Êú¨„Ç¢„Ç§„Éá„Ç¢</h3>

QÂ≠¶Áøí„ÅØ„ÄÅ<strong>QÈñ¢Êï∞„ÇíÁõ¥Êé•Â≠¶Áøí</strong>„Åô„ÇãÂº∑ÂåñÂ≠¶Áøí„Ç¢„É´„Ç¥„É™„Ç∫„É†„Åß„Åô„ÄÇ

<strong>„Éô„É´„Éû„É≥ÊñπÁ®ãÂºè</strong>:
$$
Q^*(s, a) = \mathbb{E}_{s'} \left[ r + \gamma \max_{a'} Q^*(s', a') \mid s, a \right]
$$

„Åì„Çå„ÅØ„ÄåÊúÄÈÅ©„Å™QÈñ¢Êï∞„ÅØ„ÄÅÂç≥Â∫ß„ÅÆÂ†±ÈÖ¨$r$„Å®Ê¨°„ÅÆÁä∂ÊÖã„Åß„ÅÆÊúÄÂ§ßQÂÄ§„ÅÆÂâ≤ÂºïÂíå„Å´Á≠â„Åó„ÅÑ„Äç„Å®„ÅÑ„ÅÜÊÑèÂë≥„Åß„Åô„ÄÇ

<h3>QÂ≠¶Áøí„ÅÆÊõ¥Êñ∞Âºè</h3>

Ë¶≥Ê∏¨„Åï„Çå„ÅüÈÅ∑Áßª$(s, a, r, s')$„Å´Âü∫„Å•„ÅÑ„Å¶„ÄÅQÂÄ§„ÇíÊõ¥Êñ∞Ôºö

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$

- $\alpha$: Â≠¶ÁøíÁéáÔºà0„Äú1Ôºâ
- $r + \gamma \max_{a'} Q(s', a')$: <strong>TDÁõÆÊ®ô</strong>ÔºàTemporal Difference TargetÔºâ
- $r + \gamma \max_{a'} Q(s', a') - Q(s, a)$: <strong>TDË™§Â∑Æ</strong>

<h3>Python„Å´„Çà„ÇãÂÆüË£Ö</h3>

Á∞°Âçò„Å™„Ç∞„É™„ÉÉ„Éâ„ÉØ„Éº„É´„ÉâÔºàÊùêÊñôÊé¢Á¥¢Á©∫Èñì„ÅÆ„É°„Çø„Éï„Ç°„ÉºÔºâ„ÅßQÂ≠¶Áøí„ÇíÂÆüË£Ö„Åó„Åæ„ÅôÔºö

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

class SimpleMaterialsEnv:
    """Á∞°Âçò„Å™ÊùêÊñôÊé¢Á¥¢Áí∞Â¢ÉÔºà„Ç∞„É™„ÉÉ„Éâ„ÉØ„Éº„É´„ÉâÔºâ

    - 5x5„ÅÆ„Ç∞„É™„ÉÉ„Éâ
    - ÂêÑ„Çª„É´„ÅØÊùêÊñôÂÄôË£ú„ÇíË°®„Åô
    - ÁõÆÊ®ô: ÊúÄÈ´òÁâπÊÄß„ÅÆÊùêÊñôÔºà„Ç¥„Éº„É´Ôºâ„Å´Âà∞ÈÅî
    """
    def __init__(self):
        self.grid_size = 5
        self.state = (0, 0)  # „Çπ„Çø„Éº„Éà‰ΩçÁΩÆ
        self.goal = (4, 4)   # „Ç¥„Éº„É´‰ΩçÁΩÆÔºàÊúÄÈÅ©ÊùêÊñôÔºâ

    def reset(self):
        """ÂàùÊúüÁä∂ÊÖã„Å´„É™„Çª„ÉÉ„Éà"""
        self.state = (0, 0)
        return self.state

    def step(self, action):
        """Ë°åÂãï„ÇíÂÆüË°å

        Args:
            action: 0=‰∏ä, 1=‰∏ã, 2=Â∑¶, 3=Âè≥

        Returns:
            next_state, reward, done
        """
        x, y = self.state

        # Ë°åÂãï„Å´Âøú„Åò„Å¶ÁßªÂãï
        if action == 0 and x > 0:  # ‰∏ä
            x -= 1
        elif action == 1 and x < self.grid_size - 1:  # ‰∏ã
            x += 1
        elif action == 2 and y > 0:  # Â∑¶
            y -= 1
        elif action == 3 and y < self.grid_size - 1:  # Âè≥
            y += 1

        self.state = (x, y)

        # Â†±ÈÖ¨Ë®≠Ë®à
        if self.state == self.goal:
            reward = 10.0  # „Ç¥„Éº„É´Âà∞ÈÅîÔºàÊúÄÈÅ©ÊùêÊñôÁô∫Ë¶ãÔºâ
            done = True
        else:
            reward = -0.1  # ÂêÑ„Çπ„ÉÜ„ÉÉ„Éó„ÅÆ„Ç≥„Çπ„ÉàÔºàÂÆüÈ®ì„Ç≥„Çπ„ÉàÔºâ
            done = False

        return self.state, reward, done

    def get_state_space(self):
        """Áä∂ÊÖãÁ©∫Èñì„ÅÆ„Çµ„Ç§„Ç∫"""
        return self.grid_size * self.grid_size

    def get_action_space(self):
        """Ë°åÂãïÁ©∫Èñì„ÅÆ„Çµ„Ç§„Ç∫"""
        return 4


def q_learning(env, episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):
    """QÂ≠¶Áøí„Ç¢„É´„Ç¥„É™„Ç∫„É†

    Args:
        env: Áí∞Â¢É
        episodes: „Ç®„Éî„ÇΩ„Éº„ÉâÊï∞
        alpha: Â≠¶ÁøíÁéá
        gamma: Ââ≤ÂºïÁéá
        epsilon: Œµ-greedyÊé¢Á¥¢„ÅÆÁ¢∫Áéá

    Returns:
        Â≠¶Áøí„Åó„ÅüQ-table
    """
    # Q-table„ÅÆÂàùÊúüÂåñÔºàÁä∂ÊÖã√óË°åÂãïÔºâ
    Q = np.zeros((env.grid_size, env.grid_size, env.get_action_space()))

    rewards_per_episode = []

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False

        while not done:
            # Œµ-greedyÊé¢Á¥¢
            if np.random.random() < epsilon:
                action = np.random.randint(env.get_action_space())  # „É©„É≥„ÉÄ„É†Êé¢Á¥¢
            else:
                action = np.argmax(Q[state[0], state[1], :])  # ÊúÄËâØ„ÅÆË°åÂãï„ÇíÊ¥ªÁî®

            # Ë°åÂãïÂÆüË°å
            next_state, reward, done = env.step(action)
            total_reward += reward

            # QÂÄ§Êõ¥Êñ∞Ôºà„Éô„É´„Éû„É≥ÊñπÁ®ãÂºèÔºâ
            current_q = Q[state[0], state[1], action]
            max_next_q = np.max(Q[next_state[0], next_state[1], :])
            new_q = current_q + alpha * (reward + gamma * max_next_q - current_q)
            Q[state[0], state[1], action] = new_q

            state = next_state

        rewards_per_episode.append(total_reward)

        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(rewards_per_episode[-100:])
            print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}")

    return Q, rewards_per_episode


<h1>ÂÆüË°å</h1>
env = SimpleMaterialsEnv()
Q, rewards = q_learning(env, episodes=1000)

<h1>Â≠¶ÁøíÊõ≤Á∑ö„ÅÆÂèØË¶ñÂåñ</h1>
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards, np.ones(50)/50, mode='valid'))
plt.xlabel('Episode')
plt.ylabel('Average Reward (50 episodes)')
plt.title('Q-Learning: ÊùêÊñôÊé¢Á¥¢Áí∞Â¢É„Åß„ÅÆÂ≠¶ÁøíÈÄ≤Êçó')
plt.grid(True)
plt.show()

<h1>Â≠¶Áøí„Åó„ÅüQÂÄ§„ÅÆÂèØË¶ñÂåñ</h1>
policy = np.argmax(Q, axis=2)
print("\nÂ≠¶Áøí„Åó„ÅüÊñπÁ≠ñÔºàÂêÑ„Çª„É´„Åß„ÅÆÊúÄËâØË°åÂãïÔºâ:")
print("0=‰∏ä, 1=‰∏ã, 2=Â∑¶, 3=Âè≥")
print(policy)</code></pre>

<strong>Âá∫Âäõ‰æã</strong>:
<pre><code>Episode 100: Avg Reward = -4.52
Episode 200: Avg Reward = -3.21
Episode 500: Avg Reward = -1.85
Episode 1000: Avg Reward = -1.12

Â≠¶Áøí„Åó„ÅüÊñπÁ≠ñÔºàÂêÑ„Çª„É´„Åß„ÅÆÊúÄËâØË°åÂãïÔºâ:
[[1 1 1 1 1]
 [1 1 1 1 1]
 [1 1 1 1 1]
 [1 1 1 1 1]
 [3 3 3 3 0]]</code></pre>

<strong>Ëß£Ë™¨</strong>:
- ÂàùÊúü„ÅØÂ†±ÈÖ¨„Åå‰Ωé„ÅÑÔºà-4.52Ôºâ„Åå„ÄÅÂ≠¶Áøí„ÅåÈÄ≤„ÇÄ„Å®ÊîπÂñÑÔºà-1.12Ôºâ
- ÊúÄÁµÇÁöÑ„Å´„ÄÅ„Ç¥„Éº„É´„Å∏„ÅÆÊúÄÁü≠ÁµåË∑Ø„ÇíÂ≠¶ÁøíÔºà‰∏ã‚ÜíÂè≥„ÅÆÊñπÁ≠ñÔºâ

---

<h2>1.4 Deep Q-NetworkÔºàDQNÔºâ</h2>

<h3>QÂ≠¶Áøí„ÅÆÈôêÁïå</h3>

QÂ≠¶Áøí„ÅØ„ÄÅ<strong>Áä∂ÊÖã„Å®Ë°åÂãï„ÅåÈõ¢Êï£ÁöÑ„Åã„Å§Â∞ëÊï∞</strong>„ÅÆÂ†¥Âêà„Å´ÊúâÂäπ„Åß„Åô„ÄÇ„Åó„Åã„Åó„ÄÅÊùêÊñôÁßëÂ≠¶„Åß„ÅØÔºö

- <strong>Áä∂ÊÖãÁ©∫Èñì„ÅåÂ∑®Â§ß</strong>: ÊùêÊñôË®òËø∞Â≠êÔºà100Ê¨°ÂÖÉ‰ª•‰∏äÔºâ
- <strong>ÈÄ£Á∂öÂÄ§</strong>: ÁµÑÊàêÊØîÁéá„ÄÅÊ∏©Â∫¶„ÄÅÂúßÂäõ„Å™„Å©
- <strong>Q-table„ÅåÈùûÁèæÂÆüÁöÑ</strong>: $10^{100}$ÂÄã„ÅÆ„Çª„É´„Çí‰øùÂ≠ò„Åß„Åç„Å™„ÅÑ

<h3>DQN„ÅÆËß£Ê±∫Á≠ñ</h3>

DQN„ÅØ„ÄÅ<strong>„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅßQÈñ¢Êï∞„ÇíËøë‰ºº</strong>„Åó„Åæ„ÅôÔºö

$$
Q(s, a; \theta) \approx Q^*(s, a)
$$

- $\theta$: „Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆ„Éë„É©„É°„Éº„Çø

<strong>ÊêçÂ§±Èñ¢Êï∞</strong>:
$$
L(\theta) = \mathbb{E}_{(s,a,r,s') \sim D} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2 \right]
$$

- $D$: <strong>ÁµåÈ®ìÂÜçÁîü„Éê„ÉÉ„Éï„Ç°</strong>ÔºàÈÅéÂéª„ÅÆÈÅ∑Áßª„Çí‰øùÂ≠òÔºâ
- $\theta^-$: <strong>„Çø„Éº„Ç≤„ÉÉ„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ</strong>ÔºàÂ≠¶Áøí„ÅÆÂÆâÂÆöÂåñÔºâ

<h3>DQN„ÅÆÈáçË¶ÅÊäÄË°ì</h3>

1. <strong>ÁµåÈ®ìÂÜçÁîüÔºàExperience ReplayÔºâ</strong>: ÈÅéÂéª„ÅÆÈÅ∑Áßª„Çí„É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞„Åó„ÄÅ„Éá„Éº„Çø„ÅÆÁõ∏Èñ¢„ÇíÊ∏õ„Çâ„Åô
2. <strong>„Çø„Éº„Ç≤„ÉÉ„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ</strong>: Âõ∫ÂÆö„Åï„Çå„Åü„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅßTDÁõÆÊ®ô„ÇíË®àÁÆó„Åó„ÄÅÂ≠¶Áøí„ÇíÂÆâÂÆöÂåñ
3. <strong>Œµ-greedyÊé¢Á¥¢</strong>: Êé¢Á¥¢Ôºà„É©„É≥„ÉÄ„É†Ôºâ„Å®Ê¥ªÁî®ÔºàÊúÄËâØË°åÂãïÔºâ„ÅÆ„Éê„É©„É≥„Çπ

<h3>PyTorch„Å´„Çà„ÇãDQNÂÆüË£Ö</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random

class DQN(nn.Module):
    """Deep Q-Network

    Áä∂ÊÖã„ÇíÂÖ•Âäõ„Åó„ÄÅÂêÑË°åÂãï„ÅÆQÂÄ§„ÇíÂá∫Âäõ
    """
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)


class ReplayBuffer:
    """ÁµåÈ®ìÂÜçÁîü„Éê„ÉÉ„Éï„Ç°"""
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

    def __len__(self):
        return len(self.buffer)


class DQNAgent:
    """DQN„Ç®„Éº„Ç∏„Çß„É≥„Éà"""
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min

        # „É°„Ç§„É≥„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Å®„Çø„Éº„Ç≤„ÉÉ„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ
        self.policy_net = DQN(state_dim, action_dim)
        self.target_net = DQN(state_dim, action_dim)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)
        self.buffer = ReplayBuffer()

    def select_action(self, state):
        """Œµ-greedyË°åÂãïÈÅ∏Êäû"""
        if np.random.random() < self.epsilon:
            return np.random.randint(self.action_dim)
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                q_values = self.policy_net(state_tensor)
                return q_values.argmax().item()

    def train(self, batch_size=64):
        """„Éü„Éã„Éê„ÉÉ„ÉÅÂ≠¶Áøí"""
        if len(self.buffer) < batch_size:
            return

        # „Éü„Éã„Éê„ÉÉ„ÉÅ„Çµ„É≥„Éó„É™„É≥„Ç∞
        batch = self.buffer.sample(batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions).unsqueeze(1)
        rewards = torch.FloatTensor(rewards).unsqueeze(1)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones).unsqueeze(1)

        # ÁèæÂú®„ÅÆQÂÄ§
        current_q = self.policy_net(states).gather(1, actions)

        # „Çø„Éº„Ç≤„ÉÉ„ÉàQÂÄ§
        with torch.no_grad():
            max_next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)
            target_q = rewards + (1 - dones) * self.gamma * max_next_q

        # ÊêçÂ§±Ë®àÁÆó„Å®ÊúÄÈÅ©Âåñ
        loss = nn.MSELoss()(current_q, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Œµ„ÅÆÊ∏õË°∞
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    def update_target_network(self):
        """„Çø„Éº„Ç≤„ÉÉ„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÊõ¥Êñ∞"""
        self.target_net.load_state_dict(self.policy_net.state_dict())


<h1>ÊùêÊñôÊé¢Á¥¢Áí∞Â¢ÉÔºàÈÄ£Á∂öÁä∂ÊÖãÁâàÔºâ</h1>
class ContinuousMaterialsEnv:
    """ÈÄ£Á∂öÁä∂ÊÖãÁ©∫Èñì„ÅÆÊùêÊñôÊé¢Á¥¢Áí∞Â¢É"""
    def __init__(self, state_dim=4):
        self.state_dim = state_dim
        self.target = np.array([3.0, 5.0, 2.5, 4.0])  # ÁõÆÊ®ôÁâπÊÄß
        self.state = None

    def reset(self):
        self.state = np.random.uniform(0, 10, self.state_dim)
        return self.state

    def step(self, action):
        # Ë°åÂãï: 0=Â¢óÂä†, 1=Ê∏õÂ∞ë, 2=Â§ßÂπÖÂ¢óÂä†, 3=Â§ßÂπÖÊ∏õÂ∞ë
        delta = [0.1, -0.1, 0.5, -0.5][action]

        # „É©„É≥„ÉÄ„É†„Å™Ê¨°ÂÖÉ„ÇíÂ§âÊõ¥
        dim = np.random.randint(self.state_dim)
        self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)

        # Â†±ÈÖ¨: ÁõÆÊ®ô„Å®„ÅÆË∑ùÈõ¢ÔºàË≤†„ÅÆÂÄ§„ÄÅËøë„ÅÑ„Åª„Å©ËâØ„ÅÑÔºâ
        distance = np.linalg.norm(self.state - self.target)
        reward = -distance

        # ÁµÇ‰∫ÜÊù°‰ª∂: ÁõÆÊ®ô„Å´ÂçÅÂàÜËøë„ÅÑ
        done = distance < 0.5

        return self.state, reward, done


<h1>DQNË®ìÁ∑¥</h1>
env = ContinuousMaterialsEnv()
agent = DQNAgent(state_dim=4, action_dim=4)

episodes = 500
rewards_history = []

for episode in range(episodes):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent.select_action(state)
        next_state, reward, done = env.step(action)

        agent.buffer.push(state, action, reward, next_state, done)
        agent.train()

        state = next_state
        total_reward += reward

    rewards_history.append(total_reward)

    # „Çø„Éº„Ç≤„ÉÉ„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÊõ¥Êñ∞
    if (episode + 1) % 10 == 0:
        agent.update_target_network()

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(rewards_history[-50:])
        print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}, Œµ = {agent.epsilon:.3f}")

<h1>Â≠¶ÁøíÊõ≤Á∑ö</h1>
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'))
plt.xlabel('Episode')
plt.ylabel('Average Reward (20 episodes)')
plt.title('DQN: ÈÄ£Á∂öÁä∂ÊÖãÊùêÊñôÊé¢Á¥¢„Åß„ÅÆÂ≠¶ÁøíÈÄ≤Êçó')
plt.grid(True)
plt.show()</code></pre>

<strong>Âá∫Âäõ‰æã</strong>:
<pre><code>Episode 50: Avg Reward = -45.23, Œµ = 0.779
Episode 100: Avg Reward = -32.15, Œµ = 0.606
Episode 200: Avg Reward = -18.92, Œµ = 0.365
Episode 500: Avg Reward = -8.45, Œµ = 0.010</code></pre>

<strong>Ëß£Ë™¨</strong>:
- „Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅåÈÄ£Á∂öÁä∂ÊÖã„ÅÆQÈñ¢Êï∞„ÇíÂ≠¶Áøí
- Œµ„ÅåÊ∏õË°∞„Åó„ÄÅÊé¢Á¥¢„Åã„ÇâÊ¥ªÁî®„Å∏„Ç∑„Éï„Éà
- ÊúÄÁµÇÁöÑ„Å´ÁõÆÊ®ôÁâπÊÄß„Å´Ëøë„ÅÑÊùêÊñô„ÇíÂäπÁéáÁöÑ„Å´Áô∫Ë¶ã

---

<h2>ÊºîÁøíÂïèÈ°å</h2>

<h3>ÂïèÈ°å1 (Èõ£ÊòìÂ∫¶: easy)</h3>

QÂ≠¶Áøí„ÅÆÊõ¥Êñ∞Âºè„Å´„Åä„ÅÑ„Å¶„ÄÅÂ≠¶ÁøíÁéá$\alpha$„ÇíÂ§ß„Åç„Åè„Åô„Çã„Å®‰Ωï„ÅåËµ∑„Åì„Çã„ÅãË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åæ„Åü„ÄÅ$\alpha=0$„Å®$\alpha=1$„ÅÆÊ•µÁ´Ø„Å™„Ç±„Éº„Çπ„Åß„ÅØ„Å©„ÅÜ„Å™„Çã„ÅãÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

<details>
<summary>„Éí„É≥„Éà</summary>

Â≠¶ÁøíÁéá„ÅØ„ÄåÊñ∞„Åó„ÅÑÊÉÖÂ†±„Çí„Å©„Çå„Å†„ÅëÈáçË¶ñ„Åô„Çã„Åã„Äç„ÇíÂà∂Âæ°„Åó„Åæ„Åô„ÄÇÊõ¥Êñ∞Âºè„ÇíË¶ãÁõ¥„Åó„Å¶„Åø„Åæ„Åó„Çá„ÅÜ„ÄÇ

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>

<strong>$\alpha$„ÇíÂ§ß„Åç„Åè„Åô„Çã„Å®</strong>:
- Êñ∞„Åó„ÅÑË¶≥Ê∏¨ÔºàTDÁõÆÊ®ôÔºâ„ÇíÂº∑„ÅèÂèçÊò†„Åó„ÄÅQÂÄ§„ÅåÂ§ß„Åç„ÅèÂ§âÂåñ
- Â≠¶Áøí„ÅåÈÄü„ÅÑ„Åå‰∏çÂÆâÂÆö„Å´„Å™„Çä„ÇÑ„Åô„ÅÑ

<strong>Ê•µÁ´Ø„Å™„Ç±„Éº„Çπ</strong>:
- <strong>$\alpha=0$</strong>: QÂÄ§„ÅåÂÖ®„ÅèÊõ¥Êñ∞„Åï„Çå„Å™„ÅÑÔºàÂ≠¶Áøí„Åó„Å™„ÅÑÔºâ
  $$Q(s,a) \leftarrow Q(s,a) + 0 \cdot [\cdots] = Q(s,a)$$

- <strong>$\alpha=1$</strong>: QÂÄ§„ÅåÂÆåÂÖ®„Å´TDÁõÆÊ®ô„ÅßÁΩÆ„ÅçÊèõ„Åà„Çâ„Çå„Çã
  $$Q(s,a) \leftarrow r + \gamma \max_{a'} Q(s', a')$$
  ÈÅéÂéª„ÅÆÊÉÖÂ†±„ÅåÂÆåÂÖ®„Å´Ê∂à„Åà„ÄÅÊúÄÊñ∞„ÅÆË¶≥Ê∏¨„ÅÆ„Åø„Å´‰æùÂ≠ò

<strong>ÂÆüË∑µÁöÑ„Å´„ÅØ</strong>: $\alpha = 0.01 \sim 0.1$„Åå‰∏ÄËà¨ÁöÑ

</details>

---

<h3>ÂïèÈ°å2 (Èõ£ÊòìÂ∫¶: medium)</h3>

ÊùêÊñôÊé¢Á¥¢„Å´„Åä„ÅÑ„Å¶„ÄÅÂ†±ÈÖ¨Èñ¢Êï∞„Çí‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å´Ë®≠Ë®à„Åó„Åæ„Åó„Åü„ÄÇ„Åì„ÅÆË®≠Ë®à„ÅÆÂïèÈ°åÁÇπ„Å®ÊîπÂñÑÊ°à„ÇíËø∞„Åπ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

<pre><code class="language-python">def reward_function(material_property, target=3.0):
    if material_property == target:
        return 1.0
    else:
        return 0.0</code></pre>

<details>
<summary>„Éí„É≥„Éà</summary>

„Åì„ÅÆÂ†±ÈÖ¨„ÅØ„Äå„Çπ„Éë„Éº„ÇπÂ†±ÈÖ¨„Äç„Å®Âëº„Å∞„Çå„ÄÅÁõÆÊ®ô„Å´Âà∞ÈÅî„Åó„Å™„ÅÑÈôê„Çä„Åô„Åπ„Å¶0„Åß„Åô„ÄÇÂ≠¶Áøí„Å´„Å©„ÅÜÂΩ±Èüø„Åô„Çã„ÅãËÄÉ„Åà„Å¶„Åø„Åæ„Åó„Çá„ÅÜ„ÄÇ

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>

<strong>ÂïèÈ°åÁÇπ</strong>:
1. <strong>„Çπ„Éë„Éº„ÇπÂ†±ÈÖ¨</strong>: „Åª„Å®„Çì„Å©„ÅÆÂ†¥ÂêàÂ†±ÈÖ¨„Åå0„Åß„ÄÅÂ≠¶Áøí„Ç∑„Ç∞„Éä„É´„ÅåÂº±„ÅÑ
2. <strong>Êé¢Á¥¢„ÅåÂõ∞Èõ£</strong>: „Å©„ÅÆÊñπÂêë„Å´ÈÄ≤„ÇÅ„Å∞ËâØ„ÅÑ„Åã„Çè„Åã„Çâ„Å™„ÅÑ
3. <strong>Âé≥ÂØÜ„Å™‰∏ÄËá¥</strong>: ÂÆüÊï∞ÂÄ§„ÅßÂÆåÂÖ®‰∏ÄËá¥„ÅØ„Åª„Åº‰∏çÂèØËÉΩ

<strong>ÊîπÂñÑÊ°à</strong>:
<pre><code class="language-python">def improved_reward_function(material_property, target=3.0):
    # ÁõÆÊ®ô„Å®„ÅÆË∑ùÈõ¢„Å´Âü∫„Å•„ÅèÈÄ£Á∂öÁöÑ„Å™Â†±ÈÖ¨
    distance = abs(material_property - target)

    if distance < 0.1:
        return 10.0  # ÈùûÂ∏∏„Å´Ëøë„ÅÑÔºà„Éú„Éº„Éä„ÇπÔºâ
    elif distance < 0.5:
        return 5.0   # Ëøë„ÅÑ
    else:
        return -distance  # ÈÅ†„ÅÑ„Åª„Å©„Éö„Éä„É´„ÉÜ„Ç£</code></pre>

<strong>„Åï„Çâ„Å™„ÇãÊîπÂñÑ</strong>:
- <strong>„Ç∑„Çß„Ç§„Éî„É≥„Ç∞Â†±ÈÖ¨</strong>: ÁõÆÊ®ô„Å∏„ÅÆÈÄ≤Êçó„Å´Âøú„Åò„Å¶‰∏≠ÈñìÂ†±ÈÖ¨„Çí‰∏é„Åà„Çã
- <strong>Â§öÁõÆÁöÑÂ†±ÈÖ¨</strong>: Ë§áÊï∞„ÅÆÁâπÊÄß„ÇíËÄÉÊÖÆÔºà„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó + ÂÆâÂÆöÊÄßÔºâ

</details>

---

<h3>ÂïèÈ°å3 (Èõ£ÊòìÂ∫¶: hard)</h3>

DQN„Å´„Åä„Åë„Çã„ÄåÁµåÈ®ìÂÜçÁîü„Äç„Å®„Äå„Çø„Éº„Ç≤„ÉÉ„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Äç„ÅÆÂΩπÂâ≤„ÇíË™¨Êòé„Åó„ÄÅ„Åù„Çå„Åû„Çå„Åå„Å™„ÅÑ„Å®„Å©„ÅÆ„Çà„ÅÜ„Å™ÂïèÈ°å„ÅåËµ∑„Åì„Çã„Åã„ÄÅPython„Ç≥„Éº„Éâ„ÅßÂÆüÈ®ì„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

<details>
<summary>„Éí„É≥„Éà</summary>

ÁµåÈ®ìÂÜçÁîü„Çí„Ç™„Éï„Å´„Åô„Çã„Å´„ÅØ<code>buffer.sample()</code>„ÅÆ‰ª£„Çè„Çä„Å´ÊúÄÊñ∞„ÅÆÈÅ∑Áßª„ÅÆ„Åø„Çí‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ„Çø„Éº„Ç≤„ÉÉ„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Çí„Ç™„Éï„Å´„Åô„Çã„Å´„ÅØ„ÄÅTDÁõÆÊ®ô„ÅÆË®àÁÆó„Åß<code>self.policy_net</code>„Çí‰Ωø„ÅÑ„Åæ„Åô„ÄÇ

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>

<strong>ÁµåÈ®ìÂÜçÁîü„ÅÆÂΩπÂâ≤</strong>:
- ÈÅéÂéª„ÅÆÈÅ∑Áßª„Çí„É©„É≥„ÉÄ„É†„Çµ„É≥„Éó„É™„É≥„Ç∞„Åó„ÄÅ„Éá„Éº„Çø„ÅÆÁõ∏Èñ¢„ÇíÊ∏õ„Çâ„Åô
- „Å™„ÅÑ„Å®„ÄÅÈÄ£Á∂ö„Åó„ÅüÈÅ∑Áßª„Å†„Åë„ÅßÂ≠¶Áøí„Åó„ÄÅÁâπÂÆö„ÅÆ„Éë„Çø„Éº„É≥„Å´ÈÅéÂ≠¶Áøí

<strong>„Çø„Éº„Ç≤„ÉÉ„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÂΩπÂâ≤</strong>:
- Âõ∫ÂÆö„Åï„Çå„Åü„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅßTDÁõÆÊ®ô„ÇíË®àÁÆó„Åó„ÄÅÂ≠¶Áøí„ÇíÂÆâÂÆöÂåñ
- „Å™„ÅÑ„Å®„ÄÅQÂÄ§„ÅåÊåØÂãï„Åó„Å¶ÂèéÊùü„Åó„Å´„Åè„ÅÑ

<strong>ÂÆüÈ®ì„Ç≥„Éº„Éâ</strong>:
<pre><code class="language-python"><h1>ÁµåÈ®ìÂÜçÁîü„Å™„ÅóÁâà</h1>
class DQNAgentNoReplay(DQNAgent):
    def train_no_replay(self, state, action, reward, next_state, done):
        # ÊúÄÊñ∞„ÅÆÈÅ∑Áßª„ÅÆ„Åø„ÅßÂ≠¶Áøí
        states = torch.FloatTensor([state])
        actions = torch.LongTensor([action]).unsqueeze(1)
        rewards = torch.FloatTensor([reward]).unsqueeze(1)
        next_states = torch.FloatTensor([next_state])
        dones = torch.FloatTensor([done]).unsqueeze(1)

        current_q = self.policy_net(states).gather(1, actions)
        with torch.no_grad():
            max_next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)
            target_q = rewards + (1 - dones) * self.gamma * max_next_q

        loss = nn.MSELoss()(current_q, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

<h1>„Çø„Éº„Ç≤„ÉÉ„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Å™„ÅóÁâàÔºàTDÁõÆÊ®ô„Åßpolicy_net„Çí‰ΩøÁî®Ôºâ</h1>
<h1>‚Üí Â≠¶Áøí„Åå‰∏çÂÆâÂÆö„Å´„Å™„Çã</h1>

<h1>ÁµêÊûú: ÁµåÈ®ìÂÜçÁîü„Å™„Åó„Åß„ÅØÂèéÊùü„ÅåÈÅÖ„Åè„ÄÅ„Çø„Éº„Ç≤„ÉÉ„Éà„Éç„ÉÉ„Éà„Å™„Åó„Åß„ÅØÊåØÂãï„Åô„Çã</h1></code></pre>

</details>

---

<h2>„Åì„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆ„Åæ„Å®„ÇÅ</h2>

- ÊùêÊñôÊé¢Á¥¢„ÅØÊé¢Á¥¢Á©∫Èñì„ÅåÂ∫ÉÂ§ß„Åß„ÄÅÂæìÊù•„ÅÆË©¶Ë°åÈåØË™§„ÅØÈùûÂäπÁéá
- Âº∑ÂåñÂ≠¶Áøí„ÅØ<strong>Áí∞Â¢É„Å®„ÅÆÁõ∏‰∫í‰ΩúÁî®„ÇíÈÄö„Åò„Å¶ÊúÄÈÅ©„Å™Êé¢Á¥¢Êà¶Áï•„ÇíÂ≠¶Áøí</strong>
- <strong>„Éû„É´„Ç≥„ÉïÊ±∫ÂÆöÈÅéÁ®ãÔºàMDPÔºâ</strong>„ÅåÂº∑ÂåñÂ≠¶Áøí„ÅÆÊï∞Â≠¶ÁöÑÂü∫Áõ§
- <strong>QÂ≠¶Áøí</strong>„ÅØÈõ¢Êï£Áä∂ÊÖã„ÉªË°åÂãï„ÅßÊúâÂäπ„ÄÅQ-table„Åß‰æ°ÂÄ§„ÇíË®òÈå≤
- <strong>DQN</strong>„ÅØ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅßQÈñ¢Êï∞„ÇíËøë‰ºº„Åó„ÄÅÂ∑®Â§ß„Å™Áä∂ÊÖãÁ©∫Èñì„Å´ÂØæÂøú
- <strong>ÁµåÈ®ìÂÜçÁîü</strong>„Å®<strong>„Çø„Éº„Ç≤„ÉÉ„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ</strong>„ÅåDQNÂ≠¶Áøí„ÇíÂÆâÂÆöÂåñ

Ê¨°Á´†„Åß„ÅØ„ÄÅ„Çà„ÇäÈ´òÂ∫¶„Å™ÊñπÁ≠ñÂãæÈÖçÊ≥ïÔºàPolicy GradientÔºâ„Å®Actor-CriticÊâãÊ≥ï„ÇíÂ≠¶„Å≥„Åæ„Åô„ÄÇ

---

<h2>ÂèÇËÄÉÊñáÁåÆ</h2>

1. Mnih et al. "Playing Atari with Deep Reinforcement Learning" *arXiv* (2013) - DQNÂéüË´ñÊñá
2. Sutton & Barto "Reinforcement Learning: An Introduction" MIT Press (2018) - RLÊïôÁßëÊõ∏
3. Zhou et al. "Optimization of molecules via deep reinforcement learning" *Scientific Reports* (2019)
4. Ling et al. "High-dimensional materials and process optimization using data-driven experimental design with well-calibrated uncertainty estimates" *Integrating Materials and Manufacturing Innovation* (2017)

---

<strong>Ê¨°Á´†</strong>: [Á¨¨2Á´†: Âº∑ÂåñÂ≠¶Áøí„ÅÆÂü∫Á§éÁêÜË´ñ](chapter-2.html)
<div class="navigation">
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter-2.html" class="nav-button">Á¨¨2Á´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-17</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>
