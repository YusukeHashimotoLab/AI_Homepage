<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬1ç« : ãªãœææ–™ç§‘å­¦ã«å¼·åŒ–å­¦ç¿’ã‹ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬1ç« : ãªãœææ–™ç§‘å­¦ã«å¼·åŒ–å­¦ç¿’ã‹</h1>
            
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 6å€‹</span>
            </div>
        </div>
    </header>

    <main class="container">
        
<p><h1>ç¬¬1ç« : ãªãœææ–™ç§‘å­¦ã«å¼·åŒ–å­¦ç¿’ã‹</h1></p>

<p><h2>å­¦ç¿’ç›®æ¨™</h2></p>

<p>ã“ã®ç« ã§ã¯ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã—ã¾ã™ï¼š</p>

<ul>
<li>ææ–™æ¢ç´¢ã«ãŠã‘ã‚‹å¾“æ¥æ‰‹æ³•ã®é™ç•Œã¨å¼·åŒ–å­¦ç¿’ã®å½¹å‰²</li>
<li>ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMDPï¼‰ã®åŸºæœ¬æ¦‚å¿µ</li>
<li>Qå­¦ç¿’ã¨Deep Q-Networkï¼ˆDQNï¼‰ã®ä»•çµ„ã¿</li>
<li>ç°¡å˜ãªææ–™æ¢ç´¢ã‚¿ã‚¹ã‚¯ã¸ã®å®Ÿè£…</li>
</ul>

<p>---</p>

<p><h2>1.1 ææ–™æ¢ç´¢ã®èª²é¡Œã¨å¼·åŒ–å­¦ç¿’ã®å½¹å‰²</h2></p>

<p><h3>å¾“æ¥ã®ææ–™æ¢ç´¢ã®é™ç•Œ</h3></p>

<p>æ–°ææ–™é–‹ç™ºã«ã¯ã€è†¨å¤§ãªæ¢ç´¢ç©ºé–“ï¼ˆçµ„æˆã€æ§‹é€ ã€ãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶ï¼‰ãŒã‚ã‚Šã¾ã™ï¼š</p>

<ul>
<li><strong>çµ„æˆæ¢ç´¢</strong>: å…ƒç´ å‘¨æœŸè¡¨ã‹ã‚‰3å…ƒç´ ã‚’é¸ã¶ã ã‘ã§$\binom{118}{3} \approx 267,000$é€šã‚Š</li>
<li><strong>æ§‹é€ æ¢ç´¢</strong>: çµæ™¶æ§‹é€ ã ã‘ã§230ç¨®ã®ç©ºé–“ç¾¤</li>
<li><strong>ãƒ—ãƒ­ã‚»ã‚¹æ¢ç´¢</strong>: æ¸©åº¦ãƒ»åœ§åŠ›ãƒ»æ™‚é–“ã®çµ„ã¿åˆã‚ã›ã¯ç„¡é™</li>
</ul>

<p>å¾“æ¥ã®<strong>è©¦è¡ŒéŒ¯èª¤ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</strong>ã§ã¯ï¼š</p>
<ul>
<li>ç ”ç©¶è€…ã®çµŒé¨“ã¨å‹˜ã«ä¾å­˜</li>
<li>è©•ä¾¡ã«æ™‚é–“ã¨ã‚³ã‚¹ãƒˆï¼ˆ1ææ–™ã‚ãŸã‚Šæ•°é€±é–“ã€œæ•°ãƒ¶æœˆï¼‰</li>
<li>å±€æ‰€æœ€é©è§£ã«é™¥ã‚Šã‚„ã™ã„</li>
</ul>

<p><pre><code class="language-mermaid">graph LR</p>
<p>    A[ç ”ç©¶è€…] -->|çµŒé¨“ãƒ»å‹˜| B[ææ–™å€™è£œé¸æŠ]</p>
<p>    B -->|åˆæˆãƒ»è©•ä¾¡| C[çµæœ]</p>
<p>    C -->|è§£é‡ˆ| A</p>

<p>    style A fill:#ffcccc</p>
<p>    style B fill:#ffcccc</p>
<p>    style C fill:#ffcccc</p>
<p></code></pre></p>

<p><strong>å•é¡Œç‚¹</strong>:</p>
<ol>
<li><strong>åŠ¹ç‡ãŒæ‚ªã„</strong>: åŒã˜ã‚ˆã†ãªææ–™ã‚’ç¹°ã‚Šè¿”ã—è©¦ã™</li>
<li><strong>æ¢ç´¢ãŒç‹­ã„</strong>: ç ”ç©¶è€…ã®çŸ¥è­˜ç¯„å›²ã«é™å®š</li>
<li><strong>å†ç¾æ€§ãŒä½ã„</strong>: æš—é»™çŸ¥ã«ä¾å­˜</li>
</ol>

<p><h3>å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹è§£æ±ºç­–</h3></p>

<p>å¼·åŒ–å­¦ç¿’ã¯ã€<strong>ç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦æœ€é©ãªè¡Œå‹•ã‚’å­¦ç¿’</strong>ã™ã‚‹æ çµ„ã¿ã§ã™ï¼š</p>

<p><pre><code class="language-mermaid">graph LR</p>
<p>    A[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ<br/>RL Algorithm] -->|è¡Œå‹•<br/>ææ–™å€™è£œ| B[ç’°å¢ƒ<br/>å®Ÿé¨“/è¨ˆç®—]</p>
<p>    B -->|å ±é…¬<br/>ç‰¹æ€§è©•ä¾¡| A</p>
<p>    B -->|çŠ¶æ…‹<br/>ç¾åœ¨ã®çŸ¥è¦‹| A</p>

<p>    style A fill:#e1f5ff</p>
<p>    style B fill:#ffe1cc</p>
<p></code></pre></p>

<p><strong>å¼·åŒ–å­¦ç¿’ã®åˆ©ç‚¹</strong>:</p>
<ol>
<li><strong>è‡ªå‹•æœ€é©åŒ–</strong>: è©¦è¡ŒéŒ¯èª¤ã‚’è‡ªå‹•åŒ–ã—ã€åŠ¹ç‡çš„ãªæ¢ç´¢æˆ¦ç•¥ã‚’å­¦ç¿’</li>
<li><strong>æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹</strong>: æœªçŸ¥é ˜åŸŸã®æ¢ç´¢ã¨æ—¢çŸ¥ã®è‰¯ã„é ˜åŸŸã®æ´»ç”¨ã‚’èª¿æ•´</li>
<li><strong>é€æ¬¡çš„æ”¹å–„</strong>: å„è©•ä¾¡çµæœã‹ã‚‰å­¦ç¿’ã—ã€æ¬¡ã®é¸æŠã‚’æ”¹å–„</li>
<li><strong>ã‚¯ãƒ­ãƒ¼ã‚ºãƒ‰ãƒ«ãƒ¼ãƒ—</strong>: å®Ÿé¨“è£…ç½®ã¨çµ±åˆã—24æ™‚é–“ç¨¼åƒå¯èƒ½</li>
</ol>

<p><h3>ææ–™ç§‘å­¦ã§ã®æˆåŠŸäº‹ä¾‹</h3></p>

<p><strong>ä¾‹1: Li-ioné›»æ± é›»è§£æ¶²ã®æœ€é©åŒ–</strong> (MIT, 2022)</p>
<ul>
<li><strong>èª²é¡Œ</strong>: 5æˆåˆ†ã®é…åˆæ¯”ç‡ã‚’æœ€é©åŒ–ï¼ˆæ¢ç´¢ç©ºé–“ > $10^6$ï¼‰</li>
<li><strong>æ‰‹æ³•</strong>: DQNã§é€æ¬¡çš„ã«é…åˆã‚’é¸æŠ</li>
<li><strong>çµæœ</strong>: å¾“æ¥æ‰‹æ³•ã®5å€ã®é€Ÿåº¦ã§æœ€é©è§£ç™ºè¦‹ã€ã‚¤ã‚ªãƒ³ä¼å°åº¦30%å‘ä¸Š</li>
</ul>

<p><strong>ä¾‹2: æœ‰æ©Ÿå¤ªé™½é›»æ± ãƒ‰ãƒŠãƒ¼ææ–™</strong> (Torontoå¤§, 2021)</p>
<ul>
<li><strong>èª²é¡Œ</strong>: åˆ†å­æ§‹é€ ã®æœ€é©åŒ–ï¼ˆ10^23é€šã‚Šã®å€™è£œï¼‰</li>
<li><strong>æ‰‹æ³•</strong>: Actor-Criticã§åˆ†å­ç”Ÿæˆã¨è©•ä¾¡ã‚’çµ±åˆ</li>
<li><strong>çµæœ</strong>: å…‰é›»å¤‰æ›åŠ¹ç‡15%ã®æ–°ææ–™ã‚’3ãƒ¶æœˆã§ç™ºè¦‹ï¼ˆå¾“æ¥ã¯2å¹´ï¼‰</li>
</ul>

<p>---</p>

<p><h2>1.2 ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMDPï¼‰ã®åŸºç¤</h2></p>

<p><h3>MDPã¨ã¯</h3></p>

<p>å¼·åŒ–å­¦ç¿’ã®æ•°å­¦çš„åŸºç›¤ã¯ã€<strong>ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹</strong>ï¼ˆMarkov Decision Process, MDPï¼‰ã§ã™ã€‚MDPã¯ä»¥ä¸‹ã®5ã¤çµ„ã§å®šç¾©ã•ã‚Œã¾ã™ï¼š</p>

<p>$$</p>
<p>\text{MDP} = (S, A, P, R, \gamma)</p>
<p>$$</p>

<ul>
<li>$S$: <strong>çŠ¶æ…‹ç©ºé–“</strong>ï¼ˆä¾‹: ç¾åœ¨è©¦ã—ãŸææ–™ã®ç‰¹æ€§ï¼‰</li>
<li>$A$: <strong>è¡Œå‹•ç©ºé–“</strong>ï¼ˆä¾‹: æ¬¡ã«è©¦ã™ææ–™å€™è£œï¼‰</li>
<li>$P(s'|s, a)$: <strong>çŠ¶æ…‹é·ç§»ç¢ºç‡</strong>ï¼ˆè¡Œå‹•$a$ã‚’å–ã£ãŸã¨ãã«çŠ¶æ…‹$s$ã‹ã‚‰$s'$ã¸é·ç§»ã™ã‚‹ç¢ºç‡ï¼‰</li>
<li>$R(s, a, s')$: <strong>å ±é…¬é–¢æ•°</strong>ï¼ˆçŠ¶æ…‹é·ç§»ã§å¾—ã‚‰ã‚Œã‚‹å ±é…¬ï¼‰</li>
<li>$\gamma \in [0, 1)$: <strong>å‰²å¼•ç‡</strong>ï¼ˆå°†æ¥ã®å ±é…¬ã®é‡è¦åº¦ï¼‰</li>
</ul>

<p><h3>ææ–™æ¢ç´¢ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°</h3></p>

<p>| MDPè¦ç´  | ææ–™æ¢ç´¢ã§ã®æ„å‘³ | å…·ä½“ä¾‹ |</p>
<p>|---------|-----------------|--------|</p>
<p>| çŠ¶æ…‹ $s$ | ç¾åœ¨ã®çŸ¥è¦‹ï¼ˆã“ã‚Œã¾ã§ã®è©•ä¾¡çµæœï¼‰ | "ææ–™A: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—2.1eVã€ææ–™B: 2.5eV" |</p>
<p>| è¡Œå‹• $a$ | æ¬¡ã«è©¦ã™ææ–™ | "Ti-Ni-Oçµ„æˆã®ææ–™C" |</p>
<p>| å ±é…¬ $r$ | ææ–™ç‰¹æ€§ã®è©•ä¾¡å€¤ | "ææ–™Cã®ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—2.8eVï¼ˆç›®æ¨™3.0eVã«è¿‘ã„ï¼‰" |</p>
<p>| æ–¹ç­– $\pi$ | ææ–™é¸æŠæˆ¦ç•¥ | "ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ãŒç›®æ¨™ã«è¿‘ã„å…ƒç´ çµ„æˆã‚’å„ªå…ˆ" |</p>

<p><h3>ãƒãƒ«ã‚³ãƒ•æ€§</h3></p>

<p>MDPã®é‡è¦ãªä»®å®šã¯<strong>ãƒãƒ«ã‚³ãƒ•æ€§</strong>ã§ã™ï¼š</p>

<p>$$</p>
<p>P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \dots) = P(s_{t+1}|s_t, a_t)</p>
<p>$$</p>

<p>ã¤ã¾ã‚Šã€<strong>æ¬¡ã®çŠ¶æ…‹ã¯ç¾åœ¨ã®çŠ¶æ…‹ã¨è¡Œå‹•ã®ã¿ã«ä¾å­˜ã—ã€éå»ã®å±¥æ­´ã¯ä¸è¦</strong>ã§ã™ã€‚</p>

<p>ææ–™æ¢ç´¢ã§ã¯ã€ç¾åœ¨ã®è©•ä¾¡çµæœï¼ˆçŠ¶æ…‹ï¼‰ã«åŸºã¥ã„ã¦æ¬¡ã®ææ–™ï¼ˆè¡Œå‹•ï¼‰ã‚’é¸ã¹ã°ã€éå»ã®å…¨å±¥æ­´ã‚’è¦šãˆã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</p>

<p><h3>æ–¹ç­–ã¨ä¾¡å€¤é–¢æ•°</h3></p>

<p><strong>æ–¹ç­–</strong> $\pi(a|s)$: çŠ¶æ…‹$s$ã§è¡Œå‹•$a$ã‚’é¸ã¶ç¢ºç‡</p>

<p><strong>çŠ¶æ…‹ä¾¡å€¤é–¢æ•°</strong> $V^\pi(s)$: çŠ¶æ…‹$s$ã‹ã‚‰æ–¹ç­–$\pi$ã«å¾“ã£ã¦è¡Œå‹•ã—ãŸã¨ãã®æœŸå¾…ç´¯ç©å ±é…¬</p>

<p>$$</p>
<p>V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s \right]</p>
<p>$$</p>

<p><strong>è¡Œå‹•ä¾¡å€¤é–¢æ•°ï¼ˆQé–¢æ•°ï¼‰</strong> $Q^\pi(s, a)$: çŠ¶æ…‹$s$ã§è¡Œå‹•$a$ã‚’å–ã‚Šã€ãã®å¾Œæ–¹ç­–$\pi$ã«å¾“ã£ãŸã¨ãã®æœŸå¾…ç´¯ç©å ±é…¬</p>

<p>$$</p>
<p>Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s, a_0 = a \right]</p>
<p>$$</p>

<p><strong>æœ€é©æ–¹ç­–</strong> $\pi^*$: ã™ã¹ã¦ã®çŠ¶æ…‹ã§ä¾¡å€¤é–¢æ•°ã‚’æœ€å¤§åŒ–ã™ã‚‹æ–¹ç­–</p>

<p>$$</p>
<p>\pi^* = \arg\max_\pi V^\pi(s) \quad \forall s \in S</p>
<p>$$</p>

<p>---</p>

<p><h2>1.3 Qå­¦ç¿’ï¼ˆQ-Learningï¼‰</h2></p>

<p><h3>Qå­¦ç¿’ã®åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢</h3></p>

<p>Qå­¦ç¿’ã¯ã€<strong>Qé–¢æ•°ã‚’ç›´æ¥å­¦ç¿’</strong>ã™ã‚‹å¼·åŒ–å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚</p>

<p><strong>ãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼</strong>:</p>
<p>$$</p>
<p>Q^<em>(s, a) = \mathbb{E}_{s'} \left[ r + \gamma \max_{a'} Q^</em>(s', a') \mid s, a \right]</p>
<p>$$</p>

<p>ã“ã‚Œã¯ã€Œæœ€é©ãªQé–¢æ•°ã¯ã€å³åº§ã®å ±é…¬$r$ã¨æ¬¡ã®çŠ¶æ…‹ã§ã®æœ€å¤§Qå€¤ã®å‰²å¼•å’Œã«ç­‰ã—ã„ã€ã¨ã„ã†æ„å‘³ã§ã™ã€‚</p>

<p><h3>Qå­¦ç¿’ã®æ›´æ–°å¼</h3></p>

<p>è¦³æ¸¬ã•ã‚ŒãŸé·ç§»$(s, a, r, s')$ã«åŸºã¥ã„ã¦ã€Qå€¤ã‚’æ›´æ–°ï¼š</p>

<p>$$</p>
<p>Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]</p>
<p>$$</p>

<ul>
<li>$\alpha$: å­¦ç¿’ç‡ï¼ˆ0ã€œ1ï¼‰</li>
<li>$r + \gamma \max_{a'} Q(s', a')$: <strong>TDç›®æ¨™</strong>ï¼ˆTemporal Difference Targetï¼‰</li>
<li>$r + \gamma \max_{a'} Q(s', a') - Q(s, a)$: <strong>TDèª¤å·®</strong></li>
</ul>

<p><h3>Pythonã«ã‚ˆã‚‹å®Ÿè£…</h3></p>

<p>ç°¡å˜ãªã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ï¼ˆææ–™æ¢ç´¢ç©ºé–“ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ï¼‰ã§Qå­¦ç¿’ã‚’å®Ÿè£…ã—ã¾ã™ï¼š</p>

<p><pre><code class="language-python">import numpy as np</p>
<p>import matplotlib.pyplot as plt</p>

<p>class SimpleMaterialsEnv:</p>
<p>    """ç°¡å˜ãªææ–™æ¢ç´¢ç’°å¢ƒï¼ˆã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ï¼‰</p>

<p>    - 5x5ã®ã‚°ãƒªãƒƒãƒ‰</p>
<p>    - å„ã‚»ãƒ«ã¯ææ–™å€™è£œã‚’è¡¨ã™</p>
<p>    - ç›®æ¨™: æœ€é«˜ç‰¹æ€§ã®ææ–™ï¼ˆã‚´ãƒ¼ãƒ«ï¼‰ã«åˆ°é”</p>
<p>    """</p>
<p>    def __init__(self):</p>
<p>        self.grid_size = 5</p>
<p>        self.state = (0, 0)  <h1>ã‚¹ã‚¿ãƒ¼ãƒˆä½ç½®</h1></p>
<p>        self.goal = (4, 4)   <h1>ã‚´ãƒ¼ãƒ«ä½ç½®ï¼ˆæœ€é©ææ–™ï¼‰</h1></p>

<p>    def reset(self):</p>
<p>        """åˆæœŸçŠ¶æ…‹ã«ãƒªã‚»ãƒƒãƒˆ"""</p>
<p>        self.state = (0, 0)</p>
<p>        return self.state</p>

<p>    def step(self, action):</p>
<p>        """è¡Œå‹•ã‚’å®Ÿè¡Œ</p>

<p>        Args:</p>
<p>            action: 0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³</p>

<p>        Returns:</p>
<p>            next_state, reward, done</p>
<p>        """</p>
<p>        x, y = self.state</p>

<p>        <h1>è¡Œå‹•ã«å¿œã˜ã¦ç§»å‹•</h1></p>
<p>        if action == 0 and x > 0:  <h1>ä¸Š</h1></p>
<p>            x -= 1</p>
<p>        elif action == 1 and x < self.grid_size - 1:  <h1>ä¸‹</h1></p>
<p>            x += 1</p>
<p>        elif action == 2 and y > 0:  <h1>å·¦</h1></p>
<p>            y -= 1</p>
<p>        elif action == 3 and y < self.grid_size - 1:  <h1>å³</h1></p>
<p>            y += 1</p>

<p>        self.state = (x, y)</p>

<p>        <h1>å ±é…¬è¨­è¨ˆ</h1></p>
<p>        if self.state == self.goal:</p>
<p>            reward = 10.0  <h1>ã‚´ãƒ¼ãƒ«åˆ°é”ï¼ˆæœ€é©ææ–™ç™ºè¦‹ï¼‰</h1></p>
<p>            done = True</p>
<p>        else:</p>
<p>            reward = -0.1  <h1>å„ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚³ã‚¹ãƒˆï¼ˆå®Ÿé¨“ã‚³ã‚¹ãƒˆï¼‰</h1></p>
<p>            done = False</p>

<p>        return self.state, reward, done</p>

<p>    def get_state_space(self):</p>
<p>        """çŠ¶æ…‹ç©ºé–“ã®ã‚µã‚¤ã‚º"""</p>
<p>        return self.grid_size * self.grid_size</p>

<p>    def get_action_space(self):</p>
<p>        """è¡Œå‹•ç©ºé–“ã®ã‚µã‚¤ã‚º"""</p>
<p>        return 4</p>


<p>def q_learning(env, episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):</p>
<p>    """Qå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </p>

<p>    Args:</p>
<p>        env: ç’°å¢ƒ</p>
<p>        episodes: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°</p>
<p>        alpha: å­¦ç¿’ç‡</p>
<p>        gamma: å‰²å¼•ç‡</p>
<p>        epsilon: Îµ-greedyæ¢ç´¢ã®ç¢ºç‡</p>

<p>    Returns:</p>
<p>        å­¦ç¿’ã—ãŸQ-table</p>
<p>    """</p>
<p>    <h1>Q-tableã®åˆæœŸåŒ–ï¼ˆçŠ¶æ…‹Ã—è¡Œå‹•ï¼‰</h1></p>
<p>    Q = np.zeros((env.grid_size, env.grid_size, env.get_action_space()))</p>

<p>    rewards_per_episode = []</p>

<p>    for episode in range(episodes):</p>
<p>        state = env.reset()</p>
<p>        total_reward = 0</p>
<p>        done = False</p>

<p>        while not done:</p>
<p>            <h1>Îµ-greedyæ¢ç´¢</h1></p>
<p>            if np.random.random() < epsilon:</p>
<p>                action = np.random.randint(env.get_action_space())  <h1>ãƒ©ãƒ³ãƒ€ãƒ æ¢ç´¢</h1></p>
<p>            else:</p>
<p>                action = np.argmax(Q[state[0], state[1], :])  <h1>æœ€è‰¯ã®è¡Œå‹•ã‚’æ´»ç”¨</h1></p>

<p>            <h1>è¡Œå‹•å®Ÿè¡Œ</h1></p>
<p>            next_state, reward, done = env.step(action)</p>
<p>            total_reward += reward</p>

<p>            <h1>Qå€¤æ›´æ–°ï¼ˆãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼ï¼‰</h1></p>
<p>            current_q = Q[state[0], state[1], action]</p>
<p>            max_next_q = np.max(Q[next_state[0], next_state[1], :])</p>
<p>            new_q = current_q + alpha <em> (reward + gamma </em> max_next_q - current_q)</p>
<p>            Q[state[0], state[1], action] = new_q</p>

<p>            state = next_state</p>

<p>        rewards_per_episode.append(total_reward)</p>

<p>        if (episode + 1) % 100 == 0:</p>
<p>            avg_reward = np.mean(rewards_per_episode[-100:])</p>
<p>            print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}")</p>

<p>    return Q, rewards_per_episode</p>


<p><h1>å®Ÿè¡Œ</h1></p>
<p>env = SimpleMaterialsEnv()</p>
<p>Q, rewards = q_learning(env, episodes=1000)</p>

<p><h1>å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–</h1></p>
<p>plt.figure(figsize=(10, 6))</p>
<p>plt.plot(np.convolve(rewards, np.ones(50)/50, mode='valid'))</p>
<p>plt.xlabel('Episode')</p>
<p>plt.ylabel('Average Reward (50 episodes)')</p>
<p>plt.title('Q-Learning: ææ–™æ¢ç´¢ç’°å¢ƒã§ã®å­¦ç¿’é€²æ—')</p>
<p>plt.grid(True)</p>
<p>plt.show()</p>

<p><h1>å­¦ç¿’ã—ãŸQå€¤ã®å¯è¦–åŒ–</h1></p>
<p>policy = np.argmax(Q, axis=2)</p>
<p>print("\nå­¦ç¿’ã—ãŸæ–¹ç­–ï¼ˆå„ã‚»ãƒ«ã§ã®æœ€è‰¯è¡Œå‹•ï¼‰:")</p>
<p>print("0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³")</p>
<p>print(policy)</p>
<p></code></pre></p>

<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<p><pre><code class="language-">Episode 100: Avg Reward = -4.52</p>
<p>Episode 200: Avg Reward = -3.21</p>
<p>Episode 500: Avg Reward = -1.85</p>
<p>Episode 1000: Avg Reward = -1.12</p>

<p>å­¦ç¿’ã—ãŸæ–¹ç­–ï¼ˆå„ã‚»ãƒ«ã§ã®æœ€è‰¯è¡Œå‹•ï¼‰:</p>
<p>[[1 1 1 1 1]</p>
<p> [1 1 1 1 1]</p>
<p> [1 1 1 1 1]</p>
<p> [1 1 1 1 1]</p>
<p> [3 3 3 3 0]]</p>
<p></code></pre></p>

<p><strong>è§£èª¬</strong>:</p>
<ul>
<li>åˆæœŸã¯å ±é…¬ãŒä½ã„ï¼ˆ-4.52ï¼‰ãŒã€å­¦ç¿’ãŒé€²ã‚€ã¨æ”¹å–„ï¼ˆ-1.12ï¼‰</li>
<li>æœ€çµ‚çš„ã«ã€ã‚´ãƒ¼ãƒ«ã¸ã®æœ€çŸ­çµŒè·¯ã‚’å­¦ç¿’ï¼ˆä¸‹â†’å³ã®æ–¹ç­–ï¼‰</li>
</ul>

<p>---</p>

<p><h2>1.4 Deep Q-Networkï¼ˆDQNï¼‰</h2></p>

<p><h3>Qå­¦ç¿’ã®é™ç•Œ</h3></p>

<p>Qå­¦ç¿’ã¯ã€<strong>çŠ¶æ…‹ã¨è¡Œå‹•ãŒé›¢æ•£çš„ã‹ã¤å°‘æ•°</strong>ã®å ´åˆã«æœ‰åŠ¹ã§ã™ã€‚ã—ã‹ã—ã€ææ–™ç§‘å­¦ã§ã¯ï¼š</p>

<ul>
<li><strong>çŠ¶æ…‹ç©ºé–“ãŒå·¨å¤§</strong>: ææ–™è¨˜è¿°å­ï¼ˆ100æ¬¡å…ƒä»¥ä¸Šï¼‰</li>
<li><strong>é€£ç¶šå€¤</strong>: çµ„æˆæ¯”ç‡ã€æ¸©åº¦ã€åœ§åŠ›ãªã©</li>
<li><strong>Q-tableãŒéç¾å®Ÿçš„</strong>: $10^{100}$å€‹ã®ã‚»ãƒ«ã‚’ä¿å­˜ã§ããªã„</li>
</ul>

<p><h3>DQNã®è§£æ±ºç­–</h3></p>

<p>DQNã¯ã€<strong>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§Qé–¢æ•°ã‚’è¿‘ä¼¼</strong>ã—ã¾ã™ï¼š</p>

<p>$$</p>
<p>Q(s, a; \theta) \approx Q^*(s, a)</p>
<p>$$</p>

<ul>
<li>$\theta$: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
</ul>

<p><strong>æå¤±é–¢æ•°</strong>:</p>
<p>$$</p>
<p>L(\theta) = \mathbb{E}_{(s,a,r,s') \sim D} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2 \right]</p>
<p>$$</p>

<ul>
<li>$D$: <strong>çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡</strong>ï¼ˆéå»ã®é·ç§»ã‚’ä¿å­˜ï¼‰</li>
<li>$\theta^-$: <strong>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</strong>ï¼ˆå­¦ç¿’ã®å®‰å®šåŒ–ï¼‰</li>
</ul>

<p><h3>DQNã®é‡è¦æŠ€è¡“</h3></p>

<ol>
<li><strong>çµŒé¨“å†ç”Ÿï¼ˆExperience Replayï¼‰</strong>: éå»ã®é·ç§»ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ãƒ‡ãƒ¼ã‚¿ã®ç›¸é–¢ã‚’æ¸›ã‚‰ã™</li>
<li><strong>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</strong>: å›ºå®šã•ã‚ŒãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§TDç›®æ¨™ã‚’è¨ˆç®—ã—ã€å­¦ç¿’ã‚’å®‰å®šåŒ–</li>
<li><strong>Îµ-greedyæ¢ç´¢</strong>: æ¢ç´¢ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰ã¨æ´»ç”¨ï¼ˆæœ€è‰¯è¡Œå‹•ï¼‰ã®ãƒãƒ©ãƒ³ã‚¹</li>
</ol>

<p><h3>PyTorchã«ã‚ˆã‚‹DQNå®Ÿè£…</h3></p>

<p><pre><code class="language-python">import torch</p>
<p>import torch.nn as nn</p>
<p>import torch.optim as optim</p>
<p>from collections import deque</p>
<p>import random</p>

<p>class DQN(nn.Module):</p>
<p>    """Deep Q-Network</p>

<p>    çŠ¶æ…‹ã‚’å…¥åŠ›ã—ã€å„è¡Œå‹•ã®Qå€¤ã‚’å‡ºåŠ›</p>
<p>    """</p>
<p>    def __init__(self, state_dim, action_dim, hidden_dim=64):</p>
<p>        super(DQN, self).__init__()</p>
<p>        self.fc1 = nn.Linear(state_dim, hidden_dim)</p>
<p>        self.fc2 = nn.Linear(hidden_dim, hidden_dim)</p>
<p>        self.fc3 = nn.Linear(hidden_dim, action_dim)</p>

<p>    def forward(self, x):</p>
<p>        x = torch.relu(self.fc1(x))</p>
<p>        x = torch.relu(self.fc2(x))</p>
<p>        return self.fc3(x)</p>


<p>class ReplayBuffer:</p>
<p>    """çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡"""</p>
<p>    def __init__(self, capacity=10000):</p>
<p>        self.buffer = deque(maxlen=capacity)</p>

<p>    def push(self, state, action, reward, next_state, done):</p>
<p>        self.buffer.append((state, action, reward, next_state, done))</p>

<p>    def sample(self, batch_size):</p>
<p>        return random.sample(self.buffer, batch_size)</p>

<p>    def __len__(self):</p>
<p>        return len(self.buffer)</p>


<p>class DQNAgent:</p>
<p>    """DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""</p>
<p>    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):</p>
<p>        self.action_dim = action_dim</p>
<p>        self.gamma = gamma</p>
<p>        self.epsilon = epsilon</p>
<p>        self.epsilon_decay = epsilon_decay</p>
<p>        self.epsilon_min = epsilon_min</p>

<p>        <h1>ãƒ¡ã‚¤ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</h1></p>
<p>        self.policy_net = DQN(state_dim, action_dim)</p>
<p>        self.target_net = DQN(state_dim, action_dim)</p>
<p>        self.target_net.load_state_dict(self.policy_net.state_dict())</p>
<p>        self.target_net.eval()</p>

<p>        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)</p>
<p>        self.buffer = ReplayBuffer()</p>

<p>    def select_action(self, state):</p>
<p>        """Îµ-greedyè¡Œå‹•é¸æŠ"""</p>
<p>        if np.random.random() < self.epsilon:</p>
<p>            return np.random.randint(self.action_dim)</p>
<p>        else:</p>
<p>            with torch.no_grad():</p>
<p>                state_tensor = torch.FloatTensor(state).unsqueeze(0)</p>
<p>                q_values = self.policy_net(state_tensor)</p>
<p>                return q_values.argmax().item()</p>

<p>    def train(self, batch_size=64):</p>
<p>        """ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’"""</p>
<p>        if len(self.buffer) < batch_size:</p>
<p>            return</p>

<p>        <h1>ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</h1></p>
<p>        batch = self.buffer.sample(batch_size)</p>
<p>        states, actions, rewards, next_states, dones = zip(*batch)</p>

<p>        states = torch.FloatTensor(states)</p>
<p>        actions = torch.LongTensor(actions).unsqueeze(1)</p>
<p>        rewards = torch.FloatTensor(rewards).unsqueeze(1)</p>
<p>        next_states = torch.FloatTensor(next_states)</p>
<p>        dones = torch.FloatTensor(dones).unsqueeze(1)</p>

<p>        <h1>ç¾åœ¨ã®Qå€¤</h1></p>
<p>        current_q = self.policy_net(states).gather(1, actions)</p>

<p>        <h1>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆQå€¤</h1></p>
<p>        with torch.no_grad():</p>
<p>            max_next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)</p>
<p>            target_q = rewards + (1 - dones) <em> self.gamma </em> max_next_q</p>

<p>        <h1>æå¤±è¨ˆç®—ã¨æœ€é©åŒ–</h1></p>
<p>        loss = nn.MSELoss()(current_q, target_q)</p>
<p>        self.optimizer.zero_grad()</p>
<p>        loss.backward()</p>
<p>        self.optimizer.step()</p>

<p>        <h1>Îµã®æ¸›è¡°</h1></p>
<p>        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)</p>

<p>    def update_target_network(self):</p>
<p>        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ›´æ–°"""</p>
<p>        self.target_net.load_state_dict(self.policy_net.state_dict())</p>


<p><h1>ææ–™æ¢ç´¢ç’°å¢ƒï¼ˆé€£ç¶šçŠ¶æ…‹ç‰ˆï¼‰</h1></p>
<p>class ContinuousMaterialsEnv:</p>
<p>    """é€£ç¶šçŠ¶æ…‹ç©ºé–“ã®ææ–™æ¢ç´¢ç’°å¢ƒ"""</p>
<p>    def __init__(self, state_dim=4):</p>
<p>        self.state_dim = state_dim</p>
<p>        self.target = np.array([3.0, 5.0, 2.5, 4.0])  <h1>ç›®æ¨™ç‰¹æ€§</h1></p>
<p>        self.state = None</p>

<p>    def reset(self):</p>
<p>        self.state = np.random.uniform(0, 10, self.state_dim)</p>
<p>        return self.state</p>

<p>    def step(self, action):</p>
<p>        <h1>è¡Œå‹•: 0=å¢—åŠ , 1=æ¸›å°‘, 2=å¤§å¹…å¢—åŠ , 3=å¤§å¹…æ¸›å°‘</h1></p>
<p>        delta = [0.1, -0.1, 0.5, -0.5][action]</p>

<p>        <h1>ãƒ©ãƒ³ãƒ€ãƒ ãªæ¬¡å…ƒã‚’å¤‰æ›´</h1></p>
<p>        dim = np.random.randint(self.state_dim)</p>
<p>        self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)</p>

<p>        <h1>å ±é…¬: ç›®æ¨™ã¨ã®è·é›¢ï¼ˆè² ã®å€¤ã€è¿‘ã„ã»ã©è‰¯ã„ï¼‰</h1></p>
<p>        distance = np.linalg.norm(self.state - self.target)</p>
<p>        reward = -distance</p>

<p>        <h1>çµ‚äº†æ¡ä»¶: ç›®æ¨™ã«ååˆ†è¿‘ã„</h1></p>
<p>        done = distance < 0.5</p>

<p>        return self.state, reward, done</p>


<p><h1>DQNè¨“ç·´</h1></p>
<p>env = ContinuousMaterialsEnv()</p>
<p>agent = DQNAgent(state_dim=4, action_dim=4)</p>

<p>episodes = 500</p>
<p>rewards_history = []</p>

<p>for episode in range(episodes):</p>
<p>    state = env.reset()</p>
<p>    total_reward = 0</p>
<p>    done = False</p>

<p>    while not done:</p>
<p>        action = agent.select_action(state)</p>
<p>        next_state, reward, done = env.step(action)</p>

<p>        agent.buffer.push(state, action, reward, next_state, done)</p>
<p>        agent.train()</p>

<p>        state = next_state</p>
<p>        total_reward += reward</p>

<p>    rewards_history.append(total_reward)</p>

<p>    <h1>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°</h1></p>
<p>    if (episode + 1) % 10 == 0:</p>
<p>        agent.update_target_network()</p>

<p>    if (episode + 1) % 50 == 0:</p>
<p>        avg_reward = np.mean(rewards_history[-50:])</p>
<p>        print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}, Îµ = {agent.epsilon:.3f}")</p>

<p><h1>å­¦ç¿’æ›²ç·š</h1></p>
<p>plt.figure(figsize=(10, 6))</p>
<p>plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'))</p>
<p>plt.xlabel('Episode')</p>
<p>plt.ylabel('Average Reward (20 episodes)')</p>
<p>plt.title('DQN: é€£ç¶šçŠ¶æ…‹ææ–™æ¢ç´¢ã§ã®å­¦ç¿’é€²æ—')</p>
<p>plt.grid(True)</p>
<p>plt.show()</p>
<p></code></pre></p>

<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<p><pre><code class="language-">Episode 50: Avg Reward = -45.23, Îµ = 0.779</p>
<p>Episode 100: Avg Reward = -32.15, Îµ = 0.606</p>
<p>Episode 200: Avg Reward = -18.92, Îµ = 0.365</p>
<p>Episode 500: Avg Reward = -8.45, Îµ = 0.010</p>
<p></code></pre></p>

<p><strong>è§£èª¬</strong>:</p>
<ul>
<li>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒé€£ç¶šçŠ¶æ…‹ã®Qé–¢æ•°ã‚’å­¦ç¿’</li>
<li>ÎµãŒæ¸›è¡°ã—ã€æ¢ç´¢ã‹ã‚‰æ´»ç”¨ã¸ã‚·ãƒ•ãƒˆ</li>
<li>æœ€çµ‚çš„ã«ç›®æ¨™ç‰¹æ€§ã«è¿‘ã„ææ–™ã‚’åŠ¹ç‡çš„ã«ç™ºè¦‹</li>
</ul>

<p>---</p>

<p><h2>æ¼”ç¿’å•é¡Œ</h2></p>

<p><h3>å•é¡Œ1 (é›£æ˜“åº¦: easy)</h3></p>

<p>Qå­¦ç¿’ã®æ›´æ–°å¼ã«ãŠã„ã¦ã€å­¦ç¿’ç‡$\alpha$ã‚’å¤§ããã™ã‚‹ã¨ä½•ãŒèµ·ã“ã‚‹ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€$\alpha=0$ã¨$\alpha=1$ã®æ¥µç«¯ãªã‚±ãƒ¼ã‚¹ã§ã¯ã©ã†ãªã‚‹ã‹ç­”ãˆã¦ãã ã•ã„ã€‚</p>

<p><details></p>
<p><summary>ãƒ’ãƒ³ãƒˆ</summary></p>

<p>å­¦ç¿’ç‡ã¯ã€Œæ–°ã—ã„æƒ…å ±ã‚’ã©ã‚Œã ã‘é‡è¦–ã™ã‚‹ã‹ã€ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚æ›´æ–°å¼ã‚’è¦‹ç›´ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚</p>

<p></details></p>

<p><details></p>
<p><summary>è§£ç­”ä¾‹</summary></p>

<p><strong>$\alpha$ã‚’å¤§ããã™ã‚‹ã¨</strong>:</p>
<ul>
<li>æ–°ã—ã„è¦³æ¸¬ï¼ˆTDç›®æ¨™ï¼‰ã‚’å¼·ãåæ˜ ã—ã€Qå€¤ãŒå¤§ããå¤‰åŒ–</li>
<li>å­¦ç¿’ãŒé€Ÿã„ãŒä¸å®‰å®šã«ãªã‚Šã‚„ã™ã„</li>
</ul>

<p><strong>æ¥µç«¯ãªã‚±ãƒ¼ã‚¹</strong>:</p>
<ul>
<li><strong>$\alpha=0$</strong>: Qå€¤ãŒå…¨ãæ›´æ–°ã•ã‚Œãªã„ï¼ˆå­¦ç¿’ã—ãªã„ï¼‰</li>
</ul>
<p>  $$Q(s,a) \leftarrow Q(s,a) + 0 \cdot [\cdots] = Q(s,a)$$</p>

<ul>
<li><strong>$\alpha=1$</strong>: Qå€¤ãŒå®Œå…¨ã«TDç›®æ¨™ã§ç½®ãæ›ãˆã‚‰ã‚Œã‚‹</li>
</ul>
<p>  $$Q(s,a) \leftarrow r + \gamma \max_{a'} Q(s', a')$$</p>
<p>  éå»ã®æƒ…å ±ãŒå®Œå…¨ã«æ¶ˆãˆã€æœ€æ–°ã®è¦³æ¸¬ã®ã¿ã«ä¾å­˜</p>

<p><strong>å®Ÿè·µçš„ã«ã¯</strong>: $\alpha = 0.01 \sim 0.1$ãŒä¸€èˆ¬çš„</p>

<p></details></p>

<p>---</p>

<p><h3>å•é¡Œ2 (é›£æ˜“åº¦: medium)</h3></p>

<p>ææ–™æ¢ç´¢ã«ãŠã„ã¦ã€å ±é…¬é–¢æ•°ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«è¨­è¨ˆã—ã¾ã—ãŸã€‚ã“ã®è¨­è¨ˆã®å•é¡Œç‚¹ã¨æ”¹å–„æ¡ˆã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚</p>

<p><pre><code class="language-python">def reward_function(material_property, target=3.0):</p>
<p>    if material_property == target:</p>
<p>        return 1.0</p>
<p>    else:</p>
<p>        return 0.0</p>
<p></code></pre></p>

<p><details></p>
<p><summary>ãƒ’ãƒ³ãƒˆ</summary></p>

<p>ã“ã®å ±é…¬ã¯ã€Œã‚¹ãƒ‘ãƒ¼ã‚¹å ±é…¬ã€ã¨å‘¼ã°ã‚Œã€ç›®æ¨™ã«åˆ°é”ã—ãªã„é™ã‚Šã™ã¹ã¦0ã§ã™ã€‚å­¦ç¿’ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚</p>

<p></details></p>

<p><details></p>
<p><summary>è§£ç­”ä¾‹</summary></p>

<p><strong>å•é¡Œç‚¹</strong>:</p>
<ol>
<li><strong>ã‚¹ãƒ‘ãƒ¼ã‚¹å ±é…¬</strong>: ã»ã¨ã‚“ã©ã®å ´åˆå ±é…¬ãŒ0ã§ã€å­¦ç¿’ã‚·ã‚°ãƒŠãƒ«ãŒå¼±ã„</li>
<li><strong>æ¢ç´¢ãŒå›°é›£</strong>: ã©ã®æ–¹å‘ã«é€²ã‚ã°è‰¯ã„ã‹ã‚ã‹ã‚‰ãªã„</li>
<li><strong>å³å¯†ãªä¸€è‡´</strong>: å®Ÿæ•°å€¤ã§å®Œå…¨ä¸€è‡´ã¯ã»ã¼ä¸å¯èƒ½</li>
</ol>

<p><strong>æ”¹å–„æ¡ˆ</strong>:</p>
<p><pre><code class="language-python">def improved_reward_function(material_property, target=3.0):</p>
<p>    <h1>ç›®æ¨™ã¨ã®è·é›¢ã«åŸºã¥ãé€£ç¶šçš„ãªå ±é…¬</h1></p>
<p>    distance = abs(material_property - target)</p>

<p>    if distance < 0.1:</p>
<p>        return 10.0  <h1>éå¸¸ã«è¿‘ã„ï¼ˆãƒœãƒ¼ãƒŠã‚¹ï¼‰</h1></p>
<p>    elif distance < 0.5:</p>
<p>        return 5.0   <h1>è¿‘ã„</h1></p>
<p>    else:</p>
<p>        return -distance  <h1>é ã„ã»ã©ãƒšãƒŠãƒ«ãƒ†ã‚£</h1></p>
<p></code></pre></p>

<p><strong>ã•ã‚‰ãªã‚‹æ”¹å–„</strong>:</p>
<ul>
<li><strong>ã‚·ã‚§ã‚¤ãƒ”ãƒ³ã‚°å ±é…¬</strong>: ç›®æ¨™ã¸ã®é€²æ—ã«å¿œã˜ã¦ä¸­é–“å ±é…¬ã‚’ä¸ãˆã‚‹</li>
<li><strong>å¤šç›®çš„å ±é…¬</strong>: è¤‡æ•°ã®ç‰¹æ€§ã‚’è€ƒæ…®ï¼ˆãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ— + å®‰å®šæ€§ï¼‰</li>
</ul>

<p></details></p>

<p>---</p>

<p><h3>å•é¡Œ3 (é›£æ˜“åº¦: hard)</h3></p>

<p>DQNã«ãŠã‘ã‚‹ã€ŒçµŒé¨“å†ç”Ÿã€ã¨ã€Œã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ã®å½¹å‰²ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚ŒãŒãªã„ã¨ã©ã®ã‚ˆã†ãªå•é¡ŒãŒèµ·ã“ã‚‹ã‹ã€Pythonã‚³ãƒ¼ãƒ‰ã§å®Ÿé¨“ã—ã¦ãã ã•ã„ã€‚</p>

<p><details></p>
<p><summary>ãƒ’ãƒ³ãƒˆ</summary></p>

<p>çµŒé¨“å†ç”Ÿã‚’ã‚ªãƒ•ã«ã™ã‚‹ã«ã¯<code>buffer.sample()</code>ã®ä»£ã‚ã‚Šã«æœ€æ–°ã®é·ç§»ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ã‚ªãƒ•ã«ã™ã‚‹ã«ã¯ã€TDç›®æ¨™ã®è¨ˆç®—ã§<code>self.policy_net</code>ã‚’ä½¿ã„ã¾ã™ã€‚</p>

<p></details></p>

<p><details></p>
<p><summary>è§£ç­”ä¾‹</summary></p>

<p><strong>çµŒé¨“å†ç”Ÿã®å½¹å‰²</strong>:</p>
<ul>
<li>éå»ã®é·ç§»ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ãƒ‡ãƒ¼ã‚¿ã®ç›¸é–¢ã‚’æ¸›ã‚‰ã™</li>
<li>ãªã„ã¨ã€é€£ç¶šã—ãŸé·ç§»ã ã‘ã§å­¦ç¿’ã—ã€ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«éå­¦ç¿’</li>
</ul>

<p><strong>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å½¹å‰²</strong>:</p>
<ul>
<li>å›ºå®šã•ã‚ŒãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§TDç›®æ¨™ã‚’è¨ˆç®—ã—ã€å­¦ç¿’ã‚’å®‰å®šåŒ–</li>
<li>ãªã„ã¨ã€Qå€¤ãŒæŒ¯å‹•ã—ã¦åæŸã—ã«ãã„</li>
</ul>

<p><strong>å®Ÿé¨“ã‚³ãƒ¼ãƒ‰</strong>:</p>
<p><pre><code class="language-python"><h1>çµŒé¨“å†ç”Ÿãªã—ç‰ˆ</h1></p>
<p>class DQNAgentNoReplay(DQNAgent):</p>
<p>    def train_no_replay(self, state, action, reward, next_state, done):</p>
<p>        <h1>æœ€æ–°ã®é·ç§»ã®ã¿ã§å­¦ç¿’</h1></p>
<p>        states = torch.FloatTensor([state])</p>
<p>        actions = torch.LongTensor([action]).unsqueeze(1)</p>
<p>        rewards = torch.FloatTensor([reward]).unsqueeze(1)</p>
<p>        next_states = torch.FloatTensor([next_state])</p>
<p>        dones = torch.FloatTensor([done]).unsqueeze(1)</p>

<p>        current_q = self.policy_net(states).gather(1, actions)</p>
<p>        with torch.no_grad():</p>
<p>            max_next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)</p>
<p>            target_q = rewards + (1 - dones) <em> self.gamma </em> max_next_q</p>

<p>        loss = nn.MSELoss()(current_q, target_q)</p>
<p>        self.optimizer.zero_grad()</p>
<p>        loss.backward()</p>
<p>        self.optimizer.step()</p>

<p><h1>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãªã—ç‰ˆï¼ˆTDç›®æ¨™ã§policy_netã‚’ä½¿ç”¨ï¼‰</h1></p>
<p><h1>â†’ å­¦ç¿’ãŒä¸å®‰å®šã«ãªã‚‹</h1></p>

<p><h1>çµæœ: çµŒé¨“å†ç”Ÿãªã—ã§ã¯åæŸãŒé…ãã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãªã—ã§ã¯æŒ¯å‹•ã™ã‚‹</h1></p>
<p></code></pre></p>

<p></details></p>

<p>---</p>

<p><h2>ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã¾ã¨ã‚</h2></p>

<ul>
<li>ææ–™æ¢ç´¢ã¯æ¢ç´¢ç©ºé–“ãŒåºƒå¤§ã§ã€å¾“æ¥ã®è©¦è¡ŒéŒ¯èª¤ã¯éåŠ¹ç‡</li>
<li>å¼·åŒ–å­¦ç¿’ã¯<strong>ç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦æœ€é©ãªæ¢ç´¢æˆ¦ç•¥ã‚’å­¦ç¿’</strong></li>
<li><strong>ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMDPï¼‰</strong>ãŒå¼·åŒ–å­¦ç¿’ã®æ•°å­¦çš„åŸºç›¤</li>
<li><strong>Qå­¦ç¿’</strong>ã¯é›¢æ•£çŠ¶æ…‹ãƒ»è¡Œå‹•ã§æœ‰åŠ¹ã€Q-tableã§ä¾¡å€¤ã‚’è¨˜éŒ²</li>
<li><strong>DQN</strong>ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§Qé–¢æ•°ã‚’è¿‘ä¼¼ã—ã€å·¨å¤§ãªçŠ¶æ…‹ç©ºé–“ã«å¯¾å¿œ</li>
<li><strong>çµŒé¨“å†ç”Ÿ</strong>ã¨<strong>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</strong>ãŒDQNå­¦ç¿’ã‚’å®‰å®šåŒ–</li>
</ul>

<p>æ¬¡ç« ã§ã¯ã€ã‚ˆã‚Šé«˜åº¦ãªæ–¹ç­–å‹¾é…æ³•ï¼ˆPolicy Gradientï¼‰ã¨Actor-Criticæ‰‹æ³•ã‚’å­¦ã³ã¾ã™ã€‚</p>

<p>---</p>

<p><h2>å‚è€ƒæ–‡çŒ®</h2></p>

<ol>
<li>Mnih et al. "Playing Atari with Deep Reinforcement Learning" <em>arXiv</em> (2013) - DQNåŸè«–æ–‡</li>
<li>Sutton & Barto "Reinforcement Learning: An Introduction" MIT Press (2018) - RLæ•™ç§‘æ›¸</li>
<li>Zhou et al. "Optimization of molecules via deep reinforcement learning" <em>Scientific Reports</em> (2019)</li>
<li>Ling et al. "High-dimensional materials and process optimization using data-driven experimental design with well-calibrated uncertainty estimates" <em>Integrating Materials and Manufacturing Innovation</em> (2017)</li>
</ol>

<p>---</p>

<p><strong>æ¬¡ç« </strong>: <a href="chapter-2.html">ç¬¬2ç« : å¼·åŒ–å­¦ç¿’ã®åŸºç¤ç†è«–</a></p>


        
        <div class="navigation">
            <a href="chapter-2.html" class="nav-button">æ¬¡ç« : ç¬¬2ç«  â†’</a>
            <a href="index.html" class="nav-button">â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
            
        </div>
    
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimoto(æ±åŒ—å¤§å­¦)</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>