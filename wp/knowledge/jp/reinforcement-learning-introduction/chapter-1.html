<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬1ç« : ãªãœææ–™ç§‘å­¦ã«å¼·åŒ–å­¦ç¿’ã‹ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬1ç« : ãªãœææ–™ç§‘å­¦ã«å¼·åŒ–å­¦ç¿’ã‹</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-25åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: åˆç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 6å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 3å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h1>ç¬¬1ç« : ãªãœææ–™ç§‘å­¦ã«å¼·åŒ–å­¦ç¿’ã‹</h1>

<h2>å­¦ç¿’ç›®æ¨™</h2>

ã“ã®ç« ã§ã¯ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã—ã¾ã™ï¼š

- ææ–™æ¢ç´¢ã«ãŠã‘ã‚‹å¾“æ¥æ‰‹æ³•ã®é™ç•Œã¨å¼·åŒ–å­¦ç¿’ã®å½¹å‰²
- ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMDPï¼‰ã®åŸºæœ¬æ¦‚å¿µ
- Qå­¦ç¿’ã¨Deep Q-Networkï¼ˆDQNï¼‰ã®ä»•çµ„ã¿
- ç°¡å˜ãªææ–™æ¢ç´¢ã‚¿ã‚¹ã‚¯ã¸ã®å®Ÿè£…

---

<h2>1.1 ææ–™æ¢ç´¢ã®èª²é¡Œã¨å¼·åŒ–å­¦ç¿’ã®å½¹å‰²</h2>

<h3>å¾“æ¥ã®ææ–™æ¢ç´¢ã®é™ç•Œ</h3>

æ–°ææ–™é–‹ç™ºã«ã¯ã€è†¨å¤§ãªæ¢ç´¢ç©ºé–“ï¼ˆçµ„æˆã€æ§‹é€ ã€ãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶ï¼‰ãŒã‚ã‚Šã¾ã™ï¼š

- <strong>çµ„æˆæ¢ç´¢</strong>: å…ƒç´ å‘¨æœŸè¡¨ã‹ã‚‰3å…ƒç´ ã‚’é¸ã¶ã ã‘ã§$\binom{118}{3} \approx 267,000$é€šã‚Š
- <strong>æ§‹é€ æ¢ç´¢</strong>: çµæ™¶æ§‹é€ ã ã‘ã§230ç¨®ã®ç©ºé–“ç¾¤
- <strong>ãƒ—ãƒ­ã‚»ã‚¹æ¢ç´¢</strong>: æ¸©åº¦ãƒ»åœ§åŠ›ãƒ»æ™‚é–“ã®çµ„ã¿åˆã‚ã›ã¯ç„¡é™

å¾“æ¥ã®<strong>è©¦è¡ŒéŒ¯èª¤ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</strong>ã§ã¯ï¼š
- ç ”ç©¶è€…ã®çµŒé¨“ã¨å‹˜ã«ä¾å­˜
- è©•ä¾¡ã«æ™‚é–“ã¨ã‚³ã‚¹ãƒˆï¼ˆ1ææ–™ã‚ãŸã‚Šæ•°é€±é–“ã€œæ•°ãƒ¶æœˆï¼‰
- å±€æ‰€æœ€é©è§£ã«é™¥ã‚Šã‚„ã™ã„

<div class="mermaid">graph LR
    A[ç ”ç©¶è€…] -->|çµŒé¨“ãƒ»å‹˜| B[ææ–™å€™è£œé¸æŠ]
    B -->|åˆæˆãƒ»è©•ä¾¡| C[çµæœ]
    C -->|è§£é‡ˆ| A

    style A fill:#ffcccc
    style B fill:#ffcccc
    style C fill:#ffcccc</div>

<strong>å•é¡Œç‚¹</strong>:
1. <strong>åŠ¹ç‡ãŒæ‚ªã„</strong>: åŒã˜ã‚ˆã†ãªææ–™ã‚’ç¹°ã‚Šè¿”ã—è©¦ã™
2. <strong>æ¢ç´¢ãŒç‹­ã„</strong>: ç ”ç©¶è€…ã®çŸ¥è­˜ç¯„å›²ã«é™å®š
3. <strong>å†ç¾æ€§ãŒä½ã„</strong>: æš—é»™çŸ¥ã«ä¾å­˜

<h3>å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹è§£æ±ºç­–</h3>

å¼·åŒ–å­¦ç¿’ã¯ã€<strong>ç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦æœ€é©ãªè¡Œå‹•ã‚’å­¦ç¿’</strong>ã™ã‚‹æ çµ„ã¿ã§ã™ï¼š

<div class="mermaid">graph LR
    A[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ<br/>RL Algorithm] -->|è¡Œå‹•<br/>ææ–™å€™è£œ| B[ç’°å¢ƒ<br/>å®Ÿé¨“/è¨ˆç®—]
    B -->|å ±é…¬<br/>ç‰¹æ€§è©•ä¾¡| A
    B -->|çŠ¶æ…‹<br/>ç¾åœ¨ã®çŸ¥è¦‹| A

    style A fill:#e1f5ff
    style B fill:#ffe1cc</div>

<strong>å¼·åŒ–å­¦ç¿’ã®åˆ©ç‚¹</strong>:
1. <strong>è‡ªå‹•æœ€é©åŒ–</strong>: è©¦è¡ŒéŒ¯èª¤ã‚’è‡ªå‹•åŒ–ã—ã€åŠ¹ç‡çš„ãªæ¢ç´¢æˆ¦ç•¥ã‚’å­¦ç¿’
2. <strong>æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹</strong>: æœªçŸ¥é ˜åŸŸã®æ¢ç´¢ã¨æ—¢çŸ¥ã®è‰¯ã„é ˜åŸŸã®æ´»ç”¨ã‚’èª¿æ•´
3. <strong>é€æ¬¡çš„æ”¹å–„</strong>: å„è©•ä¾¡çµæœã‹ã‚‰å­¦ç¿’ã—ã€æ¬¡ã®é¸æŠã‚’æ”¹å–„
4. <strong>ã‚¯ãƒ­ãƒ¼ã‚ºãƒ‰ãƒ«ãƒ¼ãƒ—</strong>: å®Ÿé¨“è£…ç½®ã¨çµ±åˆã—24æ™‚é–“ç¨¼åƒå¯èƒ½

<h3>ææ–™ç§‘å­¦ã§ã®æˆåŠŸäº‹ä¾‹</h3>

<strong>ä¾‹1: Li-ioné›»æ± é›»è§£æ¶²ã®æœ€é©åŒ–</strong> (MIT, 2022)
- <strong>èª²é¡Œ</strong>: 5æˆåˆ†ã®é…åˆæ¯”ç‡ã‚’æœ€é©åŒ–ï¼ˆæ¢ç´¢ç©ºé–“ > $10^6$ï¼‰
- <strong>æ‰‹æ³•</strong>: DQNã§é€æ¬¡çš„ã«é…åˆã‚’é¸æŠ
- <strong>çµæœ</strong>: å¾“æ¥æ‰‹æ³•ã®5å€ã®é€Ÿåº¦ã§æœ€é©è§£ç™ºè¦‹ã€ã‚¤ã‚ªãƒ³ä¼å°åº¦30%å‘ä¸Š

<strong>ä¾‹2: æœ‰æ©Ÿå¤ªé™½é›»æ± ãƒ‰ãƒŠãƒ¼ææ–™</strong> (Torontoå¤§, 2021)
- <strong>èª²é¡Œ</strong>: åˆ†å­æ§‹é€ ã®æœ€é©åŒ–ï¼ˆ10^23é€šã‚Šã®å€™è£œï¼‰
- <strong>æ‰‹æ³•</strong>: Actor-Criticã§åˆ†å­ç”Ÿæˆã¨è©•ä¾¡ã‚’çµ±åˆ
- <strong>çµæœ</strong>: å…‰é›»å¤‰æ›åŠ¹ç‡15%ã®æ–°ææ–™ã‚’3ãƒ¶æœˆã§ç™ºè¦‹ï¼ˆå¾“æ¥ã¯2å¹´ï¼‰

---

<h2>1.2 ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMDPï¼‰ã®åŸºç¤</h2>

<h3>MDPã¨ã¯</h3>

å¼·åŒ–å­¦ç¿’ã®æ•°å­¦çš„åŸºç›¤ã¯ã€<strong>ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹</strong>ï¼ˆMarkov Decision Process, MDPï¼‰ã§ã™ã€‚MDPã¯ä»¥ä¸‹ã®5ã¤çµ„ã§å®šç¾©ã•ã‚Œã¾ã™ï¼š

$$
\text{MDP} = (S, A, P, R, \gamma)
$$

- $S$: <strong>çŠ¶æ…‹ç©ºé–“</strong>ï¼ˆä¾‹: ç¾åœ¨è©¦ã—ãŸææ–™ã®ç‰¹æ€§ï¼‰
- $A$: <strong>è¡Œå‹•ç©ºé–“</strong>ï¼ˆä¾‹: æ¬¡ã«è©¦ã™ææ–™å€™è£œï¼‰
- $P(s'|s, a)$: <strong>çŠ¶æ…‹é·ç§»ç¢ºç‡</strong>ï¼ˆè¡Œå‹•$a$ã‚’å–ã£ãŸã¨ãã«çŠ¶æ…‹$s$ã‹ã‚‰$s'$ã¸é·ç§»ã™ã‚‹ç¢ºç‡ï¼‰
- $R(s, a, s')$: <strong>å ±é…¬é–¢æ•°</strong>ï¼ˆçŠ¶æ…‹é·ç§»ã§å¾—ã‚‰ã‚Œã‚‹å ±é…¬ï¼‰
- $\gamma \in [0, 1)$: <strong>å‰²å¼•ç‡</strong>ï¼ˆå°†æ¥ã®å ±é…¬ã®é‡è¦åº¦ï¼‰

<h3>ææ–™æ¢ç´¢ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°</h3>

| MDPè¦ç´  | ææ–™æ¢ç´¢ã§ã®æ„å‘³ | å…·ä½“ä¾‹ |
|---------|-----------------|--------|
| çŠ¶æ…‹ $s$ | ç¾åœ¨ã®çŸ¥è¦‹ï¼ˆã“ã‚Œã¾ã§ã®è©•ä¾¡çµæœï¼‰ | "ææ–™A: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—2.1eVã€ææ–™B: 2.5eV" |
| è¡Œå‹• $a$ | æ¬¡ã«è©¦ã™ææ–™ | "Ti-Ni-Oçµ„æˆã®ææ–™C" |
| å ±é…¬ $r$ | ææ–™ç‰¹æ€§ã®è©•ä¾¡å€¤ | "ææ–™Cã®ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—2.8eVï¼ˆç›®æ¨™3.0eVã«è¿‘ã„ï¼‰" |
| æ–¹ç­– $\pi$ | ææ–™é¸æŠæˆ¦ç•¥ | "ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ãŒç›®æ¨™ã«è¿‘ã„å…ƒç´ çµ„æˆã‚’å„ªå…ˆ" |

<h3>ãƒãƒ«ã‚³ãƒ•æ€§</h3>

MDPã®é‡è¦ãªä»®å®šã¯<strong>ãƒãƒ«ã‚³ãƒ•æ€§</strong>ã§ã™ï¼š

$$
P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \dots) = P(s_{t+1}|s_t, a_t)
$$

ã¤ã¾ã‚Šã€<strong>æ¬¡ã®çŠ¶æ…‹ã¯ç¾åœ¨ã®çŠ¶æ…‹ã¨è¡Œå‹•ã®ã¿ã«ä¾å­˜ã—ã€éå»ã®å±¥æ­´ã¯ä¸è¦</strong>ã§ã™ã€‚

ææ–™æ¢ç´¢ã§ã¯ã€ç¾åœ¨ã®è©•ä¾¡çµæœï¼ˆçŠ¶æ…‹ï¼‰ã«åŸºã¥ã„ã¦æ¬¡ã®ææ–™ï¼ˆè¡Œå‹•ï¼‰ã‚’é¸ã¹ã°ã€éå»ã®å…¨å±¥æ­´ã‚’è¦šãˆã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

<h3>æ–¹ç­–ã¨ä¾¡å€¤é–¢æ•°</h3>

<strong>æ–¹ç­–</strong> $\pi(a|s)$: çŠ¶æ…‹$s$ã§è¡Œå‹•$a$ã‚’é¸ã¶ç¢ºç‡

<strong>çŠ¶æ…‹ä¾¡å€¤é–¢æ•°</strong> $V^\pi(s)$: çŠ¶æ…‹$s$ã‹ã‚‰æ–¹ç­–$\pi$ã«å¾“ã£ã¦è¡Œå‹•ã—ãŸã¨ãã®æœŸå¾…ç´¯ç©å ±é…¬

$$
V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s \right]
$$

<strong>è¡Œå‹•ä¾¡å€¤é–¢æ•°ï¼ˆQé–¢æ•°ï¼‰</strong> $Q^\pi(s, a)$: çŠ¶æ…‹$s$ã§è¡Œå‹•$a$ã‚’å–ã‚Šã€ãã®å¾Œæ–¹ç­–$\pi$ã«å¾“ã£ãŸã¨ãã®æœŸå¾…ç´¯ç©å ±é…¬

$$
Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s, a_0 = a \right]
$$

<strong>æœ€é©æ–¹ç­–</strong> $\pi^*$: ã™ã¹ã¦ã®çŠ¶æ…‹ã§ä¾¡å€¤é–¢æ•°ã‚’æœ€å¤§åŒ–ã™ã‚‹æ–¹ç­–

$$
\pi^* = \arg\max_\pi V^\pi(s) \quad \forall s \in S
$$

---

<h2>1.3 Qå­¦ç¿’ï¼ˆQ-Learningï¼‰</h2>

<h3>Qå­¦ç¿’ã®åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢</h3>

Qå­¦ç¿’ã¯ã€<strong>Qé–¢æ•°ã‚’ç›´æ¥å­¦ç¿’</strong>ã™ã‚‹å¼·åŒ–å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚

<strong>ãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼</strong>:
$$
Q^*(s, a) = \mathbb{E}_{s'} \left[ r + \gamma \max_{a'} Q^*(s', a') \mid s, a \right]
$$

ã“ã‚Œã¯ã€Œæœ€é©ãªQé–¢æ•°ã¯ã€å³åº§ã®å ±é…¬$r$ã¨æ¬¡ã®çŠ¶æ…‹ã§ã®æœ€å¤§Qå€¤ã®å‰²å¼•å’Œã«ç­‰ã—ã„ã€ã¨ã„ã†æ„å‘³ã§ã™ã€‚

<h3>Qå­¦ç¿’ã®æ›´æ–°å¼</h3>

è¦³æ¸¬ã•ã‚ŒãŸé·ç§»$(s, a, r, s')$ã«åŸºã¥ã„ã¦ã€Qå€¤ã‚’æ›´æ–°ï¼š

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$

- $\alpha$: å­¦ç¿’ç‡ï¼ˆ0ã€œ1ï¼‰
- $r + \gamma \max_{a'} Q(s', a')$: <strong>TDç›®æ¨™</strong>ï¼ˆTemporal Difference Targetï¼‰
- $r + \gamma \max_{a'} Q(s', a') - Q(s, a)$: <strong>TDèª¤å·®</strong>

<h3>Pythonã«ã‚ˆã‚‹å®Ÿè£…</h3>

ç°¡å˜ãªã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ï¼ˆææ–™æ¢ç´¢ç©ºé–“ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ï¼‰ã§Qå­¦ç¿’ã‚’å®Ÿè£…ã—ã¾ã™ï¼š

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

class SimpleMaterialsEnv:
    """ç°¡å˜ãªææ–™æ¢ç´¢ç’°å¢ƒï¼ˆã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ï¼‰

    - 5x5ã®ã‚°ãƒªãƒƒãƒ‰
    - å„ã‚»ãƒ«ã¯ææ–™å€™è£œã‚’è¡¨ã™
    - ç›®æ¨™: æœ€é«˜ç‰¹æ€§ã®ææ–™ï¼ˆã‚´ãƒ¼ãƒ«ï¼‰ã«åˆ°é”
    """
    def __init__(self):
        self.grid_size = 5
        self.state = (0, 0)  # ã‚¹ã‚¿ãƒ¼ãƒˆä½ç½®
        self.goal = (4, 4)   # ã‚´ãƒ¼ãƒ«ä½ç½®ï¼ˆæœ€é©ææ–™ï¼‰

    def reset(self):
        """åˆæœŸçŠ¶æ…‹ã«ãƒªã‚»ãƒƒãƒˆ"""
        self.state = (0, 0)
        return self.state

    def step(self, action):
        """è¡Œå‹•ã‚’å®Ÿè¡Œ

        Args:
            action: 0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³

        Returns:
            next_state, reward, done
        """
        x, y = self.state

        # è¡Œå‹•ã«å¿œã˜ã¦ç§»å‹•
        if action == 0 and x > 0:  # ä¸Š
            x -= 1
        elif action == 1 and x < self.grid_size - 1:  # ä¸‹
            x += 1
        elif action == 2 and y > 0:  # å·¦
            y -= 1
        elif action == 3 and y < self.grid_size - 1:  # å³
            y += 1

        self.state = (x, y)

        # å ±é…¬è¨­è¨ˆ
        if self.state == self.goal:
            reward = 10.0  # ã‚´ãƒ¼ãƒ«åˆ°é”ï¼ˆæœ€é©ææ–™ç™ºè¦‹ï¼‰
            done = True
        else:
            reward = -0.1  # å„ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚³ã‚¹ãƒˆï¼ˆå®Ÿé¨“ã‚³ã‚¹ãƒˆï¼‰
            done = False

        return self.state, reward, done

    def get_state_space(self):
        """çŠ¶æ…‹ç©ºé–“ã®ã‚µã‚¤ã‚º"""
        return self.grid_size * self.grid_size

    def get_action_space(self):
        """è¡Œå‹•ç©ºé–“ã®ã‚µã‚¤ã‚º"""
        return 4


def q_learning(env, episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):
    """Qå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

    Args:
        env: ç’°å¢ƒ
        episodes: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°
        alpha: å­¦ç¿’ç‡
        gamma: å‰²å¼•ç‡
        epsilon: Îµ-greedyæ¢ç´¢ã®ç¢ºç‡

    Returns:
        å­¦ç¿’ã—ãŸQ-table
    """
    # Q-tableã®åˆæœŸåŒ–ï¼ˆçŠ¶æ…‹Ã—è¡Œå‹•ï¼‰
    Q = np.zeros((env.grid_size, env.grid_size, env.get_action_space()))

    rewards_per_episode = []

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False

        while not done:
            # Îµ-greedyæ¢ç´¢
            if np.random.random() < epsilon:
                action = np.random.randint(env.get_action_space())  # ãƒ©ãƒ³ãƒ€ãƒ æ¢ç´¢
            else:
                action = np.argmax(Q[state[0], state[1], :])  # æœ€è‰¯ã®è¡Œå‹•ã‚’æ´»ç”¨

            # è¡Œå‹•å®Ÿè¡Œ
            next_state, reward, done = env.step(action)
            total_reward += reward

            # Qå€¤æ›´æ–°ï¼ˆãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼ï¼‰
            current_q = Q[state[0], state[1], action]
            max_next_q = np.max(Q[next_state[0], next_state[1], :])
            new_q = current_q + alpha * (reward + gamma * max_next_q - current_q)
            Q[state[0], state[1], action] = new_q

            state = next_state

        rewards_per_episode.append(total_reward)

        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(rewards_per_episode[-100:])
            print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}")

    return Q, rewards_per_episode


<h1>å®Ÿè¡Œ</h1>
env = SimpleMaterialsEnv()
Q, rewards = q_learning(env, episodes=1000)

<h1>å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–</h1>
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards, np.ones(50)/50, mode='valid'))
plt.xlabel('Episode')
plt.ylabel('Average Reward (50 episodes)')
plt.title('Q-Learning: ææ–™æ¢ç´¢ç’°å¢ƒã§ã®å­¦ç¿’é€²æ—')
plt.grid(True)
plt.show()

<h1>å­¦ç¿’ã—ãŸQå€¤ã®å¯è¦–åŒ–</h1>
policy = np.argmax(Q, axis=2)
print("\nå­¦ç¿’ã—ãŸæ–¹ç­–ï¼ˆå„ã‚»ãƒ«ã§ã®æœ€è‰¯è¡Œå‹•ï¼‰:")
print("0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³")
print(policy)</code></pre>

<strong>å‡ºåŠ›ä¾‹</strong>:
<pre><code>Episode 100: Avg Reward = -4.52
Episode 200: Avg Reward = -3.21
Episode 500: Avg Reward = -1.85
Episode 1000: Avg Reward = -1.12

å­¦ç¿’ã—ãŸæ–¹ç­–ï¼ˆå„ã‚»ãƒ«ã§ã®æœ€è‰¯è¡Œå‹•ï¼‰:
[[1 1 1 1 1]
 [1 1 1 1 1]
 [1 1 1 1 1]
 [1 1 1 1 1]
 [3 3 3 3 0]]</code></pre>

<strong>è§£èª¬</strong>:
- åˆæœŸã¯å ±é…¬ãŒä½ã„ï¼ˆ-4.52ï¼‰ãŒã€å­¦ç¿’ãŒé€²ã‚€ã¨æ”¹å–„ï¼ˆ-1.12ï¼‰
- æœ€çµ‚çš„ã«ã€ã‚´ãƒ¼ãƒ«ã¸ã®æœ€çŸ­çµŒè·¯ã‚’å­¦ç¿’ï¼ˆä¸‹â†’å³ã®æ–¹ç­–ï¼‰

---

<h2>1.4 Deep Q-Networkï¼ˆDQNï¼‰</h2>

<h3>Qå­¦ç¿’ã®é™ç•Œ</h3>

Qå­¦ç¿’ã¯ã€<strong>çŠ¶æ…‹ã¨è¡Œå‹•ãŒé›¢æ•£çš„ã‹ã¤å°‘æ•°</strong>ã®å ´åˆã«æœ‰åŠ¹ã§ã™ã€‚ã—ã‹ã—ã€ææ–™ç§‘å­¦ã§ã¯ï¼š

- <strong>çŠ¶æ…‹ç©ºé–“ãŒå·¨å¤§</strong>: ææ–™è¨˜è¿°å­ï¼ˆ100æ¬¡å…ƒä»¥ä¸Šï¼‰
- <strong>é€£ç¶šå€¤</strong>: çµ„æˆæ¯”ç‡ã€æ¸©åº¦ã€åœ§åŠ›ãªã©
- <strong>Q-tableãŒéç¾å®Ÿçš„</strong>: $10^{100}$å€‹ã®ã‚»ãƒ«ã‚’ä¿å­˜ã§ããªã„

<h3>DQNã®è§£æ±ºç­–</h3>

DQNã¯ã€<strong>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§Qé–¢æ•°ã‚’è¿‘ä¼¼</strong>ã—ã¾ã™ï¼š

$$
Q(s, a; \theta) \approx Q^*(s, a)
$$

- $\theta$: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

<strong>æå¤±é–¢æ•°</strong>:
$$
L(\theta) = \mathbb{E}_{(s,a,r,s') \sim D} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2 \right]
$$

- $D$: <strong>çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡</strong>ï¼ˆéå»ã®é·ç§»ã‚’ä¿å­˜ï¼‰
- $\theta^-$: <strong>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</strong>ï¼ˆå­¦ç¿’ã®å®‰å®šåŒ–ï¼‰

<h3>DQNã®é‡è¦æŠ€è¡“</h3>

1. <strong>çµŒé¨“å†ç”Ÿï¼ˆExperience Replayï¼‰</strong>: éå»ã®é·ç§»ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ãƒ‡ãƒ¼ã‚¿ã®ç›¸é–¢ã‚’æ¸›ã‚‰ã™
2. <strong>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</strong>: å›ºå®šã•ã‚ŒãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§TDç›®æ¨™ã‚’è¨ˆç®—ã—ã€å­¦ç¿’ã‚’å®‰å®šåŒ–
3. <strong>Îµ-greedyæ¢ç´¢</strong>: æ¢ç´¢ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰ã¨æ´»ç”¨ï¼ˆæœ€è‰¯è¡Œå‹•ï¼‰ã®ãƒãƒ©ãƒ³ã‚¹

<h3>PyTorchã«ã‚ˆã‚‹DQNå®Ÿè£…</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random

class DQN(nn.Module):
    """Deep Q-Network

    çŠ¶æ…‹ã‚’å…¥åŠ›ã—ã€å„è¡Œå‹•ã®Qå€¤ã‚’å‡ºåŠ›
    """
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)


class ReplayBuffer:
    """çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡"""
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

    def __len__(self):
        return len(self.buffer)


class DQNAgent:
    """DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min

        # ãƒ¡ã‚¤ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.policy_net = DQN(state_dim, action_dim)
        self.target_net = DQN(state_dim, action_dim)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)
        self.buffer = ReplayBuffer()

    def select_action(self, state):
        """Îµ-greedyè¡Œå‹•é¸æŠ"""
        if np.random.random() < self.epsilon:
            return np.random.randint(self.action_dim)
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                q_values = self.policy_net(state_tensor)
                return q_values.argmax().item()

    def train(self, batch_size=64):
        """ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’"""
        if len(self.buffer) < batch_size:
            return

        # ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        batch = self.buffer.sample(batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions).unsqueeze(1)
        rewards = torch.FloatTensor(rewards).unsqueeze(1)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones).unsqueeze(1)

        # ç¾åœ¨ã®Qå€¤
        current_q = self.policy_net(states).gather(1, actions)

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆQå€¤
        with torch.no_grad():
            max_next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)
            target_q = rewards + (1 - dones) * self.gamma * max_next_q

        # æå¤±è¨ˆç®—ã¨æœ€é©åŒ–
        loss = nn.MSELoss()(current_q, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Îµã®æ¸›è¡°
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    def update_target_network(self):
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ›´æ–°"""
        self.target_net.load_state_dict(self.policy_net.state_dict())


<h1>ææ–™æ¢ç´¢ç’°å¢ƒï¼ˆé€£ç¶šçŠ¶æ…‹ç‰ˆï¼‰</h1>
class ContinuousMaterialsEnv:
    """é€£ç¶šçŠ¶æ…‹ç©ºé–“ã®ææ–™æ¢ç´¢ç’°å¢ƒ"""
    def __init__(self, state_dim=4):
        self.state_dim = state_dim
        self.target = np.array([3.0, 5.0, 2.5, 4.0])  # ç›®æ¨™ç‰¹æ€§
        self.state = None

    def reset(self):
        self.state = np.random.uniform(0, 10, self.state_dim)
        return self.state

    def step(self, action):
        # è¡Œå‹•: 0=å¢—åŠ , 1=æ¸›å°‘, 2=å¤§å¹…å¢—åŠ , 3=å¤§å¹…æ¸›å°‘
        delta = [0.1, -0.1, 0.5, -0.5][action]

        # ãƒ©ãƒ³ãƒ€ãƒ ãªæ¬¡å…ƒã‚’å¤‰æ›´
        dim = np.random.randint(self.state_dim)
        self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)

        # å ±é…¬: ç›®æ¨™ã¨ã®è·é›¢ï¼ˆè² ã®å€¤ã€è¿‘ã„ã»ã©è‰¯ã„ï¼‰
        distance = np.linalg.norm(self.state - self.target)
        reward = -distance

        # çµ‚äº†æ¡ä»¶: ç›®æ¨™ã«ååˆ†è¿‘ã„
        done = distance < 0.5

        return self.state, reward, done


<h1>DQNè¨“ç·´</h1>
env = ContinuousMaterialsEnv()
agent = DQNAgent(state_dim=4, action_dim=4)

episodes = 500
rewards_history = []

for episode in range(episodes):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent.select_action(state)
        next_state, reward, done = env.step(action)

        agent.buffer.push(state, action, reward, next_state, done)
        agent.train()

        state = next_state
        total_reward += reward

    rewards_history.append(total_reward)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°
    if (episode + 1) % 10 == 0:
        agent.update_target_network()

    if (episode + 1) % 50 == 0:
        avg_reward = np.mean(rewards_history[-50:])
        print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}, Îµ = {agent.epsilon:.3f}")

<h1>å­¦ç¿’æ›²ç·š</h1>
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'))
plt.xlabel('Episode')
plt.ylabel('Average Reward (20 episodes)')
plt.title('DQN: é€£ç¶šçŠ¶æ…‹ææ–™æ¢ç´¢ã§ã®å­¦ç¿’é€²æ—')
plt.grid(True)
plt.show()</code></pre>

<strong>å‡ºåŠ›ä¾‹</strong>:
<pre><code>Episode 50: Avg Reward = -45.23, Îµ = 0.779
Episode 100: Avg Reward = -32.15, Îµ = 0.606
Episode 200: Avg Reward = -18.92, Îµ = 0.365
Episode 500: Avg Reward = -8.45, Îµ = 0.010</code></pre>

<strong>è§£èª¬</strong>:
- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒé€£ç¶šçŠ¶æ…‹ã®Qé–¢æ•°ã‚’å­¦ç¿’
- ÎµãŒæ¸›è¡°ã—ã€æ¢ç´¢ã‹ã‚‰æ´»ç”¨ã¸ã‚·ãƒ•ãƒˆ
- æœ€çµ‚çš„ã«ç›®æ¨™ç‰¹æ€§ã«è¿‘ã„ææ–™ã‚’åŠ¹ç‡çš„ã«ç™ºè¦‹

---

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1 (é›£æ˜“åº¦: easy)</h3>

Qå­¦ç¿’ã®æ›´æ–°å¼ã«ãŠã„ã¦ã€å­¦ç¿’ç‡$\alpha$ã‚’å¤§ããã™ã‚‹ã¨ä½•ãŒèµ·ã“ã‚‹ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€$\alpha=0$ã¨$\alpha=1$ã®æ¥µç«¯ãªã‚±ãƒ¼ã‚¹ã§ã¯ã©ã†ãªã‚‹ã‹ç­”ãˆã¦ãã ã•ã„ã€‚

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

å­¦ç¿’ç‡ã¯ã€Œæ–°ã—ã„æƒ…å ±ã‚’ã©ã‚Œã ã‘é‡è¦–ã™ã‚‹ã‹ã€ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚æ›´æ–°å¼ã‚’è¦‹ç›´ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

<strong>$\alpha$ã‚’å¤§ããã™ã‚‹ã¨</strong>:
- æ–°ã—ã„è¦³æ¸¬ï¼ˆTDç›®æ¨™ï¼‰ã‚’å¼·ãåæ˜ ã—ã€Qå€¤ãŒå¤§ããå¤‰åŒ–
- å­¦ç¿’ãŒé€Ÿã„ãŒä¸å®‰å®šã«ãªã‚Šã‚„ã™ã„

<strong>æ¥µç«¯ãªã‚±ãƒ¼ã‚¹</strong>:
- <strong>$\alpha=0$</strong>: Qå€¤ãŒå…¨ãæ›´æ–°ã•ã‚Œãªã„ï¼ˆå­¦ç¿’ã—ãªã„ï¼‰
  $$Q(s,a) \leftarrow Q(s,a) + 0 \cdot [\cdots] = Q(s,a)$$

- <strong>$\alpha=1$</strong>: Qå€¤ãŒå®Œå…¨ã«TDç›®æ¨™ã§ç½®ãæ›ãˆã‚‰ã‚Œã‚‹
  $$Q(s,a) \leftarrow r + \gamma \max_{a'} Q(s', a')$$
  éå»ã®æƒ…å ±ãŒå®Œå…¨ã«æ¶ˆãˆã€æœ€æ–°ã®è¦³æ¸¬ã®ã¿ã«ä¾å­˜

<strong>å®Ÿè·µçš„ã«ã¯</strong>: $\alpha = 0.01 \sim 0.1$ãŒä¸€èˆ¬çš„

</details>

---

<h3>å•é¡Œ2 (é›£æ˜“åº¦: medium)</h3>

ææ–™æ¢ç´¢ã«ãŠã„ã¦ã€å ±é…¬é–¢æ•°ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«è¨­è¨ˆã—ã¾ã—ãŸã€‚ã“ã®è¨­è¨ˆã®å•é¡Œç‚¹ã¨æ”¹å–„æ¡ˆã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚

<pre><code class="language-python">def reward_function(material_property, target=3.0):
    if material_property == target:
        return 1.0
    else:
        return 0.0</code></pre>

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

ã“ã®å ±é…¬ã¯ã€Œã‚¹ãƒ‘ãƒ¼ã‚¹å ±é…¬ã€ã¨å‘¼ã°ã‚Œã€ç›®æ¨™ã«åˆ°é”ã—ãªã„é™ã‚Šã™ã¹ã¦0ã§ã™ã€‚å­¦ç¿’ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

<strong>å•é¡Œç‚¹</strong>:
1. <strong>ã‚¹ãƒ‘ãƒ¼ã‚¹å ±é…¬</strong>: ã»ã¨ã‚“ã©ã®å ´åˆå ±é…¬ãŒ0ã§ã€å­¦ç¿’ã‚·ã‚°ãƒŠãƒ«ãŒå¼±ã„
2. <strong>æ¢ç´¢ãŒå›°é›£</strong>: ã©ã®æ–¹å‘ã«é€²ã‚ã°è‰¯ã„ã‹ã‚ã‹ã‚‰ãªã„
3. <strong>å³å¯†ãªä¸€è‡´</strong>: å®Ÿæ•°å€¤ã§å®Œå…¨ä¸€è‡´ã¯ã»ã¼ä¸å¯èƒ½

<strong>æ”¹å–„æ¡ˆ</strong>:
<pre><code class="language-python">def improved_reward_function(material_property, target=3.0):
    # ç›®æ¨™ã¨ã®è·é›¢ã«åŸºã¥ãé€£ç¶šçš„ãªå ±é…¬
    distance = abs(material_property - target)

    if distance < 0.1:
        return 10.0  # éå¸¸ã«è¿‘ã„ï¼ˆãƒœãƒ¼ãƒŠã‚¹ï¼‰
    elif distance < 0.5:
        return 5.0   # è¿‘ã„
    else:
        return -distance  # é ã„ã»ã©ãƒšãƒŠãƒ«ãƒ†ã‚£</code></pre>

<strong>ã•ã‚‰ãªã‚‹æ”¹å–„</strong>:
- <strong>ã‚·ã‚§ã‚¤ãƒ”ãƒ³ã‚°å ±é…¬</strong>: ç›®æ¨™ã¸ã®é€²æ—ã«å¿œã˜ã¦ä¸­é–“å ±é…¬ã‚’ä¸ãˆã‚‹
- <strong>å¤šç›®çš„å ±é…¬</strong>: è¤‡æ•°ã®ç‰¹æ€§ã‚’è€ƒæ…®ï¼ˆãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ— + å®‰å®šæ€§ï¼‰

</details>

---

<h3>å•é¡Œ3 (é›£æ˜“åº¦: hard)</h3>

DQNã«ãŠã‘ã‚‹ã€ŒçµŒé¨“å†ç”Ÿã€ã¨ã€Œã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ã®å½¹å‰²ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚ŒãŒãªã„ã¨ã©ã®ã‚ˆã†ãªå•é¡ŒãŒèµ·ã“ã‚‹ã‹ã€Pythonã‚³ãƒ¼ãƒ‰ã§å®Ÿé¨“ã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

çµŒé¨“å†ç”Ÿã‚’ã‚ªãƒ•ã«ã™ã‚‹ã«ã¯<code>buffer.sample()</code>ã®ä»£ã‚ã‚Šã«æœ€æ–°ã®é·ç§»ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ã‚ªãƒ•ã«ã™ã‚‹ã«ã¯ã€TDç›®æ¨™ã®è¨ˆç®—ã§<code>self.policy_net</code>ã‚’ä½¿ã„ã¾ã™ã€‚

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

<strong>çµŒé¨“å†ç”Ÿã®å½¹å‰²</strong>:
- éå»ã®é·ç§»ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ãƒ‡ãƒ¼ã‚¿ã®ç›¸é–¢ã‚’æ¸›ã‚‰ã™
- ãªã„ã¨ã€é€£ç¶šã—ãŸé·ç§»ã ã‘ã§å­¦ç¿’ã—ã€ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«éå­¦ç¿’

<strong>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å½¹å‰²</strong>:
- å›ºå®šã•ã‚ŒãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§TDç›®æ¨™ã‚’è¨ˆç®—ã—ã€å­¦ç¿’ã‚’å®‰å®šåŒ–
- ãªã„ã¨ã€Qå€¤ãŒæŒ¯å‹•ã—ã¦åæŸã—ã«ãã„

<strong>å®Ÿé¨“ã‚³ãƒ¼ãƒ‰</strong>:
<pre><code class="language-python"><h1>çµŒé¨“å†ç”Ÿãªã—ç‰ˆ</h1>
class DQNAgentNoReplay(DQNAgent):
    def train_no_replay(self, state, action, reward, next_state, done):
        # æœ€æ–°ã®é·ç§»ã®ã¿ã§å­¦ç¿’
        states = torch.FloatTensor([state])
        actions = torch.LongTensor([action]).unsqueeze(1)
        rewards = torch.FloatTensor([reward]).unsqueeze(1)
        next_states = torch.FloatTensor([next_state])
        dones = torch.FloatTensor([done]).unsqueeze(1)

        current_q = self.policy_net(states).gather(1, actions)
        with torch.no_grad():
            max_next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)
            target_q = rewards + (1 - dones) * self.gamma * max_next_q

        loss = nn.MSELoss()(current_q, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

<h1>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãªã—ç‰ˆï¼ˆTDç›®æ¨™ã§policy_netã‚’ä½¿ç”¨ï¼‰</h1>
<h1>â†’ å­¦ç¿’ãŒä¸å®‰å®šã«ãªã‚‹</h1>

<h1>çµæœ: çµŒé¨“å†ç”Ÿãªã—ã§ã¯åæŸãŒé…ãã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãªã—ã§ã¯æŒ¯å‹•ã™ã‚‹</h1></code></pre>

</details>

---

<h2>ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã¾ã¨ã‚</h2>

- ææ–™æ¢ç´¢ã¯æ¢ç´¢ç©ºé–“ãŒåºƒå¤§ã§ã€å¾“æ¥ã®è©¦è¡ŒéŒ¯èª¤ã¯éåŠ¹ç‡
- å¼·åŒ–å­¦ç¿’ã¯<strong>ç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦æœ€é©ãªæ¢ç´¢æˆ¦ç•¥ã‚’å­¦ç¿’</strong>
- <strong>ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMDPï¼‰</strong>ãŒå¼·åŒ–å­¦ç¿’ã®æ•°å­¦çš„åŸºç›¤
- <strong>Qå­¦ç¿’</strong>ã¯é›¢æ•£çŠ¶æ…‹ãƒ»è¡Œå‹•ã§æœ‰åŠ¹ã€Q-tableã§ä¾¡å€¤ã‚’è¨˜éŒ²
- <strong>DQN</strong>ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§Qé–¢æ•°ã‚’è¿‘ä¼¼ã—ã€å·¨å¤§ãªçŠ¶æ…‹ç©ºé–“ã«å¯¾å¿œ
- <strong>çµŒé¨“å†ç”Ÿ</strong>ã¨<strong>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</strong>ãŒDQNå­¦ç¿’ã‚’å®‰å®šåŒ–

æ¬¡ç« ã§ã¯ã€ã‚ˆã‚Šé«˜åº¦ãªæ–¹ç­–å‹¾é…æ³•ï¼ˆPolicy Gradientï¼‰ã¨Actor-Criticæ‰‹æ³•ã‚’å­¦ã³ã¾ã™ã€‚

---

<h2>å‚è€ƒæ–‡çŒ®</h2>

1. Mnih et al. "Playing Atari with Deep Reinforcement Learning" *arXiv* (2013) - DQNåŸè«–æ–‡
2. Sutton & Barto "Reinforcement Learning: An Introduction" MIT Press (2018) - RLæ•™ç§‘æ›¸
3. Zhou et al. "Optimization of molecules via deep reinforcement learning" *Scientific Reports* (2019)
4. Ling et al. "High-dimensional materials and process optimization using data-driven experimental design with well-calibrated uncertainty estimates" *Integrating Materials and Manufacturing Innovation* (2017)

---

<strong>æ¬¡ç« </strong>: [ç¬¬2ç« : å¼·åŒ–å­¦ç¿’ã®åŸºç¤ç†è«–](chapter-2.html)
<div class="navigation">
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-2.html" class="nav-button">ç¬¬2ç«  â†’</a>
</div>
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>
