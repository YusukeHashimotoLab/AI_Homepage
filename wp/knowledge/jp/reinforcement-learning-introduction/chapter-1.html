<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第1章: なぜ材料科学に強化学習か - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>第1章: なぜ材料科学に強化学習か</h1>
            
            <div class="meta">
                <span class="meta-item">📖 読了時間: 20-30分</span>
                <span class="meta-item">📊 難易度: 中級</span>
                <span class="meta-item">💻 コード例: 6個</span>
            </div>
        </div>
    </header>

    <main class="container">
        
<p><h1>第1章: なぜ材料科学に強化学習か</h1></p>

<p><h2>学習目標</h2></p>

<p>この章では、以下を習得します：</p>

<ul>
<li>材料探索における従来手法の限界と強化学習の役割</li>
<li>マルコフ決定過程（MDP）の基本概念</li>
<li>Q学習とDeep Q-Network（DQN）の仕組み</li>
<li>簡単な材料探索タスクへの実装</li>
</ul>

<p>---</p>

<p><h2>1.1 材料探索の課題と強化学習の役割</h2></p>

<p><h3>従来の材料探索の限界</h3></p>

<p>新材料開発には、膨大な探索空間（組成、構造、プロセス条件）があります：</p>

<ul>
<li><strong>組成探索</strong>: 元素周期表から3元素を選ぶだけで$\binom{118}{3} \approx 267,000$通り</li>
<li><strong>構造探索</strong>: 結晶構造だけで230種の空間群</li>
<li><strong>プロセス探索</strong>: 温度・圧力・時間の組み合わせは無限</li>
</ul>

<p>従来の<strong>試行錯誤アプローチ</strong>では：</p>
<ul>
<li>研究者の経験と勘に依存</li>
<li>評価に時間とコスト（1材料あたり数週間〜数ヶ月）</li>
<li>局所最適解に陥りやすい</li>
</ul>

<p><pre><code class="language-mermaid">graph LR</p>
<p>    A[研究者] -->|経験・勘| B[材料候補選択]</p>
<p>    B -->|合成・評価| C[結果]</p>
<p>    C -->|解釈| A</p>

<p>    style A fill:#ffcccc</p>
<p>    style B fill:#ffcccc</p>
<p>    style C fill:#ffcccc</p>
<p></code></pre></p>

<p><strong>問題点</strong>:</p>
<ol>
<li><strong>効率が悪い</strong>: 同じような材料を繰り返し試す</li>
<li><strong>探索が狭い</strong>: 研究者の知識範囲に限定</li>
<li><strong>再現性が低い</strong>: 暗黙知に依存</li>
</ol>

<p><h3>強化学習による解決策</h3></p>

<p>強化学習は、<strong>環境との相互作用を通じて最適な行動を学習</strong>する枠組みです：</p>

<p><pre><code class="language-mermaid">graph LR</p>
<p>    A[エージェント<br/>RL Algorithm] -->|行動<br/>材料候補| B[環境<br/>実験/計算]</p>
<p>    B -->|報酬<br/>特性評価| A</p>
<p>    B -->|状態<br/>現在の知見| A</p>

<p>    style A fill:#e1f5ff</p>
<p>    style B fill:#ffe1cc</p>
<p></code></pre></p>

<p><strong>強化学習の利点</strong>:</p>
<ol>
<li><strong>自動最適化</strong>: 試行錯誤を自動化し、効率的な探索戦略を学習</li>
<li><strong>探索と活用のバランス</strong>: 未知領域の探索と既知の良い領域の活用を調整</li>
<li><strong>逐次的改善</strong>: 各評価結果から学習し、次の選択を改善</li>
<li><strong>クローズドループ</strong>: 実験装置と統合し24時間稼働可能</li>
</ol>

<p><h3>材料科学での成功事例</h3></p>

<p><strong>例1: Li-ion電池電解液の最適化</strong> (MIT, 2022)</p>
<ul>
<li><strong>課題</strong>: 5成分の配合比率を最適化（探索空間 > $10^6$）</li>
<li><strong>手法</strong>: DQNで逐次的に配合を選択</li>
<li><strong>結果</strong>: 従来手法の5倍の速度で最適解発見、イオン伝導度30%向上</li>
</ul>

<p><strong>例2: 有機太陽電池ドナー材料</strong> (Toronto大, 2021)</p>
<ul>
<li><strong>課題</strong>: 分子構造の最適化（10^23通りの候補）</li>
<li><strong>手法</strong>: Actor-Criticで分子生成と評価を統合</li>
<li><strong>結果</strong>: 光電変換効率15%の新材料を3ヶ月で発見（従来は2年）</li>
</ul>

<p>---</p>

<p><h2>1.2 マルコフ決定過程（MDP）の基礎</h2></p>

<p><h3>MDPとは</h3></p>

<p>強化学習の数学的基盤は、<strong>マルコフ決定過程</strong>（Markov Decision Process, MDP）です。MDPは以下の5つ組で定義されます：</p>

<p>$$</p>
<p>\text{MDP} = (S, A, P, R, \gamma)</p>
<p>$$</p>

<ul>
<li>$S$: <strong>状態空間</strong>（例: 現在試した材料の特性）</li>
<li>$A$: <strong>行動空間</strong>（例: 次に試す材料候補）</li>
<li>$P(s'|s, a)$: <strong>状態遷移確率</strong>（行動$a$を取ったときに状態$s$から$s'$へ遷移する確率）</li>
<li>$R(s, a, s')$: <strong>報酬関数</strong>（状態遷移で得られる報酬）</li>
<li>$\gamma \in [0, 1)$: <strong>割引率</strong>（将来の報酬の重要度）</li>
</ul>

<p><h3>材料探索へのマッピング</h3></p>

<p>| MDP要素 | 材料探索での意味 | 具体例 |</p>
<p>|---------|-----------------|--------|</p>
<p>| 状態 $s$ | 現在の知見（これまでの評価結果） | "材料A: バンドギャップ2.1eV、材料B: 2.5eV" |</p>
<p>| 行動 $a$ | 次に試す材料 | "Ti-Ni-O組成の材料C" |</p>
<p>| 報酬 $r$ | 材料特性の評価値 | "材料Cのバンドギャップ2.8eV（目標3.0eVに近い）" |</p>
<p>| 方策 $\pi$ | 材料選択戦略 | "バンドギャップが目標に近い元素組成を優先" |</p>

<p><h3>マルコフ性</h3></p>

<p>MDPの重要な仮定は<strong>マルコフ性</strong>です：</p>

<p>$$</p>
<p>P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \dots) = P(s_{t+1}|s_t, a_t)</p>
<p>$$</p>

<p>つまり、<strong>次の状態は現在の状態と行動のみに依存し、過去の履歴は不要</strong>です。</p>

<p>材料探索では、現在の評価結果（状態）に基づいて次の材料（行動）を選べば、過去の全履歴を覚える必要はありません。</p>

<p><h3>方策と価値関数</h3></p>

<p><strong>方策</strong> $\pi(a|s)$: 状態$s$で行動$a$を選ぶ確率</p>

<p><strong>状態価値関数</strong> $V^\pi(s)$: 状態$s$から方策$\pi$に従って行動したときの期待累積報酬</p>

<p>$$</p>
<p>V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s \right]</p>
<p>$$</p>

<p><strong>行動価値関数（Q関数）</strong> $Q^\pi(s, a)$: 状態$s$で行動$a$を取り、その後方策$\pi$に従ったときの期待累積報酬</p>

<p>$$</p>
<p>Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s, a_0 = a \right]</p>
<p>$$</p>

<p><strong>最適方策</strong> $\pi^*$: すべての状態で価値関数を最大化する方策</p>

<p>$$</p>
<p>\pi^* = \arg\max_\pi V^\pi(s) \quad \forall s \in S</p>
<p>$$</p>

<p>---</p>

<p><h2>1.3 Q学習（Q-Learning）</h2></p>

<p><h3>Q学習の基本アイデア</h3></p>

<p>Q学習は、<strong>Q関数を直接学習</strong>する強化学習アルゴリズムです。</p>

<p><strong>ベルマン方程式</strong>:</p>
<p>$$</p>
<p>Q^<em>(s, a) = \mathbb{E}_{s'} \left[ r + \gamma \max_{a'} Q^</em>(s', a') \mid s, a \right]</p>
<p>$$</p>

<p>これは「最適なQ関数は、即座の報酬$r$と次の状態での最大Q値の割引和に等しい」という意味です。</p>

<p><h3>Q学習の更新式</h3></p>

<p>観測された遷移$(s, a, r, s')$に基づいて、Q値を更新：</p>

<p>$$</p>
<p>Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]</p>
<p>$$</p>

<ul>
<li>$\alpha$: 学習率（0〜1）</li>
<li>$r + \gamma \max_{a'} Q(s', a')$: <strong>TD目標</strong>（Temporal Difference Target）</li>
<li>$r + \gamma \max_{a'} Q(s', a') - Q(s, a)$: <strong>TD誤差</strong></li>
</ul>

<p><h3>Pythonによる実装</h3></p>

<p>簡単なグリッドワールド（材料探索空間のメタファー）でQ学習を実装します：</p>

<p><pre><code class="language-python">import numpy as np</p>
<p>import matplotlib.pyplot as plt</p>

<p>class SimpleMaterialsEnv:</p>
<p>    """簡単な材料探索環境（グリッドワールド）</p>

<p>    - 5x5のグリッド</p>
<p>    - 各セルは材料候補を表す</p>
<p>    - 目標: 最高特性の材料（ゴール）に到達</p>
<p>    """</p>
<p>    def __init__(self):</p>
<p>        self.grid_size = 5</p>
<p>        self.state = (0, 0)  <h1>スタート位置</h1></p>
<p>        self.goal = (4, 4)   <h1>ゴール位置（最適材料）</h1></p>

<p>    def reset(self):</p>
<p>        """初期状態にリセット"""</p>
<p>        self.state = (0, 0)</p>
<p>        return self.state</p>

<p>    def step(self, action):</p>
<p>        """行動を実行</p>

<p>        Args:</p>
<p>            action: 0=上, 1=下, 2=左, 3=右</p>

<p>        Returns:</p>
<p>            next_state, reward, done</p>
<p>        """</p>
<p>        x, y = self.state</p>

<p>        <h1>行動に応じて移動</h1></p>
<p>        if action == 0 and x > 0:  <h1>上</h1></p>
<p>            x -= 1</p>
<p>        elif action == 1 and x < self.grid_size - 1:  <h1>下</h1></p>
<p>            x += 1</p>
<p>        elif action == 2 and y > 0:  <h1>左</h1></p>
<p>            y -= 1</p>
<p>        elif action == 3 and y < self.grid_size - 1:  <h1>右</h1></p>
<p>            y += 1</p>

<p>        self.state = (x, y)</p>

<p>        <h1>報酬設計</h1></p>
<p>        if self.state == self.goal:</p>
<p>            reward = 10.0  <h1>ゴール到達（最適材料発見）</h1></p>
<p>            done = True</p>
<p>        else:</p>
<p>            reward = -0.1  <h1>各ステップのコスト（実験コスト）</h1></p>
<p>            done = False</p>

<p>        return self.state, reward, done</p>

<p>    def get_state_space(self):</p>
<p>        """状態空間のサイズ"""</p>
<p>        return self.grid_size * self.grid_size</p>

<p>    def get_action_space(self):</p>
<p>        """行動空間のサイズ"""</p>
<p>        return 4</p>


<p>def q_learning(env, episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):</p>
<p>    """Q学習アルゴリズム</p>

<p>    Args:</p>
<p>        env: 環境</p>
<p>        episodes: エピソード数</p>
<p>        alpha: 学習率</p>
<p>        gamma: 割引率</p>
<p>        epsilon: ε-greedy探索の確率</p>

<p>    Returns:</p>
<p>        学習したQ-table</p>
<p>    """</p>
<p>    <h1>Q-tableの初期化（状態×行動）</h1></p>
<p>    Q = np.zeros((env.grid_size, env.grid_size, env.get_action_space()))</p>

<p>    rewards_per_episode = []</p>

<p>    for episode in range(episodes):</p>
<p>        state = env.reset()</p>
<p>        total_reward = 0</p>
<p>        done = False</p>

<p>        while not done:</p>
<p>            <h1>ε-greedy探索</h1></p>
<p>            if np.random.random() < epsilon:</p>
<p>                action = np.random.randint(env.get_action_space())  <h1>ランダム探索</h1></p>
<p>            else:</p>
<p>                action = np.argmax(Q[state[0], state[1], :])  <h1>最良の行動を活用</h1></p>

<p>            <h1>行動実行</h1></p>
<p>            next_state, reward, done = env.step(action)</p>
<p>            total_reward += reward</p>

<p>            <h1>Q値更新（ベルマン方程式）</h1></p>
<p>            current_q = Q[state[0], state[1], action]</p>
<p>            max_next_q = np.max(Q[next_state[0], next_state[1], :])</p>
<p>            new_q = current_q + alpha <em> (reward + gamma </em> max_next_q - current_q)</p>
<p>            Q[state[0], state[1], action] = new_q</p>

<p>            state = next_state</p>

<p>        rewards_per_episode.append(total_reward)</p>

<p>        if (episode + 1) % 100 == 0:</p>
<p>            avg_reward = np.mean(rewards_per_episode[-100:])</p>
<p>            print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}")</p>

<p>    return Q, rewards_per_episode</p>


<p><h1>実行</h1></p>
<p>env = SimpleMaterialsEnv()</p>
<p>Q, rewards = q_learning(env, episodes=1000)</p>

<p><h1>学習曲線の可視化</h1></p>
<p>plt.figure(figsize=(10, 6))</p>
<p>plt.plot(np.convolve(rewards, np.ones(50)/50, mode='valid'))</p>
<p>plt.xlabel('Episode')</p>
<p>plt.ylabel('Average Reward (50 episodes)')</p>
<p>plt.title('Q-Learning: 材料探索環境での学習進捗')</p>
<p>plt.grid(True)</p>
<p>plt.show()</p>

<p><h1>学習したQ値の可視化</h1></p>
<p>policy = np.argmax(Q, axis=2)</p>
<p>print("\n学習した方策（各セルでの最良行動）:")</p>
<p>print("0=上, 1=下, 2=左, 3=右")</p>
<p>print(policy)</p>
<p></code></pre></p>

<p><strong>出力例</strong>:</p>
<p><pre><code class="language-">Episode 100: Avg Reward = -4.52</p>
<p>Episode 200: Avg Reward = -3.21</p>
<p>Episode 500: Avg Reward = -1.85</p>
<p>Episode 1000: Avg Reward = -1.12</p>

<p>学習した方策（各セルでの最良行動）:</p>
<p>[[1 1 1 1 1]</p>
<p> [1 1 1 1 1]</p>
<p> [1 1 1 1 1]</p>
<p> [1 1 1 1 1]</p>
<p> [3 3 3 3 0]]</p>
<p></code></pre></p>

<p><strong>解説</strong>:</p>
<ul>
<li>初期は報酬が低い（-4.52）が、学習が進むと改善（-1.12）</li>
<li>最終的に、ゴールへの最短経路を学習（下→右の方策）</li>
</ul>

<p>---</p>

<p><h2>1.4 Deep Q-Network（DQN）</h2></p>

<p><h3>Q学習の限界</h3></p>

<p>Q学習は、<strong>状態と行動が離散的かつ少数</strong>の場合に有効です。しかし、材料科学では：</p>

<ul>
<li><strong>状態空間が巨大</strong>: 材料記述子（100次元以上）</li>
<li><strong>連続値</strong>: 組成比率、温度、圧力など</li>
<li><strong>Q-tableが非現実的</strong>: $10^{100}$個のセルを保存できない</li>
</ul>

<p><h3>DQNの解決策</h3></p>

<p>DQNは、<strong>ニューラルネットワークでQ関数を近似</strong>します：</p>

<p>$$</p>
<p>Q(s, a; \theta) \approx Q^*(s, a)</p>
<p>$$</p>

<ul>
<li>$\theta$: ニューラルネットワークのパラメータ</li>
</ul>

<p><strong>損失関数</strong>:</p>
<p>$$</p>
<p>L(\theta) = \mathbb{E}_{(s,a,r,s') \sim D} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2 \right]</p>
<p>$$</p>

<ul>
<li>$D$: <strong>経験再生バッファ</strong>（過去の遷移を保存）</li>
<li>$\theta^-$: <strong>ターゲットネットワーク</strong>（学習の安定化）</li>
</ul>

<p><h3>DQNの重要技術</h3></p>

<ol>
<li><strong>経験再生（Experience Replay）</strong>: 過去の遷移をランダムサンプリングし、データの相関を減らす</li>
<li><strong>ターゲットネットワーク</strong>: 固定されたネットワークでTD目標を計算し、学習を安定化</li>
<li><strong>ε-greedy探索</strong>: 探索（ランダム）と活用（最良行動）のバランス</li>
</ol>

<p><h3>PyTorchによるDQN実装</h3></p>

<p><pre><code class="language-python">import torch</p>
<p>import torch.nn as nn</p>
<p>import torch.optim as optim</p>
<p>from collections import deque</p>
<p>import random</p>

<p>class DQN(nn.Module):</p>
<p>    """Deep Q-Network</p>

<p>    状態を入力し、各行動のQ値を出力</p>
<p>    """</p>
<p>    def __init__(self, state_dim, action_dim, hidden_dim=64):</p>
<p>        super(DQN, self).__init__()</p>
<p>        self.fc1 = nn.Linear(state_dim, hidden_dim)</p>
<p>        self.fc2 = nn.Linear(hidden_dim, hidden_dim)</p>
<p>        self.fc3 = nn.Linear(hidden_dim, action_dim)</p>

<p>    def forward(self, x):</p>
<p>        x = torch.relu(self.fc1(x))</p>
<p>        x = torch.relu(self.fc2(x))</p>
<p>        return self.fc3(x)</p>


<p>class ReplayBuffer:</p>
<p>    """経験再生バッファ"""</p>
<p>    def __init__(self, capacity=10000):</p>
<p>        self.buffer = deque(maxlen=capacity)</p>

<p>    def push(self, state, action, reward, next_state, done):</p>
<p>        self.buffer.append((state, action, reward, next_state, done))</p>

<p>    def sample(self, batch_size):</p>
<p>        return random.sample(self.buffer, batch_size)</p>

<p>    def __len__(self):</p>
<p>        return len(self.buffer)</p>


<p>class DQNAgent:</p>
<p>    """DQNエージェント"""</p>
<p>    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):</p>
<p>        self.action_dim = action_dim</p>
<p>        self.gamma = gamma</p>
<p>        self.epsilon = epsilon</p>
<p>        self.epsilon_decay = epsilon_decay</p>
<p>        self.epsilon_min = epsilon_min</p>

<p>        <h1>メインネットワークとターゲットネットワーク</h1></p>
<p>        self.policy_net = DQN(state_dim, action_dim)</p>
<p>        self.target_net = DQN(state_dim, action_dim)</p>
<p>        self.target_net.load_state_dict(self.policy_net.state_dict())</p>
<p>        self.target_net.eval()</p>

<p>        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)</p>
<p>        self.buffer = ReplayBuffer()</p>

<p>    def select_action(self, state):</p>
<p>        """ε-greedy行動選択"""</p>
<p>        if np.random.random() < self.epsilon:</p>
<p>            return np.random.randint(self.action_dim)</p>
<p>        else:</p>
<p>            with torch.no_grad():</p>
<p>                state_tensor = torch.FloatTensor(state).unsqueeze(0)</p>
<p>                q_values = self.policy_net(state_tensor)</p>
<p>                return q_values.argmax().item()</p>

<p>    def train(self, batch_size=64):</p>
<p>        """ミニバッチ学習"""</p>
<p>        if len(self.buffer) < batch_size:</p>
<p>            return</p>

<p>        <h1>ミニバッチサンプリング</h1></p>
<p>        batch = self.buffer.sample(batch_size)</p>
<p>        states, actions, rewards, next_states, dones = zip(*batch)</p>

<p>        states = torch.FloatTensor(states)</p>
<p>        actions = torch.LongTensor(actions).unsqueeze(1)</p>
<p>        rewards = torch.FloatTensor(rewards).unsqueeze(1)</p>
<p>        next_states = torch.FloatTensor(next_states)</p>
<p>        dones = torch.FloatTensor(dones).unsqueeze(1)</p>

<p>        <h1>現在のQ値</h1></p>
<p>        current_q = self.policy_net(states).gather(1, actions)</p>

<p>        <h1>ターゲットQ値</h1></p>
<p>        with torch.no_grad():</p>
<p>            max_next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)</p>
<p>            target_q = rewards + (1 - dones) <em> self.gamma </em> max_next_q</p>

<p>        <h1>損失計算と最適化</h1></p>
<p>        loss = nn.MSELoss()(current_q, target_q)</p>
<p>        self.optimizer.zero_grad()</p>
<p>        loss.backward()</p>
<p>        self.optimizer.step()</p>

<p>        <h1>εの減衰</h1></p>
<p>        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)</p>

<p>    def update_target_network(self):</p>
<p>        """ターゲットネットワークの更新"""</p>
<p>        self.target_net.load_state_dict(self.policy_net.state_dict())</p>


<p><h1>材料探索環境（連続状態版）</h1></p>
<p>class ContinuousMaterialsEnv:</p>
<p>    """連続状態空間の材料探索環境"""</p>
<p>    def __init__(self, state_dim=4):</p>
<p>        self.state_dim = state_dim</p>
<p>        self.target = np.array([3.0, 5.0, 2.5, 4.0])  <h1>目標特性</h1></p>
<p>        self.state = None</p>

<p>    def reset(self):</p>
<p>        self.state = np.random.uniform(0, 10, self.state_dim)</p>
<p>        return self.state</p>

<p>    def step(self, action):</p>
<p>        <h1>行動: 0=増加, 1=減少, 2=大幅増加, 3=大幅減少</h1></p>
<p>        delta = [0.1, -0.1, 0.5, -0.5][action]</p>

<p>        <h1>ランダムな次元を変更</h1></p>
<p>        dim = np.random.randint(self.state_dim)</p>
<p>        self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)</p>

<p>        <h1>報酬: 目標との距離（負の値、近いほど良い）</h1></p>
<p>        distance = np.linalg.norm(self.state - self.target)</p>
<p>        reward = -distance</p>

<p>        <h1>終了条件: 目標に十分近い</h1></p>
<p>        done = distance < 0.5</p>

<p>        return self.state, reward, done</p>


<p><h1>DQN訓練</h1></p>
<p>env = ContinuousMaterialsEnv()</p>
<p>agent = DQNAgent(state_dim=4, action_dim=4)</p>

<p>episodes = 500</p>
<p>rewards_history = []</p>

<p>for episode in range(episodes):</p>
<p>    state = env.reset()</p>
<p>    total_reward = 0</p>
<p>    done = False</p>

<p>    while not done:</p>
<p>        action = agent.select_action(state)</p>
<p>        next_state, reward, done = env.step(action)</p>

<p>        agent.buffer.push(state, action, reward, next_state, done)</p>
<p>        agent.train()</p>

<p>        state = next_state</p>
<p>        total_reward += reward</p>

<p>    rewards_history.append(total_reward)</p>

<p>    <h1>ターゲットネットワーク更新</h1></p>
<p>    if (episode + 1) % 10 == 0:</p>
<p>        agent.update_target_network()</p>

<p>    if (episode + 1) % 50 == 0:</p>
<p>        avg_reward = np.mean(rewards_history[-50:])</p>
<p>        print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}, ε = {agent.epsilon:.3f}")</p>

<p><h1>学習曲線</h1></p>
<p>plt.figure(figsize=(10, 6))</p>
<p>plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'))</p>
<p>plt.xlabel('Episode')</p>
<p>plt.ylabel('Average Reward (20 episodes)')</p>
<p>plt.title('DQN: 連続状態材料探索での学習進捗')</p>
<p>plt.grid(True)</p>
<p>plt.show()</p>
<p></code></pre></p>

<p><strong>出力例</strong>:</p>
<p><pre><code class="language-">Episode 50: Avg Reward = -45.23, ε = 0.779</p>
<p>Episode 100: Avg Reward = -32.15, ε = 0.606</p>
<p>Episode 200: Avg Reward = -18.92, ε = 0.365</p>
<p>Episode 500: Avg Reward = -8.45, ε = 0.010</p>
<p></code></pre></p>

<p><strong>解説</strong>:</p>
<ul>
<li>ニューラルネットワークが連続状態のQ関数を学習</li>
<li>εが減衰し、探索から活用へシフト</li>
<li>最終的に目標特性に近い材料を効率的に発見</li>
</ul>

<p>---</p>

<p><h2>演習問題</h2></p>

<p><h3>問題1 (難易度: easy)</h3></p>

<p>Q学習の更新式において、学習率$\alpha$を大きくすると何が起こるか説明してください。また、$\alpha=0$と$\alpha=1$の極端なケースではどうなるか答えてください。</p>

<p><details></p>
<p><summary>ヒント</summary></p>

<p>学習率は「新しい情報をどれだけ重視するか」を制御します。更新式を見直してみましょう。</p>

<p></details></p>

<p><details></p>
<p><summary>解答例</summary></p>

<p><strong>$\alpha$を大きくすると</strong>:</p>
<ul>
<li>新しい観測（TD目標）を強く反映し、Q値が大きく変化</li>
<li>学習が速いが不安定になりやすい</li>
</ul>

<p><strong>極端なケース</strong>:</p>
<ul>
<li><strong>$\alpha=0$</strong>: Q値が全く更新されない（学習しない）</li>
</ul>
<p>  $$Q(s,a) \leftarrow Q(s,a) + 0 \cdot [\cdots] = Q(s,a)$$</p>

<ul>
<li><strong>$\alpha=1$</strong>: Q値が完全にTD目標で置き換えられる</li>
</ul>
<p>  $$Q(s,a) \leftarrow r + \gamma \max_{a'} Q(s', a')$$</p>
<p>  過去の情報が完全に消え、最新の観測のみに依存</p>

<p><strong>実践的には</strong>: $\alpha = 0.01 \sim 0.1$が一般的</p>

<p></details></p>

<p>---</p>

<p><h3>問題2 (難易度: medium)</h3></p>

<p>材料探索において、報酬関数を以下のように設計しました。この設計の問題点と改善案を述べてください。</p>

<p><pre><code class="language-python">def reward_function(material_property, target=3.0):</p>
<p>    if material_property == target:</p>
<p>        return 1.0</p>
<p>    else:</p>
<p>        return 0.0</p>
<p></code></pre></p>

<p><details></p>
<p><summary>ヒント</summary></p>

<p>この報酬は「スパース報酬」と呼ばれ、目標に到達しない限りすべて0です。学習にどう影響するか考えてみましょう。</p>

<p></details></p>

<p><details></p>
<p><summary>解答例</summary></p>

<p><strong>問題点</strong>:</p>
<ol>
<li><strong>スパース報酬</strong>: ほとんどの場合報酬が0で、学習シグナルが弱い</li>
<li><strong>探索が困難</strong>: どの方向に進めば良いかわからない</li>
<li><strong>厳密な一致</strong>: 実数値で完全一致はほぼ不可能</li>
</ol>

<p><strong>改善案</strong>:</p>
<p><pre><code class="language-python">def improved_reward_function(material_property, target=3.0):</p>
<p>    <h1>目標との距離に基づく連続的な報酬</h1></p>
<p>    distance = abs(material_property - target)</p>

<p>    if distance < 0.1:</p>
<p>        return 10.0  <h1>非常に近い（ボーナス）</h1></p>
<p>    elif distance < 0.5:</p>
<p>        return 5.0   <h1>近い</h1></p>
<p>    else:</p>
<p>        return -distance  <h1>遠いほどペナルティ</h1></p>
<p></code></pre></p>

<p><strong>さらなる改善</strong>:</p>
<ul>
<li><strong>シェイピング報酬</strong>: 目標への進捗に応じて中間報酬を与える</li>
<li><strong>多目的報酬</strong>: 複数の特性を考慮（バンドギャップ + 安定性）</li>
</ul>

<p></details></p>

<p>---</p>

<p><h3>問題3 (難易度: hard)</h3></p>

<p>DQNにおける「経験再生」と「ターゲットネットワーク」の役割を説明し、それぞれがないとどのような問題が起こるか、Pythonコードで実験してください。</p>

<p><details></p>
<p><summary>ヒント</summary></p>

<p>経験再生をオフにするには<code>buffer.sample()</code>の代わりに最新の遷移のみを使用します。ターゲットネットワークをオフにするには、TD目標の計算で<code>self.policy_net</code>を使います。</p>

<p></details></p>

<p><details></p>
<p><summary>解答例</summary></p>

<p><strong>経験再生の役割</strong>:</p>
<ul>
<li>過去の遷移をランダムサンプリングし、データの相関を減らす</li>
<li>ないと、連続した遷移だけで学習し、特定のパターンに過学習</li>
</ul>

<p><strong>ターゲットネットワークの役割</strong>:</p>
<ul>
<li>固定されたネットワークでTD目標を計算し、学習を安定化</li>
<li>ないと、Q値が振動して収束しにくい</li>
</ul>

<p><strong>実験コード</strong>:</p>
<p><pre><code class="language-python"><h1>経験再生なし版</h1></p>
<p>class DQNAgentNoReplay(DQNAgent):</p>
<p>    def train_no_replay(self, state, action, reward, next_state, done):</p>
<p>        <h1>最新の遷移のみで学習</h1></p>
<p>        states = torch.FloatTensor([state])</p>
<p>        actions = torch.LongTensor([action]).unsqueeze(1)</p>
<p>        rewards = torch.FloatTensor([reward]).unsqueeze(1)</p>
<p>        next_states = torch.FloatTensor([next_state])</p>
<p>        dones = torch.FloatTensor([done]).unsqueeze(1)</p>

<p>        current_q = self.policy_net(states).gather(1, actions)</p>
<p>        with torch.no_grad():</p>
<p>            max_next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)</p>
<p>            target_q = rewards + (1 - dones) <em> self.gamma </em> max_next_q</p>

<p>        loss = nn.MSELoss()(current_q, target_q)</p>
<p>        self.optimizer.zero_grad()</p>
<p>        loss.backward()</p>
<p>        self.optimizer.step()</p>

<p><h1>ターゲットネットワークなし版（TD目標でpolicy_netを使用）</h1></p>
<p><h1>→ 学習が不安定になる</h1></p>

<p><h1>結果: 経験再生なしでは収束が遅く、ターゲットネットなしでは振動する</h1></p>
<p></code></pre></p>

<p></details></p>

<p>---</p>

<p><h2>このセクションのまとめ</h2></p>

<ul>
<li>材料探索は探索空間が広大で、従来の試行錯誤は非効率</li>
<li>強化学習は<strong>環境との相互作用を通じて最適な探索戦略を学習</strong></li>
<li><strong>マルコフ決定過程（MDP）</strong>が強化学習の数学的基盤</li>
<li><strong>Q学習</strong>は離散状態・行動で有効、Q-tableで価値を記録</li>
<li><strong>DQN</strong>はニューラルネットワークでQ関数を近似し、巨大な状態空間に対応</li>
<li><strong>経験再生</strong>と<strong>ターゲットネットワーク</strong>がDQN学習を安定化</li>
</ul>

<p>次章では、より高度な方策勾配法（Policy Gradient）とActor-Critic手法を学びます。</p>

<p>---</p>

<p><h2>参考文献</h2></p>

<ol>
<li>Mnih et al. "Playing Atari with Deep Reinforcement Learning" <em>arXiv</em> (2013) - DQN原論文</li>
<li>Sutton & Barto "Reinforcement Learning: An Introduction" MIT Press (2018) - RL教科書</li>
<li>Zhou et al. "Optimization of molecules via deep reinforcement learning" <em>Scientific Reports</em> (2019)</li>
<li>Ling et al. "High-dimensional materials and process optimization using data-driven experimental design with well-calibrated uncertainty estimates" <em>Integrating Materials and Manufacturing Innovation</em> (2017)</li>
</ol>

<p>---</p>

<p><strong>次章</strong>: <a href="chapter-2.html">第2章: 強化学習の基礎理論</a></p>


        
        <div class="navigation">
            <a href="chapter-2.html" class="nav-button">次章: 第2章 →</a>
            <a href="index.html" class="nav-button">← シリーズ目次に戻る</a>
            
        </div>
    
    </main>

    <footer>
        <p><strong>作成者</strong>: AI Terakoya Content Team</p>
        <p><strong>監修</strong>: Dr. Yusuke Hashimoto(東北大学)</p>
        <p><strong>バージョン</strong>: 1.0 | <strong>作成日</strong>: 2025-10-17</p>
        <p><strong>ライセンス</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>