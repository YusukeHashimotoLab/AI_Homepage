<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬2ç« : å¼·åŒ–å­¦ç¿’ã®åŸºç¤ç†è«– - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬2ç« : å¼·åŒ–å­¦ç¿’ã®åŸºç¤ç†è«–</h1>
            
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
            </div>
        </div>
    </header>

    <main class="container">
        
<p><h1>ç¬¬2ç« : å¼·åŒ–å­¦ç¿’ã®åŸºç¤ç†è«–</h1></p>

<p><h2>å­¦ç¿’ç›®æ¨™</h2></p>

<p>ã“ã®ç« ã§ã¯ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã—ã¾ã™ï¼š</p>

<ul>
<li>æ–¹ç­–å‹¾é…æ³•ï¼ˆPolicy Gradient Methodsï¼‰ã®ç†è«–ã¨å®Ÿè£…</li>
<li>Actor-Criticã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ä»•çµ„ã¿</li>
<li>Proximal Policy Optimizationï¼ˆPPOï¼‰ã®è©³ç´°</li>
<li>Stable Baselines3ã«ã‚ˆã‚‹å®Ÿè·µçš„å®Ÿè£…</li>
</ul>

<p>---</p>

<p><h2>2.1 æ–¹ç­–å‹¾é…æ³•ï¼ˆPolicy Gradient Methodsï¼‰</h2></p>

<p><h3>Qå­¦ç¿’ã®é™ç•Œ</h3></p>

<p>ç¬¬1ç« ã®Qå­¦ç¿’ãƒ»DQNã¯<strong>ä¾¡å€¤ãƒ™ãƒ¼ã‚¹</strong>ã®æ‰‹æ³•ã§ã—ãŸã€‚ã“ã‚Œã‚‰ã«ã¯ä»¥ä¸‹ã®é™ç•ŒãŒã‚ã‚Šã¾ã™ï¼š</p>

<ol>
<li><strong>é›¢æ•£è¡Œå‹•ã®ã¿</strong>: $\arg\max_a Q(s,a)$ã¯é€£ç¶šè¡Œå‹•ç©ºé–“ã§å›°é›£</li>
<li><strong>æ±ºå®šçš„æ–¹ç­–</strong>: å¸¸ã«åŒã˜è¡Œå‹•ã‚’é¸æŠï¼ˆç¢ºç‡çš„æ–¹ç­–ãŒå­¦ç¿’ã§ããªã„ï¼‰</li>
<li><strong>å°ã•ãªå¤‰åŒ–ã«è„†å¼±</strong>: Qå€¤ã®å¾®å°ãªå¤‰åŒ–ã§æ–¹ç­–ãŒå¤§ããå¤‰ã‚ã‚‹</li>
</ol>

<p>ææ–™ç§‘å­¦ã§ã¯ã€<strong>é€£ç¶šçš„ãªåˆ¶å¾¡</strong>ï¼ˆæ¸©åº¦ã‚’0.5åº¦ä¸Šã’ã‚‹ã€çµ„æˆæ¯”ã‚’2%å¤‰ãˆã‚‹ï¼‰ãŒé‡è¦ã§ã™ã€‚</p>

<p><h3>æ–¹ç­–å‹¾é…æ³•ã®åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢</h3></p>

<p>æ–¹ç­–å‹¾é…æ³•ã¯ã€<strong>æ–¹ç­–ã‚’ç›´æ¥æœ€é©åŒ–</strong>ã—ã¾ã™ï¼š</p>

<p>$$</p>
<p>\pi_\theta(a|s) = P(a|s; \theta)</p>
<p>$$</p>

<ul>
<li>$\theta$: æ–¹ç­–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿ï¼‰</li>
</ul>

<p><strong>ç›®çš„</strong>: æœŸå¾…ç´¯ç©å ±é…¬$J(\theta)$ã‚’æœ€å¤§åŒ–</p>

<p>$$</p>
<p>J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T r_t \right]</p>
<p>$$</p>

<ul>
<li>$\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots)$: è»Œè·¡ï¼ˆtrajectoryï¼‰</li>
</ul>

<p><h3>æ–¹ç­–å‹¾é…å®šç†</h3></p>

<p><strong>REINFORCE</strong>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆWilliams, 1992ï¼‰ã¯ã€å‹¾é…ã‚’ä»¥ä¸‹ã§è¨ˆç®—ã—ã¾ã™ï¼š</p>

<p>$$</p>
<p>\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot R_t \right]</p>
<p>$$</p>

<ul>
<li>$R_t = \sum_{k=t}^T \gamma^{k-t} r_k$: æ™‚åˆ»$t$ã‹ã‚‰ã®ç´¯ç©å ±é…¬ï¼ˆãƒªã‚¿ãƒ¼ãƒ³ï¼‰</li>
</ul>

<p><strong>ç›´æ„Ÿçš„æ„å‘³</strong>:</p>
<ul>
<li>é«˜ã„å ±é…¬ã‚’å¾—ãŸè¡Œå‹•ã®ç¢ºç‡ã‚’ä¸Šã’ã‚‹</li>
<li>ä½ã„å ±é…¬ã‚’å¾—ãŸè¡Œå‹•ã®ç¢ºç‡ã‚’ä¸‹ã’ã‚‹</li>
</ul>

<p><h3>REINFORCEã®å®Ÿè£…</h3></p>

<p><pre><code class="language-python">import torch</p>
<p>import torch.nn as nn</p>
<p>import torch.optim as optim</p>
<p>import numpy as np</p>
<p>import matplotlib.pyplot as plt</p>

<p>class PolicyNetwork(nn.Module):</p>
<p>    """æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</p>

<p>    çŠ¶æ…‹ã‚’å…¥åŠ›ã—ã€å„è¡Œå‹•ã®ç¢ºç‡ã‚’å‡ºåŠ›</p>
<p>    """</p>
<p>    def __init__(self, state_dim, action_dim, hidden_dim=64):</p>
<p>        super(PolicyNetwork, self).__init__()</p>
<p>        self.fc1 = nn.Linear(state_dim, hidden_dim)</p>
<p>        self.fc2 = nn.Linear(hidden_dim, hidden_dim)</p>
<p>        self.fc3 = nn.Linear(hidden_dim, action_dim)</p>

<p>    def forward(self, x):</p>
<p>        x = torch.relu(self.fc1(x))</p>
<p>        x = torch.relu(self.fc2(x))</p>
<p>        return torch.softmax(self.fc3(x), dim=-1)  <h1>ç¢ºç‡åˆ†å¸ƒ</h1></p>


<p>class REINFORCEAgent:</p>
<p>    """REINFORCEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ """</p>
<p>    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):</p>
<p>        self.gamma = gamma</p>
<p>        self.policy = PolicyNetwork(state_dim, action_dim)</p>
<p>        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)</p>

<p>        <h1>ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…ã®ãƒ­ã‚°ã‚’ä¿å­˜</h1></p>
<p>        self.log_probs = []</p>
<p>        self.rewards = []</p>

<p>    def select_action(self, state):</p>
<p>        """æ–¹ç­–ã«å¾“ã£ã¦è¡Œå‹•ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""</p>
<p>        state_tensor = torch.FloatTensor(state).unsqueeze(0)</p>
<p>        probs = self.policy(state_tensor)</p>

<p>        <h1>ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</h1></p>
<p>        action_dist = torch.distributions.Categorical(probs)</p>
<p>        action = action_dist.sample()</p>

<p>        <h1>logç¢ºç‡ã‚’ä¿å­˜ï¼ˆå‹¾é…è¨ˆç®—ç”¨ï¼‰</h1></p>
<p>        self.log_probs.append(action_dist.log_prob(action))</p>

<p>        return action.item()</p>

<p>    def store_reward(self, reward):</p>
<p>        """å ±é…¬ã‚’ä¿å­˜"""</p>
<p>        self.rewards.append(reward)</p>

<p>    def update(self):</p>
<p>        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œã«æ–¹ç­–ã‚’æ›´æ–°"""</p>
<p>        <h1>ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆç´¯ç©å ±é…¬ï¼‰ã‚’è¨ˆç®—</h1></p>
<p>        returns = []</p>
<p>        R = 0</p>
<p>        for r in reversed(self.rewards):</p>
<p>            R = r + self.gamma * R</p>
<p>            returns.insert(0, R)</p>

<p>        returns = torch.FloatTensor(returns)</p>

<p>        <h1>æ­£è¦åŒ–ï¼ˆå­¦ç¿’ã‚’å®‰å®šåŒ–ï¼‰</h1></p>
<p>        returns = (returns - returns.mean()) / (returns.std() + 1e-8)</p>

<p>        <h1>æ–¹ç­–å‹¾é…</h1></p>
<p>        policy_loss = []</p>
<p>        for log_prob, R in zip(self.log_probs, returns):</p>
<p>            policy_loss.append(-log_prob * R)</p>

<p>        <h1>å‹¾é…é™ä¸‹</h1></p>
<p>        self.optimizer.zero_grad()</p>
<p>        loss = torch.stack(policy_loss).sum()</p>
<p>        loss.backward()</p>
<p>        self.optimizer.step()</p>

<p>        <h1>ãƒ­ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ</h1></p>
<p>        self.log_probs = []</p>
<p>        self.rewards = []</p>


<p><h1>ç°¡å˜ãªææ–™æ¢ç´¢ç’°å¢ƒï¼ˆé›¢æ•£è¡Œå‹•ç‰ˆï¼‰</h1></p>
<p>class DiscreteMaterialsEnv:</p>
<p>    """é›¢æ•£è¡Œå‹•ã®ææ–™æ¢ç´¢ç’°å¢ƒ"""</p>
<p>    def __init__(self, state_dim=4):</p>
<p>        self.state_dim = state_dim</p>
<p>        self.target = np.array([3.0, 5.0, 2.5, 4.0])</p>
<p>        self.state = None</p>

<p>    def reset(self):</p>
<p>        self.state = np.random.uniform(0, 10, self.state_dim)</p>
<p>        return self.state</p>

<p>    def step(self, action):</p>
<p>        <h1>è¡Œå‹•: 0=æ¬¡å…ƒ0å¢—åŠ , 1=æ¬¡å…ƒ0æ¸›å°‘, 2=æ¬¡å…ƒ1å¢—åŠ , 3=æ¬¡å…ƒ1æ¸›å°‘</h1></p>
<p>        dim = action // 2</p>
<p>        delta = 0.5 if action % 2 == 0 else -0.5</p>

<p>        self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)</p>

<p>        <h1>å ±é…¬: ç›®æ¨™ã¨ã®è·é›¢</h1></p>
<p>        distance = np.linalg.norm(self.state - self.target)</p>
<p>        reward = -distance</p>

<p>        done = distance < 0.5</p>

<p>        return self.state, reward, done</p>


<p><h1>REINFORCEã®è¨“ç·´</h1></p>
<p>env = DiscreteMaterialsEnv()</p>
<p>agent = REINFORCEAgent(state_dim=4, action_dim=4)</p>

<p>episodes = 1000</p>
<p>rewards_history = []</p>

<p>for episode in range(episodes):</p>
<p>    state = env.reset()</p>
<p>    total_reward = 0</p>
<p>    done = False</p>

<p>    while not done:</p>
<p>        action = agent.select_action(state)</p>
<p>        next_state, reward, done = env.step(action)</p>

<p>        agent.store_reward(reward)</p>
<p>        state = next_state</p>
<p>        total_reward += reward</p>

<p>    <h1>ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å¾Œã«æ›´æ–°</h1></p>
<p>    agent.update()</p>
<p>    rewards_history.append(total_reward)</p>

<p>    if (episode + 1) % 100 == 0:</p>
<p>        avg_reward = np.mean(rewards_history[-100:])</p>
<p>        print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}")</p>

<p><h1>å­¦ç¿’æ›²ç·š</h1></p>
<p>plt.figure(figsize=(10, 6))</p>
<p>plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'))</p>
<p>plt.xlabel('Episode')</p>
<p>plt.ylabel('Average Reward (20 episodes)')</p>
<p>plt.title('REINFORCE: æ–¹ç­–å‹¾é…æ³•ã«ã‚ˆã‚‹ææ–™æ¢ç´¢')</p>
<p>plt.grid(True)</p>
<p>plt.show()</p>
<p></code></pre></p>

<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<p><pre><code class="language-">Episode 100: Avg Reward = -38.24</p>
<p>Episode 200: Avg Reward = -28.15</p>
<p>Episode 500: Avg Reward = -15.32</p>
<p>Episode 1000: Avg Reward = -7.89</p>
<p></code></pre></p>

<p>---</p>

<p><h2>2.2 ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨åˆ†æ•£å‰Šæ¸›</h2></p>

<p><h3>REINFORCEã®å•é¡Œç‚¹</h3></p>

<p>REINFORCEã¯<strong>é«˜åˆ†æ•£</strong>ï¼ˆhigh varianceï¼‰ã§ã™ã€‚åŒã˜æ–¹ç­–ã§ã‚‚ã€é‹ãŒè‰¯ã„ã‹æ‚ªã„ã‹ã§ãƒªã‚¿ãƒ¼ãƒ³$R_t$ãŒå¤§ããå¤‰å‹•ã—ã¾ã™ã€‚</p>

<p><h3>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®å°å…¥</h3></p>

<p><strong>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³</strong> $b(s)$ã‚’å¼•ãã“ã¨ã§ã€åˆ†æ•£ã‚’å‰Šæ¸›ï¼š</p>

<p>$$</p>
<p>\nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot (R_t - b(s_t)) \right]</p>
<p>$$</p>

<p><strong>æœ€é©ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³</strong>: çŠ¶æ…‹ä¾¡å€¤é–¢æ•°$V(s)$</p>

<p>$$</p>
<p>b(s_t) = V(s_t) = \mathbb{E}_{\pi} \left[ \sum_{k=t}^T \gamma^{k-t} r_k \mid s_t \right]</p>
<p>$$</p>

<p><strong>ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸é–¢æ•°</strong> $A(s, a)$:</p>
<p>$$</p>
<p>A(s, a) = Q(s, a) - V(s) = R_t - V(s_t)</p>
<p>$$</p>

<p>ã€Œã“ã®è¡Œå‹•ã¯å¹³å‡ã‚ˆã‚Šã©ã‚Œã ã‘è‰¯ã„ã‹ã€ã‚’è¡¨ã—ã¾ã™ã€‚</p>

<p><h3>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä»˜ãREINFORCE</h3></p>

<p><pre><code class="language-python">class ValueNetwork(nn.Module):</p>
<p>    """ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰"""</p>
<p>    def __init__(self, state_dim, hidden_dim=64):</p>
<p>        super(ValueNetwork, self).__init__()</p>
<p>        self.fc1 = nn.Linear(state_dim, hidden_dim)</p>
<p>        self.fc2 = nn.Linear(hidden_dim, hidden_dim)</p>
<p>        self.fc3 = nn.Linear(hidden_dim, 1)  <h1>çŠ¶æ…‹ä¾¡å€¤ã‚’å‡ºåŠ›</h1></p>

<p>    def forward(self, x):</p>
<p>        x = torch.relu(self.fc1(x))</p>
<p>        x = torch.relu(self.fc2(x))</p>
<p>        return self.fc3(x)</p>


<p>class REINFORCEWithBaseline:</p>
<p>    """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä»˜ãREINFORCE"""</p>
<p>    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):</p>
<p>        self.gamma = gamma</p>
<p>        self.policy = PolicyNetwork(state_dim, action_dim)</p>
<p>        self.value = ValueNetwork(state_dim)</p>

<p>        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)</p>
<p>        self.value_optimizer = optim.Adam(self.value.parameters(), lr=lr)</p>

<p>        self.log_probs = []</p>
<p>        self.rewards = []</p>
<p>        self.states = []</p>

<p>    def select_action(self, state):</p>
<p>        state_tensor = torch.FloatTensor(state).unsqueeze(0)</p>
<p>        probs = self.policy(state_tensor)</p>

<p>        action_dist = torch.distributions.Categorical(probs)</p>
<p>        action = action_dist.sample()</p>

<p>        self.log_probs.append(action_dist.log_prob(action))</p>
<p>        self.states.append(state)</p>

<p>        return action.item()</p>

<p>    def store_reward(self, reward):</p>
<p>        self.rewards.append(reward)</p>

<p>    def update(self):</p>
<p>        <h1>ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—</h1></p>
<p>        returns = []</p>
<p>        R = 0</p>
<p>        for r in reversed(self.rewards):</p>
<p>            R = r + self.gamma * R</p>
<p>            returns.insert(0, R)</p>

<p>        returns = torch.FloatTensor(returns)</p>
<p>        states = torch.FloatTensor(self.states)</p>

<p>        <h1>ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡ºåŠ›ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰</h1></p>
<p>        values = self.value(states).squeeze()</p>

<p>        <h1>ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ = ãƒªã‚¿ãƒ¼ãƒ³ - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³</h1></p>
<p>        advantages = returns - values.detach()</p>

<p>        <h1>æ–¹ç­–å‹¾é…æå¤±</h1></p>
<p>        policy_loss = []</p>
<p>        for log_prob, adv in zip(self.log_probs, advantages):</p>
<p>            policy_loss.append(-log_prob * adv)</p>

<p>        <h1>ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æå¤±ï¼ˆMSEï¼‰</h1></p>
<p>        value_loss = nn.MSELoss()(values, returns)</p>

<p>        <h1>æœ€é©åŒ–</h1></p>
<p>        self.policy_optimizer.zero_grad()</p>
<p>        torch.stack(policy_loss).sum().backward()</p>
<p>        self.policy_optimizer.step()</p>

<p>        self.value_optimizer.zero_grad()</p>
<p>        value_loss.backward()</p>
<p>        self.value_optimizer.step()</p>

<p>        <h1>ãƒªã‚»ãƒƒãƒˆ</h1></p>
<p>        self.log_probs = []</p>
<p>        self.rewards = []</p>
<p>        self.states = []</p>


<p><h1>è¨“ç·´ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä»˜ãï¼‰</h1></p>
<p>agent_baseline = REINFORCEWithBaseline(state_dim=4, action_dim=4)</p>

<p>rewards_baseline = []</p>
<p>for episode in range(1000):</p>
<p>    state = env.reset()</p>
<p>    total_reward = 0</p>
<p>    done = False</p>

<p>    while not done:</p>
<p>        action = agent_baseline.select_action(state)</p>
<p>        next_state, reward, done = env.step(action)</p>

<p>        agent_baseline.store_reward(reward)</p>
<p>        state = next_state</p>
<p>        total_reward += reward</p>

<p>    agent_baseline.update()</p>
<p>    rewards_baseline.append(total_reward)</p>

<p><h1>æ¯”è¼ƒ</h1></p>
<p>plt.figure(figsize=(10, 6))</p>
<p>plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'), label='REINFORCE')</p>
<p>plt.plot(np.convolve(rewards_baseline, np.ones(20)/20, mode='valid'), label='REINFORCE + Baseline')</p>
<p>plt.xlabel('Episode')</p>
<p>plt.ylabel('Average Reward')</p>
<p>plt.title('ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«ã‚ˆã‚‹å­¦ç¿’å®‰å®šåŒ–')</p>
<p>plt.legend()</p>
<p>plt.grid(True)</p>
<p>plt.show()</p>
<p></code></pre></p>

<p><strong>çµæœ</strong>: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«ã‚ˆã‚Šå­¦ç¿’ãŒ<strong>ã‚ˆã‚Šå®‰å®š</strong>ã—ã€åæŸãŒ<strong>é€Ÿã</strong>ãªã‚Šã¾ã™ã€‚</p>

<p>---</p>

<p><h2>2.3 Actor-Criticã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h2></p>

<p><h3>Actor-Criticã®æ¦‚å¿µ</h3></p>

<p><strong>Actorï¼ˆæ–¹ç­–ï¼‰</strong> ã¨ <strong>Criticï¼ˆä¾¡å€¤é–¢æ•°ï¼‰</strong> ã‚’åŒæ™‚ã«å­¦ç¿’ï¼š</p>

<ul>
<li><strong>Actor</strong> $\pi_\theta(a|s)$: è¡Œå‹•ã‚’é¸æŠ</li>
<li><strong>Critic</strong> $V_\phi(s)$: çŠ¶æ…‹ã®ä¾¡å€¤ã‚’è©•ä¾¡</li>
</ul>

<p><pre><code class="language-mermaid">graph LR</p>
<p>    S[çŠ¶æ…‹ s] --> A[Actor<br/>Ï€Î¸]</p>
<p>    S --> C[Critic<br/>VÏ•]</p>
<p>    A -->|è¡Œå‹• a| E[ç’°å¢ƒ]</p>
<p>    E -->|å ±é…¬ r| C</p>
<p>    C -->|TDèª¤å·®| A</p>
<p>    C -->|ä¾¡å€¤è©•ä¾¡| C</p>

<p>    style A fill:#e1f5ff</p>
<p>    style C fill:#ffe1cc</p>
<p></code></pre></p>

<p><h3>TDèª¤å·®ã¨ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸</h3></p>

<p><strong>TDèª¤å·®</strong>ï¼ˆTemporal Difference Errorï¼‰:</p>
<p>$$</p>
<p>\delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)</p>
<p>$$</p>

<p>ã“ã‚Œã¯<strong>1ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸æ¨å®š</strong>ã¨ã—ã¦ä½¿ãˆã¾ã™ã€‚</p>

<p><h3>A2Cï¼ˆAdvantage Actor-Criticï¼‰</h3></p>

<p><pre><code class="language-python">class A2CAgent:</p>
<p>    """Advantage Actor-Critic"""</p>
<p>    def __init__(self, state_dim, action_dim, lr_actor=1e-3, lr_critic=1e-3, gamma=0.99):</p>
<p>        self.gamma = gamma</p>

<p>        self.actor = PolicyNetwork(state_dim, action_dim)</p>
<p>        self.critic = ValueNetwork(state_dim)</p>

<p>        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)</p>
<p>        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)</p>

<p>    def select_action(self, state):</p>
<p>        state_tensor = torch.FloatTensor(state).unsqueeze(0)</p>
<p>        probs = self.actor(state_tensor)</p>

<p>        action_dist = torch.distributions.Categorical(probs)</p>
<p>        action = action_dist.sample()</p>

<p>        return action.item(), action_dist.log_prob(action)</p>

<p>    def update(self, state, action_log_prob, reward, next_state, done):</p>
<p>        """1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°"""</p>
<p>        state_tensor = torch.FloatTensor(state).unsqueeze(0)</p>
<p>        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)</p>

<p>        <h1>ç¾åœ¨ã¨æ¬¡ã®çŠ¶æ…‹ä¾¡å€¤</h1></p>
<p>        value = self.critic(state_tensor)</p>
<p>        next_value = self.critic(next_state_tensor)</p>

<p>        <h1>TDç›®æ¨™ã¨TDèª¤å·®</h1></p>
<p>        td_target = reward + (1 - done) <em> self.gamma </em> next_value.item()</p>
<p>        td_error = td_target - value.item()</p>

<p>        <h1>Criticæå¤±ï¼ˆMSEï¼‰</h1></p>
<p>        critic_loss = (torch.FloatTensor([td_target]) - value).pow(2)</p>

<p>        <h1>Actoræå¤±ï¼ˆæ–¹ç­–å‹¾é… Ã— ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ï¼‰</h1></p>
<p>        actor_loss = -action_log_prob * td_error</p>

<p>        <h1>æœ€é©åŒ–</h1></p>
<p>        self.actor_optimizer.zero_grad()</p>
<p>        actor_loss.backward()</p>
<p>        self.actor_optimizer.step()</p>

<p>        self.critic_optimizer.zero_grad()</p>
<p>        critic_loss.backward()</p>
<p>        self.critic_optimizer.step()</p>


<p><h1>A2Cè¨“ç·´</h1></p>
<p>agent_a2c = A2CAgent(state_dim=4, action_dim=4)</p>

<p>rewards_a2c = []</p>
<p>for episode in range(1000):</p>
<p>    state = env.reset()</p>
<p>    total_reward = 0</p>
<p>    done = False</p>

<p>    while not done:</p>
<p>        action, log_prob = agent_a2c.select_action(state)</p>
<p>        next_state, reward, done = env.step(action)</p>

<p>        <h1>1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°</h1></p>
<p>        agent_a2c.update(state, log_prob, reward, next_state, done)</p>

<p>        state = next_state</p>
<p>        total_reward += reward</p>

<p>    rewards_a2c.append(total_reward)</p>

<p>    if (episode + 1) % 100 == 0:</p>
<p>        avg_reward = np.mean(rewards_a2c[-100:])</p>
<p>        print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}")</p>
<p></code></pre></p>

<p><strong>åˆ©ç‚¹</strong>:</p>
<ul>
<li>ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ã‚’å¾…ãŸãšã«<strong>ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’</strong></li>
<li>TDèª¤å·®ã«ã‚ˆã‚Š<strong>ä½åˆ†æ•£</strong></li>
</ul>

<p>---</p>

<p><h2>2.4 Proximal Policy Optimizationï¼ˆPPOï¼‰</h2></p>

<p><h3>Trust Region Methods</h3></p>

<p>æ–¹ç­–å‹¾é…æ³•ã§ã¯ã€<strong>æ›´æ–°ãŒå¤§ãã™ãã‚‹ã¨æ–¹ç­–ãŒå´©å£Š</strong>ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</p>

<p><strong>Trust Region Policy Optimizationï¼ˆTRPOï¼‰</strong> ã¯ã€æ–¹ç­–ã®å¤‰åŒ–ã‚’åˆ¶ç´„ï¼š</p>

<p>$$</p>
<p>\max_\theta \mathbb{E} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A(s, a) \right] \quad \text{s.t.} \quad D_{\text{KL}}(\pi_{\theta_{\text{old}}} \| \pi_\theta) \leq \delta</p>
<p>$$</p>

<p>ã—ã‹ã—ã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹åˆ¶ç´„ã®æœ€é©åŒ–ã¯è¤‡é›‘ã§ã™ã€‚</p>

<p><h3>PPOã®ç°¡ç•¥åŒ–</h3></p>

<p><strong>PPO</strong>ï¼ˆSchulman et al., 2017ï¼‰ã¯ã€åˆ¶ç´„ã‚’<strong>æå¤±é–¢æ•°å†…ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°</strong>ã§å®Ÿç¾ï¼š</p>

<p>$$</p>
<p>L^{\text{CLIP}}(\theta) = \mathbb{E} \left[ \min \left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right]</p>
<p>$$</p>

<ul>
<li>$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$: é‡è¦åº¦æ¯”ç‡ï¼ˆimportance ratioï¼‰</li>
<li>$\epsilon$: ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²ï¼ˆé€šå¸¸0.1ã€œ0.2ï¼‰</li>
</ul>

<p><strong>ç›´æ„Ÿ</strong>:</p>
<ul>
<li>ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ãŒæ­£ï¼ˆè‰¯ã„è¡Œå‹•ï¼‰â†’ $r_t$ã‚’å¢—ã‚„ã™ãŒã€$1+\epsilon$ã§ä¸Šé™</li>
<li>ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ãŒè² ï¼ˆæ‚ªã„è¡Œå‹•ï¼‰â†’ $r_t$ã‚’æ¸›ã‚‰ã™ãŒã€$1-\epsilon$ã§ä¸‹é™</li>
<li>æ€¥æ¿€ãªæ–¹ç­–å¤‰åŒ–ã‚’é˜²ã</li>
</ul>

<p><h3>ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒœãƒ¼ãƒŠã‚¹</h3></p>

<p>æ¢ç´¢ã‚’ä¿ƒé€²ã™ã‚‹ãŸã‚ã€<strong>ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼</strong>ã‚’æå¤±ã«è¿½åŠ ï¼š</p>

<p>$$</p>
<p>L^{\text{PPO}}(\theta) = L^{\text{CLIP}}(\theta) + c_1 L^{\text{VF}}(\theta) - c_2 H[\pi_\theta]</p>
<p>$$</p>

<ul>
<li>$L^{\text{VF}}$: ä¾¡å€¤é–¢æ•°ã®æå¤±</li>
<li>$H[\pi_\theta] = -\sum_a \pi_\theta(a|s) \log \pi_\theta(a|s)$: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆç¢ºç‡åˆ†å¸ƒã®ä¸ç¢ºå®Ÿæ€§ï¼‰</li>
<li>$c_2$: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ä¿‚æ•°ï¼ˆé€šå¸¸0.01ï¼‰</li>
</ul>

<p><h3>PPOã®å®Ÿè£…ï¼ˆStable Baselines3ä½¿ç”¨ï¼‰</h3></p>

<p><pre><code class="language-python">from stable_baselines3 import PPO</p>
<p>from stable_baselines3.common.vec_env import DummyVecEnv</p>
<p>import gym</p>

<p><h1>Gymç’°å¢ƒãƒ©ãƒƒãƒ‘ãƒ¼</h1></p>
<p>class GymMaterialsEnv(gym.Env):</p>
<p>    """OpenAI Gymäº’æ›ã®ææ–™æ¢ç´¢ç’°å¢ƒ"""</p>
<p>    def __init__(self):</p>
<p>        super(GymMaterialsEnv, self).__init__()</p>
<p>        self.state_dim = 4</p>
<p>        self.target = np.array([3.0, 5.0, 2.5, 4.0])</p>

<p>        <h1>è¡Œå‹•ãƒ»çŠ¶æ…‹ç©ºé–“ã®å®šç¾©</h1></p>
<p>        self.action_space = gym.spaces.Discrete(4)</p>
<p>        self.observation_space = gym.spaces.Box(</p>
<p>            low=0, high=10, shape=(self.state_dim,), dtype=np.float32</p>
<p>        )</p>

<p>        self.state = None</p>

<p>    def reset(self):</p>
<p>        self.state = np.random.uniform(0, 10, self.state_dim).astype(np.float32)</p>
<p>        return self.state</p>

<p>    def step(self, action):</p>
<p>        dim = action // 2</p>
<p>        delta = 0.5 if action % 2 == 0 else -0.5</p>

<p>        self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)</p>

<p>        distance = np.linalg.norm(self.state - self.target)</p>
<p>        reward = -distance</p>
<p>        done = distance < 0.5</p>

<p>        return self.state, reward, done, {}</p>

<p>    def render(self, mode='human'):</p>
<p>        pass</p>


<p><h1>ç’°å¢ƒä½œæˆ</h1></p>
<p>env = DummyVecEnv([lambda: GymMaterialsEnv()])</p>

<p><h1>PPOãƒ¢ãƒ‡ãƒ«</h1></p>
<p>model = PPO(</p>
<p>    "MlpPolicy",                <h1>å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³æ–¹ç­–</h1></p>
<p>    env,</p>
<p>    learning_rate=3e-4,</p>
<p>    n_steps=2048,               <h1>æ›´æ–°å‰ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°</h1></p>
<p>    batch_size=64,</p>
<p>    n_epochs=10,                <h1>å„æ›´æ–°ã§ã®æœ€é©åŒ–ã‚¨ãƒãƒƒã‚¯æ•°</h1></p>
<p>    gamma=0.99,</p>
<p>    gae_lambda=0.95,            <h1>GAEï¼ˆGeneralized Advantage Estimationï¼‰</h1></p>
<p>    clip_range=0.2,             <h1>PPOã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²</h1></p>
<p>    ent_coef=0.01,              <h1>ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ä¿‚æ•°</h1></p>
<p>    verbose=1,</p>
<p>    tensorboard_log="./ppo_materials_tensorboard/"</p>
<p>)</p>

<p><h1>è¨“ç·´</h1></p>
<p>model.learn(total_timesteps=100000)</p>

<p><h1>ä¿å­˜</h1></p>
<p>model.save("ppo_materials_agent")</p>

<p><h1>è©•ä¾¡</h1></p>
<p>eval_env = GymMaterialsEnv()</p>
<p>state = eval_env.reset()</p>
<p>total_reward = 0</p>

<p>for _ in range(100):</p>
<p>    action, _ = model.predict(state, deterministic=True)</p>
<p>    state, reward, done, _ = eval_env.step(action)</p>
<p>    total_reward += reward</p>

<p>    if done:</p>
<p>        break</p>

<p>print(f"è©•ä¾¡çµæœ: Total Reward = {total_reward:.2f}")</p>
<p>print(f"æœ€çµ‚çŠ¶æ…‹: {state}")</p>
<p>print(f"ç›®æ¨™: {eval_env.target}")</p>
<p></code></pre></p>

<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<p><pre><code class="language-">---------------------------------</p>
<p>| rollout/           |          |</p>
<p>|    ep_len_mean     | 45.2     |</p>
<p>|    ep_rew_mean     | -15.3    |</p>
<p>| time/              |          |</p>
<p>|    fps             | 1024     |</p>
<p>|    iterations      | 50       |</p>
<p>|    time_elapsed    | 97       |</p>
<p>|    total_timesteps | 102400   |</p>
<p>---------------------------------</p>

<p>è©•ä¾¡çµæœ: Total Reward = -5.23</p>
<p>æœ€çµ‚çŠ¶æ…‹: [3.02 4.98 2.47 3.95]</p>
<p>ç›®æ¨™: [3.  5.  2.5 4. ]</p>
<p></code></pre></p>

<p><strong>è§£èª¬</strong>:</p>
<ul>
<li>Stable Baselines3ã«ã‚ˆã‚Šã€ã‚ãšã‹æ•°è¡Œã§PPOã‚’å®Ÿè£…</li>
<li>TensorBoardã§å­¦ç¿’é€²æ—ã‚’å¯è¦–åŒ–å¯èƒ½</li>
<li>ç›®æ¨™ã«éå¸¸ã«è¿‘ã„ææ–™ã‚’ç™ºè¦‹</li>
</ul>

<p>---</p>

<p><h2>2.5 é€£ç¶šè¡Œå‹•ç©ºé–“ã¸ã®æ‹¡å¼µ</h2></p>

<p><h3>ã‚¬ã‚¦ã‚¹æ–¹ç­–</h3></p>

<p>ææ–™ç§‘å­¦ã§ã¯ã€æ¸©åº¦ã‚„çµ„æˆæ¯”ãªã©<strong>é€£ç¶šçš„ãªåˆ¶å¾¡</strong>ãŒå¿…è¦ã§ã™ã€‚</p>

<p>é€£ç¶šè¡Œå‹•ã«ã¯<strong>ã‚¬ã‚¦ã‚¹åˆ†å¸ƒæ–¹ç­–</strong>ã‚’ä½¿ç”¨ï¼š</p>

<p>$$</p>
<p>\pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma_\theta(s))</p>
<p>$$</p>

<ul>
<li>$\mu_\theta(s)$: å¹³å‡ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å‡ºåŠ›ï¼‰</li>
<li>$\sigma_\theta(s)$: æ¨™æº–åå·®ï¼ˆå­¦ç¿’å¯èƒ½ã¾ãŸã¯å›ºå®šï¼‰</li>
</ul>

<p><h3>é€£ç¶šè¡Œå‹•ç‰ˆPPO</h3></p>

<p><pre><code class="language-python"><h1>é€£ç¶šè¡Œå‹•ç’°å¢ƒ</h1></p>
<p>class ContinuousGymMaterialsEnv(gym.Env):</p>
<p>    """é€£ç¶šè¡Œå‹•ã®ææ–™æ¢ç´¢ç’°å¢ƒ"""</p>
<p>    def __init__(self):</p>
<p>        super(ContinuousGymMaterialsEnv, self).__init__()</p>
<p>        self.state_dim = 4</p>
<p>        self.target = np.array([3.0, 5.0, 2.5, 4.0])</p>

<p>        <h1>é€£ç¶šè¡Œå‹•ç©ºé–“ï¼ˆ4æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã€ç¯„å›² [-1, 1]ï¼‰</h1></p>
<p>        self.action_space = gym.spaces.Box(</p>
<p>            low=-1, high=1, shape=(self.state_dim,), dtype=np.float32</p>
<p>        )</p>
<p>        self.observation_space = gym.spaces.Box(</p>
<p>            low=0, high=10, shape=(self.state_dim,), dtype=np.float32</p>
<p>        )</p>

<p>        self.state = None</p>

<p>    def reset(self):</p>
<p>        self.state = np.random.uniform(0, 10, self.state_dim).astype(np.float32)</p>
<p>        return self.state</p>

<p>    def step(self, action):</p>
<p>        <h1>è¡Œå‹•ã‚’çŠ¶æ…‹å¤‰åŒ–ã«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆ-1ã€œ1 â†’ -0.5ã€œ0.5ï¼‰</h1></p>
<p>        delta = action * 0.5</p>
<p>        self.state = np.clip(self.state + delta, 0, 10)</p>

<p>        distance = np.linalg.norm(self.state - self.target)</p>
<p>        reward = -distance</p>
<p>        done = distance < 0.3</p>

<p>        return self.state, reward, done, {}</p>

<p>    def render(self, mode='human'):</p>
<p>        pass</p>


<p><h1>é€£ç¶šè¡Œå‹•ç‰ˆPPO</h1></p>
<p>env_continuous = DummyVecEnv([lambda: ContinuousGymMaterialsEnv()])</p>

<p>model_continuous = PPO(</p>
<p>    "MlpPolicy",</p>
<p>    env_continuous,</p>
<p>    learning_rate=3e-4,</p>
<p>    n_steps=2048,</p>
<p>    batch_size=64,</p>
<p>    n_epochs=10,</p>
<p>    gamma=0.99,</p>
<p>    clip_range=0.2,</p>
<p>    verbose=1</p>
<p>)</p>

<p>model_continuous.learn(total_timesteps=100000)</p>

<p><h1>è©•ä¾¡</h1></p>
<p>eval_env_cont = ContinuousGymMaterialsEnv()</p>
<p>state = eval_env_cont.reset()</p>

<p>for _ in range(50):</p>
<p>    action, _ = model_continuous.predict(state, deterministic=True)</p>
<p>    state, reward, done, _ = eval_env_cont.step(action)</p>

<p>    if done:</p>
<p>        break</p>

<p>print(f"æœ€çµ‚çŠ¶æ…‹: {state}")</p>
<p>print(f"ç›®æ¨™: {eval_env_cont.target}")</p>
<p>print(f"è·é›¢: {np.linalg.norm(state - eval_env_cont.target):.4f}")</p>
<p></code></pre></p>

<p><strong>å‡ºåŠ›ä¾‹</strong>:</p>
<p><pre><code class="language-">æœ€çµ‚çŠ¶æ…‹: [3.001 5.003 2.498 3.997]</p>
<p>ç›®æ¨™: [3.  5.  2.5 4. ]</p>
<p>è·é›¢: 0.0054</p>
<p></code></pre></p>

<p><strong>è§£èª¬</strong>: é€£ç¶šè¡Œå‹•ã«ã‚ˆã‚Šã€ç›®æ¨™ã¸ã®<strong>ç²¾å¯†ãªåˆ¶å¾¡</strong>ãŒå¯èƒ½</p>

<p>---</p>

<p><h2>æ¼”ç¿’å•é¡Œ</h2></p>

<p><h3>å•é¡Œ1 (é›£æ˜“åº¦: easy)</h3></p>

<p>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä½¿ã†ã¨åˆ†æ•£ãŒæ¸›ã‚‹ç†ç”±ã‚’ã€ä»¥ä¸‹ã®å¼ã‚’ä½¿ã£ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<p>$$</p>
<p>\text{Var}[R_t] \quad \text{vs.} \quad \text{Var}[R_t - b(s_t)]</p>
<p>$$</p>

<p><details></p>
<p><summary>ãƒ’ãƒ³ãƒˆ</summary></p>

<p>åˆ†æ•£ã®æ€§è³ª: $\text{Var}[X - c] = \text{Var}[X]$ï¼ˆå®šæ•°$c$ã‚’å¼•ã„ã¦ã‚‚åˆ†æ•£ã¯å¤‰ã‚ã‚‰ãªã„ï¼‰ã§ã™ãŒã€$b(s_t)$ã¯çŠ¶æ…‹ä¾å­˜ãªã®ã§å®šæ•°ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</p>

<p></details></p>

<p><details></p>
<p><summary>è§£ç­”ä¾‹</summary></p>

<p>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³$b(s_t)$ãŒçŠ¶æ…‹ä¾¡å€¤$V(s_t)$ã«è¿‘ã„ã¨ãï¼š</p>

<ul>
<li><strong>ãƒªã‚¿ãƒ¼ãƒ³</strong> $R_t$ã¯çŠ¶æ…‹ã«ã‚ˆã£ã¦å¤§ããå¤‰å‹•ï¼ˆé‹ã«ã‚ˆã‚‹å½±éŸ¿å¤§ï¼‰</li>
<li><strong>ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸</strong> $R_t - V(s_t)$ã¯ã€Œå¹³å‡ã‹ã‚‰ã®ã‚ºãƒ¬ã€ãªã®ã§å¤‰å‹•ãŒå°ã•ã„</li>
</ul>

<p>æ•°å­¦çš„ã«ã¯ï¼š</p>
<p>$$</p>
<p>\text{Var}[R_t - V(s_t)] \leq \text{Var}[R_t]</p>
<p>$$</p>

<p>ã“ã‚Œã¯$V(s_t)$ãŒã€ŒçŠ¶æ…‹$s_t$ã‹ã‚‰ã®æœŸå¾…ç´¯ç©å ±é…¬ã€ãªã®ã§ã€é‹ã®å½±éŸ¿ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã™ã‚‹ãŸã‚ã§ã™ã€‚</p>

<p><strong>å…·ä½“ä¾‹</strong>:</p>
<ul>
<li>çŠ¶æ…‹Aã‹ã‚‰ã®ãƒªã‚¿ãƒ¼ãƒ³: 100, 105, 95 â†’ åˆ†æ•£ = 25</li>
<li>çŠ¶æ…‹Aã®ä¾¡å€¤: 100</li>
<li>ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸: 0, 5, -5 â†’ åˆ†æ•£ = 25ï¼ˆåŒã˜ï¼‰</li>
</ul>

<p>ã—ã‹ã—ã€è¤‡æ•°ã®çŠ¶æ…‹ã‚’è€ƒãˆã‚‹ã¨ï¼š</p>
<ul>
<li>çŠ¶æ…‹Aã®ãƒªã‚¿ãƒ¼ãƒ³: 100Â±5</li>
<li>çŠ¶æ…‹Bã®ãƒªã‚¿ãƒ¼ãƒ³: 50Â±5</li>
<li>å…¨ä½“ã®åˆ†æ•£: å¤§ãã„</li>
</ul>

<p>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§çŠ¶æ…‹ã”ã¨ã®å¹³å‡ã‚’å¼•ãã¨ã€çŠ¶æ…‹é–“ã®å·®ãŒæ¶ˆãˆã€åˆ†æ•£ãŒæ¸›ã‚Šã¾ã™ã€‚</p>

<p></details></p>

<p>---</p>

<p><h3>å•é¡Œ2 (é›£æ˜“åº¦: medium)</h3></p>

<p>PPOã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²$\epsilon$ã‚’å¤§ããã™ã‚‹ã¨ä½•ãŒèµ·ã“ã‚‹ã‹ã€ã¾ãŸ$\epsilon=0$ã®æ¥µç«¯ãªã‚±ãƒ¼ã‚¹ã§ã¯ã©ã†ãªã‚‹ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<p><details></p>
<p><summary>ãƒ’ãƒ³ãƒˆ</summary></p>

<p>ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å¼ã‚’è¦‹ç›´ã—ã€$r_t(\theta)$ã®å¤‰åŒ–ãŒã©ã†åˆ¶é™ã•ã‚Œã‚‹ã‹è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚</p>

<p></details></p>

<p><details></p>
<p><summary>è§£ç­”ä¾‹</summary></p>

<p><strong>$\epsilon$ã‚’å¤§ããã™ã‚‹ã¨</strong>:</p>
<ul>
<li>ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²ãŒåºƒãŒã‚Šã€æ–¹ç­–ã®å¤‰åŒ–ãŒå¤§ãããªã‚‹</li>
<li>å­¦ç¿’ãŒé€Ÿã„ãŒä¸å®‰å®šã«ãªã‚Šã‚„ã™ã„</li>
<li>æ¥µç«¯ãªå ´åˆã€æ–¹ç­–ãŒå´©å£Šã™ã‚‹å¯èƒ½æ€§</li>
</ul>

<p><strong>$\epsilon=0$ã®å ´åˆ</strong>:</p>
<p>$$</p>
<p>\text{clip}(r_t, 1, 1) = 1</p>
<p>$$</p>

<ul>
<li>é‡è¦åº¦æ¯”ç‡ãŒå¸¸ã«1ã«ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°</li>
<li>æ–¹ç­–ãŒå…¨ãæ›´æ–°ã•ã‚Œãªã„ï¼ˆ$\pi_\theta = \pi_{\theta_{\text{old}}}$ã‚’å¼·åˆ¶ï¼‰</li>
</ul>

<p><strong>å®Ÿè·µçš„ãªå€¤</strong>: $\epsilon = 0.1 \sim 0.2$ãŒä¸€èˆ¬çš„</p>

<p><strong>å®Ÿé¨“ã‚³ãƒ¼ãƒ‰</strong>:</p>
<p><pre><code class="language-python"><h1>Îµ=0.05ï¼ˆå³ã—ã„åˆ¶ç´„ï¼‰</h1></p>
<p>model_tight = PPO("MlpPolicy", env, clip_range=0.05)</p>

<p><h1>Îµ=0.5ï¼ˆç·©ã„åˆ¶ç´„ï¼‰</h1></p>
<p>model_loose = PPO("MlpPolicy", env, clip_range=0.5)</p>

<p><h1>å­¦ç¿’æ›²ç·šã‚’æ¯”è¼ƒ</h1></p>
<p><h1>â†’ model_tightã¯å®‰å®šã ãŒé…ã„</h1></p>
<p><h1>â†’ model_looseã¯é€Ÿã„ãŒæŒ¯å‹•ã™ã‚‹</h1></p>
<p></code></pre></p>

<p></details></p>

<p>---</p>

<p><h3>å•é¡Œ3 (é›£æ˜“åº¦: hard)</h3></p>

<p>ææ–™æ¢ç´¢ã«ãŠã„ã¦ã€ä»¥ä¸‹ã®2ã¤ã®å ±é…¬è¨­è¨ˆã‚’æ¯”è¼ƒã—ã€ãã‚Œãã‚Œã®é•·æ‰€ãƒ»çŸ­æ‰€ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚ã¾ãŸã€å®Ÿéš›ã«ã‚³ãƒ¼ãƒ‰ã§å®Ÿé¨“ã—ã¦ãã ã•ã„ã€‚</p>

<p><strong>å ±é…¬Aï¼ˆç–å ±é…¬ï¼‰</strong>: ç›®æ¨™ã«åˆ°é”ã—ãŸã¨ãã®ã¿å ±é…¬1ã€ãã‚Œä»¥å¤–ã¯0</p>
<p><strong>å ±é…¬Bï¼ˆå¯†å ±é…¬ï¼‰</strong>: ç›®æ¨™ã¨ã®è·é›¢ã«å¿œã˜ãŸé€£ç¶šçš„ãªå ±é…¬</p>

<p><details></p>
<p><summary>ãƒ’ãƒ³ãƒˆ</summary></p>

<p>ç–å ±é…¬ã¯æ¢ç´¢ãŒå›°é›£ã§ã™ãŒã€å¯†å ±é…¬ã¯å±€æ‰€æœ€é©è§£ã«é™¥ã‚Šã‚„ã™ã„ã§ã™ã€‚ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒœãƒ¼ãƒŠã‚¹ã®å½±éŸ¿ã‚‚è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚</p>

<p></details></p>

<p><details></p>
<p><summary>è§£ç­”ä¾‹</summary></p>

<p><strong>å ±é…¬Aï¼ˆç–å ±é…¬ï¼‰ã®é•·æ‰€ãƒ»çŸ­æ‰€</strong>:</p>

<p><strong>é•·æ‰€</strong>:</p>
<ul>
<li>æ˜ç¢ºãªç›®æ¨™ï¼ˆæ›–æ˜§ã•ãŒãªã„ï¼‰</li>
<li>å±€æ‰€æœ€é©è§£ã«é™¥ã‚Šã«ãã„ï¼ˆä¸­é–“å ±é…¬ã«æƒ‘ã‚ã•ã‚Œãªã„ï¼‰</li>
</ul>

<p><strong>çŸ­æ‰€</strong>:</p>
<ul>
<li>æ¢ç´¢ãŒéå¸¸ã«å›°é›£ï¼ˆå­¦ç¿’ã‚·ã‚°ãƒŠãƒ«ãŒå¼±ã„ï¼‰</li>
<li>å­¦ç¿’ã«æ™‚é–“ãŒã‹ã‹ã‚‹</li>
</ul>

<p><strong>å ±é…¬Bï¼ˆå¯†å ±é…¬ï¼‰ã®é•·æ‰€ãƒ»çŸ­æ‰€</strong>:</p>

<p><strong>é•·æ‰€</strong>:</p>
<ul>
<li>æ¢ç´¢ãŒå®¹æ˜“ï¼ˆæ¯ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼‰</li>
<li>å­¦ç¿’ãŒé€Ÿã„</li>
</ul>

<p><strong>çŸ­æ‰€</strong>:</p>
<ul>
<li>å ±é…¬è¨­è¨ˆãŒé›£ã—ã„ï¼ˆè·é›¢ã ã‘ã§ã¯ä¸ååˆ†ãªå ´åˆã‚‚ï¼‰</li>
<li>å±€æ‰€æœ€é©è§£ã«é™¥ã‚Šã‚„ã™ã„</li>
</ul>

<p><strong>å®Ÿé¨“ã‚³ãƒ¼ãƒ‰</strong>:</p>
<p><pre><code class="language-python"><h1>å ±é…¬Aï¼ˆç–å ±é…¬ï¼‰</h1></p>
<p>class SparseRewardEnv(gym.Env):</p>
<p>    def step(self, action):</p>
<p>        <h1>... (çŠ¶æ…‹æ›´æ–°) ...</h1></p>
<p>        distance = np.linalg.norm(self.state - self.target)</p>

<p>        if distance < 0.5:</p>
<p>            reward = 1.0  <h1>åˆ°é”</h1></p>
<p>            done = True</p>
<p>        else:</p>
<p>            reward = 0.0  <h1>ãã‚Œä»¥å¤–</h1></p>
<p>            done = False</p>

<p>        return self.state, reward, done, {}</p>

<p><h1>å ±é…¬Bï¼ˆå¯†å ±é…¬ï¼‰</h1></p>
<p>class DenseRewardEnv(gym.Env):</p>
<p>    def step(self, action):</p>
<p>        <h1>... (çŠ¶æ…‹æ›´æ–°) ...</h1></p>
<p>        distance = np.linalg.norm(self.state - self.target)</p>
<p>        reward = -distance  <h1>é€£ç¶šçš„ãªå ±é…¬</h1></p>
<p>        done = distance < 0.5</p>

<p>        return self.state, reward, done, {}</p>

<p><h1>æ¯”è¼ƒå®Ÿé¨“</h1></p>
<p>model_sparse = PPO("MlpPolicy", DummyVecEnv([lambda: SparseRewardEnv()]))</p>
<p>model_dense = PPO("MlpPolicy", DummyVecEnv([lambda: DenseRewardEnv()]))</p>

<p>model_sparse.learn(total_timesteps=100000)</p>
<p>model_dense.learn(total_timesteps=100000)</p>

<p><h1>çµæœ: model_denseã®æ–¹ãŒå­¦ç¿’ãŒé€Ÿã„ãŒã€</h1></p>
<p><h1>è¤‡é›‘ãªç’°å¢ƒã§ã¯model_sparseã®æ–¹ãŒè‰¯ã„è§£ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã‚‚</h1></p>
<p></code></pre></p>

<p><strong>ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹</strong>: å¯†å ±é…¬ã‹ã‚‰å§‹ã‚ã€å•é¡Œã«å¿œã˜ã¦ç–å ±é…¬ã‚„<strong>å ±é…¬ã‚·ã‚§ã‚¤ãƒ”ãƒ³ã‚°</strong>ï¼ˆä¸­é–“å ±é…¬ã®è¿½åŠ ï¼‰ã‚’æ¤œè¨ã€‚</p>

<p></details></p>

<p>---</p>

<p><h2>ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã¾ã¨ã‚</h2></p>

<ul>
<li><strong>æ–¹ç­–å‹¾é…æ³•</strong>ã¯æ–¹ç­–ã‚’ç›´æ¥æœ€é©åŒ–ã—ã€é€£ç¶šè¡Œå‹•ã«å¯¾å¿œ</li>
<li><strong>REINFORCEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong>ã¯é«˜åˆ†æ•£ã ãŒã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§æ”¹å–„</li>
<li><strong>Actor-Critic</strong>ã¯Actorã¨Criticã‚’åŒæ™‚å­¦ç¿’ã—ã€ä½åˆ†æ•£ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’</li>
<li><strong>PPO</strong>ã¯ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚Šå®‰å®šã—ãŸå­¦ç¿’ã‚’å®Ÿç¾ã€æœ€å…ˆç«¯ã®å®Ÿç”¨çš„æ‰‹æ³•</li>
<li><strong>Stable Baselines3</strong>ã«ã‚ˆã‚Šã€ã‚ãšã‹æ•°è¡Œã§PPOã‚’å®Ÿè£…å¯èƒ½</li>
<li>é€£ç¶šè¡Œå‹•ç©ºé–“ã§ã¯<strong>ã‚¬ã‚¦ã‚¹æ–¹ç­–</strong>ã‚’ä½¿ç”¨</li>
</ul>

<p>æ¬¡ç« ã§ã¯ã€ææ–™æ¢ç´¢ã«ç‰¹åŒ–ã—ãŸã‚«ã‚¹ã‚¿ãƒ ç’°å¢ƒã®æ§‹ç¯‰ã¨å ±é…¬è¨­è¨ˆã‚’å­¦ã³ã¾ã™ã€‚</p>

<p>---</p>

<p><h2>å‚è€ƒæ–‡çŒ®</h2></p>

<ol>
<li>Williams "Simple statistical gradient-following algorithms for connectionist reinforcement learning" <em>Machine Learning</em> (1992) - REINFORCE</li>
<li>Mnih et al. "Asynchronous methods for deep reinforcement learning" <em>ICML</em> (2016) - A3C/A2C</li>
<li>Schulman et al. "Proximal policy optimization algorithms" <em>arXiv</em> (2017) - PPO</li>
<li>Schulman et al. "Trust region policy optimization" <em>ICML</em> (2015) - TRPO</li>
<li>Raffin et al. "Stable-Baselines3: Reliable reinforcement learning implementations" <em>JMLR</em> (2021)</li>
</ol>

<p>---</p>

<p><strong>æ¬¡ç« </strong>: <a href="chapter-3.html">ç¬¬3ç« : ææ–™æ¢ç´¢ç’°å¢ƒã®æ§‹ç¯‰</a></p>


        
        <div class="navigation">
            <a href="chapter-3.html" class="nav-button">æ¬¡ç« : ç¬¬3ç«  â†’</a>
            <a href="index.html" class="nav-button">â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
            <a href="chapter-1.html" class="nav-button">â† å‰ç« : ç¬¬1ç« </a>
        </div>
    
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimoto(æ±åŒ—å¤§å­¦)</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>