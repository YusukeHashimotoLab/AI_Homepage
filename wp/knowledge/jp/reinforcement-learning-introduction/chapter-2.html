<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Á¨¨2Á´†: Âº∑ÂåñÂ≠¶Áøí„ÅÆÂü∫Á§éÁêÜË´ñ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Á¨¨2Á´†: Âº∑ÂåñÂ≠¶Áøí„ÅÆÂü∫Á§éÁêÜË´ñ</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 20-25ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ÂàùÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 8ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 3Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">

<h1>Á¨¨2Á´†: Âº∑ÂåñÂ≠¶Áøí„ÅÆÂü∫Á§éÁêÜË´ñ</h1>

<h2>Â≠¶ÁøíÁõÆÊ®ô</h2>

„Åì„ÅÆÁ´†„Åß„ÅØ„ÄÅ‰ª•‰∏ã„ÇíÁøíÂæó„Åó„Åæ„ÅôÔºö

- ÊñπÁ≠ñÂãæÈÖçÊ≥ïÔºàPolicy Gradient MethodsÔºâ„ÅÆÁêÜË´ñ„Å®ÂÆüË£Ö
- Actor-Critic„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆ‰ªïÁµÑ„Åø
- Proximal Policy OptimizationÔºàPPOÔºâ„ÅÆË©≥Á¥∞
- Stable Baselines3„Å´„Çà„ÇãÂÆüË∑µÁöÑÂÆüË£Ö

---

<h2>2.1 ÊñπÁ≠ñÂãæÈÖçÊ≥ïÔºàPolicy Gradient MethodsÔºâ</h2>

<h3>QÂ≠¶Áøí„ÅÆÈôêÁïå</h3>

Á¨¨1Á´†„ÅÆQÂ≠¶Áøí„ÉªDQN„ÅØ<strong>‰æ°ÂÄ§„Éô„Éº„Çπ</strong>„ÅÆÊâãÊ≥ï„Åß„Åó„Åü„ÄÇ„Åì„Çå„Çâ„Å´„ÅØ‰ª•‰∏ã„ÅÆÈôêÁïå„Åå„ÅÇ„Çä„Åæ„ÅôÔºö

1. <strong>Èõ¢Êï£Ë°åÂãï„ÅÆ„Åø</strong>: $\arg\max_a Q(s,a)$„ÅØÈÄ£Á∂öË°åÂãïÁ©∫Èñì„ÅßÂõ∞Èõ£
2. <strong>Ê±∫ÂÆöÁöÑÊñπÁ≠ñ</strong>: Â∏∏„Å´Âêå„ÅòË°åÂãï„ÇíÈÅ∏ÊäûÔºàÁ¢∫ÁéáÁöÑÊñπÁ≠ñ„ÅåÂ≠¶Áøí„Åß„Åç„Å™„ÅÑÔºâ
3. <strong>Â∞è„Åï„Å™Â§âÂåñ„Å´ËÑÜÂº±</strong>: QÂÄ§„ÅÆÂæÆÂ∞è„Å™Â§âÂåñ„ÅßÊñπÁ≠ñ„ÅåÂ§ß„Åç„ÅèÂ§â„Çè„Çã

ÊùêÊñôÁßëÂ≠¶„Åß„ÅØ„ÄÅ<strong>ÈÄ£Á∂öÁöÑ„Å™Âà∂Âæ°</strong>ÔºàÊ∏©Â∫¶„Çí0.5Â∫¶‰∏ä„Åí„Çã„ÄÅÁµÑÊàêÊØî„Çí2%Â§â„Åà„ÇãÔºâ„ÅåÈáçË¶Å„Åß„Åô„ÄÇ

<h3>ÊñπÁ≠ñÂãæÈÖçÊ≥ï„ÅÆÂü∫Êú¨„Ç¢„Ç§„Éá„Ç¢</h3>

ÊñπÁ≠ñÂãæÈÖçÊ≥ï„ÅØ„ÄÅ<strong>ÊñπÁ≠ñ„ÇíÁõ¥Êé•ÊúÄÈÅ©Âåñ</strong>„Åó„Åæ„ÅôÔºö

$$
\pi_\theta(a|s) = P(a|s; \theta)
$$

- $\theta$: ÊñπÁ≠ñ„ÅÆ„Éë„É©„É°„Éº„ÇøÔºà„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÈáç„ÅøÔºâ

<strong>ÁõÆÁöÑ</strong>: ÊúüÂæÖÁ¥ØÁ©çÂ†±ÈÖ¨$J(\theta)$„ÇíÊúÄÂ§ßÂåñ

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T r_t \right]
$$

- $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots)$: ËªåË∑°ÔºàtrajectoryÔºâ

<h3>ÊñπÁ≠ñÂãæÈÖçÂÆöÁêÜ</h3>

<strong>REINFORCE</strong>„Ç¢„É´„Ç¥„É™„Ç∫„É†ÔºàWilliams, 1992Ôºâ„ÅØ„ÄÅÂãæÈÖç„Çí‰ª•‰∏ã„ÅßË®àÁÆó„Åó„Åæ„ÅôÔºö

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot R_t \right]
$$

- $R_t = \sum_{k=t}^T \gamma^{k-t} r_k$: ÊôÇÂàª$t$„Åã„Çâ„ÅÆÁ¥ØÁ©çÂ†±ÈÖ¨Ôºà„É™„Çø„Éº„É≥Ôºâ

<strong>Áõ¥ÊÑüÁöÑÊÑèÂë≥</strong>:
- È´ò„ÅÑÂ†±ÈÖ¨„ÇíÂæó„ÅüË°åÂãï„ÅÆÁ¢∫Áéá„Çí‰∏ä„Åí„Çã
- ‰Ωé„ÅÑÂ†±ÈÖ¨„ÇíÂæó„ÅüË°åÂãï„ÅÆÁ¢∫Áéá„Çí‰∏ã„Åí„Çã

<h3>REINFORCE„ÅÆÂÆüË£Ö</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

class PolicyNetwork(nn.Module):
    """ÊñπÁ≠ñ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ

    Áä∂ÊÖã„ÇíÂÖ•Âäõ„Åó„ÄÅÂêÑË°åÂãï„ÅÆÁ¢∫Áéá„ÇíÂá∫Âäõ
    """
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return torch.softmax(self.fc3(x), dim=-1)  # Á¢∫ÁéáÂàÜÂ∏É


class REINFORCEAgent:
    """REINFORCE„Ç¢„É´„Ç¥„É™„Ç∫„É†"""
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):
        self.gamma = gamma
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)

        # „Ç®„Éî„ÇΩ„Éº„ÉâÂÜÖ„ÅÆ„É≠„Ç∞„Çí‰øùÂ≠ò
        self.log_probs = []
        self.rewards = []

    def select_action(self, state):
        """ÊñπÁ≠ñ„Å´Âæì„Å£„Å¶Ë°åÂãï„Çí„Çµ„É≥„Éó„É™„É≥„Ç∞"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        probs = self.policy(state_tensor)

        # Á¢∫ÁéáÂàÜÂ∏É„Åã„Çâ„Çµ„É≥„Éó„É™„É≥„Ç∞
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()

        # logÁ¢∫Áéá„Çí‰øùÂ≠òÔºàÂãæÈÖçË®àÁÆóÁî®Ôºâ
        self.log_probs.append(action_dist.log_prob(action))

        return action.item()

    def store_reward(self, reward):
        """Â†±ÈÖ¨„Çí‰øùÂ≠ò"""
        self.rewards.append(reward)

    def update(self):
        """„Ç®„Éî„ÇΩ„Éº„ÉâÁµÇ‰∫ÜÂæå„Å´ÊñπÁ≠ñ„ÇíÊõ¥Êñ∞"""
        # „É™„Çø„Éº„É≥ÔºàÁ¥ØÁ©çÂ†±ÈÖ¨Ôºâ„ÇíË®àÁÆó
        returns = []
        R = 0
        for r in reversed(self.rewards):
            R = r + self.gamma * R
            returns.insert(0, R)

        returns = torch.FloatTensor(returns)

        # Ê≠£Ë¶èÂåñÔºàÂ≠¶Áøí„ÇíÂÆâÂÆöÂåñÔºâ
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        # ÊñπÁ≠ñÂãæÈÖç
        policy_loss = []
        for log_prob, R in zip(self.log_probs, returns):
            policy_loss.append(-log_prob * R)

        # ÂãæÈÖçÈôç‰∏ã
        self.optimizer.zero_grad()
        loss = torch.stack(policy_loss).sum()
        loss.backward()
        self.optimizer.step()

        # „É≠„Ç∞„Çí„É™„Çª„ÉÉ„Éà
        self.log_probs = []
        self.rewards = []


<h1>Á∞°Âçò„Å™ÊùêÊñôÊé¢Á¥¢Áí∞Â¢ÉÔºàÈõ¢Êï£Ë°åÂãïÁâàÔºâ</h1>
class DiscreteMaterialsEnv:
    """Èõ¢Êï£Ë°åÂãï„ÅÆÊùêÊñôÊé¢Á¥¢Áí∞Â¢É"""
    def __init__(self, state_dim=4):
        self.state_dim = state_dim
        self.target = np.array([3.0, 5.0, 2.5, 4.0])
        self.state = None

    def reset(self):
        self.state = np.random.uniform(0, 10, self.state_dim)
        return self.state

    def step(self, action):
        # Ë°åÂãï: 0=Ê¨°ÂÖÉ0Â¢óÂä†, 1=Ê¨°ÂÖÉ0Ê∏õÂ∞ë, 2=Ê¨°ÂÖÉ1Â¢óÂä†, 3=Ê¨°ÂÖÉ1Ê∏õÂ∞ë
        dim = action // 2
        delta = 0.5 if action % 2 == 0 else -0.5

        self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)

        # Â†±ÈÖ¨: ÁõÆÊ®ô„Å®„ÅÆË∑ùÈõ¢
        distance = np.linalg.norm(self.state - self.target)
        reward = -distance

        done = distance < 0.5

        return self.state, reward, done


<h1>REINFORCE„ÅÆË®ìÁ∑¥</h1>
env = DiscreteMaterialsEnv()
agent = REINFORCEAgent(state_dim=4, action_dim=4)

episodes = 1000
rewards_history = []

for episode in range(episodes):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent.select_action(state)
        next_state, reward, done = env.step(action)

        agent.store_reward(reward)
        state = next_state
        total_reward += reward

    # „Ç®„Éî„ÇΩ„Éº„ÉâÁµÇ‰∫ÜÂæå„Å´Êõ¥Êñ∞
    agent.update()
    rewards_history.append(total_reward)

    if (episode + 1) % 100 == 0:
        avg_reward = np.mean(rewards_history[-100:])
        print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}")

<h1>Â≠¶ÁøíÊõ≤Á∑ö</h1>
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'))
plt.xlabel('Episode')
plt.ylabel('Average Reward (20 episodes)')
plt.title('REINFORCE: ÊñπÁ≠ñÂãæÈÖçÊ≥ï„Å´„Çà„ÇãÊùêÊñôÊé¢Á¥¢')
plt.grid(True)
plt.show()</code></pre>

<strong>Âá∫Âäõ‰æã</strong>:
<pre><code>Episode 100: Avg Reward = -38.24
Episode 200: Avg Reward = -28.15
Episode 500: Avg Reward = -15.32
Episode 1000: Avg Reward = -7.89</code></pre>

---

<h2>2.2 „Éô„Éº„Çπ„É©„Ç§„É≥„Å®ÂàÜÊï£ÂâäÊ∏õ</h2>

<h3>REINFORCE„ÅÆÂïèÈ°åÁÇπ</h3>

REINFORCE„ÅØ<strong>È´òÂàÜÊï£</strong>Ôºàhigh varianceÔºâ„Åß„Åô„ÄÇÂêå„ÅòÊñπÁ≠ñ„Åß„ÇÇ„ÄÅÈÅã„ÅåËâØ„ÅÑ„ÅãÊÇ™„ÅÑ„Åã„Åß„É™„Çø„Éº„É≥$R_t$„ÅåÂ§ß„Åç„ÅèÂ§âÂãï„Åó„Åæ„Åô„ÄÇ

<h3>„Éô„Éº„Çπ„É©„Ç§„É≥„ÅÆÂ∞éÂÖ•</h3>

<strong>„Éô„Éº„Çπ„É©„Ç§„É≥</strong> $b(s)$„ÇíÂºï„Åè„Åì„Å®„Åß„ÄÅÂàÜÊï£„ÇíÂâäÊ∏õÔºö

$$
\nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot (R_t - b(s_t)) \right]
$$

<strong>ÊúÄÈÅ©„Å™„Éô„Éº„Çπ„É©„Ç§„É≥</strong>: Áä∂ÊÖã‰æ°ÂÄ§Èñ¢Êï∞$V(s)$

$$
b(s_t) = V(s_t) = \mathbb{E}_{\pi} \left[ \sum_{k=t}^T \gamma^{k-t} r_k \mid s_t \right]
$$

<strong>„Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏Èñ¢Êï∞</strong> $A(s, a)$:
$$
A(s, a) = Q(s, a) - V(s) = R_t - V(s_t)
$$

„Äå„Åì„ÅÆË°åÂãï„ÅØÂπ≥Âùá„Çà„Çä„Å©„Çå„Å†„ÅëËâØ„ÅÑ„Åã„Äç„ÇíË°®„Åó„Åæ„Åô„ÄÇ

<h3>„Éô„Éº„Çπ„É©„Ç§„É≥‰ªò„ÅçREINFORCE</h3>

<pre><code class="language-python">class ValueNetwork(nn.Module):
    """‰æ°ÂÄ§„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÔºà„Éô„Éº„Çπ„É©„Ç§„É≥Ôºâ"""
    def __init__(self, state_dim, hidden_dim=64):
        super(ValueNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)  # Áä∂ÊÖã‰æ°ÂÄ§„ÇíÂá∫Âäõ

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)


class REINFORCEWithBaseline:
    """„Éô„Éº„Çπ„É©„Ç§„É≥‰ªò„ÅçREINFORCE"""
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):
        self.gamma = gamma
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.value = ValueNetwork(state_dim)

        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.value_optimizer = optim.Adam(self.value.parameters(), lr=lr)

        self.log_probs = []
        self.rewards = []
        self.states = []

    def select_action(self, state):
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        probs = self.policy(state_tensor)

        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()

        self.log_probs.append(action_dist.log_prob(action))
        self.states.append(state)

        return action.item()

    def store_reward(self, reward):
        self.rewards.append(reward)

    def update(self):
        # „É™„Çø„Éº„É≥Ë®àÁÆó
        returns = []
        R = 0
        for r in reversed(self.rewards):
            R = r + self.gamma * R
            returns.insert(0, R)

        returns = torch.FloatTensor(returns)
        states = torch.FloatTensor(self.states)

        # ‰æ°ÂÄ§„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÂá∫ÂäõÔºà„Éô„Éº„Çπ„É©„Ç§„É≥Ôºâ
        values = self.value(states).squeeze()

        # „Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏ = „É™„Çø„Éº„É≥ - „Éô„Éº„Çπ„É©„Ç§„É≥
        advantages = returns - values.detach()

        # ÊñπÁ≠ñÂãæÈÖçÊêçÂ§±
        policy_loss = []
        for log_prob, adv in zip(self.log_probs, advantages):
            policy_loss.append(-log_prob * adv)

        # ‰æ°ÂÄ§„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÊêçÂ§±ÔºàMSEÔºâ
        value_loss = nn.MSELoss()(values, returns)

        # ÊúÄÈÅ©Âåñ
        self.policy_optimizer.zero_grad()
        torch.stack(policy_loss).sum().backward()
        self.policy_optimizer.step()

        self.value_optimizer.zero_grad()
        value_loss.backward()
        self.value_optimizer.step()

        # „É™„Çª„ÉÉ„Éà
        self.log_probs = []
        self.rewards = []
        self.states = []


<h1>Ë®ìÁ∑¥Ôºà„Éô„Éº„Çπ„É©„Ç§„É≥‰ªò„ÅçÔºâ</h1>
agent_baseline = REINFORCEWithBaseline(state_dim=4, action_dim=4)

rewards_baseline = []
for episode in range(1000):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent_baseline.select_action(state)
        next_state, reward, done = env.step(action)

        agent_baseline.store_reward(reward)
        state = next_state
        total_reward += reward

    agent_baseline.update()
    rewards_baseline.append(total_reward)

<h1>ÊØîËºÉ</h1>
plt.figure(figsize=(10, 6))
plt.plot(np.convolve(rewards_history, np.ones(20)/20, mode='valid'), label='REINFORCE')
plt.plot(np.convolve(rewards_baseline, np.ones(20)/20, mode='valid'), label='REINFORCE + Baseline')
plt.xlabel('Episode')
plt.ylabel('Average Reward')
plt.title('„Éô„Éº„Çπ„É©„Ç§„É≥„Å´„Çà„ÇãÂ≠¶ÁøíÂÆâÂÆöÂåñ')
plt.legend()
plt.grid(True)
plt.show()</code></pre>

<strong>ÁµêÊûú</strong>: „Éô„Éº„Çπ„É©„Ç§„É≥„Å´„Çà„ÇäÂ≠¶Áøí„Åå<strong>„Çà„ÇäÂÆâÂÆö</strong>„Åó„ÄÅÂèéÊùü„Åå<strong>ÈÄü„Åè</strong>„Å™„Çä„Åæ„Åô„ÄÇ

---

<h2>2.3 Actor-Critic„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£</h2>

<h3>Actor-Critic„ÅÆÊ¶ÇÂøµ</h3>

<strong>ActorÔºàÊñπÁ≠ñÔºâ</strong> „Å® <strong>CriticÔºà‰æ°ÂÄ§Èñ¢Êï∞Ôºâ</strong> „ÇíÂêåÊôÇ„Å´Â≠¶ÁøíÔºö

- <strong>Actor</strong> $\pi_\theta(a|s)$: Ë°åÂãï„ÇíÈÅ∏Êäû
- <strong>Critic</strong> $V_\phi(s)$: Áä∂ÊÖã„ÅÆ‰æ°ÂÄ§„ÇíË©ï‰æ°

<div class="mermaid">graph LR
    S[Áä∂ÊÖã s] --> A[Actor<br/>œÄŒ∏]
    S --> C[Critic<br/>Vœï]
    A -->|Ë°åÂãï a| E[Áí∞Â¢É]
    E -->|Â†±ÈÖ¨ r| C
    C -->|TDË™§Â∑Æ| A
    C -->|‰æ°ÂÄ§Ë©ï‰æ°| C

    style A fill:#e1f5ff
    style C fill:#ffe1cc</div>

<h3>TDË™§Â∑Æ„Å®„Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏</h3>

<strong>TDË™§Â∑Æ</strong>ÔºàTemporal Difference ErrorÔºâ:
$$
\delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)
$$

„Åì„Çå„ÅØ<strong>1„Çπ„ÉÜ„ÉÉ„Éó„ÅÆ„Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏Êé®ÂÆö</strong>„Å®„Åó„Å¶‰Ωø„Åà„Åæ„Åô„ÄÇ

<h3>A2CÔºàAdvantage Actor-CriticÔºâ</h3>

<pre><code class="language-python">class A2CAgent:
    """Advantage Actor-Critic"""
    def __init__(self, state_dim, action_dim, lr_actor=1e-3, lr_critic=1e-3, gamma=0.99):
        self.gamma = gamma

        self.actor = PolicyNetwork(state_dim, action_dim)
        self.critic = ValueNetwork(state_dim)

        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)

    def select_action(self, state):
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        probs = self.actor(state_tensor)

        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()

        return action.item(), action_dist.log_prob(action)

    def update(self, state, action_log_prob, reward, next_state, done):
        """1„Çπ„ÉÜ„ÉÉ„Éó„Åî„Å®„Å´Êõ¥Êñ∞"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)

        # ÁèæÂú®„Å®Ê¨°„ÅÆÁä∂ÊÖã‰æ°ÂÄ§
        value = self.critic(state_tensor)
        next_value = self.critic(next_state_tensor)

        # TDÁõÆÊ®ô„Å®TDË™§Â∑Æ
        td_target = reward + (1 - done) * self.gamma * next_value.item()
        td_error = td_target - value.item()

        # CriticÊêçÂ§±ÔºàMSEÔºâ
        critic_loss = (torch.FloatTensor([td_target]) - value).pow(2)

        # ActorÊêçÂ§±ÔºàÊñπÁ≠ñÂãæÈÖç √ó „Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏Ôºâ
        actor_loss = -action_log_prob * td_error

        # ÊúÄÈÅ©Âåñ
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()


<h1>A2CË®ìÁ∑¥</h1>
agent_a2c = A2CAgent(state_dim=4, action_dim=4)

rewards_a2c = []
for episode in range(1000):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action, log_prob = agent_a2c.select_action(state)
        next_state, reward, done = env.step(action)

        # 1„Çπ„ÉÜ„ÉÉ„Éó„Åî„Å®„Å´Êõ¥Êñ∞
        agent_a2c.update(state, log_prob, reward, next_state, done)

        state = next_state
        total_reward += reward

    rewards_a2c.append(total_reward)

    if (episode + 1) % 100 == 0:
        avg_reward = np.mean(rewards_a2c[-100:])
        print(f"Episode {episode+1}: Avg Reward = {avg_reward:.2f}")</code></pre>

<strong>Âà©ÁÇπ</strong>:
- „Ç®„Éî„ÇΩ„Éº„ÉâÁµÇ‰∫Ü„ÇíÂæÖ„Åü„Åö„Å´<strong>„Ç™„É≥„É©„Ç§„É≥Â≠¶Áøí</strong>
- TDË™§Â∑Æ„Å´„Çà„Çä<strong>‰ΩéÂàÜÊï£</strong>

---

<h2>2.4 Proximal Policy OptimizationÔºàPPOÔºâ</h2>

<h3>Trust Region Methods</h3>

ÊñπÁ≠ñÂãæÈÖçÊ≥ï„Åß„ÅØ„ÄÅ<strong>Êõ¥Êñ∞„ÅåÂ§ß„Åç„Åô„Åé„Çã„Å®ÊñπÁ≠ñ„ÅåÂ¥©Â£ä</strong>„Åô„Çã„Åì„Å®„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ

<strong>Trust Region Policy OptimizationÔºàTRPOÔºâ</strong> „ÅØ„ÄÅÊñπÁ≠ñ„ÅÆÂ§âÂåñ„ÇíÂà∂Á¥ÑÔºö

$$
\max_\theta \mathbb{E} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A(s, a) \right] \quad \text{s.t.} \quad D_{\text{KL}}(\pi_{\theta_{\text{old}}} \| \pi_\theta) \leq \delta
$$

„Åó„Åã„Åó„ÄÅKL„ÉÄ„Ç§„Éê„Éº„Ç∏„Çß„É≥„ÇπÂà∂Á¥Ñ„ÅÆÊúÄÈÅ©Âåñ„ÅØË§áÈõë„Åß„Åô„ÄÇ

<h3>PPO„ÅÆÁ∞°Áï•Âåñ</h3>

<strong>PPO</strong>ÔºàSchulman et al., 2017Ôºâ„ÅØ„ÄÅÂà∂Á¥Ñ„Çí<strong>ÊêçÂ§±Èñ¢Êï∞ÂÜÖ„ÅÆ„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞</strong>„ÅßÂÆüÁèæÔºö

$$
L^{\text{CLIP}}(\theta) = \mathbb{E} \left[ \min \left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right]
$$

- $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$: ÈáçË¶ÅÂ∫¶ÊØîÁéáÔºàimportance ratioÔºâ
- $\epsilon$: „ÇØ„É™„ÉÉ„Éî„É≥„Ç∞ÁØÑÂõ≤ÔºàÈÄöÂ∏∏0.1„Äú0.2Ôºâ

<strong>Áõ¥ÊÑü</strong>:
- „Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏„ÅåÊ≠£ÔºàËâØ„ÅÑË°åÂãïÔºâ‚Üí $r_t$„ÇíÂ¢ó„ÇÑ„Åô„Åå„ÄÅ$1+\epsilon$„Åß‰∏äÈôê
- „Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏„ÅåË≤†ÔºàÊÇ™„ÅÑË°åÂãïÔºâ‚Üí $r_t$„ÇíÊ∏õ„Çâ„Åô„Åå„ÄÅ$1-\epsilon$„Åß‰∏ãÈôê
- ÊÄ•ÊøÄ„Å™ÊñπÁ≠ñÂ§âÂåñ„ÇíÈò≤„Åê

<h3>„Ç®„É≥„Éà„É≠„Éî„Éº„Éú„Éº„Éä„Çπ</h3>

Êé¢Á¥¢„Çí‰øÉÈÄ≤„Åô„Çã„Åü„ÇÅ„ÄÅ<strong>„Ç®„É≥„Éà„É≠„Éî„Éº</strong>„ÇíÊêçÂ§±„Å´ËøΩÂä†Ôºö

$$
L^{\text{PPO}}(\theta) = L^{\text{CLIP}}(\theta) + c_1 L^{\text{VF}}(\theta) - c_2 H[\pi_\theta]
$$

- $L^{\text{VF}}$: ‰æ°ÂÄ§Èñ¢Êï∞„ÅÆÊêçÂ§±
- $H[\pi_\theta] = -\sum_a \pi_\theta(a|s) \log \pi_\theta(a|s)$: „Ç®„É≥„Éà„É≠„Éî„ÉºÔºàÁ¢∫ÁéáÂàÜÂ∏É„ÅÆ‰∏çÁ¢∫ÂÆüÊÄßÔºâ
- $c_2$: „Ç®„É≥„Éà„É≠„Éî„Éº‰øÇÊï∞ÔºàÈÄöÂ∏∏0.01Ôºâ

<h3>PPO„ÅÆÂÆüË£ÖÔºàStable Baselines3‰ΩøÁî®Ôºâ</h3>

<pre><code class="language-python">from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
import gym

<h1>GymÁí∞Â¢É„É©„ÉÉ„Éë„Éº</h1>
class GymMaterialsEnv(gym.Env):
    """OpenAI Gym‰∫íÊèõ„ÅÆÊùêÊñôÊé¢Á¥¢Áí∞Â¢É"""
    def __init__(self):
        super(GymMaterialsEnv, self).__init__()
        self.state_dim = 4
        self.target = np.array([3.0, 5.0, 2.5, 4.0])

        # Ë°åÂãï„ÉªÁä∂ÊÖãÁ©∫Èñì„ÅÆÂÆöÁæ©
        self.action_space = gym.spaces.Discrete(4)
        self.observation_space = gym.spaces.Box(
            low=0, high=10, shape=(self.state_dim,), dtype=np.float32
        )

        self.state = None

    def reset(self):
        self.state = np.random.uniform(0, 10, self.state_dim).astype(np.float32)
        return self.state

    def step(self, action):
        dim = action // 2
        delta = 0.5 if action % 2 == 0 else -0.5

        self.state[dim] = np.clip(self.state[dim] + delta, 0, 10)

        distance = np.linalg.norm(self.state - self.target)
        reward = -distance
        done = distance < 0.5

        return self.state, reward, done, {}

    def render(self, mode='human'):
        pass


<h1>Áí∞Â¢É‰ΩúÊàê</h1>
env = DummyVecEnv([lambda: GymMaterialsEnv()])

<h1>PPO„É¢„Éá„É´</h1>
model = PPO(
    "MlpPolicy",                # Â§öÂ±§„Éë„Éº„Çª„Éó„Éà„É≠„É≥ÊñπÁ≠ñ
    env,
    learning_rate=3e-4,
    n_steps=2048,               # Êõ¥Êñ∞Ââç„ÅÆ„Çπ„ÉÜ„ÉÉ„ÉóÊï∞
    batch_size=64,
    n_epochs=10,                # ÂêÑÊõ¥Êñ∞„Åß„ÅÆÊúÄÈÅ©Âåñ„Ç®„Éù„ÉÉ„ÇØÊï∞
    gamma=0.99,
    gae_lambda=0.95,            # GAEÔºàGeneralized Advantage EstimationÔºâ
    clip_range=0.2,             # PPO„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞ÁØÑÂõ≤
    ent_coef=0.01,              # „Ç®„É≥„Éà„É≠„Éî„Éº‰øÇÊï∞
    verbose=1,
    tensorboard_log="./ppo_materials_tensorboard/"
)

<h1>Ë®ìÁ∑¥</h1>
model.learn(total_timesteps=100000)

<h1>‰øùÂ≠ò</h1>
model.save("ppo_materials_agent")

<h1>Ë©ï‰æ°</h1>
eval_env = GymMaterialsEnv()
state = eval_env.reset()
total_reward = 0

for _ in range(100):
    action, _ = model.predict(state, deterministic=True)
    state, reward, done, _ = eval_env.step(action)
    total_reward += reward

    if done:
        break

print(f"Ë©ï‰æ°ÁµêÊûú: Total Reward = {total_reward:.2f}")
print(f"ÊúÄÁµÇÁä∂ÊÖã: {state}")
print(f"ÁõÆÊ®ô: {eval_env.target}")</code></pre>

<strong>Âá∫Âäõ‰æã</strong>:
<pre><code>---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.2     |
|    ep_rew_mean     | -15.3    |
| time/              |          |
|    fps             | 1024     |
|    iterations      | 50       |
|    time_elapsed    | 97       |
|    total_timesteps | 102400   |
---------------------------------

Ë©ï‰æ°ÁµêÊûú: Total Reward = -5.23
ÊúÄÁµÇÁä∂ÊÖã: [3.02 4.98 2.47 3.95]
ÁõÆÊ®ô: [3.  5.  2.5 4. ]</code></pre>

<strong>Ëß£Ë™¨</strong>:
- Stable Baselines3„Å´„Çà„Çä„ÄÅ„Çè„Åö„ÅãÊï∞Ë°å„ÅßPPO„ÇíÂÆüË£Ö
- TensorBoard„ÅßÂ≠¶ÁøíÈÄ≤Êçó„ÇíÂèØË¶ñÂåñÂèØËÉΩ
- ÁõÆÊ®ô„Å´ÈùûÂ∏∏„Å´Ëøë„ÅÑÊùêÊñô„ÇíÁô∫Ë¶ã

---

<h2>2.5 ÈÄ£Á∂öË°åÂãïÁ©∫Èñì„Å∏„ÅÆÊã°Âºµ</h2>

<h3>„Ç¨„Ç¶„ÇπÊñπÁ≠ñ</h3>

ÊùêÊñôÁßëÂ≠¶„Åß„ÅØ„ÄÅÊ∏©Â∫¶„ÇÑÁµÑÊàêÊØî„Å™„Å©<strong>ÈÄ£Á∂öÁöÑ„Å™Âà∂Âæ°</strong>„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ

ÈÄ£Á∂öË°åÂãï„Å´„ÅØ<strong>„Ç¨„Ç¶„ÇπÂàÜÂ∏ÉÊñπÁ≠ñ</strong>„Çí‰ΩøÁî®Ôºö

$$
\pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma_\theta(s))
$$

- $\mu_\theta(s)$: Âπ≥ÂùáÔºà„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅßÂá∫ÂäõÔºâ
- $\sigma_\theta(s)$: Ê®ôÊ∫ñÂÅèÂ∑ÆÔºàÂ≠¶ÁøíÂèØËÉΩ„Åæ„Åü„ÅØÂõ∫ÂÆöÔºâ

<h3>ÈÄ£Á∂öË°åÂãïÁâàPPO</h3>

<pre><code class="language-python"><h1>ÈÄ£Á∂öË°åÂãïÁí∞Â¢É</h1>
class ContinuousGymMaterialsEnv(gym.Env):
    """ÈÄ£Á∂öË°åÂãï„ÅÆÊùêÊñôÊé¢Á¥¢Áí∞Â¢É"""
    def __init__(self):
        super(ContinuousGymMaterialsEnv, self).__init__()
        self.state_dim = 4
        self.target = np.array([3.0, 5.0, 2.5, 4.0])

        # ÈÄ£Á∂öË°åÂãïÁ©∫ÈñìÔºà4Ê¨°ÂÖÉ„Éô„ÇØ„Éà„É´„ÄÅÁØÑÂõ≤ [-1, 1]Ôºâ
        self.action_space = gym.spaces.Box(
            low=-1, high=1, shape=(self.state_dim,), dtype=np.float32
        )
        self.observation_space = gym.spaces.Box(
            low=0, high=10, shape=(self.state_dim,), dtype=np.float32
        )

        self.state = None

    def reset(self):
        self.state = np.random.uniform(0, 10, self.state_dim).astype(np.float32)
        return self.state

    def step(self, action):
        # Ë°åÂãï„ÇíÁä∂ÊÖãÂ§âÂåñ„Å´„Éû„ÉÉ„Éî„É≥„Ç∞Ôºà-1„Äú1 ‚Üí -0.5„Äú0.5Ôºâ
        delta = action * 0.5
        self.state = np.clip(self.state + delta, 0, 10)

        distance = np.linalg.norm(self.state - self.target)
        reward = -distance
        done = distance < 0.3

        return self.state, reward, done, {}

    def render(self, mode='human'):
        pass


<h1>ÈÄ£Á∂öË°åÂãïÁâàPPO</h1>
env_continuous = DummyVecEnv([lambda: ContinuousGymMaterialsEnv()])

model_continuous = PPO(
    "MlpPolicy",
    env_continuous,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    clip_range=0.2,
    verbose=1
)

model_continuous.learn(total_timesteps=100000)

<h1>Ë©ï‰æ°</h1>
eval_env_cont = ContinuousGymMaterialsEnv()
state = eval_env_cont.reset()

for _ in range(50):
    action, _ = model_continuous.predict(state, deterministic=True)
    state, reward, done, _ = eval_env_cont.step(action)

    if done:
        break

print(f"ÊúÄÁµÇÁä∂ÊÖã: {state}")
print(f"ÁõÆÊ®ô: {eval_env_cont.target}")
print(f"Ë∑ùÈõ¢: {np.linalg.norm(state - eval_env_cont.target):.4f}")</code></pre>

<strong>Âá∫Âäõ‰æã</strong>:
<pre><code>ÊúÄÁµÇÁä∂ÊÖã: [3.001 5.003 2.498 3.997]
ÁõÆÊ®ô: [3.  5.  2.5 4. ]
Ë∑ùÈõ¢: 0.0054</code></pre>

<strong>Ëß£Ë™¨</strong>: ÈÄ£Á∂öË°åÂãï„Å´„Çà„Çä„ÄÅÁõÆÊ®ô„Å∏„ÅÆ<strong>Á≤æÂØÜ„Å™Âà∂Âæ°</strong>„ÅåÂèØËÉΩ

---

<h2>ÊºîÁøíÂïèÈ°å</h2>

<h3>ÂïèÈ°å1 (Èõ£ÊòìÂ∫¶: easy)</h3>

„Éô„Éº„Çπ„É©„Ç§„É≥„Çí‰Ωø„ÅÜ„Å®ÂàÜÊï£„ÅåÊ∏õ„ÇãÁêÜÁî±„Çí„ÄÅ‰ª•‰∏ã„ÅÆÂºè„Çí‰Ωø„Å£„Å¶Ë™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

$$
\text{Var}[R_t] \quad \text{vs.} \quad \text{Var}[R_t - b(s_t)]
$$

<details>
<summary>„Éí„É≥„Éà</summary>

ÂàÜÊï£„ÅÆÊÄßË≥™: $\text{Var}[X - c] = \text{Var}[X]$ÔºàÂÆöÊï∞$c$„ÇíÂºï„ÅÑ„Å¶„ÇÇÂàÜÊï£„ÅØÂ§â„Çè„Çâ„Å™„ÅÑÔºâ„Åß„Åô„Åå„ÄÅ$b(s_t)$„ÅØÁä∂ÊÖã‰æùÂ≠ò„Å™„ÅÆ„ÅßÂÆöÊï∞„Åß„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>

„Éô„Éº„Çπ„É©„Ç§„É≥$b(s_t)$„ÅåÁä∂ÊÖã‰æ°ÂÄ§$V(s_t)$„Å´Ëøë„ÅÑ„Å®„ÅçÔºö

- <strong>„É™„Çø„Éº„É≥</strong> $R_t$„ÅØÁä∂ÊÖã„Å´„Çà„Å£„Å¶Â§ß„Åç„ÅèÂ§âÂãïÔºàÈÅã„Å´„Çà„ÇãÂΩ±ÈüøÂ§ßÔºâ
- <strong>„Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏</strong> $R_t - V(s_t)$„ÅØ„ÄåÂπ≥Âùá„Åã„Çâ„ÅÆ„Ç∫„É¨„Äç„Å™„ÅÆ„ÅßÂ§âÂãï„ÅåÂ∞è„Åï„ÅÑ

Êï∞Â≠¶ÁöÑ„Å´„ÅØÔºö
$$
\text{Var}[R_t - V(s_t)] \leq \text{Var}[R_t]
$$

„Åì„Çå„ÅØ$V(s_t)$„Åå„ÄåÁä∂ÊÖã$s_t$„Åã„Çâ„ÅÆÊúüÂæÖÁ¥ØÁ©çÂ†±ÈÖ¨„Äç„Å™„ÅÆ„Åß„ÄÅÈÅã„ÅÆÂΩ±Èüø„Çí„Ç≠„É£„É≥„Çª„É´„Åô„Çã„Åü„ÇÅ„Åß„Åô„ÄÇ

<strong>ÂÖ∑‰Ωì‰æã</strong>:
- Áä∂ÊÖãA„Åã„Çâ„ÅÆ„É™„Çø„Éº„É≥: 100, 105, 95 ‚Üí ÂàÜÊï£ = 25
- Áä∂ÊÖãA„ÅÆ‰æ°ÂÄ§: 100
- „Ç¢„Éâ„Éê„É≥„ÉÜ„Éº„Ç∏: 0, 5, -5 ‚Üí ÂàÜÊï£ = 25ÔºàÂêå„ÅòÔºâ

„Åó„Åã„Åó„ÄÅË§áÊï∞„ÅÆÁä∂ÊÖã„ÇíËÄÉ„Åà„Çã„Å®Ôºö
- Áä∂ÊÖãA„ÅÆ„É™„Çø„Éº„É≥: 100¬±5
- Áä∂ÊÖãB„ÅÆ„É™„Çø„Éº„É≥: 50¬±5
- ÂÖ®‰Ωì„ÅÆÂàÜÊï£: Â§ß„Åç„ÅÑ

„Éô„Éº„Çπ„É©„Ç§„É≥„ÅßÁä∂ÊÖã„Åî„Å®„ÅÆÂπ≥Âùá„ÇíÂºï„Åè„Å®„ÄÅÁä∂ÊÖãÈñì„ÅÆÂ∑Æ„ÅåÊ∂à„Åà„ÄÅÂàÜÊï£„ÅåÊ∏õ„Çä„Åæ„Åô„ÄÇ

</details>

---

<h3>ÂïèÈ°å2 (Èõ£ÊòìÂ∫¶: medium)</h3>

PPO„ÅÆ„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞ÁØÑÂõ≤$\epsilon$„ÇíÂ§ß„Åç„Åè„Åô„Çã„Å®‰Ωï„ÅåËµ∑„Åì„Çã„Åã„ÄÅ„Åæ„Åü$\epsilon=0$„ÅÆÊ•µÁ´Ø„Å™„Ç±„Éº„Çπ„Åß„ÅØ„Å©„ÅÜ„Å™„Çã„ÅãË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

<details>
<summary>„Éí„É≥„Éà</summary>

„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞Âºè„ÇíË¶ãÁõ¥„Åó„ÄÅ$r_t(\theta)$„ÅÆÂ§âÂåñ„Åå„Å©„ÅÜÂà∂Èôê„Åï„Çå„Çã„ÅãËÄÉ„Åà„Å¶„Åø„Åæ„Åó„Çá„ÅÜ„ÄÇ

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>

<strong>$\epsilon$„ÇíÂ§ß„Åç„Åè„Åô„Çã„Å®</strong>:
- „ÇØ„É™„ÉÉ„Éî„É≥„Ç∞ÁØÑÂõ≤„ÅåÂ∫É„Åå„Çä„ÄÅÊñπÁ≠ñ„ÅÆÂ§âÂåñ„ÅåÂ§ß„Åç„Åè„Å™„Çã
- Â≠¶Áøí„ÅåÈÄü„ÅÑ„Åå‰∏çÂÆâÂÆö„Å´„Å™„Çä„ÇÑ„Åô„ÅÑ
- Ê•µÁ´Ø„Å™Â†¥Âêà„ÄÅÊñπÁ≠ñ„ÅåÂ¥©Â£ä„Åô„ÇãÂèØËÉΩÊÄß

<strong>$\epsilon=0$„ÅÆÂ†¥Âêà</strong>:
$$
\text{clip}(r_t, 1, 1) = 1
$$

- ÈáçË¶ÅÂ∫¶ÊØîÁéá„ÅåÂ∏∏„Å´1„Å´„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞
- ÊñπÁ≠ñ„ÅåÂÖ®„ÅèÊõ¥Êñ∞„Åï„Çå„Å™„ÅÑÔºà$\pi_\theta = \pi_{\theta_{\text{old}}}$„ÇíÂº∑Âà∂Ôºâ

<strong>ÂÆüË∑µÁöÑ„Å™ÂÄ§</strong>: $\epsilon = 0.1 \sim 0.2$„Åå‰∏ÄËà¨ÁöÑ

<strong>ÂÆüÈ®ì„Ç≥„Éº„Éâ</strong>:
<pre><code class="language-python"><h1>Œµ=0.05ÔºàÂé≥„Åó„ÅÑÂà∂Á¥ÑÔºâ</h1>
model_tight = PPO("MlpPolicy", env, clip_range=0.05)

<h1>Œµ=0.5ÔºàÁ∑©„ÅÑÂà∂Á¥ÑÔºâ</h1>
model_loose = PPO("MlpPolicy", env, clip_range=0.5)

<h1>Â≠¶ÁøíÊõ≤Á∑ö„ÇíÊØîËºÉ</h1>
<h1>‚Üí model_tight„ÅØÂÆâÂÆö„Å†„ÅåÈÅÖ„ÅÑ</h1>
<h1>‚Üí model_loose„ÅØÈÄü„ÅÑ„ÅåÊåØÂãï„Åô„Çã</h1></code></pre>

</details>

---

<h3>ÂïèÈ°å3 (Èõ£ÊòìÂ∫¶: hard)</h3>

ÊùêÊñôÊé¢Á¥¢„Å´„Åä„ÅÑ„Å¶„ÄÅ‰ª•‰∏ã„ÅÆ2„Å§„ÅÆÂ†±ÈÖ¨Ë®≠Ë®à„ÇíÊØîËºÉ„Åó„ÄÅ„Åù„Çå„Åû„Çå„ÅÆÈï∑ÊâÄ„ÉªÁü≠ÊâÄ„ÇíËø∞„Åπ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åæ„Åü„ÄÅÂÆüÈöõ„Å´„Ç≥„Éº„Éâ„ÅßÂÆüÈ®ì„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

<strong>Â†±ÈÖ¨AÔºàÁñéÂ†±ÈÖ¨Ôºâ</strong>: ÁõÆÊ®ô„Å´Âà∞ÈÅî„Åó„Åü„Å®„Åç„ÅÆ„ÅøÂ†±ÈÖ¨1„ÄÅ„Åù„Çå‰ª•Â§ñ„ÅØ0
<strong>Â†±ÈÖ¨BÔºàÂØÜÂ†±ÈÖ¨Ôºâ</strong>: ÁõÆÊ®ô„Å®„ÅÆË∑ùÈõ¢„Å´Âøú„Åò„ÅüÈÄ£Á∂öÁöÑ„Å™Â†±ÈÖ¨

<details>
<summary>„Éí„É≥„Éà</summary>

ÁñéÂ†±ÈÖ¨„ÅØÊé¢Á¥¢„ÅåÂõ∞Èõ£„Åß„Åô„Åå„ÄÅÂØÜÂ†±ÈÖ¨„ÅØÂ±ÄÊâÄÊúÄÈÅ©Ëß£„Å´Èô•„Çä„ÇÑ„Åô„ÅÑ„Åß„Åô„ÄÇ„Ç®„É≥„Éà„É≠„Éî„Éº„Éú„Éº„Éä„Çπ„ÅÆÂΩ±Èüø„ÇÇËÄÉ„Åà„Å¶„Åø„Åæ„Åó„Çá„ÅÜ„ÄÇ

</details>

<details>
<summary>Ëß£Á≠î‰æã</summary>

<strong>Â†±ÈÖ¨AÔºàÁñéÂ†±ÈÖ¨Ôºâ„ÅÆÈï∑ÊâÄ„ÉªÁü≠ÊâÄ</strong>:

<strong>Èï∑ÊâÄ</strong>:
- ÊòéÁ¢∫„Å™ÁõÆÊ®ôÔºàÊõñÊòß„Åï„Åå„Å™„ÅÑÔºâ
- Â±ÄÊâÄÊúÄÈÅ©Ëß£„Å´Èô•„Çä„Å´„Åè„ÅÑÔºà‰∏≠ÈñìÂ†±ÈÖ¨„Å´ÊÉë„Çè„Åï„Çå„Å™„ÅÑÔºâ

<strong>Áü≠ÊâÄ</strong>:
- Êé¢Á¥¢„ÅåÈùûÂ∏∏„Å´Âõ∞Èõ£ÔºàÂ≠¶Áøí„Ç∑„Ç∞„Éä„É´„ÅåÂº±„ÅÑÔºâ
- Â≠¶Áøí„Å´ÊôÇÈñì„Åå„Åã„Åã„Çã

<strong>Â†±ÈÖ¨BÔºàÂØÜÂ†±ÈÖ¨Ôºâ„ÅÆÈï∑ÊâÄ„ÉªÁü≠ÊâÄ</strong>:

<strong>Èï∑ÊâÄ</strong>:
- Êé¢Á¥¢„ÅåÂÆπÊòìÔºàÊØé„Çπ„ÉÜ„ÉÉ„Éó„Åß„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØÔºâ
- Â≠¶Áøí„ÅåÈÄü„ÅÑ

<strong>Áü≠ÊâÄ</strong>:
- Â†±ÈÖ¨Ë®≠Ë®à„ÅåÈõ£„Åó„ÅÑÔºàË∑ùÈõ¢„Å†„Åë„Åß„ÅØ‰∏çÂçÅÂàÜ„Å™Â†¥Âêà„ÇÇÔºâ
- Â±ÄÊâÄÊúÄÈÅ©Ëß£„Å´Èô•„Çä„ÇÑ„Åô„ÅÑ

<strong>ÂÆüÈ®ì„Ç≥„Éº„Éâ</strong>:
<pre><code class="language-python"><h1>Â†±ÈÖ¨AÔºàÁñéÂ†±ÈÖ¨Ôºâ</h1>
class SparseRewardEnv(gym.Env):
    def step(self, action):
        # ... (Áä∂ÊÖãÊõ¥Êñ∞) ...
        distance = np.linalg.norm(self.state - self.target)

        if distance < 0.5:
            reward = 1.0  # Âà∞ÈÅî
            done = True
        else:
            reward = 0.0  # „Åù„Çå‰ª•Â§ñ
            done = False

        return self.state, reward, done, {}

<h1>Â†±ÈÖ¨BÔºàÂØÜÂ†±ÈÖ¨Ôºâ</h1>
class DenseRewardEnv(gym.Env):
    def step(self, action):
        # ... (Áä∂ÊÖãÊõ¥Êñ∞) ...
        distance = np.linalg.norm(self.state - self.target)
        reward = -distance  # ÈÄ£Á∂öÁöÑ„Å™Â†±ÈÖ¨
        done = distance < 0.5

        return self.state, reward, done, {}

<h1>ÊØîËºÉÂÆüÈ®ì</h1>
model_sparse = PPO("MlpPolicy", DummyVecEnv([lambda: SparseRewardEnv()]))
model_dense = PPO("MlpPolicy", DummyVecEnv([lambda: DenseRewardEnv()]))

model_sparse.learn(total_timesteps=100000)
model_dense.learn(total_timesteps=100000)

<h1>ÁµêÊûú: model_dense„ÅÆÊñπ„ÅåÂ≠¶Áøí„ÅåÈÄü„ÅÑ„Åå„ÄÅ</h1>
<h1>Ë§áÈõë„Å™Áí∞Â¢É„Åß„ÅØmodel_sparse„ÅÆÊñπ„ÅåËâØ„ÅÑËß£„ÇíË¶ã„Å§„Åë„Çã„Åì„Å®„ÇÇ</h1></code></pre>

<strong>„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„Çπ</strong>: ÂØÜÂ†±ÈÖ¨„Åã„ÇâÂßã„ÇÅ„ÄÅÂïèÈ°å„Å´Âøú„Åò„Å¶ÁñéÂ†±ÈÖ¨„ÇÑ<strong>Â†±ÈÖ¨„Ç∑„Çß„Ç§„Éî„É≥„Ç∞</strong>Ôºà‰∏≠ÈñìÂ†±ÈÖ¨„ÅÆËøΩÂä†Ôºâ„ÇíÊ§úË®é„ÄÇ

</details>

---

<h2>„Åì„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆ„Åæ„Å®„ÇÅ</h2>

- <strong>ÊñπÁ≠ñÂãæÈÖçÊ≥ï</strong>„ÅØÊñπÁ≠ñ„ÇíÁõ¥Êé•ÊúÄÈÅ©Âåñ„Åó„ÄÅÈÄ£Á∂öË°åÂãï„Å´ÂØæÂøú
- <strong>REINFORCE„Ç¢„É´„Ç¥„É™„Ç∫„É†</strong>„ÅØÈ´òÂàÜÊï£„Å†„Åå„ÄÅ„Éô„Éº„Çπ„É©„Ç§„É≥„ÅßÊîπÂñÑ
- <strong>Actor-Critic</strong>„ÅØActor„Å®Critic„ÇíÂêåÊôÇÂ≠¶Áøí„Åó„ÄÅ‰ΩéÂàÜÊï£„Éª„Ç™„É≥„É©„Ç§„É≥Â≠¶Áøí
- <strong>PPO</strong>„ÅØ„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞„Å´„Çà„ÇäÂÆâÂÆö„Åó„ÅüÂ≠¶Áøí„ÇíÂÆüÁèæ„ÄÅÊúÄÂÖàÁ´Ø„ÅÆÂÆüÁî®ÁöÑÊâãÊ≥ï
- <strong>Stable Baselines3</strong>„Å´„Çà„Çä„ÄÅ„Çè„Åö„ÅãÊï∞Ë°å„ÅßPPO„ÇíÂÆüË£ÖÂèØËÉΩ
- ÈÄ£Á∂öË°åÂãïÁ©∫Èñì„Åß„ÅØ<strong>„Ç¨„Ç¶„ÇπÊñπÁ≠ñ</strong>„Çí‰ΩøÁî®

Ê¨°Á´†„Åß„ÅØ„ÄÅÊùêÊñôÊé¢Á¥¢„Å´ÁâπÂåñ„Åó„Åü„Ç´„Çπ„Çø„É†Áí∞Â¢É„ÅÆÊßãÁØâ„Å®Â†±ÈÖ¨Ë®≠Ë®à„ÇíÂ≠¶„Å≥„Åæ„Åô„ÄÇ

---

<h2>ÂèÇËÄÉÊñáÁåÆ</h2>

1. Williams "Simple statistical gradient-following algorithms for connectionist reinforcement learning" *Machine Learning* (1992) - REINFORCE
2. Mnih et al. "Asynchronous methods for deep reinforcement learning" *ICML* (2016) - A3C/A2C
3. Schulman et al. "Proximal policy optimization algorithms" *arXiv* (2017) - PPO
4. Schulman et al. "Trust region policy optimization" *ICML* (2015) - TRPO
5. Raffin et al. "Stable-Baselines3: Reliable reinforcement learning implementations" *JMLR* (2021)

---

<strong>Ê¨°Á´†</strong>: [Á¨¨3Á´†: ÊùêÊñôÊé¢Á¥¢Áí∞Â¢É„ÅÆÊßãÁØâ](chapter-3.html)
<div class="navigation">
    <a href="chapter-1.html" class="nav-button">‚Üê Á¨¨1Á´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter-3.html" class="nav-button">Á¨¨3Á´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-17</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>
