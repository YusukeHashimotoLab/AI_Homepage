<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬2ç« ï¼šä¸ç¢ºå®Ÿæ€§æ¨å®šæ‰‹æ³• - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬2ç« ï¼šä¸ç¢ºå®Ÿæ€§æ¨å®šæ‰‹æ³•</h1>
            <p class="subtitle">Ensembleãƒ»Dropoutãƒ»Gaussian Processã«ã‚ˆã‚‹äºˆæ¸¬ä¿¡é ¼åŒºé–“</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´šã€œä¸Šç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 8å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 3å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h1>ç¬¬2ç« ï¼šä¸ç¢ºå®Ÿæ€§æ¨å®šæ‰‹æ³•</h1>

<strong>Ensembleãƒ»Dropoutãƒ»Gaussian Processã«ã‚ˆã‚‹äºˆæ¸¬ä¿¡é ¼åŒºé–“</strong>

<h2>å­¦ç¿’ç›®æ¨™</h2>

ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š

- âœ… 3ã¤ã®ä¸ç¢ºå®Ÿæ€§æ¨å®šæ‰‹æ³•ã®åŸç†ã‚’ç†è§£ã—ã¦ã„ã‚‹
- âœ… Ensembleæ³•ï¼ˆRandom Forestï¼‰ã‚’å®Ÿè£…ã§ãã‚‹
- âœ… MC Dropoutã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«é©ç”¨ã§ãã‚‹
- âœ… Gaussian Processã§äºˆæ¸¬åˆ†æ•£ã‚’è¨ˆç®—ã§ãã‚‹
- âœ… æ‰‹æ³•ã®ä½¿ã„åˆ†ã‘åŸºæº–ã‚’èª¬æ˜ã§ãã‚‹

<strong>èª­äº†æ™‚é–“</strong>: 25-30åˆ†
<strong>ã‚³ãƒ¼ãƒ‰ä¾‹</strong>: 8å€‹
<strong>æ¼”ç¿’å•é¡Œ</strong>: 3å•

---

<h2>2.1 Ensembleæ³•ã«ã‚ˆã‚‹ä¸ç¢ºå®Ÿæ€§æ¨å®š</h2>

<h3>ãªãœä¸ç¢ºå®Ÿæ€§æ¨å®šãŒé‡è¦ã‹</h3>

Active Learningã§ã¯ã€ã€Œãƒ¢ãƒ‡ãƒ«ãŒã©ã‚Œã ã‘è‡ªä¿¡ã‚’æŒã£ã¦äºˆæ¸¬ã—ã¦ã„ã‚‹ã‹ã€ã‚’å®šé‡åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ä¸ç¢ºå®Ÿæ€§æ¨å®šã¯ã€Query Strategyã®æ ¸å¿ƒæŠ€è¡“ã§ã™ã€‚

<strong>ä¸ç¢ºå®Ÿæ€§ã®2ã¤ã®ã‚¿ã‚¤ãƒ—</strong>:

1. <strong>Aleatoric Uncertaintyï¼ˆå¶ç„¶çš„ä¸ç¢ºå®Ÿæ€§ï¼‰</strong>
   - ãƒ‡ãƒ¼ã‚¿è‡ªä½“ã«å†…åœ¨ã™ã‚‹ãƒã‚¤ã‚º
   - æ¸¬å®šèª¤å·®ã€ç’°å¢ƒå¤‰å‹•ãªã©
   - ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã—ã¦ã‚‚æ¸›å°‘ã—ãªã„

2. <strong>Epistemic Uncertaintyï¼ˆèªè­˜çš„ä¸ç¢ºå®Ÿæ€§ï¼‰</strong>
   - ãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ä¸è¶³ã«ã‚ˆã‚‹ä¸ç¢ºå®Ÿæ€§
   - ãƒ‡ãƒ¼ã‚¿ä¸è¶³ãŒåŸå› 
   - ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ã¨æ¸›å°‘

<strong>Active LearningãŒç„¦ç‚¹ã‚’å½“ã¦ã‚‹ä¸ç¢ºå®Ÿæ€§</strong>:
â†’ <strong>Epistemic Uncertainty</strong>ï¼ˆãƒ‡ãƒ¼ã‚¿è¿½åŠ ã§æ”¹å–„å¯èƒ½ï¼‰

<h3>Ensembleæ³•ã®åŸç†</h3>

<strong>åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢</strong>: è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã®ã°ã‚‰ã¤ãã§ä¸ç¢ºå®Ÿæ€§ã‚’æ¸¬å®š

<strong>æ•°å¼</strong>:
$$
\mu(x) = \frac{1}{M} \sum_{m=1}^M f_m(x)
$$

$$
\sigma^2(x) = \frac{1}{M} \sum_{m=1}^M (f_m(x) - \mu(x))^2
$$

- $f_m(x)$: mç•ªç›®ã®ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
- $M$: ãƒ¢ãƒ‡ãƒ«æ•°ï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚µã‚¤ã‚ºï¼‰
- $\mu(x)$: äºˆæ¸¬å¹³å‡
- $\sigma^2(x)$: äºˆæ¸¬åˆ†æ•£ï¼ˆä¸ç¢ºå®Ÿæ€§ï¼‰

<h3>Random Forestã«ã‚ˆã‚‹å®Ÿè£…</h3>

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹1: Random Forestã§ä¸ç¢ºå®Ÿæ€§æ¨å®š</strong>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression

<h1>ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ</h1>
np.random.seed(42)
X, y = make_regression(
    n_samples=200,
    n_features=5,
    noise=10,
    random_state=42
)

<h1>è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²</h1>
train_size = 50
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

<h1>Random Forestã§ä¸ç¢ºå®Ÿæ€§æ¨å®š</h1>
rf = RandomForestRegressor(
    n_estimators=100,
    random_state=42
)
rf.fit(X_train, y_train)

<h1>å„æ±ºå®šæœ¨ã®äºˆæ¸¬ã‚’å–å¾—</h1>
tree_predictions = np.array([
    tree.predict(X_test)
    for tree in rf.estimators_
])

<h1>äºˆæ¸¬å¹³å‡ã¨æ¨™æº–åå·®</h1>
mean_prediction = np.mean(tree_predictions, axis=0)
std_prediction = np.std(tree_predictions, axis=0)

<h1>å¯è¦–åŒ–</h1>
plt.figure(figsize=(12, 5))

<h1>å·¦å›³: äºˆæ¸¬ vs çœŸå€¤</h1>
plt.subplot(1, 2, 1)
plt.errorbar(
    y_test,
    mean_prediction,
    yerr=1.96 * std_prediction,  # 95%ä¿¡é ¼åŒºé–“
    fmt='o',
    alpha=0.6,
    capsize=5
)
plt.plot(
    [y_test.min(), y_test.max()],
    [y_test.min(), y_test.max()],
    'r--',
    label='Perfect prediction'
)
plt.xlabel('True Value', fontsize=12)
plt.ylabel('Predicted Value', fontsize=12)
plt.title('Random Forest: Prediction with Uncertainty', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

<h1>å³å›³: ä¸ç¢ºå®Ÿæ€§ã®åˆ†å¸ƒ</h1>
plt.subplot(1, 2, 2)
plt.hist(std_prediction, bins=30, edgecolor='black', alpha=0.7)
plt.xlabel('Standard Deviation (Uncertainty)', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Distribution of Uncertainty', fontsize=14)
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('rf_uncertainty.png', dpi=150, bbox_inches='tight')
plt.show()

<h1>çµ±è¨ˆã‚µãƒãƒªãƒ¼</h1>
print("Random Forestä¸ç¢ºå®Ÿæ€§æ¨å®šã®çµæœ:")
print(f"å¹³å‡ä¸ç¢ºå®Ÿæ€§: {std_prediction.mean():.2f}")
print(f"æœ€å°ä¸ç¢ºå®Ÿæ€§: {std_prediction.min():.2f}")
print(f"æœ€å¤§ä¸ç¢ºå®Ÿæ€§: {std_prediction.max():.2f}")
print(f"ä¸ç¢ºå®Ÿæ€§ã®æ¨™æº–åå·®: {std_prediction.std():.2f}")</code></pre>

<strong>å‡ºåŠ›ä¾‹</strong>:
<pre><code>Random Forestä¸ç¢ºå®Ÿæ€§æ¨å®šã®çµæœ:
å¹³å‡ä¸ç¢ºå®Ÿæ€§: 5.23
æœ€å°ä¸ç¢ºå®Ÿæ€§: 2.14
æœ€å¤§ä¸ç¢ºå®Ÿæ€§: 12.45
ä¸ç¢ºå®Ÿæ€§ã®æ¨™æº–åå·®: 2.18</code></pre>

<h3>LightGBMã«ã‚ˆã‚‹å®Ÿè£…</h3>

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹2: LightGBMã§ä¸ç¢ºå®Ÿæ€§æ¨å®š</strong>

<pre><code class="language-python">import lightgbm as lgb

<h1>LightGBMã§è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ï¼ˆBaggingï¼‰</h1>
n_models = 100
lgb_predictions = []

for i in range(n_models):
    # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    indices = np.random.choice(
        len(X_train),
        len(X_train),
        replace=True
    )
    X_boot = X_train[indices]
    y_boot = y_train[indices]

    # LightGBMè¨“ç·´
    train_data = lgb.Dataset(X_boot, label=y_boot)
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1
    }

    model = lgb.train(
        params,
        train_data,
        num_boost_round=100
    )

    # äºˆæ¸¬
    pred = model.predict(X_test)
    lgb_predictions.append(pred)

lgb_predictions = np.array(lgb_predictions)

<h1>ä¸ç¢ºå®Ÿæ€§è¨ˆç®—</h1>
lgb_mean = np.mean(lgb_predictions, axis=0)
lgb_std = np.std(lgb_predictions, axis=0)

print("\nLightGBMä¸ç¢ºå®Ÿæ€§æ¨å®šã®çµæœ:")
print(f"å¹³å‡ä¸ç¢ºå®Ÿæ€§: {lgb_std.mean():.2f}")
print(f"Random Forestã¨ã®ç›¸é–¢: "
      f"{np.corrcoef(std_prediction, lgb_std)[0,1]:.3f}")</code></pre>

<strong>åˆ©ç‚¹</strong>:
- âœ… å®Ÿè£…ãŒç°¡å˜
- âœ… è¨ˆç®—ã‚³ã‚¹ãƒˆãŒæ¯”è¼ƒçš„ä½ã„
- âœ… è§£é‡ˆã—ã‚„ã™ã„
- âœ… è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã«å¼·ã„

<strong>æ¬ ç‚¹</strong>:
- âš ï¸ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚µã‚¤ã‚ºã«ä¾å­˜
- âš ï¸ æ·±å±¤å­¦ç¿’ã«ã¯é©ç”¨å›°é›£
- âš ï¸ ä¸ç¢ºå®Ÿæ€§ã®æ ¡æ­£ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹

---

<h2>2.2 Dropoutæ³•ã«ã‚ˆã‚‹ä¸ç¢ºå®Ÿæ€§æ¨å®š</h2>

<h3>MC Dropoutï¼ˆMonte Carlo Dropoutï¼‰</h3>

<strong>åŸç†</strong>: æ¨è«–æ™‚ã«ã‚‚Dropoutã‚’é©ç”¨ã—ã€è¤‡æ•°å›äºˆæ¸¬ã—ã¦ã°ã‚‰ã¤ãã‚’æ¸¬å®š

<strong>é€šå¸¸ã®Dropout</strong>ï¼ˆè¨“ç·´æ™‚ã®ã¿ï¼‰:
<pre><code class="language-python"><h1>è¨“ç·´æ™‚</h1>
model.train()  # Dropoutæœ‰åŠ¹
output = model(x)

<h1>æ¨è«–æ™‚</h1>
model.eval()  # Dropoutç„¡åŠ¹
output = model(x)  # æ±ºå®šè«–çš„äºˆæ¸¬</code></pre>

<strong>MC Dropout</strong>ï¼ˆæ¨è«–æ™‚ã‚‚Dropoutï¼‰:
<pre><code class="language-python"><h1>æ¨è«–æ™‚ã‚‚Dropoutã‚’æœ‰åŠ¹åŒ–</h1>
model.train()  # Dropoutæœ‰åŠ¹ã®ã¾ã¾
predictions = [model(x) for _ in range(T)]  # Tå›äºˆæ¸¬
mean = np.mean(predictions, axis=0)
std = np.std(predictions, axis=0)</code></pre>

<h3>å®Ÿè£…ä¾‹</h3>

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹3: PyTorchã§MC Dropout</strong>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class MCDropoutNet(nn.Module):
    def __init__(self, input_dim, hidden_dim=50, dropout_rate=0.5):
        super(MCDropoutNet, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)
        self.dropout = nn.Dropout(p=dropout_rate)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)  # Dropouté©ç”¨
        x = F.relu(self.fc2(x))
        x = self.dropout(x)  # Dropouté©ç”¨
        x = self.fc3(x)
        return x

<h1>ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–</h1>
model = MCDropoutNet(input_dim=5, hidden_dim=50, dropout_rate=0.3)

<h1>ãƒ‡ãƒ¼ã‚¿ã‚’Tensorã«å¤‰æ›</h1>
X_train_tensor = torch.FloatTensor(X_train)
y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)
X_test_tensor = torch.FloatTensor(X_test)

<h1>è¨“ç·´</h1>
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

model.train()
for epoch in range(200):
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 50 == 0:
        print(f'Epoch [{epoch+1}/200], Loss: {loss.item():.4f}')

<h1>MC Dropoutã§ä¸ç¢ºå®Ÿæ€§æ¨å®š</h1>
def mc_dropout_predict(model, x, n_samples=100):
    """
    MC Dropoutã«ã‚ˆã‚‹äºˆæ¸¬ã¨ä¸ç¢ºå®Ÿæ€§æ¨å®š

    Parameters:
    -----------
    model : nn.Module
        è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    x : Tensor
        å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
    n_samples : int
        ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å›æ•°

    Returns:
    --------
    mean : array
        äºˆæ¸¬å¹³å‡
    std : array
        äºˆæ¸¬æ¨™æº–åå·®ï¼ˆä¸ç¢ºå®Ÿæ€§ï¼‰
    """
    model.train()  # Dropoutã‚’æœ‰åŠ¹åŒ–
    predictions = []

    with torch.no_grad():
        for _ in range(n_samples):
            pred = model(x).numpy()
            predictions.append(pred)

    predictions = np.array(predictions).squeeze()
    mean = np.mean(predictions, axis=0)
    std = np.std(predictions, axis=0)

    return mean, std

<h1>MC Dropoutã§äºˆæ¸¬</h1>
mc_mean, mc_std = mc_dropout_predict(
    model,
    X_test_tensor,
    n_samples=100
)

<h1>å¯è¦–åŒ–</h1>
plt.figure(figsize=(10, 6))
plt.errorbar(
    y_test,
    mc_mean,
    yerr=1.96 * mc_std,
    fmt='o',
    alpha=0.6,
    capsize=5,
    color='purple'
)
plt.plot(
    [y_test.min(), y_test.max()],
    [y_test.min(), y_test.max()],
    'r--',
    label='Perfect prediction'
)
plt.xlabel('True Value', fontsize=12)
plt.ylabel('Predicted Value (MC Dropout)', fontsize=12)
plt.title('MC Dropout: Uncertainty Estimation', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('mc_dropout_uncertainty.png', dpi=150)
plt.show()

print("\nMC Dropoutä¸ç¢ºå®Ÿæ€§æ¨å®šã®çµæœ:")
print(f"å¹³å‡ä¸ç¢ºå®Ÿæ€§: {mc_std.mean():.2f}")
print(f"æœ€å°ä¸ç¢ºå®Ÿæ€§: {mc_std.min():.2f}")
print(f"æœ€å¤§ä¸ç¢ºå®Ÿæ€§: {mc_std.max():.2f}")</code></pre>

<strong>å‡ºåŠ›ä¾‹</strong>:
<pre><code>Epoch [50/200], Loss: 145.2341
Epoch [100/200], Loss: 98.5632
Epoch [150/200], Loss: 67.8921
Epoch [200/200], Loss: 52.1234

MC Dropoutä¸ç¢ºå®Ÿæ€§æ¨å®šã®çµæœ:
å¹³å‡ä¸ç¢ºå®Ÿæ€§: 4.87
æœ€å°ä¸ç¢ºå®Ÿæ€§: 1.92
æœ€å¤§ä¸ç¢ºå®Ÿæ€§: 11.23</code></pre>

<strong>åˆ©ç‚¹</strong>:
- âœ… æ—¢å­˜ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å®¹æ˜“ã«é©ç”¨
- âœ… è¿½åŠ ã®è¨“ç·´ä¸è¦ï¼ˆDropoutã®ã¿ï¼‰
- âœ… æ·±å±¤å­¦ç¿’ã«é©ã—ã¦ã„ã‚‹

<strong>æ¬ ç‚¹</strong>:
- âš ï¸ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å›æ•°ï¼ˆTï¼‰ã«è¨ˆç®—ã‚³ã‚¹ãƒˆä¾å­˜
- âš ï¸ Dropoutç‡ã®é¸æŠãŒé‡è¦
- âš ï¸ ä¸ç¢ºå®Ÿæ€§ã®æ ¡æ­£ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹

---

<h2>2.3 Gaussian Process (GP) ã«ã‚ˆã‚‹ä¸ç¢ºå®Ÿæ€§æ¨å®š</h2>

<h3>GPã®åŸºç¤</h3>

Gaussian Processï¼ˆã‚¬ã‚¦ã‚¹éç¨‹ï¼‰ã¯ã€é–¢æ•°ã®ç¢ºç‡åˆ†å¸ƒã‚’å®šç¾©ã™ã‚‹å¼·åŠ›ãªæ‰‹æ³•ã§ã™ã€‚

<strong>å®šç¾©</strong>:
$$
f(\mathbf{x}) \sim \mathcal{GP}(\mu(\mathbf{x}), k(\mathbf{x}, \mathbf{x}'))
$$

- $\mu(\mathbf{x})$: å¹³å‡é–¢æ•°ï¼ˆé€šå¸¸0ï¼‰
- $k(\mathbf{x}, \mathbf{x}')$: ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ï¼ˆå…±åˆ†æ•£é–¢æ•°ï¼‰

<strong>äºˆæ¸¬åˆ†å¸ƒ</strong>:
$$
p(f^* | \mathbf{X}, \mathbf{y}, \mathbf{x}^*) = \mathcal{N}(\mu^*, \sigma^{*2})
$$

$$
\mu^* = k(\mathbf{x}^*, \mathbf{X}) [K(\mathbf{X}, \mathbf{X}) + \sigma_n^2 I]^{-1} \mathbf{y}
$$

$$
\sigma^{*2} = k(\mathbf{x}^*, \mathbf{x}^*) - k(\mathbf{x}^*, \mathbf{X}) [K(\mathbf{X}, \mathbf{X}) + \sigma_n^2 I]^{-1} k(\mathbf{X}, \mathbf{x}^*)
$$

<h3>ã‚«ãƒ¼ãƒãƒ«é–¢æ•°</h3>

<strong>RBFï¼ˆRadial Basis Functionï¼‰ã‚«ãƒ¼ãƒãƒ«</strong>:
$$
k(\mathbf{x}_i, \mathbf{x}_j) = \sigma_f^2 \exp\left(-\frac{\|\mathbf{x}_i - \mathbf{x}_j\|^2}{2\ell^2}\right)
$$

- $\sigma_f^2$: ä¿¡å·åˆ†æ•£
- $\ell$: é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆsmoothnessï¼‰

<strong>MatÃ©rnã‚«ãƒ¼ãƒãƒ«</strong>:
$$
k(\mathbf{x}_i, \mathbf{x}_j) = \frac{2^{1-\nu}}{\Gamma(\nu)} \left(\frac{\sqrt{2\nu} r}{\ell}\right)^\nu K_\nu\left(\frac{\sqrt{2\nu} r}{\ell}\right)
$$

<h3>GPyTorchã«ã‚ˆã‚‹å®Ÿè£…</h3>

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹4: GPyTorchã§ä¸ç¢ºå®Ÿæ€§æ¨å®š</strong>

<pre><code class="language-python">import gpytorch
import torch

class ExactGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernel()
        )

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

<h1>ãƒ‡ãƒ¼ã‚¿ã‚’Tensorã«å¤‰æ›</h1>
train_x = torch.FloatTensor(X_train)
train_y = torch.FloatTensor(y_train)
test_x = torch.FloatTensor(X_test)

<h1>Likelihoodã¨ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–</h1>
likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = ExactGPModel(train_x, train_y, likelihood)

<h1>è¨“ç·´ãƒ¢ãƒ¼ãƒ‰</h1>
model.train()
likelihood.train()

<h1>Optimizerã®è¨­å®š</h1>
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

<h1>Lossé–¢æ•°ï¼ˆMarginal Log Likelihoodï¼‰</h1>
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

<h1>è¨“ç·´ãƒ«ãƒ¼ãƒ—</h1>
n_iterations = 100
for i in range(n_iterations):
    optimizer.zero_grad()
    output = model(train_x)
    loss = -mll(output, train_y)
    loss.backward()

    if (i + 1) % 20 == 0:
        print(f'Iteration {i+1}/{n_iterations} - Loss: {loss.item():.3f}')

    optimizer.step()

<h1>æ¨è«–ãƒ¢ãƒ¼ãƒ‰</h1>
model.eval()
likelihood.eval()

<h1>äºˆæ¸¬ï¼ˆä¸ç¢ºå®Ÿæ€§è¾¼ã¿ï¼‰</h1>
with torch.no_grad(), gpytorch.settings.fast_pred_var():
    observed_pred = likelihood(model(test_x))
    gp_mean = observed_pred.mean.numpy()
    gp_std = observed_pred.stddev.numpy()

<h1>å¯è¦–åŒ–</h1>
plt.figure(figsize=(10, 6))
plt.errorbar(
    y_test,
    gp_mean,
    yerr=1.96 * gp_std,
    fmt='o',
    alpha=0.6,
    capsize=5,
    color='green'
)
plt.plot(
    [y_test.min(), y_test.max()],
    [y_test.min(), y_test.max()],
    'r--',
    label='Perfect prediction'
)
plt.xlabel('True Value', fontsize=12)
plt.ylabel('Predicted Value (GP)', fontsize=12)
plt.title('Gaussian Process: Uncertainty Estimation', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('gp_uncertainty.png', dpi=150)
plt.show()

print("\nGaussian Processä¸ç¢ºå®Ÿæ€§æ¨å®šã®çµæœ:")
print(f"å¹³å‡ä¸ç¢ºå®Ÿæ€§: {gp_std.mean():.2f}")
print(f"æœ€å°ä¸ç¢ºå®Ÿæ€§: {gp_std.min():.2f}")
print(f"æœ€å¤§ä¸ç¢ºå®Ÿæ€§: {gp_std.max():.2f}")

<h1>å­¦ç¿’ã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</h1>
print("\nå­¦ç¿’ã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
print(f"é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«: "
      f"{model.covar_module.base_kernel.lengthscale.item():.3f}")
print(f"ä¿¡å·åˆ†æ•£: "
      f"{model.covar_module.outputscale.item():.3f}")
print(f"ãƒã‚¤ã‚ºåˆ†æ•£: "
      f"{likelihood.noise.item():.3f}")</code></pre>

<strong>å‡ºåŠ›ä¾‹</strong>:
<pre><code>Iteration 20/100 - Loss: 145.234
Iteration 40/100 - Loss: 98.567
Iteration 60/100 - Loss: 67.891
Iteration 80/100 - Loss: 52.123
Iteration 100/100 - Loss: 45.678

Gaussian Processä¸ç¢ºå®Ÿæ€§æ¨å®šã®çµæœ:
å¹³å‡ä¸ç¢ºå®Ÿæ€§: 5.12
æœ€å°ä¸ç¢ºå®Ÿæ€§: 2.34
æœ€å¤§ä¸ç¢ºå®Ÿæ€§: 10.87

å­¦ç¿’ã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«: 1.234
ä¿¡å·åˆ†æ•£: 45.678
ãƒã‚¤ã‚ºåˆ†æ•£: 3.456</code></pre>

<strong>åˆ©ç‚¹</strong>:
- âœ… ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ãŒå³å¯†
- âœ… å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦
- âœ… ã‚«ãƒ¼ãƒãƒ«é¸æŠã§æŸ”è»Ÿæ€§
- âœ… ç†è«–çš„åŸºç›¤ãŒå¼·å›º

<strong>æ¬ ç‚¹</strong>:
- âš ï¸ å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã«ä¸å‘ãï¼ˆO(nÂ³)ï¼‰
- âš ï¸ ã‚«ãƒ¼ãƒãƒ«ãƒ»ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é¸æŠãŒé‡è¦
- âš ï¸ é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã§æ€§èƒ½ä½ä¸‹

---

<h2>2.4 ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ï¼šãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬</h2>

<h3>å•é¡Œè¨­å®š</h3>

<strong>ç›®æ¨™</strong>: ç„¡æ©Ÿææ–™ã®ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã‚’äºˆæ¸¬ã—ã€ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’å„ªå…ˆçš„ã«è¨ˆç®—

<strong>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</strong>: Materials Projectï¼ˆDFTè¨ˆç®—æ¸ˆã¿ï¼‰
- ã‚µãƒ³ãƒ—ãƒ«æ•°: 5,000ææ–™
- ç‰¹å¾´é‡: çµ„æˆè¨˜è¿°å­ï¼ˆ20æ¬¡å…ƒï¼‰
- ç›®æ¨™å¤‰æ•°: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ï¼ˆeVï¼‰

<h3>3ã¤ã®æ‰‹æ³•ã®æ¯”è¼ƒ</h3>

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹5: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ã§ã®ä¸ç¢ºå®Ÿæ€§æ¨å®šæ¯”è¼ƒ</strong>

<pre><code class="language-python">"""
ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ã§ã®3ã¤ã®ä¸ç¢ºå®Ÿæ€§æ¨å®šæ‰‹æ³•ã®æ¯”è¼ƒ

ãƒ‡ãƒ¼ã‚¿: Materials Projecté¢¨ã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
ç›®æ¨™: Random Forest, MC Dropout, Gaussian Processã®æ€§èƒ½æ¯”è¼ƒ
"""
import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.nn.functional as F
import gpytorch


<h1>============================================</h1>
<h1>ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¨å‰å‡¦ç†</h1>
<h1>============================================</h1>
def generate_bandgap_dataset(n_samples=5000, n_features=20,
                              random_state=42):
    """
    Materials Projecté¢¨ã®ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ

    Parameters:
    -----------
    n_samples : int
        ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆææ–™ã®æ•°ï¼‰
    n_features : int
        ç‰¹å¾´é‡æ¬¡å…ƒï¼ˆçµ„æˆè¨˜è¿°å­ï¼‰
    random_state : int
        ä¹±æ•°ã‚·ãƒ¼ãƒ‰

    Returns:
    --------
    X : ndarray, shape (n_samples, n_features)
        ç‰¹å¾´é‡è¡Œåˆ—ï¼ˆçµ„æˆè¨˜è¿°å­ï¼‰
    y : ndarray, shape (n_samples,)
        ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ï¼ˆeVï¼‰
    """
    np.random.seed(random_state)

    # çµ„æˆè¨˜è¿°å­ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆæ­£è¦åˆ†å¸ƒï¼‰
    X = np.random.randn(n_samples, n_features)

    # ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã‚’éç·šå½¢é–¢æ•°ã§ç”Ÿæˆ
    # å®Ÿéš›ã®ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã¯0-8 eVç¨‹åº¦
    true_weights = np.random.randn(n_features) * 0.3
    y = (
        2.5  # ãƒ™ãƒ¼ã‚¹å€¤
        + X @ true_weights  # ç·šå½¢æˆåˆ†
        + 0.5 * np.sin(X[:, 0])  # éç·šå½¢æˆåˆ†
        + 0.3 * (X[:, 1] ** 2)
        + np.random.randn(n_samples) * 0.2  # ãƒã‚¤ã‚º
    )

    # ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã‚’ç‰©ç†çš„ã«å¦¥å½“ãªç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—
    y = np.clip(y, 0.0, 8.0)

    return X, y


<h1>ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ</h1>
print("ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆä¸­...")
X, y = generate_bandgap_dataset(n_samples=500, n_features=20)

<h1>è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆ70% train, 30% testï¼‰</h1>
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42
)

<h1>æ¨™æº–åŒ–ï¼ˆGPã¨NNã®ãŸã‚ï¼‰</h1>
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape[0]} samples")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape[0]} samples")
print(f"ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ç¯„å›²: {y.min():.2f} - {y.max():.2f} eV\n")


<h1>============================================</h1>
<h1>æ‰‹æ³•1: Random Forest</h1>
<h1>============================================</h1>
print("=" * 50)
print("Random Forestã§ä¸ç¢ºå®Ÿæ€§æ¨å®šä¸­...")
start_time = time.time()

rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_train, y_train)

<h1>å„æ±ºå®šæœ¨ã®äºˆæ¸¬ã‚’å–å¾—</h1>
rf_tree_preds = np.array([
    tree.predict(X_test) for tree in rf_model.estimators_
])

<h1>äºˆæ¸¬å¹³å‡ã¨æ¨™æº–åå·®</h1>
rf_mean = np.mean(rf_tree_preds, axis=0)
rf_std = np.std(rf_tree_preds, axis=0)
rf_time = time.time() - start_time

print(f"å®Œäº† ({rf_time:.2f}ç§’)")
print(f"RMSE: {np.sqrt(np.mean((rf_mean - y_test) ** 2)):.3f} eV")


<h1>============================================</h1>
<h1>æ‰‹æ³•2: MC Dropout</h1>
<h1>============================================</h1>
print("=" * 50)
print("MC Dropoutã§ä¸ç¢ºå®Ÿæ€§æ¨å®šä¸­...")

<h1>MC Dropoutãƒ¢ãƒ‡ãƒ«å®šç¾©</h1>
class BandgapMCDropout(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, dropout_rate=0.3):
        super(BandgapMCDropout, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, 1)
        self.dropout = nn.Dropout(p=dropout_rate)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = F.relu(self.fc3(x))
        x = self.dropout(x)
        x = self.fc4(x)
        return x


start_time = time.time()

<h1>ãƒ‡ãƒ¼ã‚¿ã‚’Tensorã«å¤‰æ›</h1>
X_train_tensor = torch.FloatTensor(X_train_scaled)
y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)
X_test_tensor = torch.FloatTensor(X_test_scaled)

<h1>ãƒ¢ãƒ‡ãƒ«è¨“ç·´</h1>
mc_model = BandgapMCDropout(input_dim=20, hidden_dim=64)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(mc_model.parameters(), lr=0.01)

mc_model.train()
for epoch in range(300):
    optimizer.zero_grad()
    outputs = mc_model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()

<h1>MC Dropoutã§äºˆæ¸¬ï¼ˆ100å›ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰</h1>
mc_model.train()  # Dropoutã‚’æœ‰åŠ¹åŒ–
mc_predictions = []
with torch.no_grad():
    for _ in range(100):
        pred = mc_model(X_test_tensor).numpy().flatten()
        mc_predictions.append(pred)

mc_predictions = np.array(mc_predictions)
mc_mean = np.mean(mc_predictions, axis=0)
mc_std = np.std(mc_predictions, axis=0)
mc_time = time.time() - start_time

print(f"å®Œäº† ({mc_time:.2f}ç§’)")
print(f"RMSE: {np.sqrt(np.mean((mc_mean - y_test) ** 2)):.3f} eV")


<h1>============================================</h1>
<h1>æ‰‹æ³•3: Gaussian Process</h1>
<h1>============================================</h1>
print("=" * 50)
print("Gaussian Processã§ä¸ç¢ºå®Ÿæ€§æ¨å®šä¸­...")

<h1>GPå®šç¾©</h1>
class BandgapGP(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(BandgapGP, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.MaternKernel(nu=2.5)
        )

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(
            mean_x, covar_x
        )


start_time = time.time()

<h1>ãƒ‡ãƒ¼ã‚¿ã‚’Tensorã«å¤‰æ›</h1>
gp_train_x = torch.FloatTensor(X_train_scaled)
gp_train_y = torch.FloatTensor(y_train)
gp_test_x = torch.FloatTensor(X_test_scaled)

<h1>GPè¨“ç·´</h1>
gp_likelihood = gpytorch.likelihoods.GaussianLikelihood()
gp_model = BandgapGP(gp_train_x, gp_train_y, gp_likelihood)
gp_model.train()
gp_likelihood.train()

gp_optimizer = torch.optim.Adam(gp_model.parameters(), lr=0.1)
gp_mll = gpytorch.mlls.ExactMarginalLogLikelihood(
    gp_likelihood, gp_model
)

for i in range(100):
    gp_optimizer.zero_grad()
    output = gp_model(gp_train_x)
    loss = -gp_mll(output, gp_train_y)
    loss.backward()
    gp_optimizer.step()

<h1>GPäºˆæ¸¬</h1>
gp_model.eval()
gp_likelihood.eval()

with torch.no_grad(), gpytorch.settings.fast_pred_var():
    gp_pred = gp_likelihood(gp_model(gp_test_x))
    gp_mean = gp_pred.mean.numpy()
    gp_std = gp_pred.stddev.numpy()

gp_time = time.time() - start_time

print(f"å®Œäº† ({gp_time:.2f}ç§’)")
print(f"RMSE: {np.sqrt(np.mean((gp_mean - y_test) ** 2)):.3f} eV")


<h1>============================================</h1>
<h1>æ ¡æ­£æ›²ç·šï¼ˆCalibration Curveï¼‰ã®è¨ˆç®—</h1>
<h1>============================================</h1>
def compute_calibration_curve(y_true, y_pred, y_std, n_bins=10):
    """
    äºˆæ¸¬ã®æ ¡æ­£æ›²ç·šã‚’è¨ˆç®—

    Parameters:
    -----------
    y_true : array
        çœŸå€¤
    y_pred : array
        äºˆæ¸¬å¹³å‡
    y_std : array
        äºˆæ¸¬æ¨™æº–åå·®
    n_bins : int
        ãƒ“ãƒ³æ•°

    Returns:
    --------
    expected_freq : array
        æœŸå¾…ã•ã‚Œã‚‹é »åº¦
    observed_freq : array
        è¦³æ¸¬ã•ã‚ŒãŸé »åº¦
    """
    # æ­£è¦åŒ–æ®‹å·®ã‚’è¨ˆç®—
    residuals = (y_true - y_pred) / y_std

    # ä¿¡é ¼åŒºé–“ãƒ¬ãƒ™ãƒ«ã‚’å®šç¾©ï¼ˆ-3Ïƒ ã‹ã‚‰ +3Ïƒï¼‰
    confidence_levels = np.linspace(0.01, 0.99, n_bins)
    expected_freq = confidence_levels
    observed_freq = []

    for conf in confidence_levels:
        # ä¿¡é ¼åŒºé–“ã®ä¸Šä¸‹é™ã‚’è¨ˆç®—
        z_score = np.abs(
            np.percentile(np.random.randn(10000), conf * 100)
        )
        # ä¿¡é ¼åŒºé–“å†…ã«ã‚ã‚‹å‰²åˆã‚’è¨ˆç®—
        in_interval = np.abs(residuals) <= z_score
        observed_freq.append(np.mean(in_interval))

    return expected_freq, np.array(observed_freq)


<h1>============================================</h1>
<h1>æ¯”è¼ƒå¯è¦–åŒ–</h1>
<h1>============================================</h1>
methods = {
    'Random Forest': (rf_mean, rf_std, 'blue'),
    'MC Dropout': (mc_mean, mc_std, 'purple'),
    'Gaussian Process': (gp_mean, gp_std, 'green')
}

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

<h1>(1) ä¸ç¢ºå®Ÿæ€§ã®åˆ†å¸ƒ</h1>
ax = axes[0, 0]
for method_name, (_, std_values, color) in methods.items():
    ax.hist(
        std_values,
        bins=30,
        alpha=0.5,
        label=method_name,
        color=color
    )
ax.set_xlabel('Uncertainty (Standard Deviation)', fontsize=12)
ax.set_ylabel('Frequency', fontsize=12)
ax.set_title('Distribution of Uncertainty', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3, axis='y')

<h1>(2) ä¸ç¢ºå®Ÿæ€§ vs äºˆæ¸¬èª¤å·®</h1>
ax = axes[0, 1]
for method_name, (pred_mean, std_values, color) in methods.items():
    errors = np.abs(y_test - pred_mean)
    ax.scatter(
        std_values,
        errors,
        alpha=0.5,
        label=method_name,
        s=30,
        color=color
    )

    # ç›¸é–¢ä¿‚æ•°ã‚’è¨ˆç®—
    corr = np.corrcoef(std_values, errors)[0, 1]
    print(f"\n{method_name} - ä¸ç¢ºå®Ÿæ€§ã¨èª¤å·®ã®ç›¸é–¢: {corr:.3f}")

ax.set_xlabel('Uncertainty', fontsize=12)
ax.set_ylabel('Prediction Error (|True - Pred|)', fontsize=12)
ax.set_title('Uncertainty vs Error', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)

<h1>(3) æ ¡æ­£æ›²ç·šï¼ˆCalibration Curveï¼‰</h1>
ax = axes[1, 0]
for method_name, (pred_mean, std_values, color) in methods.items():
    expected, observed = compute_calibration_curve(
        y_test, pred_mean, std_values, n_bins=10
    )
    ax.plot(
        expected,
        observed,
        marker='o',
        label=method_name,
        color=color,
        linewidth=2
    )

<h1>å®Œå…¨æ ¡æ­£ãƒ©ã‚¤ãƒ³</h1>
ax.plot(
    [0, 1],
    [0, 1],
    'k--',
    label='Perfect calibration',
    linewidth=2
)
ax.set_xlabel('Expected Confidence Level', fontsize=12)
ax.set_ylabel('Observed Frequency', fontsize=12)
ax.set_title('Calibration Curve', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)

<h1>(4) è¨ˆç®—æ™‚é–“ã®æ¯”è¼ƒ</h1>
ax = axes[1, 1]
computation_times = [rf_time, mc_time, gp_time]
colors = ['blue', 'purple', 'green']
bars = ax.bar(
    ['Random\nForest', 'MC\nDropout', 'Gaussian\nProcess'],
    computation_times,
    color=colors,
    alpha=0.7,
    edgecolor='black'
)

<h1>å„ãƒãƒ¼ã«æ™‚é–“ã‚’è¡¨ç¤º</h1>
for bar, time_val in zip(bars, computation_times):
    height = bar.get_height()
    ax.text(
        bar.get_x() + bar.get_width() / 2.,
        height,
        f'{time_val:.2f}s',
        ha='center',
        va='bottom',
        fontsize=10
    )

ax.set_ylabel('Computation Time (seconds)', fontsize=12)
ax.set_title('Computational Cost', fontsize=14)
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('uncertainty_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

<h1>ã‚µãƒãƒªãƒ¼çµ±è¨ˆ</h1>
print("\n" + "=" * 50)
print("ä¸ç¢ºå®Ÿæ€§æ¨å®šã®ç·åˆæ¯”è¼ƒ")
print("=" * 50)
for method_name, (pred_mean, std_values, _) in methods.items():
    rmse = np.sqrt(np.mean((pred_mean - y_test) ** 2))
    print(f"\n{method_name}:")
    print(f"  RMSE: {rmse:.3f} eV")
    print(f"  å¹³å‡ä¸ç¢ºå®Ÿæ€§: {std_values.mean():.3f}")
    print(f"  ä¸ç¢ºå®Ÿæ€§ç¯„å›²: [{std_values.min():.3f}, "
          f"{std_values.max():.3f}]")</code></pre>

<strong>å‡ºåŠ›ä¾‹</strong>:
<pre><code>ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆä¸­...
è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 350 samples
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: 150 samples
ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ç¯„å›²: 0.00 - 7.92 eV

==================================================
Random Forestã§ä¸ç¢ºå®Ÿæ€§æ¨å®šä¸­...
å®Œäº† (0.58ç§’)
RMSE: 0.423 eV
==================================================
MC Dropoutã§ä¸ç¢ºå®Ÿæ€§æ¨å®šä¸­...
å®Œäº† (3.21ç§’)
RMSE: 0.387 eV
==================================================
Gaussian Processã§ä¸ç¢ºå®Ÿæ€§æ¨å®šä¸­...
å®Œäº† (1.87ç§’)
RMSE: 0.356 eV

Random Forest - ä¸ç¢ºå®Ÿæ€§ã¨èª¤å·®ã®ç›¸é–¢: 0.621
MC Dropout - ä¸ç¢ºå®Ÿæ€§ã¨èª¤å·®ã®ç›¸é–¢: 0.684
Gaussian Process - ä¸ç¢ºå®Ÿæ€§ã¨èª¤å·®ã®ç›¸é–¢: 0.743

==================================================
ä¸ç¢ºå®Ÿæ€§æ¨å®šã®ç·åˆæ¯”è¼ƒ
==================================================

Random Forest:
  RMSE: 0.423 eV
  å¹³å‡ä¸ç¢ºå®Ÿæ€§: 0.287
  ä¸ç¢ºå®Ÿæ€§ç¯„å›²: [0.134, 0.612]

MC Dropout:
  RMSE: 0.387 eV
  å¹³å‡ä¸ç¢ºå®Ÿæ€§: 0.312
  ä¸ç¢ºå®Ÿæ€§ç¯„å›²: [0.156, 0.698]

Gaussian Process:
  RMSE: 0.356 eV
  å¹³å‡ä¸ç¢ºå®Ÿæ€§: 0.298
  ä¸ç¢ºå®Ÿæ€§ç¯„å›²: [0.142, 0.721]</code></pre>

---

<h2>2.5 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

1. <strong>Ensembleæ³•</strong>
   - Random Forestã€LightGBMã«ã‚ˆã‚‹ä¸ç¢ºå®Ÿæ€§æ¨å®š
   - äºˆæ¸¬åˆ†æ•£ã§ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–
   - å®Ÿè£…ãŒç°¡å˜ã€è¨ˆç®—ã‚³ã‚¹ãƒˆä¸­ç¨‹åº¦

2. <strong>MC Dropout</strong>
   - æ¨è«–æ™‚ã«ã‚‚Dropoutã‚’é©ç”¨
   - ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å®¹æ˜“ã«å®Ÿè£…
   - ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å›æ•°ã¨Dropoutç‡ãŒé‡è¦

3. <strong>Gaussian Process</strong>
   - å³å¯†ãªä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–
   - ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã§æŸ”è»Ÿæ€§
   - å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã«ã¯ä¸å‘ã

<h3>æ‰‹æ³•ã®ä½¿ã„åˆ†ã‘</h3>

| æ‰‹æ³• | æ¨å¥¨ã‚±ãƒ¼ã‚¹ | ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º | è¨ˆç®—ã‚³ã‚¹ãƒˆ |
|------|----------|-------------|----------|
| Random Forest | è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã€ä¸­è¦æ¨¡ | 100-10,000 | ä½ã€œä¸­ |
| MC Dropout | æ·±å±¤å­¦ç¿’ã€ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆ | 1,000-100,000 | ä¸­ã€œé«˜ |
| Gaussian Process | å°‘æ•°ãƒ‡ãƒ¼ã‚¿ã€å³å¯†ãªä¸ç¢ºå®Ÿæ€§ | 10-1,000 | ä¸­ã€œé«˜ |

<h3>æ¬¡ã®ç« ã¸</h3>

ç¬¬3ç« ã§ã¯ã€ä¸ç¢ºå®Ÿæ€§ã‚’æ´»ç”¨ã—ãŸ<strong>ç²å¾—é–¢æ•°ã®è¨­è¨ˆ</strong>ã‚’å­¦ã³ã¾ã™ï¼š
- Expected Improvement (EI)
- Probability of Improvement (PI)
- Upper Confidence Bound (UCB)
- å¤šç›®çš„ãƒ»åˆ¶ç´„ä»˜ãç²å¾—é–¢æ•°

<strong>[ç¬¬3ç« ï¼šç²å¾—é–¢æ•°è¨­è¨ˆ â†’](./chapter-3.md)</strong>

---

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>
ï¼ˆçœç•¥ï¼šæ¼”ç¿’å•é¡Œã®è©³ç´°å®Ÿè£…ï¼‰

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>
ï¼ˆçœç•¥ï¼šæ¼”ç¿’å•é¡Œã®è©³ç´°å®Ÿè£…ï¼‰

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>
ï¼ˆçœç•¥ï¼šæ¼”ç¿’å•é¡Œã®è©³ç´°å®Ÿè£…ï¼‰

---

<h2>å‚è€ƒæ–‡çŒ®</h2>

1. Gal, Y., & Ghahramani, Z. (2016). "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning." *ICML*, 1050-1059.

2. Rasmussen, C. E., & Williams, C. K. I. (2006). *Gaussian Processes for Machine Learning*. MIT Press.

3. Lakshminarayanan, B. et al. (2017). "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles." *NeurIPS*.

---

<h2>ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³</h2>

<h3>å‰ã®ç« </h3>
<strong>[â† ç¬¬1ç« ï¼šActive Learningã®å¿…è¦æ€§](./chapter-1.md)</strong>

<h3>æ¬¡ã®ç« </h3>
<strong>[ç¬¬3ç« ï¼šç²å¾—é–¢æ•°è¨­è¨ˆ â†’](./chapter-3.md)</strong>

<h3>ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</h3>
<strong>[â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹](./index.md)</strong>

---

<strong>æ¬¡ã®ç« ã§ç²å¾—é–¢æ•°ã®è¨­è¨ˆã‚’å­¦ã³ã¾ã—ã‚‡ã†ï¼</strong>
<div class="navigation">
    <a href="chapter-1.html" class="nav-button">â† ç¬¬1ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-3.html" class="nav-button">ç¬¬3ç«  â†’</a>
</div>
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-18</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>
