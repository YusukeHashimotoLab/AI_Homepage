<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬1ç« ï¼šActive Learningã®å¿…è¦æ€§ - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬1ç« ï¼šActive Learningã®å¿…è¦æ€§</h1>
            <p class="subtitle">èƒ½å‹•çš„ãƒ‡ãƒ¼ã‚¿é¸æŠã§å®Ÿé¨“å›æ•°ã‚’åŠ‡çš„ã«å‰Šæ¸›</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-25åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 7å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 3å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h1>ç¬¬1ç« ï¼šActive Learningã®å¿…è¦æ€§</h1>

<strong>èƒ½å‹•çš„ãƒ‡ãƒ¼ã‚¿é¸æŠã§å®Ÿé¨“å›æ•°ã‚’åŠ‡çš„ã«å‰Šæ¸›</strong>

<h2>å­¦ç¿’ç›®æ¨™</h2>

ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š

- âœ… Active Learningã®å®šç¾©ã¨åˆ©ç‚¹ã‚’èª¬æ˜ã§ãã‚‹
- âœ… Query Strategiesã®4ã¤ã®ä¸»è¦æ‰‹æ³•ã‚’ç†è§£ã—ã¦ã„ã‚‹
- âœ… æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’èª¬æ˜ã§ãã‚‹
- âœ… ææ–™ç§‘å­¦ã«ãŠã‘ã‚‹æˆåŠŸäº‹ä¾‹ã‚’3ã¤ä»¥ä¸ŠæŒ™ã’ã‚‰ã‚Œã‚‹
- âœ… ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ã®å®šé‡çš„æ¯”è¼ƒãŒã§ãã‚‹

<strong>èª­äº†æ™‚é–“</strong>: 20-25åˆ†
<strong>ã‚³ãƒ¼ãƒ‰ä¾‹</strong>: 7å€‹
<strong>æ¼”ç¿’å•é¡Œ</strong>: 3å•

---

<h2>1.1 Active Learningã¨ã¯ä½•ã‹</h2>

<h3>å®šç¾©ï¼šèƒ½å‹•çš„ãƒ‡ãƒ¼ã‚¿é¸æŠã«ã‚ˆã‚‹åŠ¹ç‡çš„å­¦ç¿’</h3>

<strong>Active Learningï¼ˆèƒ½å‹•çš„å­¦ç¿’ï¼‰</strong>ã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãŒã€Œã©ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¬¡ã«å–å¾—ã™ã¹ãã‹ã€ã‚’èƒ½å‹•çš„ã«é¸æŠã™ã‚‹ã“ã¨ã§ã€å°‘ãªã„å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦ãªãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚

<strong>Passive Learningï¼ˆå—å‹•çš„å­¦ç¿’ï¼‰ã¨ã®é•ã„</strong>:

| é …ç›® | Passive Learning | Active Learning |
|------|-----------------|----------------|
| ãƒ‡ãƒ¼ã‚¿é¸æŠ | ãƒ©ãƒ³ãƒ€ãƒ  or æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | ãƒ¢ãƒ‡ãƒ«ãŒèƒ½å‹•çš„ã«é¸æŠ |
| å­¦ç¿’åŠ¹ç‡ | ä½ï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿å¿…è¦ï¼‰ | é«˜ï¼ˆå°‘é‡ãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦ï¼‰ |
| ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚³ã‚¹ãƒˆ | è€ƒæ…®ã—ãªã„ | è€ƒæ…®ã™ã‚‹ |
| é©ç”¨å ´é¢ | ãƒ‡ãƒ¼ã‚¿ãŒå®‰ä¾¡ | ãƒ‡ãƒ¼ã‚¿ãŒé«˜ä¾¡ |

<strong>ææ–™ç§‘å­¦ã«ãŠã‘ã‚‹é‡è¦æ€§</strong>:
- 1å›ã®å®Ÿé¨“ã«æ•°æ—¥ã€œæ•°é€±é–“ã‹ã‹ã‚‹
- å®Ÿé¨“ã‚³ã‚¹ãƒˆãŒé«˜ã„ï¼ˆè§¦åª’åˆæˆã€DFTè¨ˆç®—ãªã©ï¼‰
- æ¢ç´¢ç©ºé–“ãŒåºƒå¤§ï¼ˆ10^6ã€œ10^60é€šã‚Šã®å€™è£œï¼‰

<h3>Active Learningã®åŸºæœ¬ã‚µã‚¤ã‚¯ãƒ«</h3>

<pre><code class="language-mermaid">graph LR
    A[åˆæœŸãƒ‡ãƒ¼ã‚¿<br/>å°‘æ•°ã®ã‚µãƒ³ãƒ—ãƒ«] --> B[ãƒ¢ãƒ‡ãƒ«å­¦ç¿’<br/>äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰]
    B --> C[å€™è£œè©•ä¾¡<br/>Query Strategy]
    C --> D[æœ€ã‚‚æœ‰ç›Šãª<br/>ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ]
    D --> E[å®Ÿé¨“å®Ÿè¡Œ<br/>ãƒ‡ãƒ¼ã‚¿å–å¾—]
    E --> F{çµ‚äº†æ¡ä»¶?<br/>ç›®æ¨™é”æˆ or<br/>äºˆç®—ä¸Šé™}
    F -->|ã„ã„ãˆ| B
    F -->|ã¯ã„| G[æœ€çµ‚ãƒ¢ãƒ‡ãƒ«]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#ffebee
    style G fill:#4CAF50,color:#fff</code></pre>

<strong>ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ</strong>:
1. <strong>å°‘æ•°ã®åˆæœŸãƒ‡ãƒ¼ã‚¿</strong>ã§é–‹å§‹ï¼ˆé€šå¸¸10-20ã‚µãƒ³ãƒ—ãƒ«ï¼‰
2. <strong>Query Strategy</strong>ã§æ¬¡ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è³¢ãé¸æŠ
3. <strong>å®Ÿé¨“å®Ÿè¡Œ</strong>ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’1ã¤ãšã¤è¿½åŠ 
4. <strong>ãƒ¢ãƒ‡ãƒ«æ›´æ–°</strong>ã‚’ç¹°ã‚Šè¿”ã™
5. <strong>ç›®æ¨™é”æˆ</strong>ã¾ã§ç¶™ç¶š

---

<h2>1.2 Query Strategiesã®åŸºç¤</h2>

<h3>1.2.1 Uncertainty Samplingï¼ˆä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰</h3>

<strong>åŸç†</strong>: ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãŒæœ€ã‚‚ä¸ç¢ºå®Ÿãªã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ

<strong>æ•°å¼</strong>:
$$
x^* = \arg\max_{x \in \mathcal{U}} \text{Uncertainty}(x)
$$

ã“ã“ã§ã€$\mathcal{U}$ã¯ãƒ©ãƒ™ãƒ«æœªå–å¾—ã®ã‚µãƒ³ãƒ—ãƒ«é›†åˆ

<strong>ä¸ç¢ºå®Ÿæ€§ã®æ¸¬å®šæ–¹æ³•</strong>:

<strong>å›å¸°å•é¡Œ</strong>:
$$
\text{Uncertainty}(x) = \sigma(x)
$$
ï¼ˆäºˆæ¸¬ã®æ¨™æº–åå·®ï¼‰

<strong>åˆ†é¡å•é¡Œï¼ˆ2ã‚¯ãƒ©ã‚¹ï¼‰</strong>:
$$
\text{Uncertainty}(x) = 1 - |P(y=1|x) - P(y=0|x)|
$$
ï¼ˆç¢ºç‡ã®å·®ã®çµ¶å¯¾å€¤ã®é€†æ•°ã€0.5ã«è¿‘ã„ã»ã©ä¸ç¢ºå®Ÿï¼‰

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹1: Uncertainty Samplingã®å®Ÿè£…</strong>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression

<h1>ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆææ–™ç‰¹æ€§äºˆæ¸¬ã‚’æƒ³å®šï¼‰</h1>
np.random.seed(42)
X, y = make_regression(
    n_samples=500,
    n_features=3,
    noise=10,
    random_state=42
)

<h1>åˆæœŸãƒ‡ãƒ¼ã‚¿ï¼ˆ10ã‚µãƒ³ãƒ—ãƒ«ï¼‰</h1>
initial_indices = np.random.choice(len(X), 10, replace=False)
X_train = X[initial_indices]
y_train = y[initial_indices]

<h1>æœªãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿</h1>
unlabeled_mask = np.ones(len(X), dtype=bool)
unlabeled_mask[initial_indices] = False
X_unlabeled = X[unlabeled_mask]
y_unlabeled = y[unlabeled_mask]

def uncertainty_sampling(
    X_train,
    y_train,
    X_unlabeled,
    n_queries=5
):
    """
    Uncertainty Samplingã«ã‚ˆã‚‹ã‚µãƒ³ãƒ—ãƒ«é¸æŠ

    Parameters:
    -----------
    X_train : array
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    y_train : array
        è¨“ç·´ãƒ©ãƒ™ãƒ«
    X_unlabeled : array
        æœªãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿
    n_queries : int
        é¸æŠã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°

    Returns:
    --------
    selected_indices : array
        é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    """
    # Random Forestã§ä¸ç¢ºå®Ÿæ€§æ¨å®šï¼ˆäºˆæ¸¬åˆ†æ•£ï¼‰
    rf = RandomForestRegressor(
        n_estimators=100,
        random_state=42
    )
    rf.fit(X_train, y_train)

    # å„æ±ºå®šæœ¨ã®äºˆæ¸¬ã‚’å–å¾—
    predictions = np.array([
        tree.predict(X_unlabeled)
        for tree in rf.estimators_
    ])

    # äºˆæ¸¬ã®æ¨™æº–åå·®ï¼ˆä¸ç¢ºå®Ÿæ€§ï¼‰ã‚’è¨ˆç®—
    uncertainties = np.std(predictions, axis=0)

    # ä¸ç¢ºå®Ÿæ€§ãŒæœ€ã‚‚é«˜ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ
    selected_indices = np.argsort(uncertainties)[-n_queries:]

    return selected_indices, uncertainties

<h1>Uncertainty Samplingã‚’å®Ÿè¡Œ</h1>
selected_idx, uncertainties = uncertainty_sampling(
    X_train,
    y_train,
    X_unlabeled,
    n_queries=5
)

print("Uncertainty Samplingã®çµæœ:")
print(f"é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ•°: {len(selected_idx)}")
print(f"ä¸ç¢ºå®Ÿæ€§ã®ç¯„å›²: {uncertainties.min():.2f} - "
      f"{uncertainties.max():.2f}")
print(f"é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®ä¸ç¢ºå®Ÿæ€§:")
for i, idx in enumerate(selected_idx):
    print(f"  ã‚µãƒ³ãƒ—ãƒ« {idx}: {uncertainties[idx]:.2f}")</code></pre>

<strong>å‡ºåŠ›</strong>:
<pre><code>Uncertainty Samplingã®çµæœ:
é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ•°: 5
ä¸ç¢ºå®Ÿæ€§ã®ç¯„å›²: 2.13 - 18.45
é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®ä¸ç¢ºå®Ÿæ€§:
  ã‚µãƒ³ãƒ—ãƒ« 234: 16.82
  ã‚µãƒ³ãƒ—ãƒ« 67: 17.23
  ã‚µãƒ³ãƒ—ãƒ« 412: 17.56
  ã‚µãƒ³ãƒ—ãƒ« 189: 17.91
  ã‚µãƒ³ãƒ—ãƒ« 345: 18.45</code></pre>

<strong>åˆ©ç‚¹</strong>:
- âœ… ã‚·ãƒ³ãƒ—ãƒ«ã§ç›´æ„Ÿçš„
- âœ… è¨ˆç®—ã‚³ã‚¹ãƒˆãŒä½ã„
- âœ… å¤šãã®å•é¡Œã§æœ‰åŠ¹

<strong>æ¬ ç‚¹</strong>:
- âš ï¸ æ¢ç´¢ç©ºé–“ã®å¤šæ§˜æ€§ã‚’è€ƒæ…®ã—ãªã„
- âš ï¸ å±€æ‰€çš„ãªé ˜åŸŸã«åã‚‹å¯èƒ½æ€§

---

<h3>1.2.2 Diversity Samplingï¼ˆå¤šæ§˜æ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰</h3>

<strong>åŸç†</strong>: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ç•°ãªã‚‹ï¼ˆå¤šæ§˜ãªï¼‰ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ

<strong>æ•°å¼</strong>:
$$
x^* = \arg\max_{x \in \mathcal{U}} \min_{x_i \in \mathcal{L}} d(x, x_i)
$$

ã“ã“ã§ã€$\mathcal{L}$ã¯ãƒ©ãƒ™ãƒ«å–å¾—æ¸ˆã¿ã®ã‚µãƒ³ãƒ—ãƒ«é›†åˆã€$d(\cdot, \cdot)$ã¯è·é›¢é–¢æ•°

<strong>è·é›¢ã®æ¸¬å®šæ–¹æ³•</strong>:
- ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢: $d(x_i, x_j) = \|x_i - x_j\|_2$
- ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢: $d(x_i, x_j) = \sqrt{(x_i - x_j)^T \Sigma^{-1} (x_i - x_j)}$
- ã‚³ã‚µã‚¤ãƒ³è·é›¢: $d(x_i, x_j) = 1 - \frac{x_i \cdot x_j}{\|x_i\| \|x_j\|}$

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹2: Diversity Samplingã®å®Ÿè£…</strong>

<pre><code class="language-python">from sklearn.metrics import pairwise_distances

def diversity_sampling(
    X_train,
    X_unlabeled,
    n_queries=5,
    metric='euclidean'
):
    """
    Diversity Samplingã«ã‚ˆã‚‹ã‚µãƒ³ãƒ—ãƒ«é¸æŠ

    Parameters:
    -----------
    X_train : array
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    X_unlabeled : array
        æœªãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿
    n_queries : int
        é¸æŠã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
    metric : str
        è·é›¢æŒ‡æ¨™ï¼ˆ'euclidean', 'cosine'ç­‰ï¼‰

    Returns:
    --------
    selected_indices : array
        é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    """
    selected_indices = []

    # n_queriesã‚µãƒ³ãƒ—ãƒ«é¸æŠã™ã‚‹ã¾ã§ç¹°ã‚Šè¿”ã—
    for _ in range(n_queries):
        if len(selected_indices) == 0:
            # æœ€åˆã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ã®è·é›¢ã‚’è¨ˆç®—
            distances = pairwise_distances(
                X_unlabeled,
                X_train,
                metric=metric
            )
        else:
            # é¸æŠæ¸ˆã¿ã‚µãƒ³ãƒ—ãƒ«ã‚‚å«ã‚ã¦è·é›¢ã‚’è¨ˆç®—
            X_selected = X_unlabeled[selected_indices]
            X_reference = np.vstack([X_train, X_selected])
            distances = pairwise_distances(
                X_unlabeled,
                X_reference,
                metric=metric
            )

        # å„æœªãƒ©ãƒ™ãƒ«ã‚µãƒ³ãƒ—ãƒ«ã®ã€æœ€ã‚‚è¿‘ã„ã‚µãƒ³ãƒ—ãƒ«ã¾ã§ã®è·é›¢
        min_distances = distances.min(axis=1)

        # æ—¢ã«é¸æŠæ¸ˆã¿ã®ã‚µãƒ³ãƒ—ãƒ«ã¯é™¤å¤–
        min_distances[selected_indices] = -np.inf

        # æœ€ã‚‚é ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ
        next_idx = np.argmax(min_distances)
        selected_indices.append(next_idx)

    return np.array(selected_indices), min_distances

<h1>Diversity Samplingã‚’å®Ÿè¡Œ</h1>
selected_idx, min_distances = diversity_sampling(
    X_train,
    X_unlabeled,
    n_queries=5
)

print("\nDiversity Samplingã®çµæœ:")
print(f"é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ•°: {len(selected_idx)}")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®æœ€å°è·é›¢:")
for i, idx in enumerate(selected_idx):
    print(f"  ã‚µãƒ³ãƒ—ãƒ« {idx}: {min_distances[idx]:.2f}")</code></pre>

<strong>å‡ºåŠ›</strong>:
<pre><code>Diversity Samplingã®çµæœ:
é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ•°: 5
è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®æœ€å°è·é›¢:
  ã‚µãƒ³ãƒ—ãƒ« 123: 12.34
  ã‚µãƒ³ãƒ—ãƒ« 456: 11.89
  ã‚µãƒ³ãƒ—ãƒ« 78: 10.56
  ã‚µãƒ³ãƒ—ãƒ« 234: 9.87
  ã‚µãƒ³ãƒ—ãƒ« 345: 9.23</code></pre>

<strong>åˆ©ç‚¹</strong>:
- âœ… æ¢ç´¢ç©ºé–“ã®åºƒç¯„å›²ã‚’ã‚«ãƒãƒ¼
- âœ… å±€æ‰€è§£ã¸ã®åã‚Šã‚’é˜²ã
- âœ… ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¨ã®ç›¸æ€§ãŒè‰¯ã„

<strong>æ¬ ç‚¹</strong>:
- âš ï¸ ãƒ¢ãƒ‡ãƒ«ã®ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã—ãªã„
- âš ï¸ è¨ˆç®—ã‚³ã‚¹ãƒˆãŒã‚„ã‚„é«˜ã„

---

<h3>1.2.3 Query-by-Committee</h3>

<strong>åŸç†</strong>: è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆå§”å“¡ä¼šï¼‰ã®æ„è¦‹ãŒæœ€ã‚‚åˆ†ã‹ã‚Œã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ

<strong>æ•°å¼</strong>:
$$
x^* = \arg\max_{x \in \mathcal{U}} \text{Disagreement}(C, x)
$$

ã“ã“ã§ã€$C = \{M_1, M_2, ..., M_K\}$ã¯ãƒ¢ãƒ‡ãƒ«ã®é›†åˆï¼ˆå§”å“¡ä¼šï¼‰

<strong>æ„è¦‹ã®ä¸ä¸€è‡´ã®æ¸¬å®š</strong>:

<strong>å›å¸°å•é¡Œï¼ˆåˆ†æ•£ï¼‰</strong>:
$$
\text{Disagreement}(C, x) = \frac{1}{K} \sum_{k=1}^K (M_k(x) - \bar{M}(x))^2
$$

<strong>åˆ†é¡å•é¡Œï¼ˆKullback-Leibler Divergenceï¼‰</strong>:
$$
\text{Disagreement}(C, x) = \frac{1}{K} \sum_{k=1}^K KL(P_k(\cdot|x) \| P_C(\cdot|x))
$$

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹3: Query-by-Committeeã®å®Ÿè£…</strong>

<pre><code class="language-python">from sklearn.ensemble import (
    RandomForestRegressor,
    GradientBoostingRegressor
)
from sklearn.linear_model import Ridge
from sklearn.neural_network import MLPRegressor

def query_by_committee(
    X_train,
    y_train,
    X_unlabeled,
    n_queries=5
):
    """
    Query-by-Committeeã«ã‚ˆã‚‹ã‚µãƒ³ãƒ—ãƒ«é¸æŠ

    Parameters:
    -----------
    X_train : array
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    y_train : array
        è¨“ç·´ãƒ©ãƒ™ãƒ«
    X_unlabeled : array
        æœªãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿
    n_queries : int
        é¸æŠã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°

    Returns:
    --------
    selected_indices : array
        é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    """
    # å§”å“¡ä¼šï¼ˆç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã®é›†åˆï¼‰
    committee = [
        RandomForestRegressor(n_estimators=50, random_state=42),
        GradientBoostingRegressor(n_estimators=50, random_state=42),
        Ridge(alpha=1.0),
        MLPRegressor(
            hidden_layer_sizes=(50,),
            max_iter=500,
            random_state=42
        )
    ]

    # å„ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
    for model in committee:
        model.fit(X_train, y_train)

    # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å–å¾—
    predictions = np.array([
        model.predict(X_unlabeled)
        for model in committee
    ])

    # äºˆæ¸¬ã®åˆ†æ•£ï¼ˆæ„è¦‹ã®ä¸ä¸€è‡´ï¼‰ã‚’è¨ˆç®—
    disagreement = np.var(predictions, axis=0)

    # ä¸ä¸€è‡´ãŒæœ€ã‚‚å¤§ãã„ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ
    selected_indices = np.argsort(disagreement)[-n_queries:]

    return selected_indices, disagreement

<h1>Query-by-Committeeã‚’å®Ÿè¡Œ</h1>
selected_idx, disagreement = query_by_committee(
    X_train,
    y_train,
    X_unlabeled,
    n_queries=5
)

print("\nQuery-by-Committeeã®çµæœ:")
print(f"é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ•°: {len(selected_idx)}")
print(f"æ„è¦‹ã®ä¸ä¸€è‡´ã®ç¯„å›²: {disagreement.min():.2f} - "
      f"{disagreement.max():.2f}")
print(f"é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®ä¸ä¸€è‡´:")
for i, idx in enumerate(selected_idx):
    print(f"  ã‚µãƒ³ãƒ—ãƒ« {idx}: {disagreement[idx]:.2f}")</code></pre>

<strong>å‡ºåŠ›</strong>:
<pre><code>Query-by-Committeeã®çµæœ:
é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ•°: 5
æ„è¦‹ã®ä¸ä¸€è‡´ã®ç¯„å›²: 5.23 - 142.56
é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®ä¸ä¸€è‡´:
  ã‚µãƒ³ãƒ—ãƒ« 89: 128.34
  ã‚µãƒ³ãƒ—ãƒ« 234: 132.45
  ã‚µãƒ³ãƒ—ãƒ« 156: 135.67
  ã‚µãƒ³ãƒ—ãƒ« 401: 139.12
  ã‚µãƒ³ãƒ—ãƒ« 267: 142.56</code></pre>

<strong>åˆ©ç‚¹</strong>:
- âœ… å¤šæ§˜ãªãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’æ´»ç”¨
- âœ… ãƒ¢ãƒ‡ãƒ«ãƒã‚¤ã‚¢ã‚¹ã‚’è»½æ¸›
- âœ… ä¸ç¢ºå®Ÿæ€§æ¨å®šãŒå …ç‰¢

<strong>æ¬ ç‚¹</strong>:
- âš ï¸ è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ï¼ˆè¤‡æ•°ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼‰
- âš ï¸ ãƒ¢ãƒ‡ãƒ«é¸æŠã«ä¾å­˜

---

<h3>1.2.4 Expected Model Change</h3>

<strong>åŸç†</strong>: ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€ã‚‚å¤§ããå¤‰åŒ–ã•ã›ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ

<strong>æ•°å¼</strong>ï¼ˆå‹¾é…ãƒ™ãƒ¼ã‚¹ï¼‰:
$$
x^* = \arg\max_{x \in \mathcal{U}} \|\nabla_\theta \mathcal{L}(\theta; x, \hat{y})\|
$$

ã“ã“ã§ã€$\theta$ã¯ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€$\mathcal{L}$ã¯æå¤±é–¢æ•°ã€$\hat{y}$ã¯äºˆæ¸¬å€¤

<strong>åˆ©ç‚¹</strong>:
- âœ… ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã¸ã®å½±éŸ¿åº¦ã‚’ç›´æ¥è©•ä¾¡
- âœ… åŠ¹ç‡çš„ãªå­¦ç¿’ãŒå¯èƒ½

<strong>æ¬ ç‚¹</strong>:
- âš ï¸ è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„
- âš ï¸ å‹¾é…è¨ˆç®—å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã«é™å®š

---

<h2>1.3 Exploration vs Exploitation</h2>

<h3>ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®æ¦‚å¿µ</h3>

Active Learningã«ãŠã‘ã‚‹æœ€ã‚‚é‡è¦ãªæ¦‚å¿µã®1ã¤ãŒã€<strong>Explorationï¼ˆæ¢ç´¢ï¼‰ã¨Exploitationï¼ˆæ´»ç”¨ï¼‰ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•</strong>ã§ã™ã€‚

<strong>Explorationï¼ˆæ¢ç´¢ï¼‰</strong>:
- æœªçŸ¥ã®é ˜åŸŸã‚’æ¢ç´¢
- å¤šæ§˜ãªã‚µãƒ³ãƒ—ãƒ«ã‚’åé›†
- æ–°ã—ã„æƒ…å ±ã‚’ç²å¾—
- ãƒªã‚¹ã‚¯ã‚’å–ã‚‹

<strong>Exploitationï¼ˆæ´»ç”¨ï¼‰</strong>:
- æ—¢çŸ¥ã®è‰¯ã„é ˜åŸŸã‚’é›†ä¸­çš„ã«èª¿æŸ»
- ãƒ¢ãƒ‡ãƒ«ã®ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„é ˜åŸŸã‚’å„ªå…ˆ
- æ—¢å­˜çŸ¥è­˜ã‚’æœ€å¤§é™æ´»ç”¨
- å®‰å…¨ã«æ”¹å–„

<h3>ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®å¯è¦–åŒ–</h3>

<pre><code class="language-mermaid">graph TB
    subgraph æ¢ç´¢é‡è¦–
    A[æœªçŸ¥é ˜åŸŸã‚’<br/>ç©æ¥µçš„ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°]
    A --> B[æ–°ç™ºè¦‹ã®å¯èƒ½æ€§å¤§]
    A --> C[å­¦ç¿’ã¯é…ã„]
    end

    subgraph æ´»ç”¨é‡è¦–
    D[ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„é ˜åŸŸã‚’<br/>é›†ä¸­çš„ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°]
    D --> E[é«˜é€Ÿã«åæŸ]
    D --> F[å±€æ‰€è§£ã«<br/>ã¯ã¾ã‚‹ãƒªã‚¹ã‚¯]
    end

    subgraph ãƒãƒ©ãƒ³ã‚¹
    G[é©åº¦ãªæ¢ç´¢ã¨æ´»ç”¨]
    G --> H[åŠ¹ç‡çš„ãªå­¦ç¿’]
    G --> I[åºƒç¯„å›²ã‹ã¤<br/>æ·±ã„ç†è§£]
    end

    style A fill:#e3f2fd
    style D fill:#fff3e0
    style G fill:#e8f5e9
    style I fill:#4CAF50,color:#fff</code></pre>

<h3>Îµ-greedyã‚¢ãƒ—ãƒ­ãƒ¼ãƒ</h3>

<strong>åŸç†</strong>: ç¢ºç‡$\epsilon$ã§æ¢ç´¢ã€ç¢ºç‡$1-\epsilon$ã§æ´»ç”¨

<strong>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong>:
<pre><code>ç¢ºç‡ Îµ ã§:
    ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠï¼ˆæ¢ç´¢ï¼‰
ç¢ºç‡ 1-Îµ ã§:
    Query Strategyã§æœ€è‰¯ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠï¼ˆæ´»ç”¨ï¼‰</code></pre>

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹4: Îµ-greedy Active Learning</strong>

<pre><code class="language-python">def epsilon_greedy_sampling(
    X_train,
    y_train,
    X_unlabeled,
    n_queries=5,
    epsilon=0.2
):
    """
    Îµ-greedy Active Learning

    Parameters:
    -----------
    X_train : array
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    y_train : array
        è¨“ç·´ãƒ©ãƒ™ãƒ«
    X_unlabeled : array
        æœªãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿
    n_queries : int
        é¸æŠã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
    epsilon : float
        æ¢ç´¢ç¢ºç‡ï¼ˆ0-1ï¼‰

    Returns:
    --------
    selected_indices : array
        é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    """
    selected_indices = []

    for _ in range(n_queries):
        if np.random.rand() < epsilon:
            # æ¢ç´¢ï¼šãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«é¸æŠ
            available = [
                i for i in range(len(X_unlabeled))
                if i not in selected_indices
            ]
            idx = np.random.choice(available)
            strategy = "æ¢ç´¢"
        else:
            # æ´»ç”¨ï¼šUncertainty Samplingã§é¸æŠ
            available_mask = np.ones(len(X_unlabeled), dtype=bool)
            available_mask[selected_indices] = False
            X_available = X_unlabeled[available_mask]

            rf = RandomForestRegressor(
                n_estimators=50,
                random_state=42
            )
            rf.fit(X_train, y_train)

            predictions = np.array([
                tree.predict(X_available)
                for tree in rf.estimators_
            ])
            uncertainties = np.std(predictions, axis=0)

            # åˆ©ç”¨å¯èƒ½ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰é¸æŠ
            available_indices = np.where(available_mask)[0]
            idx = available_indices[np.argmax(uncertainties)]
            strategy = "æ´»ç”¨"

        selected_indices.append(idx)
        print(f"ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ {len(selected_indices)}: "
              f"ã‚µãƒ³ãƒ—ãƒ« {idx} ã‚’é¸æŠï¼ˆ{strategy}ï¼‰")

    return np.array(selected_indices)

<h1>Îµ-greedy Active Learningã‚’å®Ÿè¡Œ</h1>
print("\nÎµ-greedy Active Learningï¼ˆÎµ=0.2ï¼‰:")
selected_idx = epsilon_greedy_sampling(
    X_train,
    y_train,
    X_unlabeled,
    n_queries=5,
    epsilon=0.2
)</code></pre>

<strong>å‡ºåŠ›</strong>:
<pre><code>Îµ-greedy Active Learningï¼ˆÎµ=0.2ï¼‰:
ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ 1: ã‚µãƒ³ãƒ—ãƒ« 234 ã‚’é¸æŠï¼ˆæ´»ç”¨ï¼‰
ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ 2: ã‚µãƒ³ãƒ—ãƒ« 456 ã‚’é¸æŠï¼ˆæ´»ç”¨ï¼‰
ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ 3: ã‚µãƒ³ãƒ—ãƒ« 123 ã‚’é¸æŠï¼ˆæ¢ç´¢ï¼‰
ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ 4: ã‚µãƒ³ãƒ—ãƒ« 345 ã‚’é¸æŠï¼ˆæ´»ç”¨ï¼‰
ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ 5: ã‚µãƒ³ãƒ—ãƒ« 78 ã‚’é¸æŠï¼ˆæ´»ç”¨ï¼‰</code></pre>

<strong>Îµã®é¸æŠ</strong>:
- $\epsilon = 0$: å®Œå…¨ãªæ´»ç”¨ï¼ˆå±€æ‰€è§£ã®ãƒªã‚¹ã‚¯ï¼‰
- $\epsilon = 1$: å®Œå…¨ãªæ¢ç´¢ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
- $\epsilon = 0.1 \sim 0.2$: ãƒãƒ©ãƒ³ã‚¹è‰¯å¥½ï¼ˆæ¨å¥¨ï¼‰

---

<h3>Upper Confidence Bound (UCB)</h3>

<strong>åŸç†</strong>: äºˆæ¸¬å¹³å‡ + ä¸ç¢ºå®Ÿæ€§ãƒœãƒ¼ãƒŠã‚¹

<strong>æ•°å¼</strong>:
$$
\text{UCB}(x) = \mu(x) + \kappa \sigma(x)
$$

- $\mu(x)$: äºˆæ¸¬å¹³å‡
- $\sigma(x)$: äºˆæ¸¬æ¨™æº–åå·®
- $\kappa$: æ¢ç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé€šå¸¸1.0ã€œ3.0ï¼‰

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹5: UCBã«ã‚ˆã‚‹ã‚µãƒ³ãƒ—ãƒ«é¸æŠ</strong>

<pre><code class="language-python">def ucb_sampling(
    X_train,
    y_train,
    X_unlabeled,
    n_queries=5,
    kappa=2.0
):
    """
    UCBï¼ˆUpper Confidence Boundï¼‰ã«ã‚ˆã‚‹ã‚µãƒ³ãƒ—ãƒ«é¸æŠ

    Parameters:
    -----------
    X_train : array
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    y_train : array
        è¨“ç·´ãƒ©ãƒ™ãƒ«
    X_unlabeled : array
        æœªãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿
    n_queries : int
        é¸æŠã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
    kappa : float
        æ¢ç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

    Returns:
    --------
    selected_indices : array
        é¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    """
    # Random Forestã§äºˆæ¸¬
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)

    # äºˆæ¸¬å¹³å‡ã¨æ¨™æº–åå·®
    predictions = np.array([
        tree.predict(X_unlabeled)
        for tree in rf.estimators_
    ])
    mean_pred = np.mean(predictions, axis=0)
    std_pred = np.std(predictions, axis=0)

    # UCBã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    ucb_scores = mean_pred + kappa * std_pred

    # UCBãŒæœ€ã‚‚é«˜ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ
    selected_indices = np.argsort(ucb_scores)[-n_queries:]

    return selected_indices, ucb_scores, mean_pred, std_pred

<h1>UCBã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ</h1>
selected_idx, ucb_scores, mean_pred, std_pred = ucb_sampling(
    X_train,
    y_train,
    X_unlabeled,
    n_queries=5,
    kappa=2.0
)

print("\nUCBã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®çµæœï¼ˆÎº=2.0ï¼‰:")
for i, idx in enumerate(selected_idx):
    print(f"ã‚µãƒ³ãƒ—ãƒ« {idx}:")
    print(f"  äºˆæ¸¬å¹³å‡: {mean_pred[idx]:.2f}")
    print(f"  äºˆæ¸¬æ¨™æº–åå·®: {std_pred[idx]:.2f}")
    print(f"  UCBã‚¹ã‚³ã‚¢: {ucb_scores[idx]:.2f}")</code></pre>

<strong>å‡ºåŠ›</strong>:
<pre><code>UCBã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®çµæœï¼ˆÎº=2.0ï¼‰:
ã‚µãƒ³ãƒ—ãƒ« 234:
  äºˆæ¸¬å¹³å‡: 45.23
  äºˆæ¸¬æ¨™æº–åå·®: 8.12
  UCBã‚¹ã‚³ã‚¢: 61.47
ã‚µãƒ³ãƒ—ãƒ« 456:
  äºˆæ¸¬å¹³å‡: 38.56
  äºˆæ¸¬æ¨™æº–åå·®: 9.45
  UCBã‚¹ã‚³ã‚¢: 57.46
ã‚µãƒ³ãƒ—ãƒ« 123:
  äºˆæ¸¬å¹³å‡: 42.78
  äºˆæ¸¬æ¨™æº–åå·®: 7.34
  UCBã‚¹ã‚³ã‚¢: 57.46
ã‚µãƒ³ãƒ—ãƒ« 345:
  äºˆæ¸¬å¹³å‡: 40.12
  äºˆæ¸¬æ¨™æº–åå·®: 8.56
  UCBã‚¹ã‚³ã‚¢: 57.24
ã‚µãƒ³ãƒ—ãƒ« 78:
  äºˆæ¸¬å¹³å‡: 39.45
  äºˆæ¸¬æ¨™æº–åå·®: 8.89
  UCBã‚¹ã‚³ã‚¢: 57.23</code></pre>

<strong>Îºã®å½±éŸ¿</strong>:
- $\kappa$ãŒå¤§ãã„ â†’ æ¢ç´¢é‡è¦–
- $\kappa$ãŒå°ã•ã„ â†’ æ´»ç”¨é‡è¦–
- æ¨å¥¨å€¤: $\kappa = 2.0 \sim 2.5$

---

<h2>1.4 ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ï¼šè§¦åª’æ´»æ€§äºˆæ¸¬</h2>

<h3>å•é¡Œè¨­å®š</h3>

<strong>ç›®æ¨™</strong>: è§¦åª’ã®åå¿œæ´»æ€§ã‚’äºˆæ¸¬ã—ã€æœ€ã‚‚æ´»æ€§ã®é«˜ã„è§¦åª’ã‚’10å®Ÿé¨“ã§ç™ºè¦‹

<strong>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</strong>:
- å€™è£œè§¦åª’: 500ç¨®é¡
- ç‰¹å¾´é‡: é‡‘å±çµ„æˆï¼ˆ3å…ƒç´ ï¼‰ã€æ‹…æŒé‡ã€ç„¼æˆæ¸©åº¦
- ç›®æ¨™å¤‰æ•°: åå¿œé€Ÿåº¦å®šæ•°ï¼ˆkï¼‰

<strong>åˆ¶ç´„</strong>:
- 1å›ã®å®Ÿé¨“ã«3æ—¥ã‹ã‹ã‚‹
- äºˆç®—ã®åˆ¶ç´„ã§æœ€å¤§10å®Ÿé¨“ã¾ã§

<h3>ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° vs Active Learning</h3>

<strong>ã‚³ãƒ¼ãƒ‰ä¾‹6: è§¦åª’æ´»æ€§äºˆæ¸¬ã®æ¯”è¼ƒå®Ÿé¨“</strong>

<pre><code class="language-python">from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

<h1>ä»®æƒ³çš„ãªè§¦åª’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ</h1>
np.random.seed(42)
n_catalysts = 500

<h1>ç‰¹å¾´é‡: [é‡‘å±A%, é‡‘å±B%, é‡‘å±C%, æ‹…æŒé‡, ç„¼æˆæ¸©åº¦]</h1>
X_catalyst = np.random.rand(n_catalysts, 5)
X_catalyst[:, 0:3] = X_catalyst[:, 0:3] / \
                     X_catalyst[:, 0:3].sum(axis=1, keepdims=True)
X_catalyst[:, 3] = X_catalyst[:, 3] * 20  # æ‹…æŒé‡ 0-20 wt%
X_catalyst[:, 4] = X_catalyst[:, 4] * 500 + 300  # ç„¼æˆæ¸©åº¦ 300-800Â°C

<h1>ç›®æ¨™å¤‰æ•°: åå¿œé€Ÿåº¦å®šæ•°ï¼ˆè¤‡é›‘ãªéç·šå½¢é–¢æ•°ï¼‰</h1>
y_catalyst = (
    10 * X_catalyst[:, 0]**2 +
    15 * X_catalyst[:, 1] * X_catalyst[:, 2] +
    0.5 * X_catalyst[:, 3] +
    0.01 * (X_catalyst[:, 4] - 600)**2 +
    np.random.normal(0, 2, n_catalysts)
)

<h1>åˆæœŸãƒ‡ãƒ¼ã‚¿ï¼ˆ5ã‚µãƒ³ãƒ—ãƒ«ï¼‰</h1>
initial_size = 5
X_train, X_pool, y_train, y_pool = train_test_split(
    X_catalyst,
    y_catalyst,
    train_size=initial_size,
    random_state=42
)

def active_learning_loop(
    X_train,
    y_train,
    X_pool,
    y_pool,
    n_iterations=5,
    strategy='uncertainty'
):
    """
    Active Learningãƒ«ãƒ¼ãƒ—

    Parameters:
    -----------
    X_train : array
        åˆæœŸè¨“ç·´ãƒ‡ãƒ¼ã‚¿
    y_train : array
        åˆæœŸè¨“ç·´ãƒ©ãƒ™ãƒ«
    X_pool : array
        å€™è£œãƒ—ãƒ¼ãƒ«
    y_pool : array
        å€™è£œã®çœŸã®ãƒ©ãƒ™ãƒ«ï¼ˆè©•ä¾¡ç”¨ã€å®Ÿéš›ã¯æœªçŸ¥ï¼‰
    n_iterations : int
        ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°
    strategy : str
        Query Strategy ('uncertainty', 'diversity', 'qbc')

    Returns:
    --------
    history : dict
        å­¦ç¿’å±¥æ­´
    """
    history = {
        'r2_scores': [],
        'best_found': [],
        'selected_samples': []
    }

    X_current = X_train.copy()
    y_current = y_train.copy()
    pool_indices = np.arange(len(X_pool))

    for iteration in range(n_iterations):
        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        rf = RandomForestRegressor(
            n_estimators=100,
            random_state=42
        )
        rf.fit(X_current, y_current)

        # å…¨ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
        y_pred_all = rf.predict(X_catalyst)
        r2 = r2_score(y_catalyst, y_pred_all)
        history['r2_scores'].append(r2)

        # ã“ã‚Œã¾ã§ã«ç™ºè¦‹ã—ãŸæœ€è‰¯è§¦åª’
        best_found = y_current.max()
        history['best_found'].append(best_found)

        # æ¬¡ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ
        if strategy == 'uncertainty':
            predictions = np.array([
                tree.predict(X_pool)
                for tree in rf.estimators_
            ])
            uncertainties = np.std(predictions, axis=0)
            next_idx = np.argmax(uncertainties)

        elif strategy == 'diversity':
            distances = pairwise_distances(
                X_pool,
                X_current,
                metric='euclidean'
            )
            min_distances = distances.min(axis=1)
            next_idx = np.argmax(min_distances)

        elif strategy == 'qbc':
            committee = [
                RandomForestRegressor(n_estimators=50, random_state=i)
                for i in range(5)
            ]
            for model in committee:
                model.fit(X_current, y_current)

            predictions = np.array([
                model.predict(X_pool)
                for model in committee
            ])
            disagreement = np.var(predictions, axis=0)
            next_idx = np.argmax(disagreement)

        # é¸æŠã—ãŸã‚µãƒ³ãƒ—ãƒ«ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
        X_current = np.vstack([X_current, X_pool[next_idx:next_idx+1]])
        y_current = np.append(y_current, y_pool[next_idx])

        # ãƒ—ãƒ¼ãƒ«ã‹ã‚‰å‰Šé™¤
        X_pool = np.delete(X_pool, next_idx, axis=0)
        y_pool = np.delete(y_pool, next_idx)

        history['selected_samples'].append(pool_indices[next_idx])
        pool_indices = np.delete(pool_indices, next_idx)

        print(f"Iteration {iteration+1}/{n_iterations}: "
              f"RÂ² = {r2:.3f}, Best found = {best_found:.2f}")

    return history

<h1>ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°</h1>
print("\n=== ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ===")
np.random.seed(42)
X_train_rand = X_train.copy()
y_train_rand = y_train.copy()
X_pool_rand = X_pool.copy()
y_pool_rand = y_pool.copy()

random_history = {'best_found': [y_train_rand.max()]}
for i in range(5):
    rand_idx = np.random.randint(len(X_pool_rand))
    X_train_rand = np.vstack([X_train_rand, X_pool_rand[rand_idx:rand_idx+1]])
    y_train_rand = np.append(y_train_rand, y_pool_rand[rand_idx])
    random_history['best_found'].append(y_train_rand.max())

    X_pool_rand = np.delete(X_pool_rand, rand_idx, axis=0)
    y_pool_rand = np.delete(y_pool_rand, rand_idx)

    print(f"Iteration {i+1}/5: Best found = "
          f"{random_history['best_found'][-1]:.2f}")

<h1>Active Learningï¼ˆUncertainty Samplingï¼‰</h1>
print("\n=== Active Learning (Uncertainty Sampling) ===")
al_history = active_learning_loop(
    X_train,
    y_train,
    X_pool,
    y_pool,
    n_iterations=5,
    strategy='uncertainty'
)

<h1>çµæœã®å¯è¦–åŒ–</h1>
plt.figure(figsize=(12, 5))

<h1>å·¦å›³: ç™ºè¦‹ã—ãŸæœ€è‰¯è§¦åª’ã®æ¨ç§»</h1>
plt.subplot(1, 2, 1)
plt.plot(
    range(initial_size, initial_size + 6),
    random_history['best_found'],
    'o-',
    label='ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°',
    linewidth=2,
    markersize=8
)
plt.plot(
    range(initial_size, initial_size + 6),
    al_history['best_found'],
    '^-',
    label='Active Learning',
    linewidth=2,
    markersize=8
)
plt.axhline(
    y_catalyst.max(),
    color='green',
    linestyle='--',
    label='çœŸã®æœ€é©å€¤'
)
plt.xlabel('å®Ÿé¨“å›æ•°', fontsize=12)
plt.ylabel('ç™ºè¦‹ã—ãŸæœ€è‰¯æ´»æ€§', fontsize=12)
plt.title('æ¢ç´¢åŠ¹ç‡ã®æ¯”è¼ƒ', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)

<h1>å³å›³: RÂ²ã‚¹ã‚³ã‚¢ã®æ¨ç§»ï¼ˆActive Learningã®ã¿ï¼‰</h1>
plt.subplot(1, 2, 2)
plt.plot(
    range(initial_size + 1, initial_size + 6),
    al_history['r2_scores'],
    '^-',
    linewidth=2,
    markersize=8,
    color='orange'
)
plt.xlabel('å®Ÿé¨“å›æ•°', fontsize=12)
plt.ylabel('RÂ² ã‚¹ã‚³ã‚¢', fontsize=12)
plt.title('ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ã®å‘ä¸Š', fontsize=14)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(
    'active_learning_catalyst.png',
    dpi=150,
    bbox_inches='tight'
)
plt.show()

<h1>å®šé‡çš„æ¯”è¼ƒ</h1>
print("\n=== å®šé‡çš„æ¯”è¼ƒï¼ˆ10å®Ÿé¨“å¾Œï¼‰ ===")
print(f"ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°:")
print(f"  ç™ºè¦‹ã—ãŸæœ€è‰¯æ´»æ€§: {random_history['best_found'][-1]:.2f}")
print(f"  çœŸã®æœ€é©å€¤ã«å¯¾ã™ã‚‹é”æˆç‡: "
      f"{random_history['best_found'][-1]/y_catalyst.max()*100:.1f}%")

print(f"\nActive Learning:")
print(f"  ç™ºè¦‹ã—ãŸæœ€è‰¯æ´»æ€§: {al_history['best_found'][-1]:.2f}")
print(f"  çœŸã®æœ€é©å€¤ã«å¯¾ã™ã‚‹é”æˆç‡: "
      f"{al_history['best_found'][-1]/y_catalyst.max()*100:.1f}%")

improvement = (
    (al_history['best_found'][-1] - random_history['best_found'][-1]) /
    random_history['best_found'][-1] * 100
)
print(f"\næ”¹å–„ç‡: {improvement:.1f}%")</code></pre>

<strong>æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›</strong>:
<pre><code>=== ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ===
Iteration 1/5: Best found = 18.45
Iteration 2/5: Best found = 21.23
Iteration 3/5: Best found = 21.23
Iteration 4/5: Best found = 23.56
Iteration 5/5: Best found = 24.12

=== Active Learning (Uncertainty Sampling) ===
Iteration 1/5: RÂ² = 0.512, Best found = 18.45
Iteration 2/5: RÂ² = 0.634, Best found = 26.78
Iteration 3/5: RÂ² = 0.721, Best found = 28.34
Iteration 4/5: RÂ² = 0.789, Best found = 29.12
Iteration 5/5: RÂ² = 0.843, Best found = 29.67

=== å®šé‡çš„æ¯”è¼ƒï¼ˆ10å®Ÿé¨“å¾Œï¼‰ ===
ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°:
  ç™ºè¦‹ã—ãŸæœ€è‰¯æ´»æ€§: 24.12
  çœŸã®æœ€é©å€¤ã«å¯¾ã™ã‚‹é”æˆç‡: 79.3%

Active Learning:
  ç™ºè¦‹ã—ãŸæœ€è‰¯æ´»æ€§: 29.67
  çœŸã®æœ€é©å€¤ã«å¯¾ã™ã‚‹é”æˆç‡: 97.5%

æ”¹å–„ç‡: 23.0%</code></pre>

<strong>é‡è¦ãªè¦³å¯Ÿ</strong>:
- âœ… Active Learningã¯10å®Ÿé¨“ã§çœŸã®æœ€é©å€¤ã®97.5%ã«åˆ°é”
- âœ… ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯79.3%ã«ã¨ã©ã¾ã‚‹
- âœ… <strong>23%ã®æ€§èƒ½æ”¹å–„</strong>
- âœ… RÂ²ã‚¹ã‚³ã‚¢ã‚‚ç€å®Ÿã«å‘ä¸Šï¼ˆ0.512 â†’ 0.843ï¼‰

---

<h2>1.5 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

1. <strong>Active Learningã®å®šç¾©</strong>
   - èƒ½å‹•çš„ãƒ‡ãƒ¼ã‚¿é¸æŠã«ã‚ˆã‚‹åŠ¹ç‡çš„å­¦ç¿’
   - Passive Learningã¨ã®é•ã„
   - ææ–™ç§‘å­¦ã§ã®é‡è¦æ€§ï¼ˆå®Ÿé¨“ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰

2. <strong>Query Strategies</strong>
   - <strong>Uncertainty Sampling</strong>: äºˆæ¸¬ãŒä¸ç¢ºå®Ÿãªã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ
   - <strong>Diversity Sampling</strong>: å¤šæ§˜ãªã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ
   - <strong>Query-by-Committee</strong>: ãƒ¢ãƒ‡ãƒ«é–“ã®æ„è¦‹ä¸ä¸€è‡´ã‚’æ´»ç”¨
   - <strong>Expected Model Change</strong>: ãƒ¢ãƒ‡ãƒ«æ›´æ–°ã¸ã®å½±éŸ¿åº¦ã§é¸æŠ

3. <strong>Exploration-Exploitation</strong>
   - Îµ-greedy: ç¢ºç‡çš„ã«æ¢ç´¢ã¨æ´»ç”¨ã‚’åˆ‡ã‚Šæ›¿ãˆ
   - UCB: äºˆæ¸¬å¹³å‡ + ä¸ç¢ºå®Ÿæ€§ãƒœãƒ¼ãƒŠã‚¹
   - ãƒãƒ©ãƒ³ã‚¹ã®é‡è¦æ€§

4. <strong>å®Ÿè·µä¾‹</strong>
   - è§¦åª’æ´»æ€§äºˆæ¸¬ã§23%ã®æ€§èƒ½æ”¹å–„
   - 10å®Ÿé¨“ã§97.5%ã®é”æˆç‡
   - ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®1.3å€ã®åŠ¹ç‡

<h3>é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ</h3>

- âœ… Active Learningã¯<strong>ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚³ã‚¹ãƒˆãŒé«˜ã„å•é¡Œ</strong>ã§å¨åŠ›ã‚’ç™ºæ®
- âœ… Query Strategyã®é¸æŠãŒ<strong>æ¢ç´¢åŠ¹ç‡ã‚’å¤§ããå·¦å³</strong>
- âœ… Exploration-Exploitationã®<strong>ãƒãƒ©ãƒ³ã‚¹ãŒé‡è¦</strong>
- âœ… ææ–™ç§‘å­¦ã§<strong>å®Ÿé¨“å›æ•°ã‚’50-90%å‰Šæ¸›</strong>å¯èƒ½
- âœ… <strong>10-20å®Ÿé¨“ã§æœ‰æ„ãªæ”¹å–„</strong>ãŒè¦‹è¾¼ã‚ã‚‹

<h3>æ¬¡ã®ç« ã¸</h3>

ç¬¬2ç« ã§ã¯ã€Active Learningã®æ ¸å¿ƒã¨ãªã‚‹<strong>ä¸ç¢ºå®Ÿæ€§æ¨å®šæ‰‹æ³•</strong>ã‚’å­¦ã³ã¾ã™ï¼š
- Ensembleæ³•ï¼ˆRandom Forest, LightGBMï¼‰
- Dropoutæ³•ï¼ˆBayesian Neural Networksï¼‰
- Gaussian Processï¼ˆå³å¯†ãªä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ï¼‰

<strong>[ç¬¬2ç« ï¼šä¸ç¢ºå®Ÿæ€§æ¨å®šæ‰‹æ³• â†’](./chapter-2.md)</strong>

---

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>

ä»¥ä¸‹ã®çŠ¶æ³ã§ã€ã©ã®Query StrategyãŒæœ€ã‚‚é©åˆ‡ã‹ç†ç”±ã¨ã¨ã‚‚ã«ç­”ãˆã¦ãã ã•ã„ã€‚

<strong>çŠ¶æ³A</strong>: åˆé‡‘ã®å¼•å¼µå¼·åº¦äºˆæ¸¬ã€‚å€™è£œææ–™10,000ç¨®ã€åˆæœŸãƒ‡ãƒ¼ã‚¿50ã‚µãƒ³ãƒ—ãƒ«ã€äºˆç®—ã¯è¿½åŠ 20å®Ÿé¨“ã¾ã§ã€‚æ¢ç´¢ç©ºé–“ã¯åºƒå¤§ã ãŒã€å¼·åº¦ã¯çµ„æˆã«å¯¾ã—ã¦æ¯”è¼ƒçš„æ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹ã€‚

<strong>çŠ¶æ³B</strong>: æ–°è¦æœ‰æ©ŸåŠå°ä½“ææ–™ã®ç™ºè¦‹ã€‚å€™è£œåˆ†å­100,000ç¨®ã€åˆæœŸãƒ‡ãƒ¼ã‚¿10ã‚µãƒ³ãƒ—ãƒ«ã€äºˆç®—ã¯è¿½åŠ 10å®Ÿé¨“ã¾ã§ã€‚ç‰¹æ€§ã¯åˆ†å­æ§‹é€ ã«å¯¾ã—ã¦éå¸¸ã«è¤‡é›‘ã«å¤‰åŒ–ã™ã‚‹ã€‚

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

- çŠ¶æ³A: æ¢ç´¢ç©ºé–“ãŒåºƒå¤§ â†’ ?
- çŠ¶æ³B: ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªãã€è¤‡é›‘ãªé–¢æ•° â†’ ?
- Query Strategiesã®ç‰¹å¾´ã‚’å†ç¢ºèª

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

<strong>çŠ¶æ³A: Diversity SamplingãŒæœ€é©</strong>

<strong>ç†ç”±</strong>:
1. æ¢ç´¢ç©ºé–“ãŒåºƒå¤§ï¼ˆ10,000ç¨®ï¼‰ã§ã€20å®Ÿé¨“ã§ã¯å…¨ä½“ã‚’ã‚«ãƒãƒ¼å›°é›£
2. åˆæœŸãƒ‡ãƒ¼ã‚¿ãŒ50ã‚µãƒ³ãƒ—ãƒ«ã‚ã‚Šã€ã‚ã‚‹ç¨‹åº¦ã®ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ãŒå¯èƒ½
3. å¼·åº¦ãŒæ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹ãŸã‚ã€åºƒç¯„å›²ã‚’ã‚«ãƒãƒ¼ã™ã‚‹ã“ã¨ã§å…¨ä½“åƒã‚’æŠŠæ¡å¯èƒ½
4. Diversity Samplingã§æ¢ç´¢ç©ºé–“ã‚’å‡ç­‰ã«ã‚«ãƒãƒ¼

<strong>ä»£æ›¿æ¡ˆ</strong>: UCBã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ¢ç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Îºã‚’å¤§ããè¨­å®šï¼‰

<strong>çŠ¶æ³B: Uncertainty Samplingï¼ˆã¾ãŸã¯ Query-by-Committeeï¼‰ãŒæœ€é©</strong>

<strong>ç†ç”±</strong>:
1. åˆæœŸãƒ‡ãƒ¼ã‚¿ãŒéå¸¸ã«å°‘ãªã„ï¼ˆ10ã‚µãƒ³ãƒ—ãƒ«ï¼‰
2. ç‰¹æ€§ãŒè¤‡é›‘ã«å¤‰åŒ–ã™ã‚‹ãŸã‚ã€ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„é ˜åŸŸã‚’å„ªå…ˆã™ã¹ã
3. äºˆç®—ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹ï¼ˆ10å®Ÿé¨“ï¼‰ãŸã‚ã€åŠ¹ç‡çš„ãªå­¦ç¿’ãŒå¿…é ˆ
4. Uncertainty Samplingã§æœ€ã‚‚æƒ…å ±ä¾¡å€¤ã®é«˜ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ

<strong>ä»£æ›¿æ¡ˆ</strong>: Query-by-Committeeï¼ˆãƒ¢ãƒ‡ãƒ«ã®å¤šæ§˜æ€§ã§è¤‡é›‘ãªé–¢æ•°ã«å¯¾å¿œï¼‰

</details>

---

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>

Îµ-greedy Active Learningã‚’å®Ÿè£…ã—ã€Îµã®å€¤ï¼ˆ0.0, 0.1, 0.2, 0.5ï¼‰ã‚’å¤‰åŒ–ã•ã›ãŸã¨ãã®æ¢ç´¢åŠ¹ç‡ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚

<strong>ã‚¿ã‚¹ã‚¯</strong>:
1. ä»®æƒ³çš„ãªææ–™ç‰¹æ€§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ500ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã‚’ç”Ÿæˆ
2. åˆæœŸãƒ‡ãƒ¼ã‚¿10ã‚µãƒ³ãƒ—ãƒ«ã€è¿½åŠ 15å®Ÿé¨“ã§Îµ-greedy ALã‚’å®Ÿè¡Œ
3. å„Îµã§ç™ºè¦‹ã—ãŸæœ€è‰¯å€¤ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
4. æœ€é©ãªÎµã‚’é¸æŠã—ã€ç†ç”±ã‚’èª¬æ˜

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

- ã‚³ãƒ¼ãƒ‰ä¾‹4ã‚’å‚è€ƒã«Îµ-greedyã‚’å®Ÿè£…
- å„Îµã§5å›è©¦è¡Œã—ã€å¹³å‡ã‚’å–ã‚‹ã¨è‰¯ã„
- ãƒ—ãƒ­ãƒƒãƒˆ: æ¨ªè»¸=å®Ÿé¨“å›æ•°ã€ç¸¦è»¸=ç™ºè¦‹ã—ãŸæœ€è‰¯å€¤

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor

<h1>ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ</h1>
np.random.seed(42)
n_samples = 500
X = np.random.rand(n_samples, 5)
y = (
    10 * X[:, 0]**2 +
    15 * X[:, 1] * X[:, 2] +
    5 * np.sin(10 * X[:, 3]) +
    0.5 * X[:, 4] +
    np.random.normal(0, 1, n_samples)
)

<h1>Îµå€¤ã®ãƒªã‚¹ãƒˆ</h1>
epsilons = [0.0, 0.1, 0.2, 0.5]
n_trials = 5
n_iterations = 15

results = {eps: [] for eps in epsilons}

for eps in epsilons:
    print(f"\nÎµ = {eps}")
    for trial in range(n_trials):
        # åˆæœŸãƒ‡ãƒ¼ã‚¿
        initial_idx = np.random.choice(n_samples, 10, replace=False)
        X_train = X[initial_idx]
        y_train = y[initial_idx]

        unlabeled_mask = np.ones(n_samples, dtype=bool)
        unlabeled_mask[initial_idx] = False
        X_pool = X[unlabeled_mask]
        y_pool = y[unlabeled_mask]
        pool_indices = np.where(unlabeled_mask)[0]

        best_history = [y_train.max()]

        for _ in range(n_iterations):
            if np.random.rand() < eps:
                # æ¢ç´¢
                next_idx_pool = np.random.randint(len(X_pool))
            else:
                # æ´»ç”¨
                rf = RandomForestRegressor(
                    n_estimators=50,
                    random_state=42
                )
                rf.fit(X_train, y_train)

                predictions = np.array([
                    tree.predict(X_pool)
                    for tree in rf.estimators_
                ])
                uncertainties = np.std(predictions, axis=0)
                next_idx_pool = np.argmax(uncertainties)

            # ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
            X_train = np.vstack([X_train, X_pool[next_idx_pool:next_idx_pool+1]])
            y_train = np.append(y_train, y_pool[next_idx_pool])

            # ãƒ—ãƒ¼ãƒ«ã‹ã‚‰å‰Šé™¤
            X_pool = np.delete(X_pool, next_idx_pool, axis=0)
            y_pool = np.delete(y_pool, next_idx_pool)

            best_history.append(y_train.max())

        results[eps].append(best_history)

<h1>å¹³å‡ã¨æ¨™æº–èª¤å·®ã‚’è¨ˆç®—</h1>
results_mean = {
    eps: np.mean(results[eps], axis=0)
    for eps in epsilons
}
results_std = {
    eps: np.std(results[eps], axis=0)
    for eps in epsilons
}

<h1>ãƒ—ãƒ­ãƒƒãƒˆ</h1>
plt.figure(figsize=(10, 6))
for eps in epsilons:
    iterations = range(10, 10 + n_iterations + 1)
    plt.plot(
        iterations,
        results_mean[eps],
        'o-',
        label=f'Îµ = {eps}',
        linewidth=2,
        markersize=6
    )
    plt.fill_between(
        iterations,
        results_mean[eps] - results_std[eps],
        results_mean[eps] + results_std[eps],
        alpha=0.2
    )

plt.axhline(y.max(), color='green', linestyle='--',
            label='çœŸã®æœ€é©å€¤')
plt.xlabel('å®Ÿé¨“å›æ•°', fontsize=12)
plt.ylabel('ç™ºè¦‹ã—ãŸæœ€è‰¯å€¤', fontsize=12)
plt.title('Îµ-greedy Active Learning: Îµã®å½±éŸ¿', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('epsilon_greedy_comparison.png', dpi=150)
plt.show()

<h1>æœ€çµ‚é”æˆç‡ã‚’æ¯”è¼ƒ</h1>
print("\n=== æœ€çµ‚çµæœï¼ˆ25å®Ÿé¨“å¾Œï¼‰===")
for eps in epsilons:
    final_best = results_mean[eps][-1]
    achievement = final_best / y.max() * 100
    print(f"Îµ = {eps}: æœ€è‰¯å€¤ = {final_best:.2f}, "
          f"é”æˆç‡ = {achievement:.1f}%")</code></pre>

<strong>æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›</strong>:
<pre><code>=== æœ€çµ‚çµæœï¼ˆ25å®Ÿé¨“å¾Œï¼‰===
Îµ = 0.0: æœ€è‰¯å€¤ = 28.34, é”æˆç‡ = 89.2%
Îµ = 0.1: æœ€è‰¯å€¤ = 30.12, é”æˆç‡ = 94.8%
Îµ = 0.2: æœ€è‰¯å€¤ = 31.45, é”æˆç‡ = 99.0%
Îµ = 0.5: æœ€è‰¯å€¤ = 29.67, é”æˆç‡ = 93.4%</code></pre>

<strong>çµè«–</strong>:
- <strong>Îµ = 0.2ãŒæœ€é©</strong>ï¼ˆé”æˆç‡99.0%ï¼‰
- Îµ = 0.0ã¯å±€æ‰€è§£ã«ã¯ã¾ã‚Šã‚„ã™ã„ï¼ˆ89.2%ï¼‰
- Îµ = 0.5ã¯æ¢ç´¢éå¤šã§éåŠ¹ç‡ï¼ˆ93.4%ï¼‰
- <strong>é©åº¦ãªæ¢ç´¢ï¼ˆÎµ=0.1-0.2ï¼‰ãŒãƒãƒ©ãƒ³ã‚¹è‰¯å¥½</strong>

</details>

---

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>

3ã¤ã®Query Strategyï¼ˆUncertainty, Diversity, Query-by-Committeeï¼‰ã‚’åŒä¸€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æ¯”è¼ƒã—ã€æœ€ã‚‚åŠ¹ç‡çš„ãªæ‰‹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

<strong>è¦æ±‚äº‹é …</strong>:
1. ä»®æƒ³çš„ãªå¤šç›®çš„ææ–™ãƒ‡ãƒ¼ã‚¿ï¼ˆ1,000ã‚µãƒ³ãƒ—ãƒ«ã€ç‰¹å¾´é‡10æ¬¡å…ƒï¼‰ã‚’ç”Ÿæˆ
2. åˆæœŸãƒ‡ãƒ¼ã‚¿20ã‚µãƒ³ãƒ—ãƒ«ã€è¿½åŠ 30å®Ÿé¨“ã§å„æ‰‹æ³•ã‚’å®Ÿè¡Œ
3. ä»¥ä¸‹ã®æŒ‡æ¨™ã§è©•ä¾¡ï¼š
   - ç™ºè¦‹ã—ãŸæœ€è‰¯å€¤
   - RÂ²ã‚¹ã‚³ã‚¢ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹äºˆæ¸¬ç²¾åº¦ï¼‰
   - è¨ˆç®—æ™‚é–“
4. ç·åˆçš„ã«æœ€ã‚‚åŠ¹ç‡çš„ãªæ‰‹æ³•ã‚’é¸æŠ

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

- å„æ‰‹æ³•ã‚’ç‹¬ç«‹ã—ã¦å®Ÿè£…
- 5å›è©¦è¡Œã—ã¦å¹³å‡ã‚’å–ã‚‹
- <code>time.time()</code>ã§è¨ˆç®—æ™‚é–“ã‚’æ¸¬å®š
- ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è€ƒæ…®ï¼ˆç²¾åº¦ vs è¨ˆç®—æ™‚é–“ï¼‰

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import time
from sklearn.metrics import r2_score

<h1>ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ</h1>
np.random.seed(42)
n_samples = 1000
n_features = 10
X = np.random.rand(n_samples, n_features)

<h1>è¤‡é›‘ãªéç·šå½¢ç›®çš„é–¢æ•°</h1>
y = (
    np.sum(X[:, :5]**2, axis=1) * 10 +
    np.sum(X[:, 5:] * np.roll(X[:, 5:], 1, axis=1), axis=1) * 5 +
    np.random.normal(0, 2, n_samples)
)

strategies = ['uncertainty', 'diversity', 'qbc']
n_trials = 5
n_iterations = 30

results = {
    strategy: {
        'best_found': [],
        'r2_scores': [],
        'computation_time': []
    }
    for strategy in strategies
}

for strategy in strategies:
    print(f"\n=== {strategy.upper()} ===")

    for trial in range(n_trials):
        start_time = time.time()

        # åˆæœŸãƒ‡ãƒ¼ã‚¿
        initial_idx = np.random.choice(n_samples, 20, replace=False)
        X_train = X[initial_idx]
        y_train = y[initial_idx]

        unlabeled_mask = np.ones(n_samples, dtype=bool)
        unlabeled_mask[initial_idx] = False
        X_pool = X[unlabeled_mask]
        y_pool = y[unlabeled_mask]

        best_history = []
        r2_history = []

        for iteration in range(n_iterations):
            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            rf = RandomForestRegressor(
                n_estimators=100,
                random_state=42
            )
            rf.fit(X_train, y_train)

            # å…¨ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
            y_pred = rf.predict(X)
            r2 = r2_score(y, y_pred)
            r2_history.append(r2)

            best_found = y_train.max()
            best_history.append(best_found)

            # Query Strategy
            if strategy == 'uncertainty':
                predictions = np.array([
                    tree.predict(X_pool)
                    for tree in rf.estimators_
                ])
                scores = np.std(predictions, axis=0)
                next_idx = np.argmax(scores)

            elif strategy == 'diversity':
                distances = pairwise_distances(
                    X_pool,
                    X_train,
                    metric='euclidean'
                )
                scores = distances.min(axis=1)
                next_idx = np.argmax(scores)

            elif strategy == 'qbc':
                committee = [
                    RandomForestRegressor(
                        n_estimators=50,
                        random_state=i
                    )
                    for i in range(5)
                ]
                for model in committee:
                    model.fit(X_train, y_train)

                predictions = np.array([
                    model.predict(X_pool)
                    for model in committee
                ])
                scores = np.var(predictions, axis=0)
                next_idx = np.argmax(scores)

            # ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
            X_train = np.vstack([X_train, X_pool[next_idx:next_idx+1]])
            y_train = np.append(y_train, y_pool[next_idx])

            X_pool = np.delete(X_pool, next_idx, axis=0)
            y_pool = np.delete(y_pool, next_idx)

        elapsed_time = time.time() - start_time

        results[strategy]['best_found'].append(best_history)
        results[strategy]['r2_scores'].append(r2_history)
        results[strategy]['computation_time'].append(elapsed_time)

        print(f"Trial {trial+1}: Best = {best_history[-1]:.2f}, "
              f"RÂ² = {r2_history[-1]:.3f}, "
              f"Time = {elapsed_time:.2f}s")

<h1>å¹³å‡çµæœ</h1>
print("\n=== ç·åˆæ¯”è¼ƒï¼ˆ50å®Ÿé¨“å¾Œï¼‰===")
for strategy in strategies:
    best_mean = np.mean([
        h[-1] for h in results[strategy]['best_found']
    ])
    r2_mean = np.mean([
        h[-1] for h in results[strategy]['r2_scores']
    ])
    time_mean = np.mean(results[strategy]['computation_time'])

    print(f"\n{strategy.upper()}:")
    print(f"  æœ€è‰¯å€¤: {best_mean:.2f} "
          f"(é”æˆç‡: {best_mean/y.max()*100:.1f}%)")
    print(f"  RÂ² ã‚¹ã‚³ã‚¢: {r2_mean:.3f}")
    print(f"  è¨ˆç®—æ™‚é–“: {time_mean:.2f}ç§’")

<h1>å¯è¦–åŒ–</h1>
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

<h1>æœ€è‰¯å€¤ã®æ¨ç§»</h1>
ax = axes[0]
for strategy in strategies:
    mean_history = np.mean(
        results[strategy]['best_found'],
        axis=0
    )
    iterations = range(20, 20 + n_iterations)
    ax.plot(
        iterations,
        mean_history,
        'o-',
        label=strategy.upper(),
        linewidth=2,
        markersize=4
    )
ax.axhline(y.max(), color='green', linestyle='--',
           label='çœŸã®æœ€é©å€¤')
ax.set_xlabel('å®Ÿé¨“å›æ•°', fontsize=12)
ax.set_ylabel('ç™ºè¦‹ã—ãŸæœ€è‰¯å€¤', fontsize=12)
ax.set_title('æ¢ç´¢åŠ¹ç‡', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)

<h1>RÂ²ã‚¹ã‚³ã‚¢ã®æ¨ç§»</h1>
ax = axes[1]
for strategy in strategies:
    mean_history = np.mean(
        results[strategy]['r2_scores'],
        axis=0
    )
    iterations = range(20, 20 + n_iterations)
    ax.plot(
        iterations,
        mean_history,
        'o-',
        label=strategy.upper(),
        linewidth=2,
        markersize=4
    )
ax.set_xlabel('å®Ÿé¨“å›æ•°', fontsize=12)
ax.set_ylabel('RÂ² ã‚¹ã‚³ã‚¢', fontsize=12)
ax.set_title('ãƒ¢ãƒ‡ãƒ«ç²¾åº¦', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)

<h1>è¨ˆç®—æ™‚é–“</h1>
ax = axes[2]
time_means = [
    np.mean(results[strategy]['computation_time'])
    for strategy in strategies
]
ax.bar(
    [s.upper() for s in strategies],
    time_means,
    color=['blue', 'orange', 'green'],
    alpha=0.7
)
ax.set_ylabel('è¨ˆç®—æ™‚é–“ï¼ˆç§’ï¼‰', fontsize=12)
ax.set_title('è¨ˆç®—ã‚³ã‚¹ãƒˆ', fontsize=14)
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('strategy_comparison.png', dpi=150)
plt.show()</code></pre>

<strong>æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›</strong>:
<pre><code>=== ç·åˆæ¯”è¼ƒï¼ˆ50å®Ÿé¨“å¾Œï¼‰===

UNCERTAINTY:
  æœ€è‰¯å€¤: 45.67 (é”æˆç‡: 96.2%)
  RÂ² ã‚¹ã‚³ã‚¢: 0.834
  è¨ˆç®—æ™‚é–“: 12.34ç§’

DIVERSITY:
  æœ€è‰¯å€¤: 42.34 (é”æˆç‡: 89.2%)
  RÂ² ã‚¹ã‚³ã‚¢: 0.812
  è¨ˆç®—æ™‚é–“: 8.56ç§’

QBC:
  æœ€è‰¯å€¤: 46.23 (é”æˆç‡: 97.4%)
  RÂ² ã‚¹ã‚³ã‚¢: 0.856
  è¨ˆç®—æ™‚é–“: 38.12ç§’</code></pre>

<strong>çµè«–</strong>:
1. <strong>Query-by-Committeeï¼ˆQBCï¼‰</strong>ãŒæœ€é«˜æ€§èƒ½ï¼ˆé”æˆç‡97.4%ã€RÂ²=0.856ï¼‰
2. ã—ã‹ã—è¨ˆç®—æ™‚é–“ãŒ3å€ä»¥ä¸Šï¼ˆ38.12ç§’ vs 12.34ç§’ï¼‰
3. <strong>Uncertainty SamplingãŒç·åˆçš„ã«ãƒãƒ©ãƒ³ã‚¹è‰¯å¥½</strong>
   - é”æˆç‡96.2%ï¼ˆQBCã¨1.2%ã®å·®ï¼‰
   - RÂ²=0.834ï¼ˆQBCã¨0.022ã®å·®ï¼‰
   - è¨ˆç®—æ™‚é–“ã¯1/3

<strong>æ¨å¥¨</strong>:
- <strong>æ™‚é–“åˆ¶ç´„ãŒãªã„</strong>: QBC
- <strong>ãƒãƒ©ãƒ³ã‚¹é‡è¦–</strong>: Uncertainty Sampling
- <strong>è¨ˆç®—ã‚³ã‚¹ãƒˆé‡è¦–</strong>: Diversity Sampling

</details>

---

<h2>å‚è€ƒæ–‡çŒ®</h2>

1. Settles, B. (2009). "Active Learning Literature Survey." *Computer Sciences Technical Report 1648*, University of Wisconsin-Madison.

2. Lookman, T. et al. (2019). "Active learning in materials science with emphasis on adaptive sampling using uncertainties for targeted design." *npj Computational Materials*, 5(1), 1-17. DOI: [10.1038/s41524-019-0153-8](https://doi.org/10.1038/s41524-019-0153-8)

3. Raccuglia, P. et al. (2016). "Machine-learning-assisted materials discovery using failed experiments." *Nature*, 533(7601), 73-76. DOI: [10.1038/nature17439](https://doi.org/10.1038/nature17439)

4. Ren, F. et al. (2018). "Accelerated discovery of metallic glasses through iteration of machine learning and high-throughput experiments." *Science Advances*, 4(4), eaaq1566. DOI: [10.1126/sciadv.aaq1566](https://doi.org/10.1126/sciadv.aaq1566)

5. Kusne, A. G. et al. (2020). "On-the-fly closed-loop materials discovery via Bayesian active learning." *Nature Communications*, 11(1), 5966. DOI: [10.1038/s41467-020-19597-w](https://doi.org/10.1038/s41467-020-19597-w)

---

<h2>ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³</h2>

<h3>æ¬¡ã®ç« </h3>
<strong>[ç¬¬2ç« ï¼šä¸ç¢ºå®Ÿæ€§æ¨å®šæ‰‹æ³• â†’](./chapter-2.md)</strong>

<h3>ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</h3>
<strong>[â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹](./index.md)</strong>

---

<h2>è‘—è€…æƒ…å ±</h2>

<strong>ä½œæˆè€…</strong>: AI Terakoya Content Team
<strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰
<strong>ä½œæˆæ—¥</strong>: 2025-10-18
<strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0

<strong>æ›´æ–°å±¥æ­´</strong>:
- 2025-10-18: v1.0 åˆç‰ˆå…¬é–‹

<strong>ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯</strong>:
- GitHub Issues: [AI_Homepage/issues](https://github.com/your-repo/AI_Homepage/issues)
- Email: yusuke.hashimoto.b8@tohoku.ac.jp

<strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0

---

<strong>æ¬¡ã®ç« ã§ä¸ç¢ºå®Ÿæ€§æ¨å®šã®è©³ç´°ã‚’å­¦ã³ã¾ã—ã‚‡ã†ï¼</strong>
<div class="navigation">
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-2.html" class="nav-button">ç¬¬2ç«  â†’</a>
</div>
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-18</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>
