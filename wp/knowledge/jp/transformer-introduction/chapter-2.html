<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Chapter</h1>
            <p class="subtitle"></p>
            <div class="meta">
                <span class="meta-item">üìñ Ë™≠‰∫ÜÊôÇÈñì: 20-25ÂàÜ</span>
                <span class="meta-item">üìä Èõ£ÊòìÂ∫¶: ÂàùÁ¥ö</span>
                <span class="meta-item">üíª „Ç≥„Éº„Éâ‰æã: 0ÂÄã</span>
                <span class="meta-item">üìù ÊºîÁøíÂïèÈ°å: 0Âïè</span>
            </div>
        </div>
    </header>

    <main class="container">
<h1>Á¨¨2Á´†: ÊùêÊñôÂêë„ÅëTransformer„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£</h1>

<strong>Â≠¶ÁøíÊôÇÈñì</strong>: 30-35ÂàÜ | <strong>Èõ£ÊòìÂ∫¶</strong>: ‰∏≠Á¥ö„Äú‰∏äÁ¥ö

<h2>üìã „Åì„ÅÆÁ´†„ÅßÂ≠¶„Å∂„Åì„Å®</h2>

- ÊùêÊñôÁßëÂ≠¶„Å´ÁâπÂåñ„Åó„ÅüTransformer„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆË®≠Ë®àÂéüÁêÜ
- Matformer: Materials Transformer for Property Prediction
- CrystalFormer: Crystal Structure Representation
- ChemBERTa: ÂàÜÂ≠êSMILESË°®ÁèæÂ≠¶Áøí
- Perceiver IO: Â§öÊßò„Å™„Éá„Éº„ÇøÁµ±Âêà
- ÂÆüË£ÖÊºîÁøí: Matformer„ÅßÊùêÊñôÁâπÊÄß‰∫àÊ∏¨

---

<h2>2.1 ÊùêÊñôÁßëÂ≠¶ÁâπÂåñTransformer„ÅÆÂøÖË¶ÅÊÄß</h2>

<h3>Ê±éÁî®Transformer„ÅÆÈôêÁïå</h3>

<strong>Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜÁî®Transformer„Çí„Åù„ÅÆ„Åæ„Åæ‰Ωø„ÅÜÂïèÈ°å</strong>:
- ‚ùå ÂàÜÂ≠ê„ÉªÊùêÊñô„ÅÆ3DÊßãÈÄ†ÊÉÖÂ†±„ÅåÂ§±„Çè„Çå„Çã
- ‚ùå ÂåñÂ≠¶ÁµêÂêà„ÇÑÂéüÂ≠êÈñìË∑ùÈõ¢„ÇíËÄÉÊÖÆ„Åß„Åç„Å™„ÅÑ
- ‚ùå Âë®ÊúüÁöÑÂ¢ÉÁïåÊù°‰ª∂ÔºàÁµêÊô∂Ôºâ„ÇíÊâ±„Åà„Å™„ÅÑ
- ‚ùå Áâ©ÁêÜÁöÑÂà∂Á¥ÑÔºà‰øùÂ≠òÂâá„ÄÅÂØæÁß∞ÊÄßÔºâ„ÇíÁÑ°Ë¶ñ

<h3>ÊùêÊñôÁâπÂåñTransformer„ÅÆÁâπÂæ¥</h3>

<strong>ÂøÖË¶Å„Å™Êã°Âºµ</strong>:
- ‚úÖ <strong>3DÊßãÈÄ†„ÅÆÂüã„ÇÅËæº„Åø</strong>: ÂéüÂ≠êÂ∫ßÊ®ô„ÄÅË∑ùÈõ¢„ÄÅËßíÂ∫¶
- ‚úÖ <strong>Âë®ÊúüÁöÑÂ¢ÉÁïåÊù°‰ª∂</strong>: ÁµêÊô∂Ê†ºÂ≠ê„ÅÆÁπ∞„ÇäËøî„Åó
- ‚úÖ <strong>Áâ©ÁêÜÁöÑÂà∂Á¥Ñ</strong>: ÂØæÁß∞ÊÄß„ÄÅÁ≠âÂ§âÊÄß
- ‚úÖ <strong>Â§öÊßò„Å™„Éá„Éº„ÇøÁµ±Âêà</strong>: ÊßãÈÄ† + ÁµÑÊàê + ÂÆüÈ®ì„Éá„Éº„Çø

<div class="mermaid">graph TD
    A[Ê±éÁî®Transformer] --> B[ÊùêÊñôÁâπÂåñTransformer]
    B --> C[3DÊßãÈÄ†Âüã„ÇÅËæº„Åø]
    B --> D[Âë®ÊúüÂ¢ÉÁïåÊù°‰ª∂]
    B --> E[Áâ©ÁêÜÂà∂Á¥Ñ]
    B --> F[Â§öÊßò„Éá„Éº„ÇøÁµ±Âêà]

    C --> G[Matformer]
    D --> G
    E --> H[CrystalFormer]
    F --> I[Perceiver IO]

    style G fill:#e1f5ff
    style H fill:#ffe1f5
    style I fill:#f5ffe1</div>

---

<h2>2.2 Matformer: Materials Transformer</h2>

<h3>Ê¶ÇË¶Å</h3>

<strong>Matformer</strong> (Chen et al., 2022)„ÅØ„ÄÅÊùêÊñô„ÅÆÁµêÊô∂ÊßãÈÄ†„Åã„ÇâÁâπÊÄß„Çí‰∫àÊ∏¨„Åô„ÇãTransformer„É¢„Éá„É´„Åß„Åô„ÄÇ

<strong>ÁâπÂæ¥</strong>:
- <strong>Nested Transformer</strong>: ÂéüÂ≠ê„É¨„Éô„É´„Å®„ÇØ„É™„Çπ„Çø„É´„É¨„Éô„É´„ÅÆÈöéÂ±§ÁöÑÂá¶ÁêÜ
- <strong>Distance-aware Attention</strong>: ÂéüÂ≠êÈñìË∑ùÈõ¢„ÇíËÄÉÊÖÆ
- <strong>Elastic Inference</strong>: Ë®àÁÆóÈáè„Å®„É°„É¢„É™„ÇíÂãïÁöÑ„Å´Ë™øÊï¥

<h3>„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£</h3>

<div class="mermaid">graph TB
    subgraph Input
        A1[ÂéüÂ≠êÂ∫ßÊ®ô] --> B[ÂéüÂ≠êÂüã„ÇÅËæº„Åø]
        A2[ÂéüÂ≠êÁï™Âè∑] --> B
        A3[Ê†ºÂ≠êÂÆöÊï∞] --> B
    end

    B --> C[Positional Encoding]
    C --> D[Distance Matrix]

    subgraph "Nested Transformer"
        D --> E1[Atom-level Attention]
        E1 --> E2[Structure-level Attention]
    end

    E2 --> F[Pooling]
    F --> G[Prediction Head]
    G --> H[„Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„Éó/ÂΩ¢Êàê„Ç®„Éç„É´„ÇÆ„Éº]

    style E1 fill:#e1f5ff
    style E2 fill:#ffe1e1</div>

<h3>ÂéüÂ≠êÂüã„ÇÅËæº„ÅøÔºàAtom EmbeddingÔºâ</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
import numpy as np

class AtomEmbedding(nn.Module):
    def __init__(self, num_atoms=118, d_model=256):
        """
        ÂéüÂ≠êÂüã„ÇÅËæº„ÅøÂ±§

        Args:
            num_atoms: ÂéüÂ≠ê„ÅÆÁ®ÆÈ°ûÊï∞ÔºàÂë®ÊúüË°®„ÄÅ118ÂÖÉÁ¥†Ôºâ
            d_model: Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉ
        """
        super(AtomEmbedding, self).__init__()
        self.embedding = nn.Embedding(num_atoms, d_model)

    def forward(self, atomic_numbers):
        """
        Args:
            atomic_numbers: (batch_size, num_atoms) ÂéüÂ≠êÁï™Âè∑
        Returns:
            embeddings: (batch_size, num_atoms, d_model)
        """
        return self.embedding(atomic_numbers)

<h1>‰ΩøÁî®‰æã: NaClÁµêÊô∂</h1>
batch_size = 2
num_atoms = 8  # Âçò‰ΩçÊ†ºÂ≠êÂÜÖ„ÅÆÂéüÂ≠êÊï∞

<h1>ÂéüÂ≠êÁï™Âè∑: Na(11), Cl(17)</h1>
atomic_numbers = torch.tensor([
    [11, 17, 11, 17, 11, 17, 11, 17],  # „Çµ„É≥„Éó„É´1
    [11, 17, 11, 17, 11, 17, 11, 17]   # „Çµ„É≥„Éó„É´2
])

atom_emb = AtomEmbedding(num_atoms=118, d_model=256)
embeddings = atom_emb(atomic_numbers)
print(f"Atom embeddings shape: {embeddings.shape}")  # (2, 8, 256)</code></pre>

<h3>Distance-aware Attention</h3>

<strong>ÂéüÂ≠êÈñìË∑ùÈõ¢„ÇíËÄÉÊÖÆ„Åó„ÅüAttention</strong>:

<pre><code class="language-python">class DistanceAwareAttention(nn.Module):
    def __init__(self, d_model, num_heads, max_distance=10.0):
        """
        Ë∑ùÈõ¢„ÇíËÄÉÊÖÆ„Åó„ÅüAttention

        Args:
            d_model: „É¢„Éá„É´Ê¨°ÂÖÉ
            num_heads: Attention„Éò„ÉÉ„ÉâÊï∞
            max_distance: ÊúÄÂ§ßË∑ùÈõ¢Ôºà√ÖÔºâ
        """
        super(DistanceAwareAttention, self).__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.max_distance = max_distance

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

        # Ë∑ùÈõ¢Âüã„ÇÅËæº„Åø
        self.distance_embedding = nn.Linear(1, num_heads)

    def forward(self, x, distance_matrix):
        """
        Args:
            x: (batch_size, num_atoms, d_model)
            distance_matrix: (batch_size, num_atoms, num_atoms) ÂéüÂ≠êÈñìË∑ùÈõ¢Ôºà√ÖÔºâ
        """
        batch_size = x.size(0)

        # Q, K, V
        Q = self.W_q(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        # Attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)

        # Ë∑ùÈõ¢„Éê„Ç§„Ç¢„Çπ
        # Ë∑ùÈõ¢„ÅåËøë„ÅÑ„Åª„Å©Â§ß„Åç„Å™ÂÄ§„ÄÅÈÅ†„ÅÑ„Åª„Å©Â∞è„Åï„Å™ÂÄ§
        distance_bias = self.distance_embedding(distance_matrix.unsqueeze(-1))  # (batch, num_atoms, num_atoms, num_heads)
        distance_bias = distance_bias.permute(0, 3, 1, 2)  # (batch, num_heads, num_atoms, num_atoms)

        # „Ç¨„Ç¶„ÇπÈñ¢Êï∞„ÅßË∑ùÈõ¢„ÇíÂ§âÊèõÔºàËøë„ÅÑÂéüÂ≠ê„Åª„Å©È´ò„ÅÑ„Çπ„Ç≥„Ç¢Ôºâ
        distance_factor = torch.exp(-distance_matrix.unsqueeze(1) / 2.0)  # (batch, 1, num_atoms, num_atoms)

        scores = scores + distance_bias * distance_factor

        # Softmax
        attention_weights = torch.softmax(scores, dim=-1)

        # Attention„ÅÆÈÅ©Áî®
        output = torch.matmul(attention_weights, V)
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        output = self.W_o(output)

        return output, attention_weights

<h1>‰ΩøÁî®‰æã</h1>
d_model = 256
num_heads = 8
num_atoms = 8

dist_attn = DistanceAwareAttention(d_model, num_heads)

x = torch.randn(2, num_atoms, d_model)
<h1>NaClÁµêÊô∂„ÅÆÂéüÂ≠êÈñìË∑ùÈõ¢ÔºàÁ∞°Áï•ÁâàÔºâ</h1>
distance_matrix = torch.tensor([
    [[0.0, 2.8, 3.9, 4.8, 3.9, 5.5, 4.8, 6.7],  # ÂéüÂ≠ê1„Åã„Çâ„ÅÆË∑ùÈõ¢
     [2.8, 0.0, 2.8, 3.9, 5.5, 3.9, 6.7, 4.8],
     # ... ÁúÅÁï•
     [6.7, 4.8, 5.5, 3.9, 4.8, 3.9, 2.8, 0.0]]
]).repeat(2, 1, 1)  # batch_sizeÂàÜË§áË£Ω

output, attn_weights = dist_attn(x, distance_matrix)
print(f"Output shape: {output.shape}")  # (2, 8, 256)</code></pre>

<h3>Matformer„Éñ„É≠„ÉÉ„ÇØ</h3>

<pre><code class="language-python">class MatformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff=1024, dropout=0.1):
        """
        Matformer„ÅÆÂü∫Êú¨„Éñ„É≠„ÉÉ„ÇØ

        Args:
            d_model: „É¢„Éá„É´Ê¨°ÂÖÉ
            num_heads: Attention„Éò„ÉÉ„ÉâÊï∞
            d_ff: Feed-ForwardÂ±§„ÅÆ‰∏≠ÈñìÊ¨°ÂÖÉ
            dropout: „Éâ„É≠„ÉÉ„Éó„Ç¢„Ç¶„ÉàÁéá
        """
        super(MatformerBlock, self).__init__()

        self.distance_attention = DistanceAwareAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)

        # Feed-Forward Network
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, distance_matrix):
        # Distance-aware Attention + Residual
        attn_output, _ = self.distance_attention(x, distance_matrix)
        x = self.norm1(x + self.dropout1(attn_output))

        # Feed-Forward + Residual
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout2(ffn_output))

        return x</code></pre>

---

<h2>2.3 CrystalFormer: ÁµêÊô∂ÊßãÈÄ†Transformer</h2>

<h3>Ê¶ÇË¶Å</h3>

<strong>CrystalFormer</strong>„ÅØ„ÄÅÁµêÊô∂„ÅÆÂë®ÊúüÁöÑÂ¢ÉÁïåÊù°‰ª∂„ÇíËÄÉÊÖÆ„Åó„ÅüTransformer„Åß„Åô„ÄÇ

<strong>ÁâπÂæ¥</strong>:
- <strong>Wyckoff‰ΩçÁΩÆÂüã„ÇÅËæº„Åø</strong>: ÁµêÊô∂„ÅÆÂØæÁß∞ÊÄß„ÇíËÄÉÊÖÆ
- <strong>Fractional Coordinates</strong>: ÂàÜÊï∞Â∫ßÊ®ô„Åß„ÅÆË°®Áèæ
- <strong>Space Group Encoding</strong>: Á©∫ÈñìÁæ§ÊÉÖÂ†±„ÅÆÂüã„ÇÅËæº„Åø

<h3>ÂàÜÊï∞Â∫ßÊ®ôÂüã„ÇÅËæº„Åø</h3>

<pre><code class="language-python">class FractionalCoordinateEncoding(nn.Module):
    def __init__(self, d_model):
        super(FractionalCoordinateEncoding, self).__init__()
        self.coord_linear = nn.Linear(3, d_model)

    def forward(self, fractional_coords):
        """
        Args:
            fractional_coords: (batch_size, num_atoms, 3) ÂàÜÊï∞Â∫ßÊ®ô [0, 1)
        Returns:
            encoding: (batch_size, num_atoms, d_model)
        """
        # ‰∏âËßíÈñ¢Êï∞Âüã„ÇÅËæº„Åø
        freqs = torch.arange(1, d_model // 6 + 1, dtype=torch.float32)
        coords_expanded = fractional_coords.unsqueeze(-1) * freqs

        encoding = torch.cat([
            torch.sin(2 * np.pi * coords_expanded),
            torch.cos(2 * np.pi * coords_expanded)
        ], dim=-1)

        # Á∑öÂΩ¢Â§âÊèõ„ÅßÊ¨°ÂÖÉË™øÊï¥
        encoding = encoding.flatten(start_dim=2)
        encoding = self.coord_linear(encoding)

        return encoding</code></pre>

<h3>Âë®ÊúüÂ¢ÉÁïåÊù°‰ª∂„ÅÆËÄÉÊÖÆ</h3>

<pre><code class="language-python">def compute_periodic_distance(coords1, coords2, lattice_matrix):
    """
    Âë®ÊúüÂ¢ÉÁïåÊù°‰ª∂„ÇíËÄÉÊÖÆ„Åó„ÅüË∑ùÈõ¢Ë®àÁÆó

    Args:
        coords1: (num_atoms1, 3) ÂàÜÊï∞Â∫ßÊ®ô
        coords2: (num_atoms2, 3) ÂàÜÊï∞Â∫ßÊ®ô
        lattice_matrix: (3, 3) Ê†ºÂ≠ê„Éô„ÇØ„Éà„É´Ë°åÂàó
    Returns:
        distances: (num_atoms1, num_atoms2) ÊúÄÁü≠Ë∑ùÈõ¢Ôºà√ÖÔºâ
    """
    # „Éá„Ç´„É´„ÉàÂ∫ßÊ®ô„Å´Â§âÊèõ
    cart1 = torch.matmul(coords1, lattice_matrix)
    cart2 = torch.matmul(coords2, lattice_matrix)

    # „Åô„Åπ„Å¶„ÅÆÂë®Êúü„Ç§„É°„Éº„Ç∏„ÇíËÄÉÊÖÆÔºà-1, 0, 1„ÅÆÁØÑÂõ≤Ôºâ
    offsets = torch.tensor([
        [i, j, k] for i in [-1, 0, 1]
                  for j in [-1, 0, 1]
                  for k in [-1, 0, 1]
    ], dtype=torch.float32)  # 27ÈÄö„Çä

    min_distances = []
    for offset in offsets:
        offset_cart = torch.matmul(offset, lattice_matrix)
        shifted_cart2 = cart2 + offset_cart

        # Ë∑ùÈõ¢Ë®àÁÆó
        diff = cart1.unsqueeze(1) - shifted_cart2.unsqueeze(0)
        distances = torch.norm(diff, dim=-1)
        min_distances.append(distances)

    # ÊúÄÁü≠Ë∑ùÈõ¢„ÇíÈÅ∏Êäû
    min_distances = torch.stack(min_distances, dim=-1)
    min_distances, _ = torch.min(min_distances, dim=-1)

    return min_distances

<h1>‰ΩøÁî®‰æã: ÂçòÁ¥îÁ´ãÊñπÊ†ºÂ≠ê</h1>
fractional_coords = torch.tensor([
    [0.0, 0.0, 0.0],  # ÂéüÂ≠ê1
    [0.5, 0.5, 0.5]   # ÂéüÂ≠ê2
])

lattice_matrix = torch.tensor([
    [5.0, 0.0, 0.0],
    [0.0, 5.0, 0.0],
    [0.0, 0.0, 5.0]
])  # 5√Ö„ÅÆÁ´ãÊñπÊ†ºÂ≠ê

distances = compute_periodic_distance(fractional_coords, fractional_coords, lattice_matrix)
print("Distance matrix (√Ö):")
print(distances)</code></pre>

---

<h2>2.4 ChemBERTa: ÂàÜÂ≠êSMILESË°®ÁèæÂ≠¶Áøí</h2>

<h3>Ê¶ÇË¶Å</h3>

<strong>ChemBERTa</strong>„ÅØ„ÄÅÂàÜÂ≠ê„ÅÆSMILESÊñáÂ≠óÂàó„ÇíBERT„ÅßÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ

<strong>ÁâπÂæ¥</strong>:
- <strong>RoBERTa</strong>„Éô„Éº„ÇπÔºàBERTÊîπËâØÁâàÔºâ
- <strong>10MÂàÜÂ≠ê</strong>„Åß‰∫ãÂâçÂ≠¶Áøí
- <strong>Ëª¢ÁßªÂ≠¶Áøí</strong>„ÅßÂ∞ëÈáè„Éá„Éº„Çø„Åß„ÇÇÈ´òÁ≤æÂ∫¶

<h3>SMILES„Éà„Éº„ÇØ„É≥Âåñ</h3>

<pre><code class="language-python">from transformers import RobertaTokenizer

class SMILESTokenizer:
    def __init__(self):
        # ChemBERTaÁî®„ÅÆ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂
        self.tokenizer = RobertaTokenizer.from_pretrained("seyonec/ChemBERTa-zinc-base-v1")

    def encode(self, smiles_list):
        """
        SMILESÊñáÂ≠óÂàó„Çí„Éà„Éº„ÇØ„É≥Âåñ

        Args:
            smiles_list: SMILES„ÅÆ„É™„Çπ„Éà
        Returns:
            input_ids: „Éà„Éº„ÇØ„É≥ID
            attention_mask: „Éû„Çπ„ÇØ
        """
        encoded = self.tokenizer(
            smiles_list,
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors='pt'
        )
        return encoded['input_ids'], encoded['attention_mask']

<h1>‰ΩøÁî®‰æã</h1>
smiles_list = [
    'CCO',  # „Ç®„Çø„Éé„Éº„É´
    'CC(C)Cc1ccc(cc1)C(C)C(=O)O',  # „Ç§„Éñ„Éó„É≠„Éï„Çß„É≥
    'CN1C=NC2=C1C(=O)N(C(=O)N2C)C'  # „Ç´„Éï„Çß„Ç§„É≥
]

tokenizer = SMILESTokenizer()
input_ids, attention_mask = tokenizer.encode(smiles_list)

print(f"Input IDs shape: {input_ids.shape}")
print(f"Attention mask shape: {attention_mask.shape}")
print(f"First molecule tokens: {input_ids[0][:10]}")</code></pre>

<h3>ChemBERTa„É¢„Éá„É´„ÅÆ‰ΩøÁî®</h3>

<pre><code class="language-python">from transformers import RobertaModel

class ChemBERTaEmbedding(nn.Module):
    def __init__(self, pretrained_model="seyonec/ChemBERTa-zinc-base-v1"):
        super(ChemBERTaEmbedding, self).__init__()
        self.bert = RobertaModel.from_pretrained(pretrained_model)

    def forward(self, input_ids, attention_mask):
        """
        Args:
            input_ids: (batch_size, seq_len)
            attention_mask: (batch_size, seq_len)
        Returns:
            embeddings: (batch_size, hidden_size)
        """
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)

        # [CLS]„Éà„Éº„ÇØ„É≥„ÅÆÂüã„ÇÅËæº„Åø„Çí‰ΩøÁî®
        cls_embedding = outputs.last_hidden_state[:, 0, :]

        return cls_embedding

<h1>ÂàÜÂ≠êÁâπÊÄß‰∫àÊ∏¨„É¢„Éá„É´</h1>
class MoleculePropertyPredictor(nn.Module):
    def __init__(self, hidden_size=768, num_properties=1):
        super(MoleculePropertyPredictor, self).__init__()
        self.chemberta = ChemBERTaEmbedding()
        self.predictor = nn.Sequential(
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, num_properties)
        )

    def forward(self, input_ids, attention_mask):
        embeddings = self.chemberta(input_ids, attention_mask)
        predictions = self.predictor(embeddings)
        return predictions

<h1>‰ΩøÁî®‰æã</h1>
model = MoleculePropertyPredictor(num_properties=1)  # ‰æã: logP‰∫àÊ∏¨
predictions = model(input_ids, attention_mask)
print(f"Predictions shape: {predictions.shape}")  # (3, 1)</code></pre>

---

<h2>2.5 Perceiver IO: Â§öÊßò„Å™„Éá„Éº„ÇøÁµ±Âêà</h2>

<h3>Ê¶ÇË¶Å</h3>

<strong>Perceiver IO</strong>„ÅØ„ÄÅÁï∞„Å™„ÇãÁ®ÆÈ°û„ÅÆ„Éá„Éº„Çø„ÇíÁµ±Âêà„Åó„Å¶Âá¶ÁêÜ„Åß„Åç„ÇãTransformer„Åß„Åô„ÄÇ

<strong>ÊùêÊñôÁßëÂ≠¶„Åß„ÅÆÂøúÁî®</strong>:
- ÊßãÈÄ†„Éá„Éº„Çø + ÁµÑÊàê„Éá„Éº„Çø
- ÂÆüÈ®ì„Éá„Éº„Çø + Ë®àÁÆó„Éá„Éº„Çø
- ÁîªÂÉè + „ÉÜ„Ç≠„Çπ„Éà + Êï∞ÂÄ§

<h3>„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£</h3>

<div class="mermaid">graph TB
    A1[ÊßãÈÄ†„Éá„Éº„Çø] --> C[Cross-Attention]
    A2[ÁµÑÊàê„Éá„Éº„Çø] --> C
    A3[ÂÆüÈ®ì„Éá„Éº„Çø] --> C

    B[Latent Array] --> C
    C --> D[Latent Transformer]
    D --> E[Cross-Attention Decoder]
    E --> F[‰∫àÊ∏¨ÁµêÊûú]

    style C fill:#e1f5ff
    style D fill:#ffe1e1</div>

<h3>Á∞°ÊòìÂÆüË£Ö</h3>

<pre><code class="language-python">class PerceiverBlock(nn.Module):
    def __init__(self, latent_dim, input_dim, num_latents=64):
        super(PerceiverBlock, self).__init__()
        self.num_latents = num_latents
        self.latent_dim = latent_dim

        # Latent arrayÔºàÂ≠¶ÁøíÂèØËÉΩÔºâ
        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))

        # Cross-Attention: Latent ‚Üí Input
        self.cross_attn = nn.MultiheadAttention(latent_dim, num_heads=8, batch_first=True)

        # Self-Attention: Latent ‚Üí Latent
        self.self_attn = nn.MultiheadAttention(latent_dim, num_heads=8, batch_first=True)

        # ÂÖ•Âäõ„ÇíÂüã„ÇÅËæº„Åø
        self.input_projection = nn.Linear(input_dim, latent_dim)

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, input_dim) ÂÖ•Âäõ„Éá„Éº„Çø
        Returns:
            latents: (batch_size, num_latents, latent_dim)
        """
        batch_size = x.size(0)

        # ÂÖ•Âäõ„ÇíÂüã„ÇÅËæº„Åø
        x_embed = self.input_projection(x)

        # Latent„ÇíË§áË£Ω
        latents = self.latents.unsqueeze(0).repeat(batch_size, 1, 1)

        # Cross-Attention: Latent (Query) ‚Üê Input (Key, Value)
        latents, _ = self.cross_attn(latents, x_embed, x_embed)

        # Self-Attention: LatentÂÜÖÈÉ®
        latents, _ = self.self_attn(latents, latents, latents)

        return latents

<h1>‰ΩøÁî®‰æã: ÊßãÈÄ†„Éá„Éº„Çø„Å®ÁµÑÊàê„Éá„Éº„Çø„ÇíÁµ±Âêà</h1>
batch_size = 2
seq_len = 20
input_dim = 128
latent_dim = 256

perceiver = PerceiverBlock(latent_dim, input_dim, num_latents=32)

<h1>ÊßãÈÄ†„Éá„Éº„ÇøÔºà‰æã: ÂéüÂ≠êÂ∫ßÊ®ôÔºâ</h1>
structure_data = torch.randn(batch_size, seq_len, input_dim)

latents = perceiver(structure_data)
print(f"Latent representation shape: {latents.shape}")  # (2, 32, 256)</code></pre>

---

<h2>2.6 ÂÆüË£ÖÊºîÁøí: Matformer„ÅßÊùêÊñôÁâπÊÄß‰∫àÊ∏¨</h2>

<h3>ÂÆåÂÖ®„Å™ÂÆüË£Ö‰æã</h3>

<pre><code class="language-python">import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

<h1>„Éá„Éº„Çø„Çª„ÉÉ„Éà</h1>
class MaterialsDataset(Dataset):
    def __init__(self, num_samples=100):
        self.num_samples = num_samples

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        # „ÉÄ„Éü„Éº„Éá„Éº„ÇøÔºàÂÆüÈöõ„ÅØMaterials Project„Å™„Å©„Åã„ÇâÂèñÂæóÔºâ
        num_atoms = 8
        atomic_numbers = torch.randint(1, 30, (num_atoms,))  # ÂéüÂ≠êÁï™Âè∑
        positions = torch.randn(num_atoms, 3)  # ÂéüÂ≠êÂ∫ßÊ®ôÔºà√ÖÔºâ
        distance_matrix = torch.cdist(positions, positions)  # Ë∑ùÈõ¢Ë°åÂàó

        # „Çø„Éº„Ç≤„ÉÉ„Éà: „Éê„É≥„Éâ„ÇÆ„É£„ÉÉ„ÉóÔºàeVÔºâ
        target = torch.randn(1)

        return atomic_numbers, distance_matrix, target

<h1>Matformer„É¢„Éá„É´ÔºàÁ∞°Áï•ÁâàÔºâ</h1>
class SimpleMatformer(nn.Module):
    def __init__(self, d_model=256, num_heads=8, num_layers=4):
        super(SimpleMatformer, self).__init__()

        self.atom_embedding = AtomEmbedding(num_atoms=118, d_model=d_model)

        self.layers = nn.ModuleList([
            MatformerBlock(d_model, num_heads)
            for _ in range(num_layers)
        ])

        self.pooling = nn.AdaptiveAvgPool1d(1)
        self.predictor = nn.Sequential(
            nn.Linear(d_model, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, atomic_numbers, distance_matrix):
        # ÂéüÂ≠êÂüã„ÇÅËæº„Åø
        x = self.atom_embedding(atomic_numbers)

        # Matformer„Éñ„É≠„ÉÉ„ÇØ
        for layer in self.layers:
            x = layer(x, distance_matrix)

        # Global pooling
        x = x.transpose(1, 2)  # (batch, d_model, num_atoms)
        x = self.pooling(x).squeeze(-1)  # (batch, d_model)

        # ‰∫àÊ∏¨
        output = self.predictor(x)
        return output

<h1>Ë®ìÁ∑¥</h1>
def train_matformer():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # „Éá„Éº„Çø
    dataset = MaterialsDataset(num_samples=100)
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

    # „É¢„Éá„É´
    model = SimpleMatformer(d_model=256, num_heads=8, num_layers=4).to(device)

    # ÊúÄÈÅ©Âåñ
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    # Ë®ìÁ∑¥„É´„Éº„Éó
    model.train()
    for epoch in range(5):
        total_loss = 0
        for atomic_numbers, distance_matrix, target in dataloader:
            atomic_numbers = atomic_numbers.to(device)
            distance_matrix = distance_matrix.to(device)
            target = target.to(device)

            # Forward
            predictions = model(atomic_numbers, distance_matrix)
            loss = criterion(predictions, target)

            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")

    return model

<h1>ÂÆüË°å</h1>
trained_model = train_matformer()</code></pre>

---

<h2>2.7 „Åæ„Å®„ÇÅ</h2>

<h3>ÈáçË¶Å„Éù„Ç§„É≥„Éà</h3>

1. <strong>Matformer</strong>: Ë∑ùÈõ¢„ÇíËÄÉÊÖÆ„Åó„ÅüAttention„ÄÅÈöéÂ±§ÁöÑÊßãÈÄ†
2. <strong>CrystalFormer</strong>: Âë®ÊúüÂ¢ÉÁïåÊù°‰ª∂„ÄÅÂàÜÊï∞Â∫ßÊ®ô„ÄÅÁ©∫ÈñìÁæ§
3. <strong>ChemBERTa</strong>: SMILESË°®ÁèæÂ≠¶Áøí„ÄÅËª¢ÁßªÂ≠¶Áøí
4. <strong>Perceiver IO</strong>: Â§öÊßò„Å™„Éá„Éº„ÇøÁµ±Âêà

<h3>Ê¨°Á´†„Å∏„ÅÆÊ∫ñÂÇô</h3>

Á¨¨3Á´†„Åß„ÅØ„ÄÅ‰∫ãÂâçÂ≠¶Áøí„É¢„Éá„É´ÔºàMatBERT„ÄÅMolBERTÔºâ„Å®„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÂ≠¶„Å≥„Åæ„Åô„ÄÇ

---

<h2>üìù ÊºîÁøíÂïèÈ°å</h2>

<h3>ÂïèÈ°å1: Ê¶ÇÂøµÁêÜËß£</h3>
Distance-aware Attention„ÅåÈÄöÂ∏∏„ÅÆAttention„Çà„ÇäÊùêÊñôÁßëÂ≠¶„ÅßÂÑ™„Çå„Å¶„ÅÑ„ÇãÁêÜÁî±„Çí3„Å§Êåô„Åí„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

<details>
<summary>Ëß£Á≠î‰æã</summary>

1. <strong>ÂåñÂ≠¶ÁµêÂêà„ÅÆËÄÉÊÖÆ</strong>: ÂéüÂ≠êÈñìË∑ùÈõ¢„ÅåËøë„ÅÑ„Åª„Å©Áõ∏‰∫í‰ΩúÁî®„ÅåÂº∑„ÅÑ„Å®„ÅÑ„ÅÜÁâ©ÁêÜÊ≥ïÂâá„ÇíÂèçÊò†
2. <strong>Èï∑Ë∑ùÈõ¢Áõ∏‰∫í‰ΩúÁî®„ÅÆÊäëÂà∂</strong>: ÈÅ†„ÅÑÂéüÂ≠ê„Å∏„ÅÆ‰∏çË¶Å„Å™Attention„ÇíÊ∏õ„Çâ„Åó„ÄÅË®àÁÆóÂäπÁéáÂêë‰∏ä
3. <strong>Ëß£ÈáàÊÄß„ÅÆÂêë‰∏ä</strong>: AttentionÈáç„Åø„ÅåÂåñÂ≠¶ÁöÑ„Å´ÊÑèÂë≥„ÅÆ„ÅÇ„ÇãÁµêÂêàÂº∑Â∫¶„Å®ÂØæÂøú
</details>

<h3>ÂïèÈ°å2: ÂÆüË£Ö</h3>
Âë®ÊúüÂ¢ÉÁïåÊù°‰ª∂„ÇíËÄÉÊÖÆ„Åõ„Åö„Å´Ë∑ùÈõ¢„ÇíË®àÁÆó„Åô„ÇãÂçòÁ¥î„Å™Èñ¢Êï∞„ÇíÂÆüË£Ö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

<pre><code class="language-python">def compute_simple_distance(coords1, coords2):
    """
    ÂçòÁ¥î„Å™Ë∑ùÈõ¢Ë®àÁÆóÔºàÂë®ÊúüÂ¢ÉÁïåÊù°‰ª∂„Å™„ÅóÔºâ

    Args:
        coords1: (num_atoms1, 3)
        coords2: (num_atoms2, 3)
    Returns:
        distances: (num_atoms1, num_atoms2)
    """
    # „Åì„Åì„Å´ÂÆüË£Ö
    pass</code></pre>

<details>
<summary>Ëß£Á≠î‰æã</summary>

<pre><code class="language-python">def compute_simple_distance(coords1, coords2):
    diff = coords1.unsqueeze(1) - coords2.unsqueeze(0)
    distances = torch.norm(diff, dim=-1)
    return distances</code></pre>
</details>

<h3>ÂïèÈ°å3: ÂøúÁî®</h3>
ChemBERTa„Çí‰Ωø„Å£„Å¶„ÄÅÂàÜÂ≠ê„ÅÆÊ∞¥Ê∫∂Ëß£Â∫¶„Çí‰∫àÊ∏¨„Åô„Çã„É¢„Éá„É´„ÇíË®≠Ë®à„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÂøÖË¶Å„Å™Â±§„Å®ÊßãÊàê„ÇíË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

<details>
<summary>Ëß£Á≠î‰æã</summary>

<pre><code class="language-python">class SolubilityPredictor(nn.Module):
    def __init__(self):
        super(SolubilityPredictor, self).__init__()
        self.chemberta = ChemBERTaEmbedding()  # 768Ê¨°ÂÖÉ

        self.predictor = nn.Sequential(
            nn.Linear(768, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 1)  # Ê∫∂Ëß£Â∫¶ÔºàÈÄ£Á∂öÂÄ§Ôºâ
        )

    def forward(self, input_ids, attention_mask):
        embeddings = self.chemberta(input_ids, attention_mask)
        solubility = self.predictor(embeddings)
        return solubility</code></pre>

<strong>Ë®≠Ë®àÁêÜÁî±</strong>:
- ChemBERTa„ÅßÂàÜÂ≠ê„ÅÆ‰∏ÄËà¨ÁöÑ„Å™ÁâπÂæ¥„ÇíÊäΩÂá∫
- 3Â±§„ÅÆÂÖ®ÁµêÂêàÂ±§„ÅßÊ∫∂Ëß£Â∫¶„Å´ÁâπÂåñ„Åó„ÅüË°®Áèæ„Å´Â§âÊèõ
- Dropout„ÅßÈÅéÂ≠¶Áøí„ÇíÈò≤Ê≠¢
- Âá∫Âäõ„ÅØÈÄ£Á∂öÂÄ§Ôºàlog10(mol/L)„Å™„Å©Ôºâ
</details>

---

<strong>Ê¨°Á´†</strong>: <strong>[Á¨¨3Á´†: ‰∫ãÂâçÂ≠¶Áøí„É¢„Éá„É´„Å®Ëª¢ÁßªÂ≠¶Áøí](chapter-3.md)</strong>

---

<strong>‰ΩúÊàêËÄÖ</strong>: Ê©ãÊú¨‰Ωë‰ªãÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ
<strong>ÊúÄÁµÇÊõ¥Êñ∞</strong>: 2025Âπ¥10Êúà17Êó•
<div class="navigation">
    <a href="chapter-1.html" class="nav-button">‚Üê Á¨¨1Á´†</a>
    <a href="index.html" class="nav-button">„Ç∑„É™„Éº„Ç∫ÁõÆÊ¨°„Å´Êàª„Çã</a>
    <a href="chapter-3.html" class="nav-button">Á¨¨3Á´† ‚Üí</a>
</div>
    </main>

    <footer>
        <p><strong>‰ΩúÊàêËÄÖ</strong>: AI Terakoya Content Team</p>
        <p><strong>Áõ£‰øÆ</strong>: Dr. Yusuke HashimotoÔºàÊù±ÂåóÂ§ßÂ≠¶Ôºâ</p>
        <p><strong>„Éê„Éº„Ç∏„Éß„É≥</strong>: 1.0 | <strong>‰ΩúÊàêÊó•</strong>: 2025-10-17</p>
        <p><strong>„É©„Ç§„Çª„É≥„Çπ</strong>: Creative Commons BY 4.0</p>
        <p>¬© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>
