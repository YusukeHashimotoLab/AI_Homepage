# ç¬¬3ç« : äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨è»¢ç§»å­¦ç¿’

**å­¦ç¿’æ™‚é–“**: 25-30åˆ† | **é›£æ˜“åº¦**: ä¸­ç´šã€œä¸Šç´š

## ğŸ“‹ ã“ã®ç« ã§å­¦ã¶ã“ã¨

- äº‹å‰å­¦ç¿’ï¼ˆPre-trainingï¼‰ã®é‡è¦æ€§ã¨åŸç†
- MatBERTã€MolBERTãªã©ææ–™ç§‘å­¦å‘ã‘äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«
- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆFine-tuningï¼‰ã®æˆ¦ç•¥
- Few-shotå­¦ç¿’ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
- ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œï¼ˆDomain Adaptationï¼‰

---

## 3.1 äº‹å‰å­¦ç¿’ã®é‡è¦æ€§

### ãªãœäº‹å‰å­¦ç¿’ãŒå¿…è¦ã‹

**ææ–™ç§‘å­¦ã®èª²é¡Œ**:
- âŒ ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ï¼ˆå®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã¯é«˜ã‚³ã‚¹ãƒˆï¼‰
- âŒ ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®çŸ¥è­˜ãŒå¿…è¦
- âŒ ã‚¼ãƒ­ã‹ã‚‰å­¦ç¿’ã™ã‚‹ã¨æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹

**äº‹å‰å­¦ç¿’ã®åˆ©ç‚¹**:
- âœ… å¤§è¦æ¨¡ãª**ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿**ã§ä¸€èˆ¬çš„ãªçŸ¥è­˜ã‚’ç²å¾—
- âœ… å°‘é‡ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã§**é«˜ç²¾åº¦**ã‚’å®Ÿç¾
- âœ… é–‹ç™ºæœŸé–“ã®**å¤§å¹…çŸ­ç¸®**ï¼ˆæ•°é€±é–“â†’æ•°æ™‚é–“ï¼‰

```mermaid
graph LR
    A[å¤§è¦æ¨¡ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿] --> B[äº‹å‰å­¦ç¿’]
    B --> C[æ±ç”¨è¡¨ç¾ãƒ¢ãƒ‡ãƒ«]
    C --> D[ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°]
    E[å°‘é‡ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿] --> D
    D --> F[ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«]

    style B fill:#e1f5ff
    style D fill:#ffe1e1
```

### äº‹å‰å­¦ç¿’ã®ã‚¿ã‚¹ã‚¯

**è‡ªç„¶è¨€èªå‡¦ç†ã§ã®ä¾‹**:
- **Masked Language Model (MLM)**: ä¸€éƒ¨ã®å˜èªã‚’ãƒã‚¹ã‚¯ã—ã¦äºˆæ¸¬
- **Next Sentence Prediction (NSP)**: 2æ–‡ã®é€£ç¶šæ€§ã‚’äºˆæ¸¬

**ææ–™ç§‘å­¦ã§ã®å¿œç”¨**:
- **Masked Atom Prediction**: ä¸€éƒ¨ã®åŸå­ã‚’ãƒã‚¹ã‚¯ã—ã¦äºˆæ¸¬
- **Property Prediction**: è¤‡æ•°ã®ææ–™ç‰¹æ€§ã‚’åŒæ™‚äºˆæ¸¬
- **Contrastive Learning**: é¡ä¼¼ææ–™ã‚’è¿‘ãã€ç•°ãªã‚‹ææ–™ã‚’é ãã«é…ç½®

---

## 3.2 MatBERT: Materials BERT

### æ¦‚è¦

**MatBERT**ã¯ã€ææ–™ã®çµ„æˆå¼ã‚’BERTã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚

**ç‰¹å¾´**:
- **500kææ–™**ã®çµ„æˆå¼ã§äº‹å‰å­¦ç¿’
- **ãƒã‚¹ã‚¯åŸå­äºˆæ¸¬**ã‚¿ã‚¹ã‚¯
- è»¢ç§»å­¦ç¿’ã§æ§˜ã€…ãªç‰¹æ€§äºˆæ¸¬ã«é©ç”¨å¯èƒ½

### çµ„æˆå¼ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–

```python
import torch
import torch.nn as nn
from transformers import BertTokenizer, BertModel

class CompositionTokenizer:
    def __init__(self):
        # ã‚«ã‚¹ã‚¿ãƒ èªå½™ï¼ˆå‘¨æœŸè¡¨ã®å…ƒç´ ï¼‰
        self.vocab = ['[PAD]', '[CLS]', '[SEP]', '[MASK]'] + [
            'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne',
            'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar', 'K', 'Ca',
            # ... å…¨å…ƒç´ 
        ]
        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}
        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}

    def tokenize(self, composition):
        """
        çµ„æˆå¼ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–

        Args:
            composition: 'Fe2O3' ã®ã‚ˆã†ãªçµ„æˆå¼
        Returns:
            tokens: ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ
        """
        import re
        # å…ƒç´ ã¨æ•°å­—ã‚’åˆ†å‰²
        pattern = r'([A-Z][a-z]?)(\d*\.?\d*)'
        matches = re.findall(pattern, composition)

        tokens = ['[CLS]']
        for element, count in matches:
            if element in self.vocab:
                # å…ƒç´ ã‚’è¿½åŠ 
                tokens.append(element)
                # æ•°ãŒ1ã‚ˆã‚Šå¤§ãã„å ´åˆã€ãã®å›æ•°ã ã‘ç¹°ã‚Šè¿”ã™ï¼ˆç°¡ç•¥åŒ–ï¼‰
                if count and float(count) > 1:
                    for _ in range(int(float(count)) - 1):
                        tokens.append(element)
        tokens.append('[SEP]')

        return tokens

    def encode(self, compositions, max_length=32):
        """
        çµ„æˆå¼ã‚’IDã«å¤‰æ›

        Args:
            compositions: çµ„æˆå¼ã®ãƒªã‚¹ãƒˆ
            max_length: æœ€å¤§é•·
        Returns:
            input_ids: (batch_size, max_length)
            attention_mask: (batch_size, max_length)
        """
        batch_input_ids = []
        batch_attention_mask = []

        for comp in compositions:
            tokens = self.tokenize(comp)
            ids = [self.token_to_id.get(token, 0) for token in tokens]

            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            attention_mask = [1] * len(ids)
            while len(ids) < max_length:
                ids.append(0)  # [PAD]
                attention_mask.append(0)

            # ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ã‚·ãƒ§ãƒ³
            ids = ids[:max_length]
            attention_mask = attention_mask[:max_length]

            batch_input_ids.append(ids)
            batch_attention_mask.append(attention_mask)

        return torch.tensor(batch_input_ids), torch.tensor(batch_attention_mask)

# ä½¿ç”¨ä¾‹
tokenizer = CompositionTokenizer()

compositions = [
    'Fe2O3',     # é…¸åŒ–é‰„
    'LiCoO2',    # ãƒªãƒã‚¦ãƒ ã‚³ãƒãƒ«ãƒˆé…¸åŒ–ç‰©ï¼ˆé›»æ± ææ–™ï¼‰
    'BaTiO3'     # ãƒã‚¿ãƒ³é…¸ãƒãƒªã‚¦ãƒ ï¼ˆèª˜é›»ä½“ï¼‰
]

input_ids, attention_mask = tokenizer.encode(compositions)
print(f"Input IDs shape: {input_ids.shape}")
print(f"First composition tokens: {input_ids[0][:10]}")
```

### MatBERTãƒ¢ãƒ‡ãƒ«

```python
class MatBERT(nn.Module):
    def __init__(self, vocab_size, d_model=512, num_layers=6, num_heads=8):
        super(MatBERT, self).__init__()

        # Embedding
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(512, d_model)

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dim_feedforward=2048,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)

        self.d_model = d_model

    def forward(self, input_ids, attention_mask):
        """
        Args:
            input_ids: (batch_size, seq_len)
            attention_mask: (batch_size, seq_len)
        Returns:
            embeddings: (batch_size, seq_len, d_model)
        """
        batch_size, seq_len = input_ids.shape

        # Token embedding
        token_embeddings = self.embedding(input_ids)

        # Positional embedding
        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)
        position_embeddings = self.position_embedding(positions)

        # åˆè¨ˆ
        embeddings = token_embeddings + position_embeddings

        # Transformer
        # attention_maskã‚’Transformerç”¨ã«å¤‰æ›ï¼ˆ0â†’-inf, 1â†’0ï¼‰
        transformer_mask = (1 - attention_mask).bool()
        output = self.transformer_encoder(embeddings, src_key_padding_mask=transformer_mask)

        return output

# ä½¿ç”¨ä¾‹
vocab_size = len(tokenizer.vocab)
model = MatBERT(vocab_size, d_model=512, num_layers=6, num_heads=8)

embeddings = model(input_ids, attention_mask)
print(f"Embeddings shape: {embeddings.shape}")  # (3, 32, 512)
```

### äº‹å‰å­¦ç¿’: Masked Atom Prediction

```python
def masked_atom_prediction_loss(model, input_ids, attention_mask, mask_prob=0.15):
    """
    ãƒã‚¹ã‚¯åŸå­äºˆæ¸¬ã«ã‚ˆã‚‹äº‹å‰å­¦ç¿’

    Args:
        model: MatBERTãƒ¢ãƒ‡ãƒ«
        input_ids: (batch_size, seq_len)
        attention_mask: (batch_size, seq_len)
        mask_prob: ãƒã‚¹ã‚¯ã™ã‚‹ç¢ºç‡
    Returns:
        loss: æå¤±
    """
    batch_size, seq_len = input_ids.shape

    # ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒã‚¹ã‚¯
    mask_token_id = tokenizer.token_to_id['[MASK]']
    mask = torch.rand(batch_size, seq_len) < mask_prob
    mask = mask & (attention_mask == 1)  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†ã¯é™¤å¤–

    # å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿å­˜
    original_input_ids = input_ids.clone()

    # ãƒã‚¹ã‚¯ã‚’é©ç”¨
    input_ids[mask] = mask_token_id

    # Forward
    embeddings = model(input_ids, attention_mask)

    # äºˆæ¸¬ãƒ˜ãƒƒãƒ‰
    prediction_head = nn.Linear(model.d_model, vocab_size)
    logits = prediction_head(embeddings)

    # æå¤±è¨ˆç®—ï¼ˆãƒã‚¹ã‚¯ã•ã‚ŒãŸä½ç½®ã®ã¿ï¼‰
    criterion = nn.CrossEntropyLoss(ignore_index=-100)
    labels = original_input_ids.clone()
    labels[~mask] = -100  # ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†ã¯ç„¡è¦–

    loss = criterion(logits.view(-1, vocab_size), labels.view(-1))

    return loss

# äº‹å‰å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
def pretrain_matbert(model, dataloader, epochs=10):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for input_ids, attention_mask in dataloader:
            loss = masked_atom_prediction_loss(model, input_ids, attention_mask)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1}, Pretraining Loss: {avg_loss:.4f}")

    return model
```

---

## 3.3 ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥

### ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã¯

**å®šç¾©**: äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®šã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ã‚‹è¿½åŠ å­¦ç¿’

**æˆ¦ç•¥**:
1. **Full Fine-tuning**: ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
2. **Feature Extraction**: åŸ‹ã‚è¾¼ã¿å±¤ã®ã¿ä½¿ç”¨ã€äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ã®ã¿å­¦ç¿’
3. **Partial Fine-tuning**: ä¸€éƒ¨ã®å±¤ã®ã¿æ›´æ–°

```mermaid
graph TD
    A[äº‹å‰å­¦ç¿’æ¸ˆã¿MatBERT] --> B{ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥}
    B --> C[Full Fine-tuning]
    B --> D[Feature Extraction]
    B --> E[Partial Fine-tuning]

    C --> F[å…¨å±¤ã‚’æ›´æ–°]
    D --> G[åŸ‹ã‚è¾¼ã¿å›ºå®šã€äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ã®ã¿å­¦ç¿’]
    E --> H[ä¸Šä½å±¤ã®ã¿æ›´æ–°]

    style C fill:#ffe1e1
    style D fill:#e1f5ff
    style E fill:#f5ffe1
```

### å®Ÿè£…: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬

```python
class MatBERTForBandgap(nn.Module):
    def __init__(self, matbert_model, d_model=512):
        super(MatBERTForBandgap, self).__init__()
        self.matbert = matbert_model

        # äºˆæ¸¬ãƒ˜ãƒƒãƒ‰
        self.bandgap_predictor = nn.Sequential(
            nn.Linear(d_model, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 1)
        )

    def forward(self, input_ids, attention_mask):
        # MatBERTåŸ‹ã‚è¾¼ã¿
        embeddings = self.matbert(input_ids, attention_mask)

        # [CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨
        cls_embedding = embeddings[:, 0, :]

        # ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬
        bandgap = self.bandgap_predictor(cls_embedding)
        return bandgap

# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
def finetune_for_bandgap(pretrained_model, train_loader, val_loader, strategy='full'):
    """
    ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ã¸ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

    Args:
        pretrained_model: äº‹å‰å­¦ç¿’æ¸ˆã¿MatBERT
        train_loader: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        val_loader: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        strategy: 'full', 'feature', 'partial'
    """
    model = MatBERTForBandgap(pretrained_model)

    # æˆ¦ç•¥ã«å¿œã˜ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å›ºå®š
    if strategy == 'feature':
        # MatBERTã‚’å›ºå®š
        for param in model.matbert.parameters():
            param.requires_grad = False
    elif strategy == 'partial':
        # ä¸‹ä½å±¤ã‚’å›ºå®šã€ä¸Šä½å±¤ã®ã¿æ›´æ–°
        for i, layer in enumerate(model.matbert.transformer_encoder.layers):
            if i < 3:  # ä¸‹ä½3å±¤ã‚’å›ºå®š
                for param in layer.parameters():
                    param.requires_grad = False

    # æœ€é©åŒ–
    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)
    criterion = nn.MSELoss()

    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    best_val_loss = float('inf')
    for epoch in range(20):
        model.train()
        train_loss = 0
        for input_ids, attention_mask, bandgaps in train_loader:
            predictions = model(input_ids, attention_mask)
            loss = criterion(predictions, bandgaps)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        # æ¤œè¨¼
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for input_ids, attention_mask, bandgaps in val_loader:
                predictions = model(input_ids, attention_mask)
                loss = criterion(predictions, bandgaps)
                val_loss += loss.item()

        train_loss /= len(train_loader)
        val_loss /= len(val_loader)

        print(f"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_matbert_bandgap.pt')

    return model
```

---

## 3.4 Few-shotå­¦ç¿’

### æ¦‚è¦

**Few-shotå­¦ç¿’**: å°‘é‡ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæ•°å€‹ã€œæ•°åå€‹ï¼‰ã§æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’å­¦ç¿’

**ææ–™ç§‘å­¦ã§ã®é‡è¦æ€§**:
- æ–°è¦ææ–™ã®ãƒ‡ãƒ¼ã‚¿ã¯éå¸¸ã«å°‘ãªã„
- å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¯é«˜ã‚³ã‚¹ãƒˆ
- è¿…é€Ÿãªãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°ãŒå¿…è¦

### Prototypical Networks

```python
class PrototypicalNetwork(nn.Module):
    def __init__(self, matbert_model, d_model=512):
        super(PrototypicalNetwork, self).__init__()
        self.encoder = matbert_model

    def forward(self, support_ids, support_mask, query_ids, query_mask, support_labels):
        """
        Prototypical Networksã«ã‚ˆã‚‹åˆ†é¡

        Args:
            support_ids: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆå…¥åŠ› (n_support, seq_len)
            support_mask: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆãƒã‚¹ã‚¯
            query_ids: ã‚¯ã‚¨ãƒªå…¥åŠ› (n_query, seq_len)
            query_mask: ã‚¯ã‚¨ãƒªãƒã‚¹ã‚¯
            support_labels: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆãƒ©ãƒ™ãƒ« (n_support,)
        Returns:
            predictions: ã‚¯ã‚¨ãƒªã®äºˆæ¸¬ãƒ©ãƒ™ãƒ«
        """
        # ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆã¨ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿
        support_embeddings = self.encoder(support_ids, support_mask)[:, 0, :]  # [CLS]
        query_embeddings = self.encoder(query_ids, query_mask)[:, 0, :]

        # å„ã‚¯ãƒ©ã‚¹ã®ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆå¹³å‡åŸ‹ã‚è¾¼ã¿ï¼‰ã‚’è¨ˆç®—
        unique_labels = torch.unique(support_labels)
        prototypes = []
        for label in unique_labels:
            mask = (support_labels == label)
            prototype = support_embeddings[mask].mean(dim=0)
            prototypes.append(prototype)

        prototypes = torch.stack(prototypes)  # (num_classes, d_model)

        # ã‚¯ã‚¨ãƒªã¨ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—é–“ã®è·é›¢
        distances = torch.cdist(query_embeddings, prototypes)  # (n_query, num_classes)

        # æœ€ã‚‚è¿‘ã„ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã®ã‚¯ãƒ©ã‚¹ã‚’äºˆæ¸¬
        predictions = torch.argmin(distances, dim=1)

        return predictions

# ä½¿ç”¨ä¾‹: 3-way 5-shotåˆ†é¡
# 3ã‚¯ãƒ©ã‚¹ã€å„ã‚¯ãƒ©ã‚¹5ã‚µãƒ³ãƒ—ãƒ«
n_classes = 3
n_support_per_class = 5
n_query = 10

support_ids = torch.randint(0, vocab_size, (n_classes * n_support_per_class, 32))
support_mask = torch.ones_like(support_ids)
support_labels = torch.arange(n_classes).repeat_interleave(n_support_per_class)

query_ids = torch.randint(0, vocab_size, (n_query, 32))
query_mask = torch.ones_like(query_ids)

proto_net = PrototypicalNetwork(model)
predictions = proto_net(support_ids, support_mask, query_ids, query_mask, support_labels)
print(f"Predictions: {predictions}")
```

---

## 3.5 ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

### ææ–™ç§‘å­¦ã§ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: ãƒ¢ãƒ‡ãƒ«ã«è¿½åŠ æƒ…å ±ã‚’ä¸ãˆã¦æ€§èƒ½ã‚’å‘ä¸Š

**ä¾‹**:
```python
# é€šå¸¸: 'Fe2O3'
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ã: '[OXIDE] Fe2O3 [BANDGAP]'
```

### å®Ÿè£…

```python
class PromptedMatBERT(nn.Module):
    def __init__(self, matbert_model, d_model=512):
        super(PromptedMatBERT, self).__init__()
        self.matbert = matbert_model

        # ã‚¿ã‚¹ã‚¯åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåŸ‹ã‚è¾¼ã¿ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
        self.task_prompts = nn.Parameter(torch.randn(10, d_model))  # 10ç¨®é¡ã®ã‚¿ã‚¹ã‚¯

    def forward(self, input_ids, attention_mask, task_id=0):
        """
        Args:
            input_ids: (batch_size, seq_len)
            attention_mask: (batch_size, seq_len)
            task_id: ã‚¿ã‚¹ã‚¯ID (0-9)
        """
        batch_size = input_ids.size(0)

        # é€šå¸¸ã®åŸ‹ã‚è¾¼ã¿
        embeddings = self.matbert(input_ids, attention_mask)

        # ã‚¿ã‚¹ã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…ˆé ­ã«è¿½åŠ 
        task_prompt = self.task_prompts[task_id].unsqueeze(0).expand(batch_size, -1, -1)
        embeddings = torch.cat([task_prompt, embeddings], dim=1)

        return embeddings

# ä½¿ç”¨ä¾‹
prompted_model = PromptedMatBERT(model)

# ã‚¿ã‚¹ã‚¯0: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬
embeddings_task0 = prompted_model(input_ids, attention_mask, task_id=0)

# ã‚¿ã‚¹ã‚¯1: å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼äºˆæ¸¬
embeddings_task1 = prompted_model(input_ids, attention_mask, task_id=1)

print(f"Embeddings with prompt shape: {embeddings_task0.shape}")
```

---

## 3.6 ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ

### æ¦‚è¦

**ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ**: ã‚½ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ã§è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³ã«é©å¿œ

**ä¾‹**:
- ã‚½ãƒ¼ã‚¹: ç„¡æ©Ÿææ–™ãƒ‡ãƒ¼ã‚¿
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: æœ‰æ©Ÿåˆ†å­ãƒ‡ãƒ¼ã‚¿

### Adversarial Domain Adaptation

```python
class DomainClassifier(nn.Module):
    def __init__(self, d_model=512):
        super(DomainClassifier, self).__init__()
        self.classifier = nn.Sequential(
            nn.Linear(d_model, 256),
            nn.ReLU(),
            nn.Linear(256, 2)  # ã‚½ãƒ¼ã‚¹ or ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
        )

    def forward(self, embeddings):
        return self.classifier(embeddings)

class DomainAdaptiveMatBERT(nn.Module):
    def __init__(self, matbert_model):
        super(DomainAdaptiveMatBERT, self).__init__()
        self.matbert = matbert_model
        self.domain_classifier = DomainClassifier()
        self.task_predictor = nn.Linear(512, 1)  # ä¾‹: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬

    def forward(self, input_ids, attention_mask, alpha=1.0):
        """
        Args:
            alpha: ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã®å¼·ã•
        """
        embeddings = self.matbert(input_ids, attention_mask)[:, 0, :]

        # ã‚¿ã‚¹ã‚¯äºˆæ¸¬
        task_output = self.task_predictor(embeddings)

        # ãƒ‰ãƒ¡ã‚¤ãƒ³äºˆæ¸¬ï¼ˆå‹¾é…åè»¢å±¤ã‚’ä½¿ç”¨ï¼‰
        # ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã®ãŸã‚çœç•¥
        domain_output = self.domain_classifier(embeddings)

        return task_output, domain_output

# è¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
def train_domain_adaptive(model, source_loader, target_loader, epochs=20):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
    task_criterion = nn.MSELoss()
    domain_criterion = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        for (source_ids, source_mask, source_labels), (target_ids, target_mask, _) in zip(source_loader, target_loader):
            # ã‚½ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³
            source_task, source_domain = model(source_ids, source_mask)
            source_domain_labels = torch.zeros(source_ids.size(0), dtype=torch.long)  # ã‚½ãƒ¼ã‚¹ = 0

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³
            target_task, target_domain = model(target_ids, target_mask)
            target_domain_labels = torch.ones(target_ids.size(0), dtype=torch.long)  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ = 1

            # æå¤±
            task_loss = task_criterion(source_task, source_labels)
            domain_loss = domain_criterion(source_domain, source_domain_labels) + \
                          domain_criterion(target_domain, target_domain_labels)

            total_loss = task_loss + 0.1 * domain_loss

            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()

        print(f"Epoch {epoch+1}, Task Loss: {task_loss.item():.4f}, Domain Loss: {domain_loss.item():.4f}")
```

---

## 3.7 ã¾ã¨ã‚

### é‡è¦ãƒã‚¤ãƒ³ãƒˆ

1. **äº‹å‰å­¦ç¿’**: å¤§è¦æ¨¡ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã§ä¸€èˆ¬çš„çŸ¥è­˜ã‚’ç²å¾—
2. **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§ã‚¿ã‚¹ã‚¯ç‰¹åŒ–
3. **Few-shotå­¦ç¿’**: æ•°å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã§æ–°ã‚¿ã‚¹ã‚¯å­¦ç¿’
4. **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**: ã‚¿ã‚¹ã‚¯æƒ…å ±ã‚’åŸ‹ã‚è¾¼ã¿ã§è¡¨ç¾
5. **ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ**: ç•°ãªã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³é–“ã§çŸ¥è­˜è»¢ç§»

### æ¬¡ç« ã¸ã®æº–å‚™

ç¬¬4ç« ã§ã¯ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹åˆ†å­ç”Ÿæˆã¨ææ–™é€†è¨­è¨ˆã‚’å­¦ã³ã¾ã™ã€‚

---

## ğŸ“ æ¼”ç¿’å•é¡Œ

### å•é¡Œ1: æ¦‚å¿µç†è§£
ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®3ã¤ã®æˆ¦ç•¥ï¼ˆFullã€Feature Extractionã€Partialï¼‰ã«ã¤ã„ã¦ã€ãã‚Œãã‚Œã©ã®ã‚ˆã†ãªå ´åˆã«é©ã—ã¦ã„ã‚‹ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

<details>
<summary>è§£ç­”ä¾‹</summary>

1. **Full Fine-tuning**:
   - **é©ç”¨å ´é¢**: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ãŒæ¯”è¼ƒçš„å¤šã„ï¼ˆæ•°åƒã‚µãƒ³ãƒ—ãƒ«ä»¥ä¸Šï¼‰
   - **åˆ©ç‚¹**: æœ€é«˜ç²¾åº¦ã‚’é”æˆå¯èƒ½
   - **æ¬ ç‚¹**: éå­¦ç¿’ãƒªã‚¹ã‚¯ã€è¨ˆç®—ã‚³ã‚¹ãƒˆå¤§

2. **Feature Extraction**:
   - **é©ç”¨å ´é¢**: ãƒ‡ãƒ¼ã‚¿ãŒéå¸¸ã«å°‘ãªã„ï¼ˆæ•°åã€œæ•°ç™¾ã‚µãƒ³ãƒ—ãƒ«ï¼‰
   - **åˆ©ç‚¹**: éå­¦ç¿’ã‚’é˜²ãã‚„ã™ã„ã€é«˜é€Ÿ
   - **æ¬ ç‚¹**: ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒå¤§ããç•°ãªã‚‹å ´åˆã¯ç²¾åº¦ä½ä¸‹

3. **Partial Fine-tuning**:
   - **é©ç”¨å ´é¢**: ä¸­ç¨‹åº¦ã®ãƒ‡ãƒ¼ã‚¿é‡ã€ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒé¡ä¼¼
   - **åˆ©ç‚¹**: ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸæ€§èƒ½ã¨ã‚³ã‚¹ãƒˆ
   - **æ¬ ç‚¹**: ã©ã®å±¤ã‚’æ›´æ–°ã™ã‚‹ã‹é¸æŠãŒé›£ã—ã„
</details>

### å•é¡Œ2: å®Ÿè£…
ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®ç©ºæ¬„ã‚’åŸ‹ã‚ã¦ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹é–¢æ•°ã‚’å®Œæˆã•ã›ã¦ãã ã•ã„ã€‚

```python
def load_and_finetune(pretrained_path, train_loader, val_loader):
    # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    matbert = MatBERT(vocab_size=______, d_model=512)
    matbert.load_state_dict(torch.load(______))

    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
    model = MatBERTForBandgap(______)

    # æœ€é©åŒ–
    optimizer = torch.optim.Adam(______.parameters(), lr=1e-5)
    criterion = nn.MSELoss()

    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    for epoch in range(10):
        model.train()
        for input_ids, attention_mask, targets in train_loader:
            predictions = model(______, ______)
            loss = ______(predictions, targets)

            optimizer.zero_grad()
            ______.backward()
            optimizer.step()

    return model
```

<details>
<summary>è§£ç­”ä¾‹</summary>

```python
def load_and_finetune(pretrained_path, train_loader, val_loader):
    # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    matbert = MatBERT(vocab_size=len(tokenizer.vocab), d_model=512)
    matbert.load_state_dict(torch.load(pretrained_path))

    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
    model = MatBERTForBandgap(matbert)

    # æœ€é©åŒ–
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
    criterion = nn.MSELoss()

    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    for epoch in range(10):
        model.train()
        for input_ids, attention_mask, targets in train_loader:
            predictions = model(input_ids, attention_mask)
            loss = criterion(predictions, targets)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    return model
```
</details>

### å•é¡Œ3: å¿œç”¨
ææ–™ç§‘å­¦ã§ Few-shotå­¦ç¿’ãŒç‰¹ã«æœ‰ç”¨ãª3ã¤ã®ã‚·ãƒŠãƒªã‚ªã‚’æŒ™ã’ã€ãã‚Œãã‚Œã®ç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

<details>
<summary>è§£ç­”ä¾‹</summary>

1. **æ–°è¦ææ–™ã®è¿…é€Ÿè©•ä¾¡**:
   - **ã‚·ãƒŠãƒªã‚ª**: æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã®ææ–™ï¼ˆä¾‹: æ–°å‹ãƒšãƒ­ãƒ–ã‚¹ã‚«ã‚¤ãƒˆï¼‰
   - **ç†ç”±**: å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãŒã¾ã å°‘ãªãã€æ•°ã‚µãƒ³ãƒ—ãƒ«ã§ç‰¹æ€§äºˆæ¸¬ãŒå¿…è¦

2. **å®Ÿé¨“è¨ˆç”»ã®åŠ¹ç‡åŒ–**:
   - **ã‚·ãƒŠãƒªã‚ª**: é«˜ã‚³ã‚¹ãƒˆãªå®Ÿé¨“ï¼ˆå˜çµæ™¶æˆé•·ã€é«˜åœ§åˆæˆï¼‰
   - **ç†ç”±**: å°‘æ•°ã®å®Ÿé¨“çµæœã‹ã‚‰æ¬¡ã®å®Ÿé¨“æ¡ä»¶ã‚’ææ¡ˆ

3. **ä¼æ¥­ã®ç‹¬è‡ªææ–™é–‹ç™º**:
   - **ã‚·ãƒŠãƒªã‚ª**: ç«¶åˆã«å…¬é–‹ã§ããªã„ç‹¬è‡ªææ–™
   - **ç†ç”±**: ç¤¾å†…ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§å­¦ç¿’ã€å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã¯ä½¿ãˆãªã„
</details>

---

**æ¬¡ç« **: **[ç¬¬4ç« : ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¨é€†è¨­è¨ˆ](chapter-4.md)**

---

**ä½œæˆè€…**: æ©‹æœ¬ä½‘ä»‹ï¼ˆæ±åŒ—å¤§å­¦ï¼‰
**æœ€çµ‚æ›´æ–°**: 2025å¹´10æœˆ17æ—¥
