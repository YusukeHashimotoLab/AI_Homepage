<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Untitled</h1>
            
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 5å€‹</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p><h1>ç¬¬1ç« : Transformeré©å‘½ã¨ææ–™ç§‘å­¦</h1></p>

<p><strong>å­¦ç¿’æ™‚é–“</strong>: 20-30åˆ† | <strong>é›£æ˜“åº¦</strong>: ä¸­ç´š</p>

<p><h2>ğŸ“‹ ã“ã®ç« ã§å­¦ã¶ã“ã¨</h2></p>

<ul>
<li>Attentionæ©Ÿæ§‹ã®åŸç†ã¨æ•°å­¦çš„ç†è§£</li>
<li>Self-Attentionã¨Multi-Head Attentionã®ä»•çµ„ã¿</li>
<li>TransformerãŒRNN/CNNã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ç†ç”±</li>
<li>BERTã€GPTã®åŸºæœ¬æ§‹é€ ã¨é•ã„</li>
<li>ææ–™ç§‘å­¦ã§ã®æˆåŠŸäº‹ä¾‹</li>
</ul>

<p>---</p>

<p><h2>1.1 ãªãœTransformerãŒé©å‘½ã‚’èµ·ã“ã—ãŸã®ã‹</h2></p>

<p><h3>å¾“æ¥ã®RNN/CNNã®é™ç•Œ</h3></p>

<p><strong>RNNï¼ˆRecurrent Neural Networkï¼‰ã®å•é¡Œ</strong>:</p>
<ul>
<li>é•·ã„ç³»åˆ—ã§ã®å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™º</li>
<li>ä¸¦åˆ—åŒ–ãŒå›°é›£ï¼ˆé€æ¬¡å‡¦ç†ãŒå¿…è¦ï¼‰</li>
<li>é•·æœŸä¾å­˜é–¢ä¿‚ã®æ•æ‰ãŒé›£ã—ã„</li>
</ul>

<p><strong>CNNï¼ˆConvolutional Neural Networkï¼‰ã®å•é¡Œ</strong>:</p>
<ul>
<li>å±€æ‰€çš„ãªç‰¹å¾´ã—ã‹æ‰ãˆã‚‰ã‚Œãªã„</li>
<li>é•·è·é›¢ã®é–¢ä¿‚æ€§ã‚’æ‰ãˆã‚‹ã«ã¯æ·±ã„å±¤ãŒå¿…è¦</li>
<li>åˆ†å­ãƒ»ææ–™ã®ã‚ˆã†ãªä¸è¦å‰‡ãªæ§‹é€ ã«ã¯ä¸å‘ã</li>
</ul>

<p><h3>Transformerã®é©æ–°æ€§</h3></p>

<p><strong>2017å¹´ã€"Attention Is All You Need"è«–æ–‡ã§ç™»å ´</strong>:</p>
<ul>
<li>âœ… <strong>å…¨è¦ç´ é–“ã®é–¢ä¿‚ã‚’ç›´æ¥ãƒ¢ãƒ‡ãƒ«åŒ–</strong>ï¼ˆAttentionæ©Ÿæ§‹ï¼‰</li>
<li>âœ… <strong>å®Œå…¨ä¸¦åˆ—åŒ–å¯èƒ½</strong>ï¼ˆGPUã‚’æœ€å¤§é™æ´»ç”¨ï¼‰</li>
<li>âœ… <strong>é•·è·é›¢ä¾å­˜é–¢ä¿‚ã‚’åŠ¹ç‡çš„ã«æ•æ‰</strong></li>
<li>âœ… <strong>è§£é‡ˆæ€§</strong>ï¼ˆAttentioné‡ã¿ã§é‡è¦ãªéƒ¨åˆ†ã‚’å¯è¦–åŒ–ï¼‰</li>
</ul>

<p><pre><code class="language-mermaid">graph LR</p>
<p>    A[å…¥åŠ›ç³»åˆ—] --> B[Self-Attention]</p>
<p>    B --> C[Feed Forward]</p>
<p>    C --> D[å‡ºåŠ›]</p>

<p>    B -.ã™ã¹ã¦ã®è¦ç´ é–“ã®é–¢ä¿‚ã‚’è¨ˆç®—.-> B</p>

<p>    style B fill:#e1f5ff</p>
<p></code></pre></p>

<p>---</p>

<p><h2>1.2 Attentionæ©Ÿæ§‹ã®åŸç†</h2></p>

<p><h3>Attentionæ©Ÿæ§‹ã¨ã¯</h3></p>

<p><strong>åŸºæœ¬æ¦‚å¿µ</strong>: å…¥åŠ›ã®ä¸­ã§ã€Œã©ã“ã«æ³¨ç›®ã™ã¹ãã‹ã€ã‚’å­¦ç¿’ã™ã‚‹ä»•çµ„ã¿</p>

<p><strong>æ•°å¼</strong>:</p>
<p>$$</p>
<p>\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V</p>
<p>$$</p>

<ul>
<li><strong>Q (Query)</strong>: ã€Œä½•ã‚’æ¢ã—ã¦ã„ã‚‹ã‹ã€</li>
<li><strong>K (Key)</strong>: ã€Œä½•ã‚’æŒã£ã¦ã„ã‚‹ã‹ã€</li>
<li><strong>V (Value)</strong>: ã€Œå®Ÿéš›ã®å†…å®¹ã€</li>
<li>$d_k$: Keyã®æ¬¡å…ƒï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å› å­ï¼‰</li>
</ul>

<p><h3>ç›´æ„Ÿçš„ç†è§£</h3></p>

<p><strong>å›³æ›¸é¤¨ã®ä¾‹ãˆ</strong>:</p>
<ul>
<li><strong>Query</strong>: ã€Œæ©Ÿæ¢°å­¦ç¿’ã®æœ¬ã‚’æ¢ã—ã¦ã„ã‚‹ã€</li>
<li><strong>Key</strong>: å„æœ¬ã®ç›®æ¬¡ãƒ»ã‚¿ã‚¤ãƒˆãƒ«</li>
<li><strong>Value</strong>: æœ¬ã®å®Ÿéš›ã®å†…å®¹</li>
<li><strong>Attention</strong>: é–¢é€£æ€§ãŒé«˜ã„æœ¬ã«ã€Œæ³¨ç›®ã€ã—ã¦èª­ã‚€</li>
</ul>

<p><h3>Pythonå®Ÿè£…: åŸºæœ¬çš„ãªAttention</h3></p>

<p><pre><code class="language-python">import torch</p>
<p>import torch.nn.functional as F</p>

<p>def scaled_dot_product_attention(Q, K, V, mask=None):</p>
<p>    """</p>
<p>    Scaled Dot-Product Attention</p>

<p>    Args:</p>
<p>        Q: Query (batch_size, seq_len, d_k)</p>
<p>        K: Key (batch_size, seq_len, d_k)</p>
<p>        V: Value (batch_size, seq_len, d_v)</p>
<p>        mask: ãƒã‚¹ã‚¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰</p>
<p>    """</p>
<p>    d_k = Q.size(-1)</p>

<p>    <h1>1. Qã¨Kã®å†…ç©ã‚’è¨ˆç®—ï¼ˆé¡ä¼¼åº¦ï¼‰</h1></p>
<p>    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))</p>
<p>    <h1>scores shape: (batch_size, seq_len_q, seq_len_k)</h1></p>

<p>    <h1>2. ãƒã‚¹ã‚¯é©ç”¨ï¼ˆå¿…è¦ãªå ´åˆï¼‰</h1></p>
<p>    if mask is not None:</p>
<p>        scores = scores.masked_fill(mask == 0, -1e9)</p>

<p>    <h1>3. Softmaxã§æ­£è¦åŒ–ï¼ˆAttentioné‡ã¿ï¼‰</h1></p>
<p>    attention_weights = F.softmax(scores, dim=-1)</p>

<p>    <h1>4. Attentioné‡ã¿ã§Valueã‚’é‡ã¿ä»˜ã‘å’Œ</h1></p>
<p>    output = torch.matmul(attention_weights, V)</p>

<p>    return output, attention_weights</p>

<p><h1>ä½¿ç”¨ä¾‹</h1></p>
<p>batch_size, seq_len, d_model = 2, 5, 64</p>
<p>Q = torch.randn(batch_size, seq_len, d_model)</p>
<p>K = torch.randn(batch_size, seq_len, d_model)</p>
<p>V = torch.randn(batch_size, seq_len, d_model)</p>

<p>output, attn_weights = scaled_dot_product_attention(Q, K, V)</p>
<p>print(f"Output shape: {output.shape}")  <h1>(2, 5, 64)</h1></p>
<p>print(f"Attention weights shape: {attn_weights.shape}")  <h1>(2, 5, 5)</h1></p>
<p></code></pre></p>

<p><h3>Attentioné‡ã¿ã®å¯è¦–åŒ–</h3></p>

<p><pre><code class="language-python">import matplotlib.pyplot as plt</p>
<p>import seaborn as sns</p>

<p>def visualize_attention(attention_weights, tokens=None):</p>
<p>    """</p>
<p>    Attentioné‡ã¿ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–</p>

<p>    Args:</p>
<p>        attention_weights: (seq_len, seq_len)ã®Attentioné‡ã¿</p>
<p>        tokens: ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰</p>
<p>    """</p>
<p>    plt.figure(figsize=(8, 6))</p>

<p>    <h1>æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã®æœ€åˆã®ãƒ˜ãƒƒãƒ‰ã®Attentioné‡ã¿ã‚’å–å¾—</h1></p>
<p>    attn = attention_weights[0].detach().numpy()</p>

<p>    sns.heatmap(attn, cmap='YlOrRd', cbar=True, square=True,</p>
<p>                xticklabels=tokens if tokens else range(attn.shape[0]),</p>
<p>                yticklabels=tokens if tokens else range(attn.shape[0]))</p>

<p>    plt.xlabel('Key (å‚ç…§å…ˆ)')</p>
<p>    plt.ylabel('Query (æ³¨ç›®å…ƒ)')</p>
<p>    plt.title('Attention Weights')</p>
<p>    plt.tight_layout()</p>
<p>    plt.show()</p>

<p><h1>ä½¿ç”¨ä¾‹</h1></p>
<p>tokens = ['H', 'C', 'C', 'O', 'H']</p>
<p>visualize_attention(attn_weights, tokens)</p>
<p></code></pre></p>

<p>---</p>

<p><h2>1.3 Self-Attention: è‡ªå·±æ³¨æ„æ©Ÿæ§‹</h2></p>

<p><h3>Self-Attentionã¨ã¯</h3></p>

<p><strong>å®šç¾©</strong>: å…¥åŠ›ç³»åˆ—è‡ªèº«ã«å¯¾ã—ã¦Attentionã‚’é©ç”¨ã™ã‚‹ä»•çµ„ã¿</p>

<p><strong>ç‰¹å¾´</strong>:</p>
<ul>
<li>Queryã€Keyã€Valueã™ã¹ã¦åŒã˜å…¥åŠ›ã‹ã‚‰ç”Ÿæˆ</li>
<li>ç³»åˆ—å†…ã®ä»»æ„ã®2è¦ç´ é–“ã®é–¢ä¿‚ã‚’ç›´æ¥ãƒ¢ãƒ‡ãƒ«åŒ–</li>
<li>ä½ç½®ã«é–¢ã‚ã‚‰ãšã€é–¢é€£æ€§ã®é«˜ã„è¦ç´ ã«æ³¨ç›®</li>
</ul>

<p><h3>åˆ†å­ã«ãŠã‘ã‚‹Self-Attentionã®ä¾‹</h3></p>

<p><strong>ãƒ¡ã‚¿ãƒãƒ¼ãƒ« (CHâ‚ƒOH)ã®ä¾‹</strong>:</p>
<p><pre><code class="language-python"><h1>åŸå­: C, H, H, H, O, H</h1></p>
<p><h1>Self-Attentionã§å„åŸå­ãŒä»–ã®åŸå­ã¨ã®é–¢ä¿‚ã‚’å­¦ç¿’</h1></p>
<p><h1>ä¾‹: OåŸå­ã¯CåŸå­ã¨å¼·ã„é–¢ä¿‚æ€§ã‚’æŒã¤</h1></p>
<p></code></pre></p>

<p><h3>Self-Attentionå®Ÿè£…</h3></p>

<p><pre><code class="language-python">import torch.nn as nn</p>

<p>class SelfAttention(nn.Module):</p>
<p>    def __init__(self, d_model):</p>
<p>        super(SelfAttention, self).__init__()</p>
<p>        self.d_model = d_model</p>

<p>        <h1>Q, K, Vã¸ã®ç·šå½¢å¤‰æ›</h1></p>
<p>        self.W_q = nn.Linear(d_model, d_model)</p>
<p>        self.W_k = nn.Linear(d_model, d_model)</p>
<p>        self.W_v = nn.Linear(d_model, d_model)</p>

<p>    def forward(self, x):</p>
<p>        """</p>
<p>        Args:</p>
<p>            x: (batch_size, seq_len, d_model)</p>
<p>        """</p>
<p>        <h1>Q, K, Vã‚’ç”Ÿæˆ</h1></p>
<p>        Q = self.W_q(x)</p>
<p>        K = self.W_k(x)</p>
<p>        V = self.W_v(x)</p>

<p>        <h1>Scaled Dot-Product Attention</h1></p>
<p>        output, attn_weights = scaled_dot_product_attention(Q, K, V)</p>

<p>        return output, attn_weights</p>

<p><h1>ä½¿ç”¨ä¾‹</h1></p>
<p>d_model = 128</p>
<p>seq_len = 10</p>
<p>batch_size = 4</p>

<p>self_attn = SelfAttention(d_model)</p>
<p>x = torch.randn(batch_size, seq_len, d_model)</p>
<p>output, attn_weights = self_attn(x)</p>

<p>print(f"Input shape: {x.shape}")          <h1>(4, 10, 128)</h1></p>
<p>print(f"Output shape: {output.shape}")    <h1>(4, 10, 128)</h1></p>
<p>print(f"Attention shape: {attn_weights.shape}")  <h1>(4, 10, 10)</h1></p>
<p></code></pre></p>

<p>---</p>

<p><h2>1.4 Multi-Head Attention: å¤šé ­æ³¨æ„æ©Ÿæ§‹</h2></p>

<p><h3>ãªãœMulti-HeadãŒå¿…è¦ã‹</h3></p>

<p><strong>å˜ä¸€ã®Attentionãƒ˜ãƒƒãƒ‰ã®é™ç•Œ</strong>:</p>
<ul>
<li>1ã¤ã®è¦–ç‚¹ã‹ã‚‰ã—ã‹é–¢ä¿‚æ€§ã‚’è¦‹ã‚‰ã‚Œãªã„</li>
<li>è¤‡é›‘ãªé–¢ä¿‚æ€§ï¼ˆåŒ–å­¦çµåˆã€ç«‹ä½“é…åº§ãªã©ï¼‰ã‚’æ‰ãˆãã‚Œãªã„</li>
</ul>

<p><strong>Multi-Head Attentionã®åˆ©ç‚¹</strong>:</p>
<ul>
<li>è¤‡æ•°ã®ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰é–¢ä¿‚æ€§ã‚’å­¦ç¿’</li>
<li>å„ãƒ˜ãƒƒãƒ‰ãŒç•°ãªã‚‹ç‰¹å¾´ï¼ˆçµåˆã€è·é›¢ã€è§’åº¦ãªã©ï¼‰ã‚’æ‰ãˆã‚‹</li>
<li>ã‚ˆã‚Šè±Šã‹ãªè¡¨ç¾ãŒå¯èƒ½</li>
</ul>

<p><h3>æ•°å¼</h3></p>

<p>$$</p>
<p>\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O</p>
<p>$$</p>

<p>where $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$</p>

<p><h3>å®Ÿè£…</h3></p>

<p><pre><code class="language-python">class MultiHeadAttention(nn.Module):</p>
<p>    def __init__(self, d_model, num_heads):</p>
<p>        super(MultiHeadAttention, self).__init__()</p>
<p>        assert d_model % num_heads == 0, "d_modelã¯num_headsã§å‰²ã‚Šåˆ‡ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"</p>

<p>        self.d_model = d_model</p>
<p>        self.num_heads = num_heads</p>
<p>        self.d_k = d_model // num_heads</p>

<p>        <h1>Q, K, Vå¤‰æ›</h1></p>
<p>        self.W_q = nn.Linear(d_model, d_model)</p>
<p>        self.W_k = nn.Linear(d_model, d_model)</p>
<p>        self.W_v = nn.Linear(d_model, d_model)</p>

<p>        <h1>å‡ºåŠ›å¤‰æ›</h1></p>
<p>        self.W_o = nn.Linear(d_model, d_model)</p>

<p>    def forward(self, x, mask=None):</p>
<p>        batch_size = x.size(0)</p>

<p>        <h1>1. Q, K, Vã‚’ç”Ÿæˆã—ã¦ã€ãƒ˜ãƒƒãƒ‰ã”ã¨ã«åˆ†å‰²</h1></p>
<p>        Q = self.W_q(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)</p>
<p>        K = self.W_k(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)</p>
<p>        V = self.W_v(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)</p>
<p>        <h1>Shape: (batch_size, num_heads, seq_len, d_k)</h1></p>

<p>        <h1>2. å„ãƒ˜ãƒƒãƒ‰ã§Scaled Dot-Product Attention</h1></p>
<p>        output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)</p>
<p>        <h1>output: (batch_size, num_heads, seq_len, d_k)</h1></p>

<p>        <h1>3. ãƒ˜ãƒƒãƒ‰ã‚’é€£çµ</h1></p>
<p>        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)</p>
<p>        <h1>Shape: (batch_size, seq_len, d_model)</h1></p>

<p>        <h1>4. å‡ºåŠ›å¤‰æ›</h1></p>
<p>        output = self.W_o(output)</p>

<p>        return output, attn_weights</p>

<p><h1>ä½¿ç”¨ä¾‹</h1></p>
<p>d_model = 512</p>
<p>num_heads = 8</p>
<p>seq_len = 20</p>
<p>batch_size = 2</p>

<p>mha = MultiHeadAttention(d_model, num_heads)</p>
<p>x = torch.randn(batch_size, seq_len, d_model)</p>
<p>output, attn_weights = mha(x)</p>

<p>print(f"Input shape: {x.shape}")          <h1>(2, 20, 512)</h1></p>
<p>print(f"Output shape: {output.shape}")    <h1>(2, 20, 512)</h1></p>
<p>print(f"Attention shape: {attn_weights.shape}")  <h1>(2, 8, 20, 20)</h1></p>
<p></code></pre></p>

<p>---</p>

<p><h2>1.5 Positional Encoding: ä½ç½®æƒ…å ±ã®åŸ‹ã‚è¾¼ã¿</h2></p>

<p><h3>ãªãœå¿…è¦ã‹</h3></p>

<p><strong>å•é¡Œ</strong>: Self-Attentionã«ã¯é †åºã®æ¦‚å¿µãŒãªã„</p>
<ul>
<li>"H-C-O" ã¨ "O-C-H" ã‚’åŒºåˆ¥ã§ããªã„</li>
<li>åˆ†å­ã‚„ææ–™ã§ã¯åŸå­ã®é…ç½®é †åºãŒé‡è¦</li>
</ul>

<p><strong>è§£æ±ºç­–</strong>: Positional Encodingã§ä½ç½®æƒ…å ±ã‚’è¿½åŠ </p>

<p><h3>æ•°å¼</h3></p>

<p>$$</p>
<p>PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)</p>
<p>$$</p>

<p>$$</p>
<p>PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)</p>
<p>$$</p>

<p><h3>å®Ÿè£…</h3></p>

<p><pre><code class="language-python">class PositionalEncoding(nn.Module):</p>
<p>    def __init__(self, d_model, max_len=5000):</p>
<p>        super(PositionalEncoding, self).__init__()</p>

<p>        <h1>ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¡Œåˆ—ã‚’ä½œæˆ</h1></p>
<p>        pe = torch.zeros(max_len, d_model)</p>
<p>        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)</p>
<p>        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))</p>

<p>        pe[:, 0::2] = torch.sin(position * div_term)</p>
<p>        pe[:, 1::2] = torch.cos(position * div_term)</p>

<p>        pe = pe.unsqueeze(0)  <h1>(1, max_len, d_model)</h1></p>
<p>        self.register_buffer('pe', pe)</p>

<p>    def forward(self, x):</p>
<p>        """</p>
<p>        Args:</p>
<p>            x: (batch_size, seq_len, d_model)</p>
<p>        """</p>
<p>        seq_len = x.size(1)</p>
<p>        x = x + self.pe[:, :seq_len, :]</p>
<p>        return x</p>

<p><h1>ä½¿ç”¨ä¾‹ã¨å¯è¦–åŒ–</h1></p>
<p>d_model = 128</p>
<p>max_len = 100</p>

<p>pos_enc = PositionalEncoding(d_model, max_len)</p>

<p><h1>ãƒ€ãƒŸãƒ¼å…¥åŠ›</h1></p>
<p>x = torch.zeros(1, 50, d_model)</p>
<p>output = pos_enc(x)</p>

<p><h1>å¯è¦–åŒ–</h1></p>
<p>plt.figure(figsize=(12, 4))</p>
<p>plt.plot(pos_enc.pe[0, :50, :8].numpy())</p>
<p>plt.xlabel('Position')</p>
<p>plt.ylabel('Encoding Value')</p>
<p>plt.title('Positional Encoding (first 8 dimensions)')</p>
<p>plt.legend([f'dim {i}' for i in range(8)])</p>
<p>plt.tight_layout()</p>
<p>plt.show()</p>
<p></code></pre></p>

<p>---</p>

<p><h2>1.6 Transformerã¨BERT/GPT</h2></p>

<p><h3>Transformerå…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h3></p>

<p><pre><code class="language-mermaid">graph TB</p>
<p>    subgraph Encoder</p>
<p>        E1[Input Embedding] --> E2[Positional Encoding]</p>
<p>        E2 --> E3[Multi-Head Attention]</p>
<p>        E3 --> E4[Add & Norm]</p>
<p>        E4 --> E5[Feed Forward]</p>
<p>        E5 --> E6[Add & Norm]</p>
<p>    end</p>

<p>    subgraph Decoder</p>
<p>        D1[Output Embedding] --> D2[Positional Encoding]</p>
<p>        D2 --> D3[Masked Multi-Head Attention]</p>
<p>        D3 --> D4[Add & Norm]</p>
<p>        D4 --> D5[Multi-Head Attention]</p>
<p>        D5 --> D6[Add & Norm]</p>
<p>        D6 --> D7[Feed Forward]</p>
<p>        D7 --> D8[Add & Norm]</p>
<p>    end</p>

<p>    E6 -.Encoderå‡ºåŠ›.-> D5</p>
<p>    D8 --> O[Output]</p>

<p>    style E3 fill:#e1f5ff</p>
<p>    style D3 fill:#ffe1e1</p>
<p>    style D5 fill:#e1ffe1</p>
<p></code></pre></p>

<p><h3>BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰</h3></p>

<p><strong>ç‰¹å¾´</strong>:</p>
<ul>
<li><strong>Encoderã®ã¿</strong>ä½¿ç”¨</li>
<li><strong>åŒæ–¹å‘</strong>ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç†è§£</li>
<li><strong>äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯</strong>: Masked Language Model (MLM) + Next Sentence Prediction (NSP)</li>
<li><strong>ç”¨é€”</strong>: åˆ†é¡ã€ç‰¹å¾´æŠ½å‡ºã€è³ªå•å¿œç­”</li>
</ul>

<p><strong>ææ–™ç§‘å­¦ã§ã®å¿œç”¨</strong>:</p>
<ul>
<li>MatBERT: ææ–™ã®çµ„æˆå¼ã‹ã‚‰ç‰¹æ€§äºˆæ¸¬</li>
<li>ChemBERTa: åˆ†å­SMILESè¡¨ç¾å­¦ç¿’</li>
</ul>

<p><h3>GPTï¼ˆGenerative Pre-trained Transformerï¼‰</h3></p>

<p><strong>ç‰¹å¾´</strong>:</p>
<ul>
<li><strong>Decoderã®ã¿</strong>ä½¿ç”¨</li>
<li><strong>å˜æ–¹å‘</strong>ï¼ˆå·¦ã‹ã‚‰å³ï¼‰ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ</li>
<li><strong>äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯</strong>: æ¬¡ã®å˜èªäºˆæ¸¬</li>
<li><strong>ç”¨é€”</strong>: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã€å¯¾è©±ã€å‰µé€ çš„ã‚¿ã‚¹ã‚¯</li>
</ul>

<p><strong>ææ–™ç§‘å­¦ã§ã®å¿œç”¨</strong>:</p>
<ul>
<li>åˆ†å­ç”Ÿæˆï¼ˆSMILESæ–‡å­—åˆ—ç”Ÿæˆï¼‰</li>
<li>ææ–™è¨˜è¿°æ–‡ã®è‡ªå‹•ç”Ÿæˆ</li>
<li>åˆæˆçµŒè·¯ã®ææ¡ˆ</li>
</ul>

<p>---</p>

<p><h2>1.7 ææ–™ç§‘å­¦ã§ã®æˆåŠŸäº‹ä¾‹</h2></p>

<p><h3>1. ChemBERTa: åˆ†å­è¡¨ç¾å­¦ç¿’</h3></p>

<p><strong>æ¦‚è¦</strong>: SMILESã‚’BERTã§å­¦ç¿’</p>
<p><pre><code class="language-python"><h1>åˆ†å­SMILES: CC(C)Cc1ccc(cc1)C(C)C(=O)O (ã‚¤ãƒ–ãƒ—ãƒ­ãƒ•ã‚§ãƒ³)</h1></p>
<p><h1>ChemBERTaã§åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ› â†’ ç‰¹æ€§äºˆæ¸¬</h1></p>
<p></code></pre></p>

<p><strong>æˆæœ</strong>:</p>
<ul>
<li>å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®é«˜ç²¾åº¦äºˆæ¸¬</li>
<li>è»¢ç§»å­¦ç¿’ã«ã‚ˆã‚Šé–‹ç™ºæœŸé–“çŸ­ç¸®</li>
<li>è§£é‡ˆå¯èƒ½æ€§ï¼ˆAttentionã§é‡è¦éƒ¨åˆ†å¯è¦–åŒ–ï¼‰</li>
</ul>

<p><h3>2. Matformer: ææ–™ç‰¹æ€§äºˆæ¸¬</h3></p>

<p><strong>æ¦‚è¦</strong>: çµæ™¶æ§‹é€ ã‚’Transformerã§å‡¦ç†</p>
<p><pre><code class="language-python"><h1>å…¥åŠ›: åŸå­åº§æ¨™ã€åŸå­ç•ªå·ã€æ ¼å­å®šæ•°</h1></p>
<p><h1>å‡ºåŠ›: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã€å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼</h1></p>
<p></code></pre></p>

<p><strong>æˆæœ</strong>:</p>
<ul>
<li>Materials Projectãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦</li>
<li>GNNã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½</li>
<li>è¨ˆç®—åŠ¹ç‡ãŒè‰¯ã„</li>
</ul>

<p><h3>3. æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹åˆ†å­ç”Ÿæˆ</h3></p>

<p><strong>æ¦‚è¦</strong>: æ¡ä»¶ä»˜ãæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã§æ–°è¦åˆ†å­ç”Ÿæˆ</p>
<p><pre><code class="language-python"><h1>æ¡ä»¶: æº¶è§£åº¦ > 5 mg/mL, LogP < 3</h1></p>
<p><h1>ç”Ÿæˆ: æ¡ä»¶ã‚’æº€ãŸã™åˆ†å­SMILES</h1></p>
<p></code></pre></p>

<p><strong>æˆæœ</strong>:</p>
<ul>
<li>å‰µè–¬ã§æœ‰æœ›ãªå€™è£œåˆ†å­ç™ºè¦‹</li>
<li>å¾“æ¥æ‰‹æ³•ã‚ˆã‚Šå¤šæ§˜æ€§ãŒé«˜ã„</li>
<li>åˆæˆå¯èƒ½æ€§ã‚‚è€ƒæ…®</li>
</ul>

<p>---</p>

<p><h2>1.8 ã¾ã¨ã‚</h2></p>

<p><h3>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</h3></p>

<ol>
<li><strong>Attentionæ©Ÿæ§‹</strong>: ç³»åˆ—å†…ã®ä»»æ„ã®è¦ç´ é–“ã®é–¢ä¿‚ã‚’ç›´æ¥ãƒ¢ãƒ‡ãƒ«åŒ–</li>
<li><strong>Self-Attention</strong>: å…¥åŠ›ç³»åˆ—è‡ªèº«ã«å¯¾ã™ã‚‹Attention</li>
<li><strong>Multi-Head Attention</strong>: è¤‡æ•°ã®è¦–ç‚¹ã‹ã‚‰é–¢ä¿‚æ€§ã‚’å­¦ç¿’</li>
<li><strong>Positional Encoding</strong>: ä½ç½®æƒ…å ±ã‚’åŸ‹ã‚è¾¼ã¿</li>
<li><strong>BERT/GPT</strong>: Transformer based ã®ä»£è¡¨çš„äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«</li>
<li><strong>ææ–™ç§‘å­¦å¿œç”¨</strong>: åˆ†å­ãƒ»ææ–™è¡¨ç¾å­¦ç¿’ã€ç‰¹æ€§äºˆæ¸¬ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«</li>
</ol>

<p><h3>æ¬¡ç« ã¸ã®æº–å‚™</h3></p>

<p>ç¬¬2ç« ã§ã¯ã€ææ–™ç§‘å­¦ã«ç‰¹åŒ–ã—ãŸTransformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆMatformerã€CrystalFormerã€ChemBERTaï¼‰ã‚’è©³ã—ãå­¦ã³ã¾ã™ã€‚</p>

<p>---</p>

<p><h2>ğŸ“ æ¼”ç¿’å•é¡Œ</h2></p>

<p><h3>å•é¡Œ1: åŸºç¤ç†è§£ï¼ˆæ¦‚å¿µï¼‰</h3></p>
<p>Attentionæ©Ÿæ§‹ã«ãŠã‘ã‚‹ Queryã€Keyã€Value ã®å½¹å‰²ã‚’ã€å›³æ›¸é¤¨ã®ä¾‹ãˆä»¥å¤–ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<p><details></p>
<p><summary>è§£ç­”ä¾‹</summary></p>

<p><strong>æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®ä¾‹ãˆ</strong>:</p>
<ul>
<li><strong>Query</strong>: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰</li>
<li><strong>Key</strong>: å„Webãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€è¦ç´„ï¼‰</li>
<li><strong>Value</strong>: Webãƒšãƒ¼ã‚¸ã®å®Ÿéš›ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„</li>
<li><strong>Attention</strong>: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã®é–¢é€£æ€§ãŒé«˜ã„ãƒšãƒ¼ã‚¸ã‚’ä¸Šä½è¡¨ç¤º</li>
</ul>

<p><strong>åˆ†å­ã®ä¾‹ãˆ</strong>:</p>
<ul>
<li><strong>Query</strong>: ã‚ã‚‹åŸå­ãŒã€Œã©ã®åŸå­ã¨ç›¸äº’ä½œç”¨ã—ãŸã„ã‹ã€</li>
<li><strong>Key</strong>: å„åŸå­ã®ç‰¹å¾´ï¼ˆåŸå­ç•ªå·ã€é›»è·ã€ä½ç½®ï¼‰</li>
<li><strong>Value</strong>: å„åŸå­ã®è©³ç´°ãªæƒ…å ±</li>
<li><strong>Attention</strong>: åŒ–å­¦çµåˆã‚„ç›¸äº’ä½œç”¨ã®å¼·ã•ã‚’è¡¨ç¾</li>
</ul>
<p></details></p>

<p><h3>å•é¡Œ2: å®Ÿè£…ï¼ˆã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰</h3></p>
<p>ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®ç©ºæ¬„ã‚’åŸ‹ã‚ã¦ã€Simple Attentionï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãªã—ã€ãƒã‚¹ã‚¯ãªã—ï¼‰ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚</p>

<p><pre><code class="language-python">def simple_attention(Q, K, V):</p>
<p>    """</p>
<p>    ã‚·ãƒ³ãƒ—ãƒ«ãªAttentionæ©Ÿæ§‹</p>

<p>    Args:</p>
<p>        Q: Query (batch_size, seq_len, d_k)</p>
<p>        K: Key (batch_size, seq_len, d_k)</p>
<p>        V: Value (batch_size, seq_len, d_v)</p>

<p>    Returns:</p>
<p>        output: (batch_size, seq_len, d_v)</p>
<p>        attention_weights: (batch_size, seq_len, seq_len)</p>
<p>    """</p>
<p>    <h1>1. Qã¨Kã®å†…ç©ã‚’è¨ˆç®—</h1></p>
<p>    scores = torch.matmul(______, ______.transpose(-2, -1))</p>

<p>    <h1>2. Softmaxã§æ­£è¦åŒ–</h1></p>
<p>    attention_weights = F.softmax(______, dim=-1)</p>

<p>    <h1>3. Attentioné‡ã¿ã§Valueã‚’é‡ã¿ä»˜ã‘å’Œ</h1></p>
<p>    output = torch.matmul(______, ______)</p>

<p>    return output, attention_weights</p>
<p></code></pre></p>

<p><details></p>
<p><summary>è§£ç­”ä¾‹</summary></p>

<p><pre><code class="language-python">def simple_attention(Q, K, V):</p>
<p>    <h1>1. Qã¨Kã®å†…ç©ã‚’è¨ˆç®—</h1></p>
<p>    scores = torch.matmul(Q, K.transpose(-2, -1))</p>

<p>    <h1>2. Softmaxã§æ­£è¦åŒ–</h1></p>
<p>    attention_weights = F.softmax(scores, dim=-1)</p>

<p>    <h1>3. Attentioné‡ã¿ã§Valueã‚’é‡ã¿ä»˜ã‘å’Œ</h1></p>
<p>    output = torch.matmul(attention_weights, V)</p>

<p>    return output, attention_weights</p>
<p></code></pre></p>
<p></details></p>

<p><h3>å•é¡Œ3: å¿œç”¨ï¼ˆè€ƒå¯Ÿï¼‰</h3></p>
<p>åˆ†å­ "CCO"ï¼ˆã‚¨ã‚¿ãƒãƒ¼ãƒ«ï¼‰ã«ãŠã‘ã‚‹Self-Attentionã‚’è€ƒãˆã¾ã™ã€‚ä»¥ä¸‹ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ï¼š</p>

<ol>
<li>ã©ã®åŸå­é–“ã®Attentioné‡ã¿ãŒæœ€ã‚‚é«˜ããªã‚‹ã¨äºˆæƒ³ã•ã‚Œã¾ã™ã‹ï¼Ÿ</li>
<li>ãã®ç†ç”±ã‚’åŒ–å­¦çš„è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</li>
<li>Multi-Head Attentionã§ã¯ã€å„ãƒ˜ãƒƒãƒ‰ãŒã©ã®ã‚ˆã†ãªç•°ãªã‚‹æƒ…å ±ã‚’æ‰ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ</li>
</ol>

<p><details></p>
<p><summary>è§£ç­”ä¾‹</summary></p>

<ol>
<li><strong>æœ€ã‚‚é«˜ã„Attentioné‡ã¿</strong>: C-Cçµåˆã€C-Oçµåˆ</li>
</ol>

<ol>
<li><strong>åŒ–å­¦çš„ç†ç”±</strong>:</li>
</ol>
<p>   - å…±æœ‰çµåˆã«ã‚ˆã‚Šå¼·ã„ç›¸äº’ä½œç”¨ãŒã‚ã‚‹</p>
<p>   - é›»å­ã®å…±æœ‰ã«ã‚ˆã‚Šé›»å­å¯†åº¦ãŒé«˜ã„</p>
<p>   - OåŸå­ã¯CåŸå­ã¨æ¥µæ€§çµåˆã‚’å½¢æˆ</p>

<ol>
<li><strong>å„ãƒ˜ãƒƒãƒ‰ãŒæ‰ãˆã‚‹æƒ…å ±ã®ä¾‹</strong>:</li>
</ol>
<p>   - <strong>ãƒ˜ãƒƒãƒ‰1</strong>: åŒ–å­¦çµåˆï¼ˆ1æ¬¡çµåˆï¼‰</p>
<p>   - <strong>ãƒ˜ãƒƒãƒ‰2</strong>: 2æ¬¡çµåˆï¼ˆC-C-Oè§’åº¦ï¼‰</p>
<p>   - <strong>ãƒ˜ãƒƒãƒ‰3</strong>: é›»å­å¯†åº¦åˆ†å¸ƒ</p>
<p>   - <strong>ãƒ˜ãƒƒãƒ‰4</strong>: åŸå­ã®ç¨®é¡ï¼ˆC vs O vs Hï¼‰</p>
<p>   - <strong>ãƒ˜ãƒƒãƒ‰5</strong>: ç«‹ä½“é…åº§æƒ…å ±</p>
<p>   - <strong>ãƒ˜ãƒƒãƒ‰6</strong>: æ¥µæ€§ç›¸äº’ä½œç”¨</p>

<p>   å„ãƒ˜ãƒƒãƒ‰ãŒç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰åˆ†å­ã‚’ç†è§£ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šè±Šã‹ãªè¡¨ç¾ãŒå¯èƒ½ã«ãªã‚‹ã€‚</p>
<p></details></p>

<p>---</p>

<p><h2>ğŸ”— å‚è€ƒè³‡æ–™</h2></p>

<p><h3>è«–æ–‡</h3></p>
<ul>
<li>Vaswani et al. (2017) "Attention Is All You Need" <a href="https://arxiv.org/abs/1706.03762">arXiv:1706.03762</a></li>
<li>Devlin et al. (2019) "BERT: Pre-training of Deep Bidirectional Transformers" <a href="https://arxiv.org/abs/1810.04805">arXiv:1810.04805</a></li>
</ul>

<p><h3>ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«</h3></p>
<ul>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
<li><a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html">PyTorch Transformer Tutorial</a></li>
</ul>

<p><h3>æ¬¡ç« </h3></p>
<p><strong><a href="chapter-2.md">ç¬¬2ç« : ææ–™å‘ã‘Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</a></strong> ã§ã€Matformerã€ChemBERTaãªã©ææ–™ç§‘å­¦ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ã³ã¾ã™ã€‚</p>

<p>---</p>

<p><strong>ä½œæˆè€…</strong>: æ©‹æœ¬ä½‘ä»‹ï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
<p><strong>æœ€çµ‚æ›´æ–°</strong>: 2025å¹´10æœˆ17æ—¥</p>


        
        <div class="navigation">
            <a href="chapter-2.html" class="nav-button">æ¬¡ç« : ç¬¬2ç«  â†’</a>
            <a href="index.html" class="nav-button">â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
            
        </div>
    
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimoto(æ±åŒ—å¤§å­¦)</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>