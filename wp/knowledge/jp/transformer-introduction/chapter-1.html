<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Untitled</h1>
            
            <div class="meta">
                <span class="meta-item">📖 読了時間: 20-30分</span>
                <span class="meta-item">📊 難易度: 中級</span>
                <span class="meta-item">💻 コード例: 5個</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p><h1>第1章: Transformer革命と材料科学</h1></p>

<p><strong>学習時間</strong>: 20-30分 | <strong>難易度</strong>: 中級</p>

<p><h2>📋 この章で学ぶこと</h2></p>

<ul>
<li>Attention機構の原理と数学的理解</li>
<li>Self-AttentionとMulti-Head Attentionの仕組み</li>
<li>TransformerがRNN/CNNより優れている理由</li>
<li>BERT、GPTの基本構造と違い</li>
<li>材料科学での成功事例</li>
</ul>

<p>---</p>

<p><h2>1.1 なぜTransformerが革命を起こしたのか</h2></p>

<p><h3>従来のRNN/CNNの限界</h3></p>

<p><strong>RNN（Recurrent Neural Network）の問題</strong>:</p>
<ul>
<li>長い系列での勾配消失・爆発</li>
<li>並列化が困難（逐次処理が必要）</li>
<li>長期依存関係の捕捉が難しい</li>
</ul>

<p><strong>CNN（Convolutional Neural Network）の問題</strong>:</p>
<ul>
<li>局所的な特徴しか捉えられない</li>
<li>長距離の関係性を捉えるには深い層が必要</li>
<li>分子・材料のような不規則な構造には不向き</li>
</ul>

<p><h3>Transformerの革新性</h3></p>

<p><strong>2017年、"Attention Is All You Need"論文で登場</strong>:</p>
<ul>
<li>✅ <strong>全要素間の関係を直接モデル化</strong>（Attention機構）</li>
<li>✅ <strong>完全並列化可能</strong>（GPUを最大限活用）</li>
<li>✅ <strong>長距離依存関係を効率的に捕捉</strong></li>
<li>✅ <strong>解釈性</strong>（Attention重みで重要な部分を可視化）</li>
</ul>

<p><pre><code class="language-mermaid">graph LR</p>
<p>    A[入力系列] --> B[Self-Attention]</p>
<p>    B --> C[Feed Forward]</p>
<p>    C --> D[出力]</p>

<p>    B -.すべての要素間の関係を計算.-> B</p>

<p>    style B fill:#e1f5ff</p>
<p></code></pre></p>

<p>---</p>

<p><h2>1.2 Attention機構の原理</h2></p>

<p><h3>Attention機構とは</h3></p>

<p><strong>基本概念</strong>: 入力の中で「どこに注目すべきか」を学習する仕組み</p>

<p><strong>数式</strong>:</p>
<p>$$</p>
<p>\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V</p>
<p>$$</p>

<ul>
<li><strong>Q (Query)</strong>: 「何を探しているか」</li>
<li><strong>K (Key)</strong>: 「何を持っているか」</li>
<li><strong>V (Value)</strong>: 「実際の内容」</li>
<li>$d_k$: Keyの次元（スケーリング因子）</li>
</ul>

<p><h3>直感的理解</h3></p>

<p><strong>図書館の例え</strong>:</p>
<ul>
<li><strong>Query</strong>: 「機械学習の本を探している」</li>
<li><strong>Key</strong>: 各本の目次・タイトル</li>
<li><strong>Value</strong>: 本の実際の内容</li>
<li><strong>Attention</strong>: 関連性が高い本に「注目」して読む</li>
</ul>

<p><h3>Python実装: 基本的なAttention</h3></p>

<p><pre><code class="language-python">import torch</p>
<p>import torch.nn.functional as F</p>

<p>def scaled_dot_product_attention(Q, K, V, mask=None):</p>
<p>    """</p>
<p>    Scaled Dot-Product Attention</p>

<p>    Args:</p>
<p>        Q: Query (batch_size, seq_len, d_k)</p>
<p>        K: Key (batch_size, seq_len, d_k)</p>
<p>        V: Value (batch_size, seq_len, d_v)</p>
<p>        mask: マスク（オプション）</p>
<p>    """</p>
<p>    d_k = Q.size(-1)</p>

<p>    <h1>1. QとKの内積を計算（類似度）</h1></p>
<p>    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))</p>
<p>    <h1>scores shape: (batch_size, seq_len_q, seq_len_k)</h1></p>

<p>    <h1>2. マスク適用（必要な場合）</h1></p>
<p>    if mask is not None:</p>
<p>        scores = scores.masked_fill(mask == 0, -1e9)</p>

<p>    <h1>3. Softmaxで正規化（Attention重み）</h1></p>
<p>    attention_weights = F.softmax(scores, dim=-1)</p>

<p>    <h1>4. Attention重みでValueを重み付け和</h1></p>
<p>    output = torch.matmul(attention_weights, V)</p>

<p>    return output, attention_weights</p>

<p><h1>使用例</h1></p>
<p>batch_size, seq_len, d_model = 2, 5, 64</p>
<p>Q = torch.randn(batch_size, seq_len, d_model)</p>
<p>K = torch.randn(batch_size, seq_len, d_model)</p>
<p>V = torch.randn(batch_size, seq_len, d_model)</p>

<p>output, attn_weights = scaled_dot_product_attention(Q, K, V)</p>
<p>print(f"Output shape: {output.shape}")  <h1>(2, 5, 64)</h1></p>
<p>print(f"Attention weights shape: {attn_weights.shape}")  <h1>(2, 5, 5)</h1></p>
<p></code></pre></p>

<p><h3>Attention重みの可視化</h3></p>

<p><pre><code class="language-python">import matplotlib.pyplot as plt</p>
<p>import seaborn as sns</p>

<p>def visualize_attention(attention_weights, tokens=None):</p>
<p>    """</p>
<p>    Attention重みをヒートマップで可視化</p>

<p>    Args:</p>
<p>        attention_weights: (seq_len, seq_len)のAttention重み</p>
<p>        tokens: トークンのリスト（オプション）</p>
<p>    """</p>
<p>    plt.figure(figsize=(8, 6))</p>

<p>    <h1>最初のサンプルの最初のヘッドのAttention重みを取得</h1></p>
<p>    attn = attention_weights[0].detach().numpy()</p>

<p>    sns.heatmap(attn, cmap='YlOrRd', cbar=True, square=True,</p>
<p>                xticklabels=tokens if tokens else range(attn.shape[0]),</p>
<p>                yticklabels=tokens if tokens else range(attn.shape[0]))</p>

<p>    plt.xlabel('Key (参照先)')</p>
<p>    plt.ylabel('Query (注目元)')</p>
<p>    plt.title('Attention Weights')</p>
<p>    plt.tight_layout()</p>
<p>    plt.show()</p>

<p><h1>使用例</h1></p>
<p>tokens = ['H', 'C', 'C', 'O', 'H']</p>
<p>visualize_attention(attn_weights, tokens)</p>
<p></code></pre></p>

<p>---</p>

<p><h2>1.3 Self-Attention: 自己注意機構</h2></p>

<p><h3>Self-Attentionとは</h3></p>

<p><strong>定義</strong>: 入力系列自身に対してAttentionを適用する仕組み</p>

<p><strong>特徴</strong>:</p>
<ul>
<li>Query、Key、Valueすべて同じ入力から生成</li>
<li>系列内の任意の2要素間の関係を直接モデル化</li>
<li>位置に関わらず、関連性の高い要素に注目</li>
</ul>

<p><h3>分子におけるSelf-Attentionの例</h3></p>

<p><strong>メタノール (CH₃OH)の例</strong>:</p>
<p><pre><code class="language-python"><h1>原子: C, H, H, H, O, H</h1></p>
<p><h1>Self-Attentionで各原子が他の原子との関係を学習</h1></p>
<p><h1>例: O原子はC原子と強い関係性を持つ</h1></p>
<p></code></pre></p>

<p><h3>Self-Attention実装</h3></p>

<p><pre><code class="language-python">import torch.nn as nn</p>

<p>class SelfAttention(nn.Module):</p>
<p>    def __init__(self, d_model):</p>
<p>        super(SelfAttention, self).__init__()</p>
<p>        self.d_model = d_model</p>

<p>        <h1>Q, K, Vへの線形変換</h1></p>
<p>        self.W_q = nn.Linear(d_model, d_model)</p>
<p>        self.W_k = nn.Linear(d_model, d_model)</p>
<p>        self.W_v = nn.Linear(d_model, d_model)</p>

<p>    def forward(self, x):</p>
<p>        """</p>
<p>        Args:</p>
<p>            x: (batch_size, seq_len, d_model)</p>
<p>        """</p>
<p>        <h1>Q, K, Vを生成</h1></p>
<p>        Q = self.W_q(x)</p>
<p>        K = self.W_k(x)</p>
<p>        V = self.W_v(x)</p>

<p>        <h1>Scaled Dot-Product Attention</h1></p>
<p>        output, attn_weights = scaled_dot_product_attention(Q, K, V)</p>

<p>        return output, attn_weights</p>

<p><h1>使用例</h1></p>
<p>d_model = 128</p>
<p>seq_len = 10</p>
<p>batch_size = 4</p>

<p>self_attn = SelfAttention(d_model)</p>
<p>x = torch.randn(batch_size, seq_len, d_model)</p>
<p>output, attn_weights = self_attn(x)</p>

<p>print(f"Input shape: {x.shape}")          <h1>(4, 10, 128)</h1></p>
<p>print(f"Output shape: {output.shape}")    <h1>(4, 10, 128)</h1></p>
<p>print(f"Attention shape: {attn_weights.shape}")  <h1>(4, 10, 10)</h1></p>
<p></code></pre></p>

<p>---</p>

<p><h2>1.4 Multi-Head Attention: 多頭注意機構</h2></p>

<p><h3>なぜMulti-Headが必要か</h3></p>

<p><strong>単一のAttentionヘッドの限界</strong>:</p>
<ul>
<li>1つの視点からしか関係性を見られない</li>
<li>複雑な関係性（化学結合、立体配座など）を捉えきれない</li>
</ul>

<p><strong>Multi-Head Attentionの利点</strong>:</p>
<ul>
<li>複数の異なる視点から関係性を学習</li>
<li>各ヘッドが異なる特徴（結合、距離、角度など）を捉える</li>
<li>より豊かな表現が可能</li>
</ul>

<p><h3>数式</h3></p>

<p>$$</p>
<p>\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O</p>
<p>$$</p>

<p>where $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$</p>

<p><h3>実装</h3></p>

<p><pre><code class="language-python">class MultiHeadAttention(nn.Module):</p>
<p>    def __init__(self, d_model, num_heads):</p>
<p>        super(MultiHeadAttention, self).__init__()</p>
<p>        assert d_model % num_heads == 0, "d_modelはnum_headsで割り切れる必要があります"</p>

<p>        self.d_model = d_model</p>
<p>        self.num_heads = num_heads</p>
<p>        self.d_k = d_model // num_heads</p>

<p>        <h1>Q, K, V変換</h1></p>
<p>        self.W_q = nn.Linear(d_model, d_model)</p>
<p>        self.W_k = nn.Linear(d_model, d_model)</p>
<p>        self.W_v = nn.Linear(d_model, d_model)</p>

<p>        <h1>出力変換</h1></p>
<p>        self.W_o = nn.Linear(d_model, d_model)</p>

<p>    def forward(self, x, mask=None):</p>
<p>        batch_size = x.size(0)</p>

<p>        <h1>1. Q, K, Vを生成して、ヘッドごとに分割</h1></p>
<p>        Q = self.W_q(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)</p>
<p>        K = self.W_k(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)</p>
<p>        V = self.W_v(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)</p>
<p>        <h1>Shape: (batch_size, num_heads, seq_len, d_k)</h1></p>

<p>        <h1>2. 各ヘッドでScaled Dot-Product Attention</h1></p>
<p>        output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)</p>
<p>        <h1>output: (batch_size, num_heads, seq_len, d_k)</h1></p>

<p>        <h1>3. ヘッドを連結</h1></p>
<p>        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)</p>
<p>        <h1>Shape: (batch_size, seq_len, d_model)</h1></p>

<p>        <h1>4. 出力変換</h1></p>
<p>        output = self.W_o(output)</p>

<p>        return output, attn_weights</p>

<p><h1>使用例</h1></p>
<p>d_model = 512</p>
<p>num_heads = 8</p>
<p>seq_len = 20</p>
<p>batch_size = 2</p>

<p>mha = MultiHeadAttention(d_model, num_heads)</p>
<p>x = torch.randn(batch_size, seq_len, d_model)</p>
<p>output, attn_weights = mha(x)</p>

<p>print(f"Input shape: {x.shape}")          <h1>(2, 20, 512)</h1></p>
<p>print(f"Output shape: {output.shape}")    <h1>(2, 20, 512)</h1></p>
<p>print(f"Attention shape: {attn_weights.shape}")  <h1>(2, 8, 20, 20)</h1></p>
<p></code></pre></p>

<p>---</p>

<p><h2>1.5 Positional Encoding: 位置情報の埋め込み</h2></p>

<p><h3>なぜ必要か</h3></p>

<p><strong>問題</strong>: Self-Attentionには順序の概念がない</p>
<ul>
<li>"H-C-O" と "O-C-H" を区別できない</li>
<li>分子や材料では原子の配置順序が重要</li>
</ul>

<p><strong>解決策</strong>: Positional Encodingで位置情報を追加</p>

<p><h3>数式</h3></p>

<p>$$</p>
<p>PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)</p>
<p>$$</p>

<p>$$</p>
<p>PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)</p>
<p>$$</p>

<p><h3>実装</h3></p>

<p><pre><code class="language-python">class PositionalEncoding(nn.Module):</p>
<p>    def __init__(self, d_model, max_len=5000):</p>
<p>        super(PositionalEncoding, self).__init__()</p>

<p>        <h1>位置エンコーディング行列を作成</h1></p>
<p>        pe = torch.zeros(max_len, d_model)</p>
<p>        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)</p>
<p>        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))</p>

<p>        pe[:, 0::2] = torch.sin(position * div_term)</p>
<p>        pe[:, 1::2] = torch.cos(position * div_term)</p>

<p>        pe = pe.unsqueeze(0)  <h1>(1, max_len, d_model)</h1></p>
<p>        self.register_buffer('pe', pe)</p>

<p>    def forward(self, x):</p>
<p>        """</p>
<p>        Args:</p>
<p>            x: (batch_size, seq_len, d_model)</p>
<p>        """</p>
<p>        seq_len = x.size(1)</p>
<p>        x = x + self.pe[:, :seq_len, :]</p>
<p>        return x</p>

<p><h1>使用例と可視化</h1></p>
<p>d_model = 128</p>
<p>max_len = 100</p>

<p>pos_enc = PositionalEncoding(d_model, max_len)</p>

<p><h1>ダミー入力</h1></p>
<p>x = torch.zeros(1, 50, d_model)</p>
<p>output = pos_enc(x)</p>

<p><h1>可視化</h1></p>
<p>plt.figure(figsize=(12, 4))</p>
<p>plt.plot(pos_enc.pe[0, :50, :8].numpy())</p>
<p>plt.xlabel('Position')</p>
<p>plt.ylabel('Encoding Value')</p>
<p>plt.title('Positional Encoding (first 8 dimensions)')</p>
<p>plt.legend([f'dim {i}' for i in range(8)])</p>
<p>plt.tight_layout()</p>
<p>plt.show()</p>
<p></code></pre></p>

<p>---</p>

<p><h2>1.6 TransformerとBERT/GPT</h2></p>

<p><h3>Transformer全体アーキテクチャ</h3></p>

<p><pre><code class="language-mermaid">graph TB</p>
<p>    subgraph Encoder</p>
<p>        E1[Input Embedding] --> E2[Positional Encoding]</p>
<p>        E2 --> E3[Multi-Head Attention]</p>
<p>        E3 --> E4[Add & Norm]</p>
<p>        E4 --> E5[Feed Forward]</p>
<p>        E5 --> E6[Add & Norm]</p>
<p>    end</p>

<p>    subgraph Decoder</p>
<p>        D1[Output Embedding] --> D2[Positional Encoding]</p>
<p>        D2 --> D3[Masked Multi-Head Attention]</p>
<p>        D3 --> D4[Add & Norm]</p>
<p>        D4 --> D5[Multi-Head Attention]</p>
<p>        D5 --> D6[Add & Norm]</p>
<p>        D6 --> D7[Feed Forward]</p>
<p>        D7 --> D8[Add & Norm]</p>
<p>    end</p>

<p>    E6 -.Encoder出力.-> D5</p>
<p>    D8 --> O[Output]</p>

<p>    style E3 fill:#e1f5ff</p>
<p>    style D3 fill:#ffe1e1</p>
<p>    style D5 fill:#e1ffe1</p>
<p></code></pre></p>

<p><h3>BERT（Bidirectional Encoder Representations from Transformers）</h3></p>

<p><strong>特徴</strong>:</p>
<ul>
<li><strong>Encoderのみ</strong>使用</li>
<li><strong>双方向</strong>でコンテキストを理解</li>
<li><strong>事前学習タスク</strong>: Masked Language Model (MLM) + Next Sentence Prediction (NSP)</li>
<li><strong>用途</strong>: 分類、特徴抽出、質問応答</li>
</ul>

<p><strong>材料科学での応用</strong>:</p>
<ul>
<li>MatBERT: 材料の組成式から特性予測</li>
<li>ChemBERTa: 分子SMILES表現学習</li>
</ul>

<p><h3>GPT（Generative Pre-trained Transformer）</h3></p>

<p><strong>特徴</strong>:</p>
<ul>
<li><strong>Decoderのみ</strong>使用</li>
<li><strong>単方向</strong>（左から右）でテキスト生成</li>
<li><strong>事前学習タスク</strong>: 次の単語予測</li>
<li><strong>用途</strong>: テキスト生成、対話、創造的タスク</li>
</ul>

<p><strong>材料科学での応用</strong>:</p>
<ul>
<li>分子生成（SMILES文字列生成）</li>
<li>材料記述文の自動生成</li>
<li>合成経路の提案</li>
</ul>

<p>---</p>

<p><h2>1.7 材料科学での成功事例</h2></p>

<p><h3>1. ChemBERTa: 分子表現学習</h3></p>

<p><strong>概要</strong>: SMILESをBERTで学習</p>
<p><pre><code class="language-python"><h1>分子SMILES: CC(C)Cc1ccc(cc1)C(C)C(=O)O (イブプロフェン)</h1></p>
<p><h1>ChemBERTaで埋め込みベクトルに変換 → 特性予測</h1></p>
<p></code></pre></p>

<p><strong>成果</strong>:</p>
<ul>
<li>小規模データでの高精度予測</li>
<li>転移学習により開発期間短縮</li>
<li>解釈可能性（Attentionで重要部分可視化）</li>
</ul>

<p><h3>2. Matformer: 材料特性予測</h3></p>

<p><strong>概要</strong>: 結晶構造をTransformerで処理</p>
<p><pre><code class="language-python"><h1>入力: 原子座標、原子番号、格子定数</h1></p>
<p><h1>出力: バンドギャップ、形成エネルギー</h1></p>
<p></code></pre></p>

<p><strong>成果</strong>:</p>
<ul>
<li>Materials Projectデータで高精度</li>
<li>GNNと同等以上の性能</li>
<li>計算効率が良い</li>
</ul>

<p><h3>3. 拡散モデルによる分子生成</h3></p>

<p><strong>概要</strong>: 条件付き拡散モデルで新規分子生成</p>
<p><pre><code class="language-python"><h1>条件: 溶解度 > 5 mg/mL, LogP < 3</h1></p>
<p><h1>生成: 条件を満たす分子SMILES</h1></p>
<p></code></pre></p>

<p><strong>成果</strong>:</p>
<ul>
<li>創薬で有望な候補分子発見</li>
<li>従来手法より多様性が高い</li>
<li>合成可能性も考慮</li>
</ul>

<p>---</p>

<p><h2>1.8 まとめ</h2></p>

<p><h3>重要ポイント</h3></p>

<ol>
<li><strong>Attention機構</strong>: 系列内の任意の要素間の関係を直接モデル化</li>
<li><strong>Self-Attention</strong>: 入力系列自身に対するAttention</li>
<li><strong>Multi-Head Attention</strong>: 複数の視点から関係性を学習</li>
<li><strong>Positional Encoding</strong>: 位置情報を埋め込み</li>
<li><strong>BERT/GPT</strong>: Transformer based の代表的事前学習モデル</li>
<li><strong>材料科学応用</strong>: 分子・材料表現学習、特性予測、生成モデル</li>
</ol>

<p><h3>次章への準備</h3></p>

<p>第2章では、材料科学に特化したTransformerアーキテクチャ（Matformer、CrystalFormer、ChemBERTa）を詳しく学びます。</p>

<p>---</p>

<p><h2>📝 演習問題</h2></p>

<p><h3>問題1: 基礎理解（概念）</h3></p>
<p>Attention機構における Query、Key、Value の役割を、図書館の例え以外で説明してください。</p>

<p><details></p>
<p><summary>解答例</summary></p>

<p><strong>検索エンジンの例え</strong>:</p>
<ul>
<li><strong>Query</strong>: ユーザーが入力した検索キーワード</li>
<li><strong>Key</strong>: 各Webページのメタデータ（タイトル、要約）</li>
<li><strong>Value</strong>: Webページの実際のコンテンツ</li>
<li><strong>Attention</strong>: 検索キーワードとの関連性が高いページを上位表示</li>
</ul>

<p><strong>分子の例え</strong>:</p>
<ul>
<li><strong>Query</strong>: ある原子が「どの原子と相互作用したいか」</li>
<li><strong>Key</strong>: 各原子の特徴（原子番号、電荷、位置）</li>
<li><strong>Value</strong>: 各原子の詳細な情報</li>
<li><strong>Attention</strong>: 化学結合や相互作用の強さを表現</li>
</ul>
<p></details></p>

<p><h3>問題2: 実装（コーディング）</h3></p>
<p>以下のコードの空欄を埋めて、Simple Attention（スケーリングなし、マスクなし）を実装してください。</p>

<p><pre><code class="language-python">def simple_attention(Q, K, V):</p>
<p>    """</p>
<p>    シンプルなAttention機構</p>

<p>    Args:</p>
<p>        Q: Query (batch_size, seq_len, d_k)</p>
<p>        K: Key (batch_size, seq_len, d_k)</p>
<p>        V: Value (batch_size, seq_len, d_v)</p>

<p>    Returns:</p>
<p>        output: (batch_size, seq_len, d_v)</p>
<p>        attention_weights: (batch_size, seq_len, seq_len)</p>
<p>    """</p>
<p>    <h1>1. QとKの内積を計算</h1></p>
<p>    scores = torch.matmul(______, ______.transpose(-2, -1))</p>

<p>    <h1>2. Softmaxで正規化</h1></p>
<p>    attention_weights = F.softmax(______, dim=-1)</p>

<p>    <h1>3. Attention重みでValueを重み付け和</h1></p>
<p>    output = torch.matmul(______, ______)</p>

<p>    return output, attention_weights</p>
<p></code></pre></p>

<p><details></p>
<p><summary>解答例</summary></p>

<p><pre><code class="language-python">def simple_attention(Q, K, V):</p>
<p>    <h1>1. QとKの内積を計算</h1></p>
<p>    scores = torch.matmul(Q, K.transpose(-2, -1))</p>

<p>    <h1>2. Softmaxで正規化</h1></p>
<p>    attention_weights = F.softmax(scores, dim=-1)</p>

<p>    <h1>3. Attention重みでValueを重み付け和</h1></p>
<p>    output = torch.matmul(attention_weights, V)</p>

<p>    return output, attention_weights</p>
<p></code></pre></p>
<p></details></p>

<p><h3>問題3: 応用（考察）</h3></p>
<p>分子 "CCO"（エタノール）におけるSelf-Attentionを考えます。以下の質問に答えてください：</p>

<ol>
<li>どの原子間のAttention重みが最も高くなると予想されますか？</li>
<li>その理由を化学的観点から説明してください。</li>
<li>Multi-Head Attentionでは、各ヘッドがどのような異なる情報を捉える可能性がありますか？</li>
</ol>

<p><details></p>
<p><summary>解答例</summary></p>

<ol>
<li><strong>最も高いAttention重み</strong>: C-C結合、C-O結合</li>
</ol>

<ol>
<li><strong>化学的理由</strong>:</li>
</ol>
<p>   - 共有結合により強い相互作用がある</p>
<p>   - 電子の共有により電子密度が高い</p>
<p>   - O原子はC原子と極性結合を形成</p>

<ol>
<li><strong>各ヘッドが捉える情報の例</strong>:</li>
</ol>
<p>   - <strong>ヘッド1</strong>: 化学結合（1次結合）</p>
<p>   - <strong>ヘッド2</strong>: 2次結合（C-C-O角度）</p>
<p>   - <strong>ヘッド3</strong>: 電子密度分布</p>
<p>   - <strong>ヘッド4</strong>: 原子の種類（C vs O vs H）</p>
<p>   - <strong>ヘッド5</strong>: 立体配座情報</p>
<p>   - <strong>ヘッド6</strong>: 極性相互作用</p>

<p>   各ヘッドが異なる視点から分子を理解することで、より豊かな表現が可能になる。</p>
<p></details></p>

<p>---</p>

<p><h2>🔗 参考資料</h2></p>

<p><h3>論文</h3></p>
<ul>
<li>Vaswani et al. (2017) "Attention Is All You Need" <a href="https://arxiv.org/abs/1706.03762">arXiv:1706.03762</a></li>
<li>Devlin et al. (2019) "BERT: Pre-training of Deep Bidirectional Transformers" <a href="https://arxiv.org/abs/1810.04805">arXiv:1810.04805</a></li>
</ul>

<p><h3>チュートリアル</h3></p>
<ul>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
<li><a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html">PyTorch Transformer Tutorial</a></li>
</ul>

<p><h3>次章</h3></p>
<p><strong><a href="chapter-2.md">第2章: 材料向けTransformerアーキテクチャ</a></strong> で、Matformer、ChemBERTaなど材料科学特化モデルを学びます。</p>

<p>---</p>

<p><strong>作成者</strong>: 橋本佑介（東北大学）</p>
<p><strong>最終更新</strong>: 2025年10月17日</p>


        
        <div class="navigation">
            <a href="chapter-2.html" class="nav-button">次章: 第2章 →</a>
            <a href="index.html" class="nav-button">← シリーズ目次に戻る</a>
            
        </div>
    
    </main>

    <footer>
        <p><strong>作成者</strong>: AI Terakoya Content Team</p>
        <p><strong>監修</strong>: Dr. Yusuke Hashimoto(東北大学)</p>
        <p><strong>バージョン</strong>: 1.0 | <strong>作成日</strong>: 2025-10-17</p>
        <p><strong>ライセンス</strong>: Creative Commons BY 4.0</p>
        <p>© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>