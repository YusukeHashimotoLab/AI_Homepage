<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Untitled</h1>
            
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 20-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 5å€‹</span>
            </div>
        </div>
    </header>

    <main class="container">
        <p><h1>ç¬¬3ç« : äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨è»¢ç§»å­¦ç¿’</h1></p>

<p><strong>å­¦ç¿’æ™‚é–“</strong>: 25-30åˆ† | <strong>é›£æ˜“åº¦</strong>: ä¸­ç´šã€œä¸Šç´š</p>

<p><h2>ğŸ“‹ ã“ã®ç« ã§å­¦ã¶ã“ã¨</h2></p>

<ul>
<li>äº‹å‰å­¦ç¿’ï¼ˆPre-trainingï¼‰ã®é‡è¦æ€§ã¨åŸç†</li>
<li>MatBERTã€MolBERTãªã©ææ–™ç§‘å­¦å‘ã‘äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«</li>
<li>ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆFine-tuningï¼‰ã®æˆ¦ç•¥</li>
<li>Few-shotå­¦ç¿’ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</li>
<li>ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œï¼ˆDomain Adaptationï¼‰</li>
</ul>

<p>---</p>

<p><h2>3.1 äº‹å‰å­¦ç¿’ã®é‡è¦æ€§</h2></p>

<p><h3>ãªãœäº‹å‰å­¦ç¿’ãŒå¿…è¦ã‹</h3></p>

<p><strong>ææ–™ç§‘å­¦ã®èª²é¡Œ</strong>:</p>
<ul>
<li>âŒ ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ï¼ˆå®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã¯é«˜ã‚³ã‚¹ãƒˆï¼‰</li>
<li>âŒ ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®çŸ¥è­˜ãŒå¿…è¦</li>
<li>âŒ ã‚¼ãƒ­ã‹ã‚‰å­¦ç¿’ã™ã‚‹ã¨æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹</li>
</ul>

<p><strong>äº‹å‰å­¦ç¿’ã®åˆ©ç‚¹</strong>:</p>
<ul>
<li>âœ… å¤§è¦æ¨¡ãª<strong>ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿</strong>ã§ä¸€èˆ¬çš„ãªçŸ¥è­˜ã‚’ç²å¾—</li>
<li>âœ… å°‘é‡ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã§<strong>é«˜ç²¾åº¦</strong>ã‚’å®Ÿç¾</li>
<li>âœ… é–‹ç™ºæœŸé–“ã®<strong>å¤§å¹…çŸ­ç¸®</strong>ï¼ˆæ•°é€±é–“â†’æ•°æ™‚é–“ï¼‰</li>
</ul>

<p><pre><code class="language-mermaid">graph LR</p>
<p>    A[å¤§è¦æ¨¡ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿] --> B[äº‹å‰å­¦ç¿’]</p>
<p>    B --> C[æ±ç”¨è¡¨ç¾ãƒ¢ãƒ‡ãƒ«]</p>
<p>    C --> D[ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°]</p>
<p>    E[å°‘é‡ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿] --> D</p>
<p>    D --> F[ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«]</p>

<p>    style B fill:#e1f5ff</p>
<p>    style D fill:#ffe1e1</p>
<p></code></pre></p>

<p><h3>äº‹å‰å­¦ç¿’ã®ã‚¿ã‚¹ã‚¯</h3></p>

<p><strong>è‡ªç„¶è¨€èªå‡¦ç†ã§ã®ä¾‹</strong>:</p>
<ul>
<li><strong>Masked Language Model (MLM)</strong>: ä¸€éƒ¨ã®å˜èªã‚’ãƒã‚¹ã‚¯ã—ã¦äºˆæ¸¬</li>
<li><strong>Next Sentence Prediction (NSP)</strong>: 2æ–‡ã®é€£ç¶šæ€§ã‚’äºˆæ¸¬</li>
</ul>

<p><strong>ææ–™ç§‘å­¦ã§ã®å¿œç”¨</strong>:</p>
<ul>
<li><strong>Masked Atom Prediction</strong>: ä¸€éƒ¨ã®åŸå­ã‚’ãƒã‚¹ã‚¯ã—ã¦äºˆæ¸¬</li>
<li><strong>Property Prediction</strong>: è¤‡æ•°ã®ææ–™ç‰¹æ€§ã‚’åŒæ™‚äºˆæ¸¬</li>
<li><strong>Contrastive Learning</strong>: é¡ä¼¼ææ–™ã‚’è¿‘ãã€ç•°ãªã‚‹ææ–™ã‚’é ãã«é…ç½®</li>
</ul>

<p>---</p>

<p><h2>3.2 MatBERT: Materials BERT</h2></p>

<p><h3>æ¦‚è¦</h3></p>

<p><strong>MatBERT</strong>ã¯ã€ææ–™ã®çµ„æˆå¼ã‚’BERTã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</p>

<p><strong>ç‰¹å¾´</strong>:</p>
<ul>
<li><strong>500kææ–™</strong>ã®çµ„æˆå¼ã§äº‹å‰å­¦ç¿’</li>
<li><strong>ãƒã‚¹ã‚¯åŸå­äºˆæ¸¬</strong>ã‚¿ã‚¹ã‚¯</li>
<li>è»¢ç§»å­¦ç¿’ã§æ§˜ã€…ãªç‰¹æ€§äºˆæ¸¬ã«é©ç”¨å¯èƒ½</li>
</ul>

<p><h3>çµ„æˆå¼ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–</h3></p>

<p><pre><code class="language-python">import torch</p>
<p>import torch.nn as nn</p>
<p>from transformers import BertTokenizer, BertModel</p>

<p>class CompositionTokenizer:</p>
<p>    def __init__(self):</p>
<p>        <h1>ã‚«ã‚¹ã‚¿ãƒ èªå½™ï¼ˆå‘¨æœŸè¡¨ã®å…ƒç´ ï¼‰</h1></p>
<p>        self.vocab = ['[PAD]', '[CLS]', '[SEP]', '[MASK]'] + [</p>
<p>            'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne',</p>
<p>            'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar', 'K', 'Ca',</p>
<p>            <h1>... å…¨å…ƒç´ </h1></p>
<p>        ]</p>
<p>        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}</p>
<p>        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}</p>

<p>    def tokenize(self, composition):</p>
<p>        """</p>
<p>        çµ„æˆå¼ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–</p>

<p>        Args:</p>
<p>            composition: 'Fe2O3' ã®ã‚ˆã†ãªçµ„æˆå¼</p>
<p>        Returns:</p>
<p>            tokens: ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ</p>
<p>        """</p>
<p>        import re</p>
<p>        <h1>å…ƒç´ ã¨æ•°å­—ã‚’åˆ†å‰²</h1></p>
<p>        pattern = r'([A-Z][a-z]?)(\d<em>\.?\d</em>)'</p>
<p>        matches = re.findall(pattern, composition)</p>

<p>        tokens = ['[CLS]']</p>
<p>        for element, count in matches:</p>
<p>            if element in self.vocab:</p>
<p>                <h1>å…ƒç´ ã‚’è¿½åŠ </h1></p>
<p>                tokens.append(element)</p>
<p>                <h1>æ•°ãŒ1ã‚ˆã‚Šå¤§ãã„å ´åˆã€ãã®å›æ•°ã ã‘ç¹°ã‚Šè¿”ã™ï¼ˆç°¡ç•¥åŒ–ï¼‰</h1></p>
<p>                if count and float(count) > 1:</p>
<p>                    for _ in range(int(float(count)) - 1):</p>
<p>                        tokens.append(element)</p>
<p>        tokens.append('[SEP]')</p>

<p>        return tokens</p>

<p>    def encode(self, compositions, max_length=32):</p>
<p>        """</p>
<p>        çµ„æˆå¼ã‚’IDã«å¤‰æ›</p>

<p>        Args:</p>
<p>            compositions: çµ„æˆå¼ã®ãƒªã‚¹ãƒˆ</p>
<p>            max_length: æœ€å¤§é•·</p>
<p>        Returns:</p>
<p>            input_ids: (batch_size, max_length)</p>
<p>            attention_mask: (batch_size, max_length)</p>
<p>        """</p>
<p>        batch_input_ids = []</p>
<p>        batch_attention_mask = []</p>

<p>        for comp in compositions:</p>
<p>            tokens = self.tokenize(comp)</p>
<p>            ids = [self.token_to_id.get(token, 0) for token in tokens]</p>

<p>            <h1>ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°</h1></p>
<p>            attention_mask = [1] * len(ids)</p>
<p>            while len(ids) < max_length:</p>
<p>                ids.append(0)  <h1>[PAD]</h1></p>
<p>                attention_mask.append(0)</p>

<p>            <h1>ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ã‚·ãƒ§ãƒ³</h1></p>
<p>            ids = ids[:max_length]</p>
<p>            attention_mask = attention_mask[:max_length]</p>

<p>            batch_input_ids.append(ids)</p>
<p>            batch_attention_mask.append(attention_mask)</p>

<p>        return torch.tensor(batch_input_ids), torch.tensor(batch_attention_mask)</p>

<p><h1>ä½¿ç”¨ä¾‹</h1></p>
<p>tokenizer = CompositionTokenizer()</p>

<p>compositions = [</p>
<p>    'Fe2O3',     <h1>é…¸åŒ–é‰„</h1></p>
<p>    'LiCoO2',    <h1>ãƒªãƒã‚¦ãƒ ã‚³ãƒãƒ«ãƒˆé…¸åŒ–ç‰©ï¼ˆé›»æ± ææ–™ï¼‰</h1></p>
<p>    'BaTiO3'     <h1>ãƒã‚¿ãƒ³é…¸ãƒãƒªã‚¦ãƒ ï¼ˆèª˜é›»ä½“ï¼‰</h1></p>
<p>]</p>

<p>input_ids, attention_mask = tokenizer.encode(compositions)</p>
<p>print(f"Input IDs shape: {input_ids.shape}")</p>
<p>print(f"First composition tokens: {input_ids[0][:10]}")</p>
<p></code></pre></p>

<p><h3>MatBERTãƒ¢ãƒ‡ãƒ«</h3></p>

<p><pre><code class="language-python">class MatBERT(nn.Module):</p>
<p>    def __init__(self, vocab_size, d_model=512, num_layers=6, num_heads=8):</p>
<p>        super(MatBERT, self).__init__()</p>

<p>        <h1>Embedding</h1></p>
<p>        self.embedding = nn.Embedding(vocab_size, d_model)</p>
<p>        self.position_embedding = nn.Embedding(512, d_model)</p>

<p>        <h1>Transformer Encoder</h1></p>
<p>        encoder_layer = nn.TransformerEncoderLayer(</p>
<p>            d_model=d_model,</p>
<p>            nhead=num_heads,</p>
<p>            dim_feedforward=2048,</p>
<p>            batch_first=True</p>
<p>        )</p>
<p>        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)</p>

<p>        self.d_model = d_model</p>

<p>    def forward(self, input_ids, attention_mask):</p>
<p>        """</p>
<p>        Args:</p>
<p>            input_ids: (batch_size, seq_len)</p>
<p>            attention_mask: (batch_size, seq_len)</p>
<p>        Returns:</p>
<p>            embeddings: (batch_size, seq_len, d_model)</p>
<p>        """</p>
<p>        batch_size, seq_len = input_ids.shape</p>

<p>        <h1>Token embedding</h1></p>
<p>        token_embeddings = self.embedding(input_ids)</p>

<p>        <h1>Positional embedding</h1></p>
<p>        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)</p>
<p>        position_embeddings = self.position_embedding(positions)</p>

<p>        <h1>åˆè¨ˆ</h1></p>
<p>        embeddings = token_embeddings + position_embeddings</p>

<p>        <h1>Transformer</h1></p>
<p>        <h1>attention_maskã‚’Transformerç”¨ã«å¤‰æ›ï¼ˆ0â†’-inf, 1â†’0ï¼‰</h1></p>
<p>        transformer_mask = (1 - attention_mask).bool()</p>
<p>        output = self.transformer_encoder(embeddings, src_key_padding_mask=transformer_mask)</p>

<p>        return output</p>

<p><h1>ä½¿ç”¨ä¾‹</h1></p>
<p>vocab_size = len(tokenizer.vocab)</p>
<p>model = MatBERT(vocab_size, d_model=512, num_layers=6, num_heads=8)</p>

<p>embeddings = model(input_ids, attention_mask)</p>
<p>print(f"Embeddings shape: {embeddings.shape}")  <h1>(3, 32, 512)</h1></p>
<p></code></pre></p>

<p><h3>äº‹å‰å­¦ç¿’: Masked Atom Prediction</h3></p>

<p><pre><code class="language-python">def masked_atom_prediction_loss(model, input_ids, attention_mask, mask_prob=0.15):</p>
<p>    """</p>
<p>    ãƒã‚¹ã‚¯åŸå­äºˆæ¸¬ã«ã‚ˆã‚‹äº‹å‰å­¦ç¿’</p>

<p>    Args:</p>
<p>        model: MatBERTãƒ¢ãƒ‡ãƒ«</p>
<p>        input_ids: (batch_size, seq_len)</p>
<p>        attention_mask: (batch_size, seq_len)</p>
<p>        mask_prob: ãƒã‚¹ã‚¯ã™ã‚‹ç¢ºç‡</p>
<p>    Returns:</p>
<p>        loss: æå¤±</p>
<p>    """</p>
<p>    batch_size, seq_len = input_ids.shape</p>

<p>    <h1>ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒã‚¹ã‚¯</h1></p>
<p>    mask_token_id = tokenizer.token_to_id['[MASK]']</p>
<p>    mask = torch.rand(batch_size, seq_len) < mask_prob</p>
<p>    mask = mask & (attention_mask == 1)  <h1>ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†ã¯é™¤å¤–</h1></p>

<p>    <h1>å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿å­˜</h1></p>
<p>    original_input_ids = input_ids.clone()</p>

<p>    <h1>ãƒã‚¹ã‚¯ã‚’é©ç”¨</h1></p>
<p>    input_ids[mask] = mask_token_id</p>

<p>    <h1>Forward</h1></p>
<p>    embeddings = model(input_ids, attention_mask)</p>

<p>    <h1>äºˆæ¸¬ãƒ˜ãƒƒãƒ‰</h1></p>
<p>    prediction_head = nn.Linear(model.d_model, vocab_size)</p>
<p>    logits = prediction_head(embeddings)</p>

<p>    <h1>æå¤±è¨ˆç®—ï¼ˆãƒã‚¹ã‚¯ã•ã‚ŒãŸä½ç½®ã®ã¿ï¼‰</h1></p>
<p>    criterion = nn.CrossEntropyLoss(ignore_index=-100)</p>
<p>    labels = original_input_ids.clone()</p>
<p>    labels[~mask] = -100  <h1>ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†ã¯ç„¡è¦–</h1></p>

<p>    loss = criterion(logits.view(-1, vocab_size), labels.view(-1))</p>

<p>    return loss</p>

<p><h1>äº‹å‰å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆç°¡ç•¥ç‰ˆï¼‰</h1></p>
<p>def pretrain_matbert(model, dataloader, epochs=10):</p>
<p>    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)</p>

<p>    model.train()</p>
<p>    for epoch in range(epochs):</p>
<p>        total_loss = 0</p>
<p>        for input_ids, attention_mask in dataloader:</p>
<p>            loss = masked_atom_prediction_loss(model, input_ids, attention_mask)</p>

<p>            optimizer.zero_grad()</p>
<p>            loss.backward()</p>
<p>            optimizer.step()</p>

<p>            total_loss += loss.item()</p>

<p>        avg_loss = total_loss / len(dataloader)</p>
<p>        print(f"Epoch {epoch+1}, Pretraining Loss: {avg_loss:.4f}")</p>

<p>    return model</p>
<p></code></pre></p>

<p>---</p>

<p><h2>3.3 ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥</h2></p>

<p><h3>ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã¯</h3></p>

<p><strong>å®šç¾©</strong>: äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®šã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ã‚‹è¿½åŠ å­¦ç¿’</p>

<p><strong>æˆ¦ç•¥</strong>:</p>
<ol>
<li><strong>Full Fine-tuning</strong>: ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°</li>
<li><strong>Feature Extraction</strong>: åŸ‹ã‚è¾¼ã¿å±¤ã®ã¿ä½¿ç”¨ã€äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ã®ã¿å­¦ç¿’</li>
<li><strong>Partial Fine-tuning</strong>: ä¸€éƒ¨ã®å±¤ã®ã¿æ›´æ–°</li>
</ol>

<p><pre><code class="language-mermaid">graph TD</p>
<p>    A[äº‹å‰å­¦ç¿’æ¸ˆã¿MatBERT] --> B{ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥}</p>
<p>    B --> C[Full Fine-tuning]</p>
<p>    B --> D[Feature Extraction]</p>
<p>    B --> E[Partial Fine-tuning]</p>

<p>    C --> F[å…¨å±¤ã‚’æ›´æ–°]</p>
<p>    D --> G[åŸ‹ã‚è¾¼ã¿å›ºå®šã€äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ã®ã¿å­¦ç¿’]</p>
<p>    E --> H[ä¸Šä½å±¤ã®ã¿æ›´æ–°]</p>

<p>    style C fill:#ffe1e1</p>
<p>    style D fill:#e1f5ff</p>
<p>    style E fill:#f5ffe1</p>
<p></code></pre></p>

<p><h3>å®Ÿè£…: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬</h3></p>

<p><pre><code class="language-python">class MatBERTForBandgap(nn.Module):</p>
<p>    def __init__(self, matbert_model, d_model=512):</p>
<p>        super(MatBERTForBandgap, self).__init__()</p>
<p>        self.matbert = matbert_model</p>

<p>        <h1>äºˆæ¸¬ãƒ˜ãƒƒãƒ‰</h1></p>
<p>        self.bandgap_predictor = nn.Sequential(</p>
<p>            nn.Linear(d_model, 256),</p>
<p>            nn.ReLU(),</p>
<p>            nn.Dropout(0.2),</p>
<p>            nn.Linear(256, 1)</p>
<p>        )</p>

<p>    def forward(self, input_ids, attention_mask):</p>
<p>        <h1>MatBERTåŸ‹ã‚è¾¼ã¿</h1></p>
<p>        embeddings = self.matbert(input_ids, attention_mask)</p>

<p>        <h1>[CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨</h1></p>
<p>        cls_embedding = embeddings[:, 0, :]</p>

<p>        <h1>ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬</h1></p>
<p>        bandgap = self.bandgap_predictor(cls_embedding)</p>
<p>        return bandgap</p>

<p><h1>ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</h1></p>
<p>def finetune_for_bandgap(pretrained_model, train_loader, val_loader, strategy='full'):</p>
<p>    """</p>
<p>    ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ã¸ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</p>

<p>    Args:</p>
<p>        pretrained_model: äº‹å‰å­¦ç¿’æ¸ˆã¿MatBERT</p>
<p>        train_loader: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼</p>
<p>        val_loader: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼</p>
<p>        strategy: 'full', 'feature', 'partial'</p>
<p>    """</p>
<p>    model = MatBERTForBandgap(pretrained_model)</p>

<p>    <h1>æˆ¦ç•¥ã«å¿œã˜ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å›ºå®š</h1></p>
<p>    if strategy == 'feature':</p>
<p>        <h1>MatBERTã‚’å›ºå®š</h1></p>
<p>        for param in model.matbert.parameters():</p>
<p>            param.requires_grad = False</p>
<p>    elif strategy == 'partial':</p>
<p>        <h1>ä¸‹ä½å±¤ã‚’å›ºå®šã€ä¸Šä½å±¤ã®ã¿æ›´æ–°</h1></p>
<p>        for i, layer in enumerate(model.matbert.transformer_encoder.layers):</p>
<p>            if i < 3:  <h1>ä¸‹ä½3å±¤ã‚’å›ºå®š</h1></p>
<p>                for param in layer.parameters():</p>
<p>                    param.requires_grad = False</p>

<p>    <h1>æœ€é©åŒ–</h1></p>
<p>    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)</p>
<p>    criterion = nn.MSELoss()</p>

<p>    <h1>è¨“ç·´ãƒ«ãƒ¼ãƒ—</h1></p>
<p>    best_val_loss = float('inf')</p>
<p>    for epoch in range(20):</p>
<p>        model.train()</p>
<p>        train_loss = 0</p>
<p>        for input_ids, attention_mask, bandgaps in train_loader:</p>
<p>            predictions = model(input_ids, attention_mask)</p>
<p>            loss = criterion(predictions, bandgaps)</p>

<p>            optimizer.zero_grad()</p>
<p>            loss.backward()</p>
<p>            optimizer.step()</p>

<p>            train_loss += loss.item()</p>

<p>        <h1>æ¤œè¨¼</h1></p>
<p>        model.eval()</p>
<p>        val_loss = 0</p>
<p>        with torch.no_grad():</p>
<p>            for input_ids, attention_mask, bandgaps in val_loader:</p>
<p>                predictions = model(input_ids, attention_mask)</p>
<p>                loss = criterion(predictions, bandgaps)</p>
<p>                val_loss += loss.item()</p>

<p>        train_loss /= len(train_loader)</p>
<p>        val_loss /= len(val_loader)</p>

<p>        print(f"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")</p>

<p>        if val_loss < best_val_loss:</p>
<p>            best_val_loss = val_loss</p>
<p>            torch.save(model.state_dict(), 'best_matbert_bandgap.pt')</p>

<p>    return model</p>
<p></code></pre></p>

<p>---</p>

<p><h2>3.4 Few-shotå­¦ç¿’</h2></p>

<p><h3>æ¦‚è¦</h3></p>

<p><strong>Few-shotå­¦ç¿’</strong>: å°‘é‡ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæ•°å€‹ã€œæ•°åå€‹ï¼‰ã§æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’å­¦ç¿’</p>

<p><strong>ææ–™ç§‘å­¦ã§ã®é‡è¦æ€§</strong>:</p>
<ul>
<li>æ–°è¦ææ–™ã®ãƒ‡ãƒ¼ã‚¿ã¯éå¸¸ã«å°‘ãªã„</li>
<li>å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¯é«˜ã‚³ã‚¹ãƒˆ</li>
<li>è¿…é€Ÿãªãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°ãŒå¿…è¦</li>
</ul>

<p><h3>Prototypical Networks</h3></p>

<p><pre><code class="language-python">class PrototypicalNetwork(nn.Module):</p>
<p>    def __init__(self, matbert_model, d_model=512):</p>
<p>        super(PrototypicalNetwork, self).__init__()</p>
<p>        self.encoder = matbert_model</p>

<p>    def forward(self, support_ids, support_mask, query_ids, query_mask, support_labels):</p>
<p>        """</p>
<p>        Prototypical Networksã«ã‚ˆã‚‹åˆ†é¡</p>

<p>        Args:</p>
<p>            support_ids: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆå…¥åŠ› (n_support, seq_len)</p>
<p>            support_mask: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆãƒã‚¹ã‚¯</p>
<p>            query_ids: ã‚¯ã‚¨ãƒªå…¥åŠ› (n_query, seq_len)</p>
<p>            query_mask: ã‚¯ã‚¨ãƒªãƒã‚¹ã‚¯</p>
<p>            support_labels: ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆãƒ©ãƒ™ãƒ« (n_support,)</p>
<p>        Returns:</p>
<p>            predictions: ã‚¯ã‚¨ãƒªã®äºˆæ¸¬ãƒ©ãƒ™ãƒ«</p>
<p>        """</p>
<p>        <h1>ã‚µãƒãƒ¼ãƒˆã‚»ãƒƒãƒˆã¨ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿</h1></p>
<p>        support_embeddings = self.encoder(support_ids, support_mask)[:, 0, :]  <h1>[CLS]</h1></p>
<p>        query_embeddings = self.encoder(query_ids, query_mask)[:, 0, :]</p>

<p>        <h1>å„ã‚¯ãƒ©ã‚¹ã®ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆå¹³å‡åŸ‹ã‚è¾¼ã¿ï¼‰ã‚’è¨ˆç®—</h1></p>
<p>        unique_labels = torch.unique(support_labels)</p>
<p>        prototypes = []</p>
<p>        for label in unique_labels:</p>
<p>            mask = (support_labels == label)</p>
<p>            prototype = support_embeddings[mask].mean(dim=0)</p>
<p>            prototypes.append(prototype)</p>

<p>        prototypes = torch.stack(prototypes)  <h1>(num_classes, d_model)</h1></p>

<p>        <h1>ã‚¯ã‚¨ãƒªã¨ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—é–“ã®è·é›¢</h1></p>
<p>        distances = torch.cdist(query_embeddings, prototypes)  <h1>(n_query, num_classes)</h1></p>

<p>        <h1>æœ€ã‚‚è¿‘ã„ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã®ã‚¯ãƒ©ã‚¹ã‚’äºˆæ¸¬</h1></p>
<p>        predictions = torch.argmin(distances, dim=1)</p>

<p>        return predictions</p>

<p><h1>ä½¿ç”¨ä¾‹: 3-way 5-shotåˆ†é¡</h1></p>
<p><h1>3ã‚¯ãƒ©ã‚¹ã€å„ã‚¯ãƒ©ã‚¹5ã‚µãƒ³ãƒ—ãƒ«</h1></p>
<p>n_classes = 3</p>
<p>n_support_per_class = 5</p>
<p>n_query = 10</p>

<p>support_ids = torch.randint(0, vocab_size, (n_classes * n_support_per_class, 32))</p>
<p>support_mask = torch.ones_like(support_ids)</p>
<p>support_labels = torch.arange(n_classes).repeat_interleave(n_support_per_class)</p>

<p>query_ids = torch.randint(0, vocab_size, (n_query, 32))</p>
<p>query_mask = torch.ones_like(query_ids)</p>

<p>proto_net = PrototypicalNetwork(model)</p>
<p>predictions = proto_net(support_ids, support_mask, query_ids, query_mask, support_labels)</p>
<p>print(f"Predictions: {predictions}")</p>
<p></code></pre></p>

<p>---</p>

<p><h2>3.5 ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</h2></p>

<p><h3>ææ–™ç§‘å­¦ã§ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ</h3></p>

<p><strong>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ</strong>: ãƒ¢ãƒ‡ãƒ«ã«è¿½åŠ æƒ…å ±ã‚’ä¸ãˆã¦æ€§èƒ½ã‚’å‘ä¸Š</p>

<p><strong>ä¾‹</strong>:</p>
<p><pre><code class="language-python"><h1>é€šå¸¸: 'Fe2O3'</h1></p>
<p><h1>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ã: '[OXIDE] Fe2O3 [BANDGAP]'</h1></p>
<p></code></pre></p>

<p><h3>å®Ÿè£…</h3></p>

<p><pre><code class="language-python">class PromptedMatBERT(nn.Module):</p>
<p>    def __init__(self, matbert_model, d_model=512):</p>
<p>        super(PromptedMatBERT, self).__init__()</p>
<p>        self.matbert = matbert_model</p>

<p>        <h1>ã‚¿ã‚¹ã‚¯åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåŸ‹ã‚è¾¼ã¿ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰</h1></p>
<p>        self.task_prompts = nn.Parameter(torch.randn(10, d_model))  <h1>10ç¨®é¡ã®ã‚¿ã‚¹ã‚¯</h1></p>

<p>    def forward(self, input_ids, attention_mask, task_id=0):</p>
<p>        """</p>
<p>        Args:</p>
<p>            input_ids: (batch_size, seq_len)</p>
<p>            attention_mask: (batch_size, seq_len)</p>
<p>            task_id: ã‚¿ã‚¹ã‚¯ID (0-9)</p>
<p>        """</p>
<p>        batch_size = input_ids.size(0)</p>

<p>        <h1>é€šå¸¸ã®åŸ‹ã‚è¾¼ã¿</h1></p>
<p>        embeddings = self.matbert(input_ids, attention_mask)</p>

<p>        <h1>ã‚¿ã‚¹ã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…ˆé ­ã«è¿½åŠ </h1></p>
<p>        task_prompt = self.task_prompts[task_id].unsqueeze(0).expand(batch_size, -1, -1)</p>
<p>        embeddings = torch.cat([task_prompt, embeddings], dim=1)</p>

<p>        return embeddings</p>

<p><h1>ä½¿ç”¨ä¾‹</h1></p>
<p>prompted_model = PromptedMatBERT(model)</p>

<p><h1>ã‚¿ã‚¹ã‚¯0: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬</h1></p>
<p>embeddings_task0 = prompted_model(input_ids, attention_mask, task_id=0)</p>

<p><h1>ã‚¿ã‚¹ã‚¯1: å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼äºˆæ¸¬</h1></p>
<p>embeddings_task1 = prompted_model(input_ids, attention_mask, task_id=1)</p>

<p>print(f"Embeddings with prompt shape: {embeddings_task0.shape}")</p>
<p></code></pre></p>

<p>---</p>

<p><h2>3.6 ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ</h2></p>

<p><h3>æ¦‚è¦</h3></p>

<p><strong>ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ</strong>: ã‚½ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ã§è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³ã«é©å¿œ</p>

<p><strong>ä¾‹</strong>:</p>
<ul>
<li>ã‚½ãƒ¼ã‚¹: ç„¡æ©Ÿææ–™ãƒ‡ãƒ¼ã‚¿</li>
<li>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: æœ‰æ©Ÿåˆ†å­ãƒ‡ãƒ¼ã‚¿</li>
</ul>

<p><h3>Adversarial Domain Adaptation</h3></p>

<p><pre><code class="language-python">class DomainClassifier(nn.Module):</p>
<p>    def __init__(self, d_model=512):</p>
<p>        super(DomainClassifier, self).__init__()</p>
<p>        self.classifier = nn.Sequential(</p>
<p>            nn.Linear(d_model, 256),</p>
<p>            nn.ReLU(),</p>
<p>            nn.Linear(256, 2)  <h1>ã‚½ãƒ¼ã‚¹ or ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ</h1></p>
<p>        )</p>

<p>    def forward(self, embeddings):</p>
<p>        return self.classifier(embeddings)</p>

<p>class DomainAdaptiveMatBERT(nn.Module):</p>
<p>    def __init__(self, matbert_model):</p>
<p>        super(DomainAdaptiveMatBERT, self).__init__()</p>
<p>        self.matbert = matbert_model</p>
<p>        self.domain_classifier = DomainClassifier()</p>
<p>        self.task_predictor = nn.Linear(512, 1)  <h1>ä¾‹: ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬</h1></p>

<p>    def forward(self, input_ids, attention_mask, alpha=1.0):</p>
<p>        """</p>
<p>        Args:</p>
<p>            alpha: ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã®å¼·ã•</p>
<p>        """</p>
<p>        embeddings = self.matbert(input_ids, attention_mask)[:, 0, :]</p>

<p>        <h1>ã‚¿ã‚¹ã‚¯äºˆæ¸¬</h1></p>
<p>        task_output = self.task_predictor(embeddings)</p>

<p>        <h1>ãƒ‰ãƒ¡ã‚¤ãƒ³äºˆæ¸¬ï¼ˆå‹¾é…åè»¢å±¤ã‚’ä½¿ç”¨ï¼‰</h1></p>
<p>        <h1>ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã®ãŸã‚çœç•¥</h1></p>
<p>        domain_output = self.domain_classifier(embeddings)</p>

<p>        return task_output, domain_output</p>

<p><h1>è¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆç°¡ç•¥ç‰ˆï¼‰</h1></p>
<p>def train_domain_adaptive(model, source_loader, target_loader, epochs=20):</p>
<p>    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)</p>
<p>    task_criterion = nn.MSELoss()</p>
<p>    domain_criterion = nn.CrossEntropyLoss()</p>

<p>    for epoch in range(epochs):</p>
<p>        for (source_ids, source_mask, source_labels), (target_ids, target_mask, _) in zip(source_loader, target_loader):</p>
<p>            <h1>ã‚½ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³</h1></p>
<p>            source_task, source_domain = model(source_ids, source_mask)</p>
<p>            source_domain_labels = torch.zeros(source_ids.size(0), dtype=torch.long)  <h1>ã‚½ãƒ¼ã‚¹ = 0</h1></p>

<p>            <h1>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³</h1></p>
<p>            target_task, target_domain = model(target_ids, target_mask)</p>
<p>            target_domain_labels = torch.ones(target_ids.size(0), dtype=torch.long)  <h1>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ = 1</h1></p>

<p>            <h1>æå¤±</h1></p>
<p>            task_loss = task_criterion(source_task, source_labels)</p>
<p>            domain_loss = domain_criterion(source_domain, source_domain_labels) + \</p>
<p>                          domain_criterion(target_domain, target_domain_labels)</p>

<p>            total_loss = task_loss + 0.1 * domain_loss</p>

<p>            optimizer.zero_grad()</p>
<p>            total_loss.backward()</p>
<p>            optimizer.step()</p>

<p>        print(f"Epoch {epoch+1}, Task Loss: {task_loss.item():.4f}, Domain Loss: {domain_loss.item():.4f}")</p>
<p></code></pre></p>

<p>---</p>

<p><h2>3.7 ã¾ã¨ã‚</h2></p>

<p><h3>é‡è¦ãƒã‚¤ãƒ³ãƒˆ</h3></p>

<ol>
<li><strong>äº‹å‰å­¦ç¿’</strong>: å¤§è¦æ¨¡ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã§ä¸€èˆ¬çš„çŸ¥è­˜ã‚’ç²å¾—</li>
<li><strong>ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°</strong>: å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§ã‚¿ã‚¹ã‚¯ç‰¹åŒ–</li>
<li><strong>Few-shotå­¦ç¿’</strong>: æ•°å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã§æ–°ã‚¿ã‚¹ã‚¯å­¦ç¿’</li>
<li><strong>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°</strong>: ã‚¿ã‚¹ã‚¯æƒ…å ±ã‚’åŸ‹ã‚è¾¼ã¿ã§è¡¨ç¾</li>
<li><strong>ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ</strong>: ç•°ãªã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³é–“ã§çŸ¥è­˜è»¢ç§»</li>
</ol>

<p><h3>æ¬¡ç« ã¸ã®æº–å‚™</h3></p>

<p>ç¬¬4ç« ã§ã¯ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹åˆ†å­ç”Ÿæˆã¨ææ–™é€†è¨­è¨ˆã‚’å­¦ã³ã¾ã™ã€‚</p>

<p>---</p>

<p><h2>ğŸ“ æ¼”ç¿’å•é¡Œ</h2></p>

<p><h3>å•é¡Œ1: æ¦‚å¿µç†è§£</h3></p>
<p>ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®3ã¤ã®æˆ¦ç•¥ï¼ˆFullã€Feature Extractionã€Partialï¼‰ã«ã¤ã„ã¦ã€ãã‚Œãã‚Œã©ã®ã‚ˆã†ãªå ´åˆã«é©ã—ã¦ã„ã‚‹ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<p><details></p>
<p><summary>è§£ç­”ä¾‹</summary></p>

<ol>
<li><strong>Full Fine-tuning</strong>:</li>
</ol>
<p>   - <strong>é©ç”¨å ´é¢</strong>: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ãŒæ¯”è¼ƒçš„å¤šã„ï¼ˆæ•°åƒã‚µãƒ³ãƒ—ãƒ«ä»¥ä¸Šï¼‰</p>
<p>   - <strong>åˆ©ç‚¹</strong>: æœ€é«˜ç²¾åº¦ã‚’é”æˆå¯èƒ½</p>
<p>   - <strong>æ¬ ç‚¹</strong>: éå­¦ç¿’ãƒªã‚¹ã‚¯ã€è¨ˆç®—ã‚³ã‚¹ãƒˆå¤§</p>

<ol>
<li><strong>Feature Extraction</strong>:</li>
</ol>
<p>   - <strong>é©ç”¨å ´é¢</strong>: ãƒ‡ãƒ¼ã‚¿ãŒéå¸¸ã«å°‘ãªã„ï¼ˆæ•°åã€œæ•°ç™¾ã‚µãƒ³ãƒ—ãƒ«ï¼‰</p>
<p>   - <strong>åˆ©ç‚¹</strong>: éå­¦ç¿’ã‚’é˜²ãã‚„ã™ã„ã€é«˜é€Ÿ</p>
<p>   - <strong>æ¬ ç‚¹</strong>: ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒå¤§ããç•°ãªã‚‹å ´åˆã¯ç²¾åº¦ä½ä¸‹</p>

<ol>
<li><strong>Partial Fine-tuning</strong>:</li>
</ol>
<p>   - <strong>é©ç”¨å ´é¢</strong>: ä¸­ç¨‹åº¦ã®ãƒ‡ãƒ¼ã‚¿é‡ã€ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒé¡ä¼¼</p>
<p>   - <strong>åˆ©ç‚¹</strong>: ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸæ€§èƒ½ã¨ã‚³ã‚¹ãƒˆ</p>
<p>   - <strong>æ¬ ç‚¹</strong>: ã©ã®å±¤ã‚’æ›´æ–°ã™ã‚‹ã‹é¸æŠãŒé›£ã—ã„</p>
<p></details></p>

<p><h3>å•é¡Œ2: å®Ÿè£…</h3></p>
<p>ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®ç©ºæ¬„ã‚’åŸ‹ã‚ã¦ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹é–¢æ•°ã‚’å®Œæˆã•ã›ã¦ãã ã•ã„ã€‚</p>

<p><pre><code class="language-python">def load_and_finetune(pretrained_path, train_loader, val_loader):</p>
<p>    <h1>äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰</h1></p>
<p>    matbert = MatBERT(vocab_size=______, d_model=512)</p>
<p>    matbert.load_state_dict(torch.load(______))</p>

<p>    <h1>ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰</h1></p>
<p>    model = MatBERTForBandgap(______)</p>

<p>    <h1>æœ€é©åŒ–</h1></p>
<p>    optimizer = torch.optim.Adam(______.parameters(), lr=1e-5)</p>
<p>    criterion = nn.MSELoss()</p>

<p>    <h1>è¨“ç·´ãƒ«ãƒ¼ãƒ—</h1></p>
<p>    for epoch in range(10):</p>
<p>        model.train()</p>
<p>        for input_ids, attention_mask, targets in train_loader:</p>
<p>            predictions = model(______, ______)</p>
<p>            loss = ______(predictions, targets)</p>

<p>            optimizer.zero_grad()</p>
<p>            ______.backward()</p>
<p>            optimizer.step()</p>

<p>    return model</p>
<p></code></pre></p>

<p><details></p>
<p><summary>è§£ç­”ä¾‹</summary></p>

<p><pre><code class="language-python">def load_and_finetune(pretrained_path, train_loader, val_loader):</p>
<p>    <h1>äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰</h1></p>
<p>    matbert = MatBERT(vocab_size=len(tokenizer.vocab), d_model=512)</p>
<p>    matbert.load_state_dict(torch.load(pretrained_path))</p>

<p>    <h1>ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰</h1></p>
<p>    model = MatBERTForBandgap(matbert)</p>

<p>    <h1>æœ€é©åŒ–</h1></p>
<p>    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)</p>
<p>    criterion = nn.MSELoss()</p>

<p>    <h1>è¨“ç·´ãƒ«ãƒ¼ãƒ—</h1></p>
<p>    for epoch in range(10):</p>
<p>        model.train()</p>
<p>        for input_ids, attention_mask, targets in train_loader:</p>
<p>            predictions = model(input_ids, attention_mask)</p>
<p>            loss = criterion(predictions, targets)</p>

<p>            optimizer.zero_grad()</p>
<p>            loss.backward()</p>
<p>            optimizer.step()</p>

<p>    return model</p>
<p></code></pre></p>
<p></details></p>

<p><h3>å•é¡Œ3: å¿œç”¨</h3></p>
<p>ææ–™ç§‘å­¦ã§ Few-shotå­¦ç¿’ãŒç‰¹ã«æœ‰ç”¨ãª3ã¤ã®ã‚·ãƒŠãƒªã‚ªã‚’æŒ™ã’ã€ãã‚Œãã‚Œã®ç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚</p>

<p><details></p>
<p><summary>è§£ç­”ä¾‹</summary></p>

<ol>
<li><strong>æ–°è¦ææ–™ã®è¿…é€Ÿè©•ä¾¡</strong>:</li>
</ol>
<p>   - <strong>ã‚·ãƒŠãƒªã‚ª</strong>: æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã®ææ–™ï¼ˆä¾‹: æ–°å‹ãƒšãƒ­ãƒ–ã‚¹ã‚«ã‚¤ãƒˆï¼‰</p>
<p>   - <strong>ç†ç”±</strong>: å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãŒã¾ã å°‘ãªãã€æ•°ã‚µãƒ³ãƒ—ãƒ«ã§ç‰¹æ€§äºˆæ¸¬ãŒå¿…è¦</p>

<ol>
<li><strong>å®Ÿé¨“è¨ˆç”»ã®åŠ¹ç‡åŒ–</strong>:</li>
</ol>
<p>   - <strong>ã‚·ãƒŠãƒªã‚ª</strong>: é«˜ã‚³ã‚¹ãƒˆãªå®Ÿé¨“ï¼ˆå˜çµæ™¶æˆé•·ã€é«˜åœ§åˆæˆï¼‰</p>
<p>   - <strong>ç†ç”±</strong>: å°‘æ•°ã®å®Ÿé¨“çµæœã‹ã‚‰æ¬¡ã®å®Ÿé¨“æ¡ä»¶ã‚’ææ¡ˆ</p>

<ol>
<li><strong>ä¼æ¥­ã®ç‹¬è‡ªææ–™é–‹ç™º</strong>:</li>
</ol>
<p>   - <strong>ã‚·ãƒŠãƒªã‚ª</strong>: ç«¶åˆã«å…¬é–‹ã§ããªã„ç‹¬è‡ªææ–™</p>
<p>   - <strong>ç†ç”±</strong>: ç¤¾å†…ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§å­¦ç¿’ã€å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã¯ä½¿ãˆãªã„</p>
<p></details></p>

<p>---</p>

<p><strong>æ¬¡ç« </strong>: <strong><a href="chapter-4.md">ç¬¬4ç« : ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¨é€†è¨­è¨ˆ</a></strong></p>

<p>---</p>

<p><strong>ä½œæˆè€…</strong>: æ©‹æœ¬ä½‘ä»‹ï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
<p><strong>æœ€çµ‚æ›´æ–°</strong>: 2025å¹´10æœˆ17æ—¥</p>


        
        <div class="navigation">
            <a href="chapter-4.html" class="nav-button">æ¬¡ç« : ç¬¬4ç«  â†’</a>
            <a href="index.html" class="nav-button">â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
            <a href="chapter-2.html" class="nav-button">â† å‰ç« : ç¬¬2ç« </a>
        </div>
    
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimoto(æ±åŒ—å¤§å­¦)</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>