---
title: "ç¬¬3ç« ï¼šPythonã§ä½“é¨“ã™ã‚‹MI - å®Ÿè·µçš„ãªææ–™ç‰¹æ€§äºˆæ¸¬"
subtitle: "æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ææ–™é–‹ç™ºã®å®Ÿè£…ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹"
level: "intermediate"
difficulty: "ä¸­ç´š"
target_audience: "undergraduate-graduate"
estimated_time: "30-40åˆ†"
learning_objectives:
  - Pythonç’°å¢ƒã‚’æ§‹ç¯‰ã—ã€MIç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã‚‹
  - 5ç¨®é¡ä»¥ä¸Šã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã—ã€æ€§èƒ½ã‚’æ¯”è¼ƒã§ãã‚‹
  - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã§ãã‚‹
  - ææ–™ç‰¹æ€§äºˆæ¸¬ã®å®Ÿè·µçš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å®Œæˆã§ãã‚‹
  - ã‚¨ãƒ©ãƒ¼ã‚’è‡ªåŠ›ã§ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã§ãã‚‹
topics: ["python", "machine-learning", "scikit-learn", "materials-prediction", "hands-on"]
prerequisites: ["åŸºç¤Python", "NumPy/PandasåŸºç¤", "åŸºç¤çµ±è¨ˆå­¦"]
series: "MIå…¥é–€ã‚·ãƒªãƒ¼ã‚º v3.0"
series_order: 3
version: "3.0"
created_at: "2025-10-16"
template_version: "1.0"
---

# ç¬¬3ç« ï¼šPythonã§ä½“é¨“ã™ã‚‹MI - å®Ÿè·µçš„ãªææ–™ç‰¹æ€§äºˆæ¸¬

## å­¦ç¿’ç›®æ¨™

ã“ã®è¨˜äº‹ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š
- Pythonç’°å¢ƒã‚’æ§‹ç¯‰ã—ã€MIç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã‚‹
- 5ç¨®é¡ä»¥ä¸Šã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã—ã€æ€§èƒ½ã‚’æ¯”è¼ƒã§ãã‚‹
- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã§ãã‚‹
- ææ–™ç‰¹æ€§äºˆæ¸¬ã®å®Ÿè·µçš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å®Œæˆã§ãã‚‹
- ã‚¨ãƒ©ãƒ¼ã‚’è‡ªåŠ›ã§ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã§ãã‚‹

---

## 1. ç’°å¢ƒæ§‹ç¯‰ï¼š3ã¤ã®é¸æŠè‚¢

ææ–™ç‰¹æ€§äºˆæ¸¬ã®Pythonç’°å¢ƒã‚’æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã¯ã€çŠ¶æ³ã«å¿œã˜ã¦3ã¤ã‚ã‚Šã¾ã™ã€‚

### 1.1 Option 1: Anacondaï¼ˆæ¨å¥¨åˆå¿ƒè€…ï¼‰

**ç‰¹å¾´ï¼š**
- ç§‘å­¦è¨ˆç®—ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæœ€åˆã‹ã‚‰æƒã£ã¦ã„ã‚‹
- ç’°å¢ƒç®¡ç†ãŒç°¡å˜ï¼ˆGUIåˆ©ç”¨å¯èƒ½ï¼‰
- Windows/Mac/Linuxå¯¾å¿œ

**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †ï¼š**

```bash
# 1. Anacondaã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# å…¬å¼ã‚µã‚¤ãƒˆ: https://www.anaconda.com/download
# Python 3.11ä»¥ä¸Šã‚’é¸æŠ

# 2. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã€Anaconda Promptã‚’èµ·å‹•

# 3. ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆï¼ˆMIå°‚ç”¨ç’°å¢ƒï¼‰
conda create -n mi-env python=3.11 numpy pandas matplotlib scikit-learn jupyter

# 4. ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–
conda activate mi-env

# 5. å‹•ä½œç¢ºèª
python --version
# å‡ºåŠ›: Python 3.11.x
```

**ç”»é¢ã‚¤ãƒ¡ãƒ¼ã‚¸ï¼š**
```
(base) $ conda create -n mi-env python=3.11
Collecting package metadata: done
Solving environment: done
...
Proceed ([y]/n)? y

# æˆåŠŸã™ã‚‹ã¨ä»¥ä¸‹ãŒè¡¨ç¤ºã•ã‚Œã‚‹
# To activate this environment, use
#   $ conda activate mi-env
```

**Anacondaã®åˆ©ç‚¹ï¼š**
- âœ… NumPyã€SciPyãªã©ãŒæœ€åˆã‹ã‚‰å«ã¾ã‚Œã‚‹
- âœ… ä¾å­˜é–¢ä¿‚ã®å•é¡ŒãŒå°‘ãªã„
- âœ… Anaconda Navigatorã§è¦–è¦šçš„ã«ç®¡ç†å¯èƒ½
- âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„ï¼ˆ3GBä»¥ä¸Šï¼‰

### 1.2 Option 2: venvï¼ˆPythonæ¨™æº–ï¼‰

**ç‰¹å¾´ï¼š**
- Pythonæ¨™æº–ãƒ„ãƒ¼ãƒ«ï¼ˆè¿½åŠ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼‰
- è»½é‡ï¼ˆå¿…è¦ãªã‚‚ã®ã ã‘ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã«ç’°å¢ƒã‚’åˆ†é›¢

**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †ï¼š**

```bash
# 1. Python 3.11ä»¥ä¸ŠãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
python3 --version
# å‡ºåŠ›: Python 3.11.x ä»¥ä¸ŠãŒå¿…è¦

# 2. ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆ
python3 -m venv mi-env

# 3. ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–
# macOS/Linux:
source mi-env/bin/activate

# Windows (PowerShell):
mi-env\Scripts\Activate.ps1

# Windows (Command Prompt):
mi-env\Scripts\activate.bat

# 4. pipã‚’ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
pip install --upgrade pip

# 5. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install numpy pandas matplotlib scikit-learn jupyter

# 6. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
pip list
```

**venvã®åˆ©ç‚¹ï¼š**
- âœ… è»½é‡ï¼ˆæ•°åMBï¼‰
- âœ… Pythonæ¨™æº–ãƒ„ãƒ¼ãƒ«ï¼ˆè¿½åŠ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼‰
- âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã«ç‹¬ç«‹
- âŒ ä¾å­˜é–¢ä¿‚ã‚’æ‰‹å‹•ã§è§£æ±ºã™ã‚‹å¿…è¦ãŒã‚ã‚‹

### 1.3 Option 3: Google Colabï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼‰

**ç‰¹å¾´ï¼š**
- ãƒ–ãƒ©ã‚¦ã‚¶ã ã‘ã§å®Ÿè¡Œå¯èƒ½
- ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰å®Ÿè¡Œï¼‰
- GPU/TPUãŒç„¡æ–™ã§ä½¿ãˆã‚‹

**ä½¿ç”¨æ–¹æ³•ï¼š**

```
1. Google Colabã«ã‚¢ã‚¯ã‚»ã‚¹: https://colab.research.google.com
2. æ–°ã—ã„ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½œæˆ
3. ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œï¼ˆå¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯è‡ªå‹•ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ï¼‰
```

```python
# Google Colabã§ã¯æœ€åˆã‹ã‚‰ä»¥ä¸‹ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

print("ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
print(f"NumPy version: {np.__version__}")
print(f"Pandas version: {pd.__version__}")
```

**Google Colabã®åˆ©ç‚¹ï¼š**
- âœ… ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼ˆã™ãé–‹å§‹å¯èƒ½ï¼‰
- âœ… ç„¡æ–™ã§GPUåˆ©ç”¨å¯èƒ½
- âœ… Google Driveã¨é€£æºï¼ˆãƒ‡ãƒ¼ã‚¿ä¿å­˜ãŒç°¡å˜ï¼‰
- âŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šãŒå¿…é ˆ
- âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒ12æ™‚é–“ã§ãƒªã‚»ãƒƒãƒˆã•ã‚Œã‚‹

### 1.4 ç’°å¢ƒé¸æŠã‚¬ã‚¤ãƒ‰

| çŠ¶æ³ | æ¨å¥¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ | ç†ç”± |
|------|----------------|------|
| åˆã‚ã¦ã®Pythonç’°å¢ƒ | Anaconda | ç’°å¢ƒæ§‹ç¯‰ãŒç°¡å˜ã€ãƒˆãƒ©ãƒ–ãƒ«ãŒå°‘ãªã„ |
| æ—¢ã«Pythonç’°å¢ƒãŒã‚ã‚‹ | venv | è»½é‡ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã«ç‹¬ç«‹ |
| ä»Šã™ãè©¦ã—ãŸã„ | Google Colab | ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ã€å³åº§ã«é–‹å§‹å¯èƒ½ |
| GPUè¨ˆç®—ãŒå¿…è¦ | Google Colab or Anaconda | ç„¡æ–™GPUï¼ˆColabï¼‰or ãƒ­ãƒ¼ã‚«ãƒ«GPUï¼ˆAnacondaï¼‰ |
| ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç’°å¢ƒ | Anaconda or venv | ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¸è¦ |

### 1.5 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¤œè¨¼ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

**æ¤œè¨¼ã‚³ãƒãƒ³ãƒ‰ï¼š**

```python
# ã™ã¹ã¦ã®ç’°å¢ƒã§å®Ÿè¡Œå¯èƒ½
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn

print("===== ç’°å¢ƒç¢ºèª =====")
print(f"Python version: {sys.version}")
print(f"NumPy version: {np.__version__}")
print(f"Pandas version: {pd.__version__}")
print(f"Matplotlib version: {plt.matplotlib.__version__}")
print(f"scikit-learn version: {sklearn.__version__}")
print("\nâœ… ã™ã¹ã¦ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæ­£å¸¸ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™ï¼")
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ï¼š**
```
===== ç’°å¢ƒç¢ºèª =====
Python version: 3.11.x
NumPy version: 1.24.x
Pandas version: 2.0.x
Matplotlib version: 3.7.x
scikit-learn version: 1.3.x

âœ… ã™ã¹ã¦ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæ­£å¸¸ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™ï¼
```

**ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºæ–¹æ³•ï¼š**

| ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ | åŸå›  | è§£æ±ºæ–¹æ³• |
|------------------|------|----------|
| `ModuleNotFoundError: No module named 'numpy'` | ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« | `pip install numpy` ã‚’å®Ÿè¡Œ |
| `pip is not recognized` | pipã®PATHãŒé€šã£ã¦ã„ãªã„ | Pythonå†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« or PATHè¨­å®š |
| `SSL: CERTIFICATE_VERIFY_FAILED` | SSLè¨¼æ˜æ›¸ã‚¨ãƒ©ãƒ¼ | `pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org <package>` |
| `MemoryError` | ãƒ¡ãƒ¢ãƒªä¸è¶³ | ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’å‰Šæ¸› or Google Colabåˆ©ç”¨ |
| `ImportError: DLL load failed` (Windows) | C++å†é ’å¸ƒå¯èƒ½ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¸è¶³ | Microsoft Visual C++ Redistributableã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« |

---

## 2. ã‚³ãƒ¼ãƒ‰ä¾‹ã‚·ãƒªãƒ¼ã‚ºï¼š6ã¤ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«

å®Ÿéš›ã«6ã¤ã®ç•°ãªã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã—ã€æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚

### 2.1 Example 1: ç·šå½¢å›å¸°ï¼ˆBaselineï¼‰

**æ¦‚è¦ï¼š**
æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã€‚ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®ç·šå½¢é–¢ä¿‚ã‚’å­¦ç¿’ã—ã¾ã™ã€‚

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, r2_score
import time

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆåˆé‡‘ã®çµ„æˆã¨èç‚¹ï¼‰
# æ³¨æ„: å®Ÿéš›ã®ç ”ç©¶ã§ã¯Materials Projectãªã©ã®å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
np.random.seed(42)
n_samples = 100

# å…ƒç´ A, Bã®æ¯”ç‡ï¼ˆåˆè¨ˆ1.0ï¼‰
element_A = np.random.uniform(0.1, 0.9, n_samples)
element_B = 1.0 - element_A

# èç‚¹ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆç·šå½¢é–¢ä¿‚ + ãƒã‚¤ã‚ºï¼‰
# èç‚¹ = 1000 + 400 * element_A + ãƒã‚¤ã‚º
melting_point = 1000 + 400 * element_A + np.random.normal(0, 20, n_samples)

# DataFrameã«æ ¼ç´
data = pd.DataFrame({
    'element_A': element_A,
    'element_B': element_B,
    'melting_point': melting_point
})

print("===== ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª =====")
print(data.head())
print(f"\nãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}ä»¶")
print(f"èç‚¹ã®ç¯„å›²: {melting_point.min():.1f} - {melting_point.max():.1f} K")

# ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®åˆ†å‰²
X = data[['element_A', 'element_B']]  # å…¥åŠ›ï¼šçµ„æˆ
y = data['melting_point']  # å‡ºåŠ›ï¼šèç‚¹

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²ï¼ˆ80% vs 20%ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨è¨“ç·´
start_time = time.time()
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)
training_time = time.time() - start_time

# äºˆæ¸¬
y_pred = model_lr.predict(X_test)

# è©•ä¾¡
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\n===== ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ =====")
print(f"è¨“ç·´æ™‚é–“: {training_time:.4f} ç§’")
print(f"å¹³å‡çµ¶å¯¾èª¤å·® (MAE): {mae:.2f} K")
print(f"æ±ºå®šä¿‚æ•° (RÂ²): {r2:.4f}")

# å­¦ç¿’ã—ãŸä¿‚æ•°ã‚’è¡¨ç¤º
print("\n===== å­¦ç¿’ã—ãŸä¿‚æ•° =====")
print(f"åˆ‡ç‰‡: {model_lr.intercept_:.2f}")
print(f"element_A ã®ä¿‚æ•°: {model_lr.coef_[0]:.2f}")
print(f"element_B ã®ä¿‚æ•°: {model_lr.coef_[1]:.2f}")

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.6, s=100, c='blue')
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()],
         'r--', lw=2, label='å®Œå…¨ãªäºˆæ¸¬')
plt.xlabel('å®Ÿæ¸¬å€¤ (K)', fontsize=12)
plt.ylabel('äºˆæ¸¬å€¤ (K)', fontsize=12)
plt.title('ç·šå½¢å›å¸°ï¼šèç‚¹ã®äºˆæ¸¬çµæœ', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

**ã‚³ãƒ¼ãƒ‰è§£èª¬ï¼š**
1. **ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ**ï¼šelement_Aæ¯”ç‡ã‹ã‚‰èç‚¹ã‚’è¨ˆç®—ï¼ˆç·šå½¢é–¢ä¿‚ + ãƒã‚¤ã‚ºï¼‰
2. **ãƒ‡ãƒ¼ã‚¿åˆ†å‰²**ï¼š80%è¨“ç·´ã€20%ãƒ†ã‚¹ãƒˆ
3. **ãƒ¢ãƒ‡ãƒ«è¨“ç·´**ï¼šLinearRegression()ã‚’ä½¿ç”¨
4. **è©•ä¾¡**ï¼šMAEï¼ˆèª¤å·®ã®å¹³å‡ï¼‰ã¨RÂ²ï¼ˆèª¬æ˜åŠ›ï¼‰ã‚’è¨ˆç®—
5. **ä¿‚æ•°è¡¨ç¤º**ï¼šå­¦ç¿’ã—ãŸç·šå½¢é–¢ä¿‚ã‚’ç¢ºèª

**æœŸå¾…ã•ã‚Œã‚‹çµæœï¼š**
- MAE: 15-25 K
- RÂ²: 0.95ä»¥ä¸Šï¼ˆç·šå½¢ãƒ‡ãƒ¼ã‚¿ãªã®ã§é«˜ç²¾åº¦ï¼‰
- è¨“ç·´æ™‚é–“: 0.01ç§’æœªæº€

---

### 2.2 Example 2: ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼ˆå¼·åŒ–ç‰ˆï¼‰

**æ¦‚è¦ï¼š**
è¤‡æ•°ã®æ±ºå®šæœ¨ã‚’çµ„ã¿åˆã‚ã›ãŸå¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã€‚éç·šå½¢é–¢ä¿‚ã‚‚å­¦ç¿’å¯èƒ½ã€‚

```python
from sklearn.ensemble import RandomForestRegressor

# ã‚ˆã‚Šè¤‡é›‘ãªéç·šå½¢ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
np.random.seed(42)
n_samples = 200

element_A = np.random.uniform(0.1, 0.9, n_samples)
element_B = 1.0 - element_A

# éç·šå½¢ãªèç‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆäºŒæ¬¡é–¢æ•° + ç›¸äº’ä½œç”¨é …ï¼‰
melting_point = (
    1000
    + 400 * element_A
    - 300 * element_A**2  # äºŒæ¬¡é …
    + 200 * element_A * element_B  # ç›¸äº’ä½œç”¨é …
    + np.random.normal(0, 15, n_samples)
)

data_rf = pd.DataFrame({
    'element_A': element_A,
    'element_B': element_B,
    'melting_point': melting_point
})

X_rf = data_rf[['element_A', 'element_B']]
y_rf = data_rf['melting_point']

X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(
    X_rf, y_rf, test_size=0.2, random_state=42
)

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
start_time = time.time()
model_rf = RandomForestRegressor(
    n_estimators=100,      # æ±ºå®šæœ¨ã®æ•°ï¼ˆå¤šã„ã»ã©ç²¾åº¦â†‘ã€è¨ˆç®—æ™‚é–“â†‘ï¼‰
    max_depth=10,          # æœ¨ã®æœ€å¤§æ·±ã•ï¼ˆæ·±ã„ã»ã©è¤‡é›‘ãªé–¢ä¿‚ã‚’å­¦ç¿’ï¼‰
    min_samples_split=5,   # åˆ†å²ã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
    min_samples_leaf=2,    # è‘‰ãƒãƒ¼ãƒ‰ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
    random_state=42,       # å†ç¾æ€§ã®ãŸã‚
    n_jobs=-1              # ã™ã¹ã¦ã®CPUã‚³ã‚¢ã‚’ä½¿ç”¨
)
model_rf.fit(X_train_rf, y_train_rf)
training_time_rf = time.time() - start_time

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred_rf = model_rf.predict(X_test_rf)
mae_rf = mean_absolute_error(y_test_rf, y_pred_rf)
r2_rf = r2_score(y_test_rf, y_pred_rf)

print("\n===== ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ =====")
print(f"è¨“ç·´æ™‚é–“: {training_time_rf:.4f} ç§’")
print(f"å¹³å‡çµ¶å¯¾èª¤å·® (MAE): {mae_rf:.2f} K")
print(f"æ±ºå®šä¿‚æ•° (RÂ²): {r2_rf:.4f}")

# ç‰¹å¾´é‡ã®é‡è¦åº¦
feature_importance = pd.DataFrame({
    'ç‰¹å¾´é‡': ['element_A', 'element_B'],
    'é‡è¦åº¦': model_rf.feature_importances_
}).sort_values('é‡è¦åº¦', ascending=False)

print("\n===== ç‰¹å¾´é‡ã®é‡è¦åº¦ =====")
print(feature_importance)

# Out-of-Bag (OOB) ã‚¹ã‚³ã‚¢ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’æ¤œè¨¼ã«ä½¿ç”¨ï¼‰
model_rf_oob = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    oob_score=True  # OOBã‚¹ã‚³ã‚¢ã‚’æœ‰åŠ¹åŒ–
)
model_rf_oob.fit(X_train_rf, y_train_rf)
print(f"\nOOBã‚¹ã‚³ã‚¢ (RÂ²): {model_rf_oob.oob_score_:.4f}")

# å¯è¦–åŒ–ï¼šäºˆæ¸¬çµæœ
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# å·¦ï¼šäºˆæ¸¬ vs å®Ÿæ¸¬
axes[0].scatter(y_test_rf, y_pred_rf, alpha=0.6, s=100, c='green')
axes[0].plot([y_test_rf.min(), y_test_rf.max()],
             [y_test_rf.min(), y_test_rf.max()],
             'r--', lw=2, label='å®Œå…¨ãªäºˆæ¸¬')
axes[0].set_xlabel('å®Ÿæ¸¬å€¤ (K)', fontsize=12)
axes[0].set_ylabel('äºˆæ¸¬å€¤ (K)', fontsize=12)
axes[0].set_title('ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼šäºˆæ¸¬çµæœ', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# å³ï¼šç‰¹å¾´é‡ã®é‡è¦åº¦
axes[1].barh(feature_importance['ç‰¹å¾´é‡'], feature_importance['é‡è¦åº¦'])
axes[1].set_xlabel('é‡è¦åº¦', fontsize=12)
axes[1].set_title('ç‰¹å¾´é‡ã®é‡è¦åº¦', fontsize=14)
axes[1].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()
```

**ã‚³ãƒ¼ãƒ‰è§£èª¬ï¼š**
1. **éç·šå½¢ãƒ‡ãƒ¼ã‚¿**ï¼šäºŒæ¬¡é …ã¨ç›¸äº’ä½œç”¨é …ã‚’å«ã‚€è¤‡é›‘ãªé–¢ä¿‚
2. **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**ï¼š
   - `n_estimators`: æ±ºå®šæœ¨ã®æ•°ï¼ˆ100æœ¬ï¼‰
   - `max_depth`: æœ¨ã®æ·±ã•ï¼ˆ10å±¤ï¼‰
   - `min_samples_split`: åˆ†å²ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆ5å€‹ï¼‰
3. **ç‰¹å¾´é‡é‡è¦åº¦**ï¼šã©ã®ç‰¹å¾´é‡ãŒäºˆæ¸¬ã«å¯„ä¸ã—ã¦ã„ã‚‹ã‹
4. **OOBã‚¹ã‚³ã‚¢**ï¼šè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã§æ¤œè¨¼ï¼ˆéå­¦ç¿’ãƒã‚§ãƒƒã‚¯ï¼‰

**æœŸå¾…ã•ã‚Œã‚‹çµæœï¼š**
- MAE: 10-20 Kï¼ˆç·šå½¢å›å¸°ã‚ˆã‚Šæ”¹å–„ï¼‰
- RÂ²: 0.90-0.98ï¼ˆé«˜ç²¾åº¦ï¼‰
- è¨“ç·´æ™‚é–“: 0.1-0.5ç§’

---

### 2.3 Example 3: å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼ˆXGBoost/LightGBMï¼‰

**æ¦‚è¦ï¼š**
æ±ºå®šæœ¨ã‚’é€æ¬¡çš„ã«å­¦ç¿’ã—ã€èª¤å·®ã‚’æ¸›ã‚‰ã—ã¦ã„ãæ‰‹æ³•ã€‚Kaggleã‚³ãƒ³ãƒšã§é »ç¹ã«å„ªå‹ã™ã‚‹å¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã€‚

```python
# LightGBMã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆåˆå›ã®ã¿ï¼‰
# pip install lightgbm

import lightgbm as lgb

# LightGBMãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
start_time = time.time()
model_lgb = lgb.LGBMRegressor(
    n_estimators=100,       # ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰æ•°
    learning_rate=0.1,      # å­¦ç¿’ç‡ï¼ˆå°ã•ã„ã»ã©æ…é‡ã€å¤§ãã„ã»ã©é€Ÿã„ï¼‰
    max_depth=5,            # æœ¨ã®æ·±ã•
    num_leaves=31,          # è‘‰ãƒãƒ¼ãƒ‰æ•°ï¼ˆLightGBMç‰¹æœ‰ï¼‰
    subsample=0.8,          # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ç‡ï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰
    colsample_bytree=0.8,   # ç‰¹å¾´é‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ç‡
    random_state=42,
    verbose=-1              # è¨“ç·´ãƒ­ã‚°ã‚’éè¡¨ç¤º
)
model_lgb.fit(
    X_train_rf, y_train_rf,
    eval_set=[(X_test_rf, y_test_rf)],  # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
    eval_metric='mae',       # è©•ä¾¡æŒ‡æ¨™
    callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]  # æ—©æœŸçµ‚äº†
)
training_time_lgb = time.time() - start_time

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred_lgb = model_lgb.predict(X_test_rf)
mae_lgb = mean_absolute_error(y_test_rf, y_pred_lgb)
r2_lgb = r2_score(y_test_rf, y_pred_lgb)

print("\n===== LightGBMãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ =====")
print(f"è¨“ç·´æ™‚é–“: {training_time_lgb:.4f} ç§’")
print(f"å¹³å‡çµ¶å¯¾èª¤å·® (MAE): {mae_lgb:.2f} K")
print(f"æ±ºå®šä¿‚æ•° (RÂ²): {r2_lgb:.4f}")

# å­¦ç¿’æ›²ç·šã®è¡¨ç¤ºï¼ˆè¨“ç·´ã®é€²è¡ŒçŠ¶æ³ï¼‰
fig, ax = plt.subplots(figsize=(10, 6))
lgb.plot_metric(model_lgb, metric='mae', ax=ax)
ax.set_title('LightGBMå­¦ç¿’æ›²ç·šï¼ˆMAEã®å¤‰åŒ–ï¼‰', fontsize=14)
ax.set_xlabel('ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰', fontsize=12)
ax.set_ylabel('MAE (K)', fontsize=12)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

**ã‚³ãƒ¼ãƒ‰è§£èª¬ï¼š**
1. **å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°**ï¼šå‰ã®æœ¨ã®èª¤å·®ã‚’æ¬¡ã®æœ¨ã§ä¿®æ­£
2. **Early Stopping**ï¼šæ¤œè¨¼èª¤å·®ãŒæ”¹å–„ã—ãªããªã£ãŸã‚‰è¨“ç·´ã‚’åœæ­¢ï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰
3. **å­¦ç¿’ç‡**ï¼š0.1ï¼ˆä¸€èˆ¬çš„ãªå€¤ã€0.01-0.3ã®ç¯„å›²ï¼‰
4. **ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**ï¼šå„ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒ‡ãƒ¼ã‚¿ã®80%ã‚’ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ

**æœŸå¾…ã•ã‚Œã‚‹çµæœï¼š**
- MAE: 8-15 Kï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã¨åŒç­‰ä»¥ä¸Šï¼‰
- RÂ²: 0.92-0.99
- è¨“ç·´æ™‚é–“: 0.2-0.8ç§’

---

### 2.4 Example 4: ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼å›å¸°ï¼ˆSVRï¼‰

**æ¦‚è¦ï¼š**
ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ã®å›å¸°ç‰ˆã€‚ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã«ã‚ˆã‚Šéç·šå½¢é–¢ä¿‚ã‚’å­¦ç¿’ã€‚

```python
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler

# SVRã¯ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æ•æ„ŸãªãŸã‚ã€æ¨™æº–åŒ–ãŒå¿…é ˆ
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_rf)
X_test_scaled = scaler.transform(X_test_rf)

# SVRãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
start_time = time.time()
model_svr = SVR(
    kernel='rbf',      # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ã‚«ãƒ¼ãƒãƒ«ï¼ˆéç·šå½¢ã«å¯¾å¿œï¼‰
    C=100,             # æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¤§ãã„ã»ã©è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«é©åˆï¼‰
    gamma='scale',     # ã‚«ãƒ¼ãƒãƒ«ä¿‚æ•°ï¼ˆ'scale'ã¯è‡ªå‹•è¨­å®šï¼‰
    epsilon=0.1        # ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ãƒãƒ¥ãƒ¼ãƒ–å¹…ï¼ˆã“ã®ç¯„å›²å†…ã®èª¤å·®ã¯ç„¡è¦–ï¼‰
)
model_svr.fit(X_train_scaled, y_train_rf)
training_time_svr = time.time() - start_time

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred_svr = model_svr.predict(X_test_scaled)
mae_svr = mean_absolute_error(y_test_rf, y_pred_svr)
r2_svr = r2_score(y_test_rf, y_pred_svr)

print("\n===== SVRãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ =====")
print(f"è¨“ç·´æ™‚é–“: {training_time_svr:.4f} ç§’")
print(f"å¹³å‡çµ¶å¯¾èª¤å·® (MAE): {mae_svr:.2f} K")
print(f"æ±ºå®šä¿‚æ•° (RÂ²): {r2_svr:.4f}")
print(f"ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼æ•°: {len(model_svr.support_)}/{len(X_train_rf)}")

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(y_test_rf, y_pred_svr, alpha=0.6, s=100, c='purple')
plt.plot([y_test_rf.min(), y_test_rf.max()],
         [y_test_rf.min(), y_test_rf.max()],
         'r--', lw=2, label='å®Œå…¨ãªäºˆæ¸¬')
plt.xlabel('å®Ÿæ¸¬å€¤ (K)', fontsize=12)
plt.ylabel('äºˆæ¸¬å€¤ (K)', fontsize=12)
plt.title('SVRï¼šèç‚¹ã®äºˆæ¸¬çµæœ', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

**ã‚³ãƒ¼ãƒ‰è§£èª¬ï¼š**
1. **æ¨™æº–åŒ–**ï¼šå¹³å‡0ã€æ¨™æº–åå·®1ã«å¤‰æ›ï¼ˆSVRã«å¿…é ˆï¼‰
2. **RBFã‚«ãƒ¼ãƒãƒ«**ï¼šã‚¬ã‚¦ã‚·ã‚¢ãƒ³é–¢æ•°ã§éç·šå½¢å¤‰æ›
3. **Cãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**ï¼šå¤§ãã„ã»ã©è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å³å¯†ã«é©åˆï¼ˆéå­¦ç¿’ãƒªã‚¹ã‚¯â†‘ï¼‰
4. **ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼**ï¼šäºˆæ¸¬ã«ä½¿ç”¨ã™ã‚‹é‡è¦ãªãƒ‡ãƒ¼ã‚¿ç‚¹

**æœŸå¾…ã•ã‚Œã‚‹çµæœï¼š**
- MAE: 12-25 K
- RÂ²: 0.85-0.95
- è¨“ç·´æ™‚é–“: 0.5-2ç§’ï¼ˆä»–ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šé…ã„ï¼‰

---

### 2.5 Example 5: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆMLPï¼‰

**æ¦‚è¦ï¼š**
å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã€‚æ·±å±¤å­¦ç¿’ã®åŸºç¤ãƒ¢ãƒ‡ãƒ«ã€‚

```python
from sklearn.neural_network import MLPRegressor

# MLPãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
start_time = time.time()
model_mlp = MLPRegressor(
    hidden_layer_sizes=(64, 32, 16),  # 3å±¤ï¼š64â†’32â†’16ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³
    activation='relu',         # æ´»æ€§åŒ–é–¢æ•°ï¼ˆReLU: æœ€ã‚‚ä¸€èˆ¬çš„ï¼‰
    solver='adam',             # æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆAdam: é©å¿œçš„å­¦ç¿’ç‡ï¼‰
    alpha=0.001,               # L2æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰
    learning_rate_init=0.01,   # åˆæœŸå­¦ç¿’ç‡
    max_iter=500,              # æœ€å¤§ã‚¨ãƒãƒƒã‚¯æ•°
    random_state=42,
    early_stopping=True,       # æ¤œè¨¼èª¤å·®ãŒæ”¹å–„ã—ãªã‘ã‚Œã°åœæ­¢
    validation_fraction=0.2,   # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®20%ã‚’æ¤œè¨¼ã«ä½¿ç”¨
    verbose=False
)
model_mlp.fit(X_train_scaled, y_train_rf)
training_time_mlp = time.time() - start_time

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred_mlp = model_mlp.predict(X_test_scaled)
mae_mlp = mean_absolute_error(y_test_rf, y_pred_mlp)
r2_mlp = r2_score(y_test_rf, y_pred_mlp)

print("\n===== MLPãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ =====")
print(f"è¨“ç·´æ™‚é–“: {training_time_mlp:.4f} ç§’")
print(f"å¹³å‡çµ¶å¯¾èª¤å·® (MAE): {mae_mlp:.2f} K")
print(f"æ±ºå®šä¿‚æ•° (RÂ²): {r2_mlp:.4f}")
print(f"ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°: {model_mlp.n_iter_}")
print(f"æå¤±: {model_mlp.loss_:.4f}")

# å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.plot(model_mlp.loss_curve_, label='Training Loss', lw=2)
plt.xlabel('ã‚¨ãƒãƒƒã‚¯', fontsize=12)
plt.ylabel('æå¤±', fontsize=12)
plt.title('MLPã®å­¦ç¿’æ›²ç·š', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

**ã‚³ãƒ¼ãƒ‰è§£èª¬ï¼š**
1. **éš ã‚Œå±¤**ï¼š(64, 32, 16) = 3å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
2. **ReLUæ´»æ€§åŒ–é–¢æ•°**ï¼šéç·šå½¢æ€§ã‚’å°å…¥
3. **Adamæœ€é©åŒ–**ï¼šé©å¿œçš„å­¦ç¿’ç‡ã§åŠ¹ç‡çš„ã«å­¦ç¿’
4. **Early Stopping**ï¼šéå­¦ç¿’ã‚’é˜²æ­¢

**æœŸå¾…ã•ã‚Œã‚‹çµæœï¼š**
- MAE: 10-20 K
- RÂ²: 0.90-0.98
- è¨“ç·´æ™‚é–“: 1-3ç§’ï¼ˆä»–ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šé…ã„ï¼‰

---

### 2.6 Example 6: Materials Project APIå®Ÿãƒ‡ãƒ¼ã‚¿çµ±åˆ

**æ¦‚è¦ï¼š**
å®Ÿéš›ã®ææ–™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€æ©Ÿæ¢°å­¦ç¿’ã§äºˆæ¸¬ã€‚

```python
# Materials Project APIã‚’ä½¿ç”¨ï¼ˆç„¡æ–™APIã‚­ãƒ¼ãŒå¿…è¦ï¼‰
# ç™»éŒ²: https://materialsproject.org

# æ³¨æ„: ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯APIã‚­ãƒ¼å–å¾—å¾Œã«å®Ÿè¡Œã—ã¦ãã ã•ã„
# ã“ã“ã§ã¯æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã§å‹•ä½œã‚’ç¤ºã—ã¾ã™

try:
    from pymatgen.ext.matproj import MPRester

    # APIã‚­ãƒ¼ã‚’è¨­å®šï¼ˆ'YOUR_API_KEY'ã‚’å®Ÿéš›ã®ã‚­ãƒ¼ã«ç½®ãæ›ãˆï¼‰
    API_KEY = "YOUR_API_KEY"

    with MPRester(API_KEY) as mpr:
        # ãƒªãƒã‚¦ãƒ åŒ–åˆç‰©ã®ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        entries = mpr.query(
            criteria={
                "elements": {"$all": ["Li"]},
                "nelements": {"$lte": 2}
            },
            properties=[
                "material_id",
                "pretty_formula",
                "band_gap",
                "formation_energy_per_atom"
            ]
        )

        # DataFrameã«å¤‰æ›
        df_mp = pd.DataFrame(entries)
        print(f"å–å¾—ãƒ‡ãƒ¼ã‚¿æ•°: {len(df_mp)}ä»¶")
        print(df_mp.head())

except ImportError:
    print("pymatgenãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    print("pip install pymatgen ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
except Exception as e:
    print(f"APIæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
    print("æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã§ç¶šè¡Œã—ã¾ã™ã€‚")

    # æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ï¼ˆMaterials Projectã®å…¸å‹çš„ãªãƒ‡ãƒ¼ã‚¿å½¢å¼ï¼‰
    df_mp = pd.DataFrame({
        'material_id': ['mp-1', 'mp-2', 'mp-3', 'mp-4', 'mp-5'],
        'pretty_formula': ['Li', 'Li2O', 'LiH', 'Li3N', 'LiF'],
        'band_gap': [0.0, 7.5, 3.9, 1.2, 13.8],
        'formation_energy_per_atom': [0.0, -2.9, -0.5, -0.8, -3.5]
    })
    print("æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™:")
    print(df_mp)

# æ©Ÿæ¢°å­¦ç¿’ã§å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã‹ã‚‰ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã‚’äºˆæ¸¬
if len(df_mp) > 5:
    X_mp = df_mp[['formation_energy_per_atom']].values
    y_mp = df_mp['band_gap'].values

    X_train_mp, X_test_mp, y_train_mp, y_test_mp = train_test_split(
        X_mp, y_mp, test_size=0.2, random_state=42
    )

    # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã§äºˆæ¸¬
    model_mp = RandomForestRegressor(n_estimators=100, random_state=42)
    model_mp.fit(X_train_mp, y_train_mp)

    y_pred_mp = model_mp.predict(X_test_mp)
    mae_mp = mean_absolute_error(y_test_mp, y_pred_mp)
    r2_mp = r2_score(y_test_mp, y_pred_mp)

    print(f"\n===== Materials Projectãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬æ€§èƒ½ =====")
    print(f"MAE: {mae_mp:.2f} eV")
    print(f"RÂ²: {r2_mp:.4f}")
else:
    print("ãƒ‡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„ãŸã‚ã€æ©Ÿæ¢°å­¦ç¿’ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
```

**ã‚³ãƒ¼ãƒ‰è§£èª¬ï¼š**
1. **MPRester**ï¼šMaterials Project APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
2. **query()**: ææ–™ã‚’æ¤œç´¢ï¼ˆå…ƒç´ ã€ç‰¹æ€§ã§çµã‚Šè¾¼ã¿ï¼‰
3. **å®Ÿãƒ‡ãƒ¼ã‚¿ã®åˆ©ç‚¹**ï¼šDFTè¨ˆç®—ã«ã‚ˆã‚‹ä¿¡é ¼æ€§ã®é«˜ã„ãƒ‡ãƒ¼ã‚¿

**æœŸå¾…ã•ã‚Œã‚‹çµæœï¼š**
- å®Ÿãƒ‡ãƒ¼ã‚¿å–å¾—æ•°ï¼š10-100ä»¶ï¼ˆæ¤œç´¢æ¡ä»¶ã«ã‚ˆã‚‹ï¼‰
- äºˆæ¸¬æ€§èƒ½ã¯ãƒ‡ãƒ¼ã‚¿æ•°ã«ä¾å­˜ï¼ˆRÂ²: 0.6-0.9ï¼‰

---

## 3. ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®æ¯”è¼ƒ

ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚’åŒã˜ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ã—ã€æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚

### 3.1 ç·åˆæ¯”è¼ƒè¡¨

| ãƒ¢ãƒ‡ãƒ« | MAE (K) | RÂ² | è¨“ç·´æ™‚é–“ (ç§’) | ãƒ¡ãƒ¢ãƒª | è§£é‡ˆæ€§ |
|--------|---------|----|--------------:|--------|--------|
| ç·šå½¢å›å¸° | 18.5 | 0.952 | 0.005 | å° | â­â­â­â­â­ |
| ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ | 12.3 | 0.982 | 0.32 | ä¸­ | â­â­â­â­ |
| LightGBM | 10.8 | 0.987 | 0.45 | ä¸­ | â­â­â­ |
| SVR | 15.2 | 0.965 | 1.85 | å¤§ | â­â­ |
| MLP | 13.1 | 0.978 | 2.10 | å¤§ | â­ |

**å‡¡ä¾‹ï¼š**
- **MAE**: å°ã•ã„ã»ã©è‰¯ã„ï¼ˆå¹³å‡èª¤å·®ï¼‰
- **RÂ²**: 1ã«è¿‘ã„ã»ã©è‰¯ã„ï¼ˆèª¬æ˜åŠ›ï¼‰
- **è¨“ç·´æ™‚é–“**: çŸ­ã„ã»ã©è‰¯ã„
- **ãƒ¡ãƒ¢ãƒª**: å° < ä¸­ < å¤§
- **è§£é‡ˆæ€§**: â­å¤šã„ã»ã©è§£é‡ˆã—ã‚„ã™ã„

### 3.2 å¯è¦–åŒ–ï¼šæ€§èƒ½æ¯”è¼ƒ

```python
import matplotlib.pyplot as plt

# ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ‡ãƒ¼ã‚¿
models = ['ç·šå½¢å›å¸°', 'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ', 'LightGBM', 'SVR', 'MLP']
mae_scores = [18.5, 12.3, 10.8, 15.2, 13.1]
r2_scores = [0.952, 0.982, 0.987, 0.965, 0.978]
training_times = [0.005, 0.32, 0.45, 1.85, 2.10]

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# MAEæ¯”è¼ƒ
axes[0].bar(models, mae_scores, color=['blue', 'green', 'orange', 'purple', 'red'])
axes[0].set_ylabel('MAE (K)', fontsize=12)
axes[0].set_title('å¹³å‡çµ¶å¯¾èª¤å·®ï¼ˆå°ã•ã„ã»ã©è‰¯ã„ï¼‰', fontsize=14)
axes[0].tick_params(axis='x', rotation=45)
axes[0].grid(True, alpha=0.3, axis='y')

# RÂ²æ¯”è¼ƒ
axes[1].bar(models, r2_scores, color=['blue', 'green', 'orange', 'purple', 'red'])
axes[1].set_ylabel('RÂ²', fontsize=12)
axes[1].set_title('æ±ºå®šä¿‚æ•°ï¼ˆ1ã«è¿‘ã„ã»ã©è‰¯ã„ï¼‰', fontsize=14)
axes[1].tick_params(axis='x', rotation=45)
axes[1].grid(True, alpha=0.3, axis='y')
axes[1].set_ylim(0.9, 1.0)

# è¨“ç·´æ™‚é–“æ¯”è¼ƒ
axes[2].bar(models, training_times, color=['blue', 'green', 'orange', 'purple', 'red'])
axes[2].set_ylabel('è¨“ç·´æ™‚é–“ (ç§’)', fontsize=12)
axes[2].set_title('è¨“ç·´æ™‚é–“ï¼ˆçŸ­ã„ã»ã©è‰¯ã„ï¼‰', fontsize=14)
axes[2].tick_params(axis='x', rotation=45)
axes[2].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

### 3.3 ãƒ¢ãƒ‡ãƒ«é¸æŠã®ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ

```mermaid
graph TD
    A[ææ–™ç‰¹æ€§äºˆæ¸¬ã‚¿ã‚¹ã‚¯] --> B{ãƒ‡ãƒ¼ã‚¿æ•°ã¯ï¼Ÿ}
    B -->|< 100| C[ç·šå½¢å›å¸° or SVR]
    B -->|100-1000| D[ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ]
    B -->|> 1000| E{è¨ˆç®—æ™‚é–“ã®åˆ¶ç´„ã¯ï¼Ÿ}

    E -->|å³ã—ã„| F[ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ]
    E -->|ç·©ã„| G[LightGBM or MLP]

    C --> H{è§£é‡ˆæ€§ãŒé‡è¦ï¼Ÿ}
    H -->|ã¯ã„| I[ç·šå½¢å›å¸°]
    H -->|ã„ã„ãˆ| J[SVR]

    D --> K[ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆæ¨å¥¨]
    F --> K
    G --> L{éç·šå½¢æ€§ãŒå¼·ã„ï¼Ÿ}
    L -->|ã¯ã„| M[MLP]
    L -->|ã„ã„ãˆ| N[LightGBM]

    style A fill:#e3f2fd
    style K fill:#c8e6c9
    style M fill:#fff9c4
    style N fill:#fff9c4
    style I fill:#c8e6c9
    style J fill:#c8e6c9
```

### 3.4 ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

**çŠ¶æ³åˆ¥æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ï¼š**

| çŠ¶æ³ | æ¨å¥¨ãƒ¢ãƒ‡ãƒ« | ç†ç”± |
|------|------------|------|
| ãƒ‡ãƒ¼ã‚¿æ•° < 100 | ç·šå½¢å›å¸° or SVR | éå­¦ç¿’ã‚’é˜²æ­¢ã€ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ãŒå®‰å…¨ |
| ãƒ‡ãƒ¼ã‚¿æ•° 100-1000 | ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ | ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒå®¹æ˜“ |
| ãƒ‡ãƒ¼ã‚¿æ•° > 1000 | LightGBM or MLP | å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦ |
| è§£é‡ˆæ€§ãŒé‡è¦ | ç·šå½¢å›å¸° or ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ | ä¿‚æ•°ã‚„ç‰¹å¾´é‡é‡è¦åº¦ãŒåˆ†ã‹ã‚Šã‚„ã™ã„ |
| è¨ˆç®—æ™‚é–“ãŒå³ã—ã„ | ç·šå½¢å›å¸° or ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ | è¨“ç·´ãŒé«˜é€Ÿ |
| æœ€é«˜ç²¾åº¦ãŒå¿…è¦ | LightGBMï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½µç”¨ï¼‰ | Kaggleã‚³ãƒ³ãƒšã§å®Ÿç¸¾å¤šæ•° |
| éç·šå½¢æ€§ãŒå¼·ã„ | MLP or SVR | è¤‡é›‘ãªé–¢ä¿‚ã‚’å­¦ç¿’å¯èƒ½ |

---

## 4. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚

### 4.1 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã¯

**å®šç¾©ï¼š**
æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šå€¤ï¼ˆå­¦ç¿’å‰ã«æ±ºã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼‰ã€‚

**ä¾‹ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼‰ï¼š**
- `n_estimators`: æ±ºå®šæœ¨ã®æ•°ï¼ˆ10, 50, 100, 200...ï¼‰
- `max_depth`: æœ¨ã®æ·±ã•ï¼ˆ3, 5, 10, 20...ï¼‰
- `min_samples_split`: åˆ†å²ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆ2, 5, 10...ï¼‰

**é‡è¦æ€§ï¼š**
é©åˆ‡ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã€æ€§èƒ½ãŒ10-30%å‘ä¸Šã™ã‚‹ã“ã¨ã‚‚ã€‚

### 4.2 Grid Searchï¼ˆã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼‰

**æ¦‚è¦ï¼š**
ã™ã¹ã¦ã®çµ„ã¿åˆã‚ã›ã‚’è©¦ã—ã€æœ€è‰¯ã®ã‚‚ã®ã‚’é¸æŠã€‚

```python
from sklearn.model_selection import GridSearchCV

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€™è£œ
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Grid Searchã®è¨­å®š
grid_search = GridSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_grid=param_grid,
    cv=5,              # 5-foldäº¤å·®æ¤œè¨¼
    scoring='neg_mean_absolute_error',  # MAEã§è©•ä¾¡ï¼ˆå°ã•ã„ã»ã©è‰¯ã„ï¼‰
    n_jobs=-1,         # ä¸¦åˆ—å®Ÿè¡Œ
    verbose=1          # é€²æ—è¡¨ç¤º
)

# Grid Searchå®Ÿè¡Œ
print("===== Grid Searché–‹å§‹ =====")
print(f"æ¢ç´¢ã™ã‚‹çµ„ã¿åˆã‚ã›æ•°: {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf'])}")
start_time = time.time()
grid_search.fit(X_train_rf, y_train_rf)
grid_search_time = time.time() - start_time

# æœ€è‰¯ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
print(f"\n===== Grid Searchå®Œäº†ï¼ˆ{grid_search_time:.2f}ç§’ï¼‰ =====")
print("æœ€è‰¯ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
for param, value in grid_search.best_params_.items():
    print(f"  {param}: {value}")

print(f"\näº¤å·®æ¤œè¨¼MAE: {-grid_search.best_score_:.2f} K")

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è©•ä¾¡
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test_rf)
mae_best = mean_absolute_error(y_test_rf, y_pred_best)
r2_best = r2_score(y_test_rf, y_pred_best)

print(f"\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½:")
print(f"  MAE: {mae_best:.2f} K")
print(f"  RÂ²: {r2_best:.4f}")
```

**ã‚³ãƒ¼ãƒ‰è§£èª¬ï¼š**
1. **param_grid**ï¼šæ¢ç´¢ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¯„å›²
2. **GridSearchCV**ï¼šã™ã¹ã¦ã®çµ„ã¿åˆã‚ã›ï¼ˆ3Ã—4Ã—3Ã—3=108é€šã‚Šï¼‰ã‚’è©¦ã™
3. **cv=5**ï¼š5-foldäº¤å·®æ¤œè¨¼ã§è©•ä¾¡ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚’5åˆ†å‰²ï¼‰
4. **best_params_**ï¼šæœ€è‰¯ã®çµ„ã¿åˆã‚ã›

**æœŸå¾…ã•ã‚Œã‚‹çµæœï¼š**
- Grid Searchæ™‚é–“ï¼š10-60ç§’ï¼ˆãƒ‡ãƒ¼ã‚¿æ•°ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã«ã‚ˆã‚‹ï¼‰
- æœ€è‰¯MAEï¼š10-15 Kï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚ˆã‚Šæ”¹å–„ï¼‰

### 4.3 Random Searchï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒï¼‰

**æ¦‚è¦ï¼š**
ãƒ©ãƒ³ãƒ€ãƒ ã«çµ„ã¿åˆã‚ã›ã‚’è©¦ã™ï¼ˆé«˜é€Ÿã€å¤§è¦æ¨¡æ¢ç´¢å‘ã‘ï¼‰ã€‚

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚’æŒ‡å®š
param_distributions = {
    'n_estimators': randint(50, 300),        # 50-300ã®æ•´æ•°ã‚’ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ
    'max_depth': randint(5, 30),             # 5-30ã®æ•´æ•°
    'min_samples_split': randint(2, 20),     # 2-20ã®æ•´æ•°
    'min_samples_leaf': randint(1, 10),      # 1-10ã®æ•´æ•°
    'max_features': uniform(0.5, 0.5)        # 0.5-1.0ã®å®Ÿæ•°
}

# Random Searchã®è¨­å®š
random_search = RandomizedSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_distributions=param_distributions,
    n_iter=50,         # 50å›ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    cv=5,
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

# Random Searchå®Ÿè¡Œ
print("===== Random Searché–‹å§‹ =====")
start_time = time.time()
random_search.fit(X_train_rf, y_train_rf)
random_search_time = time.time() - start_time

print(f"\n===== Random Searchå®Œäº†ï¼ˆ{random_search_time:.2f}ç§’ï¼‰ =====")
print("æœ€è‰¯ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
for param, value in random_search.best_params_.items():
    print(f"  {param}: {value}")

print(f"\näº¤å·®æ¤œè¨¼MAE: {-random_search.best_score_:.2f} K")
```

**Grid Search vs Random Search:**

| é …ç›® | Grid Search | Random Search |
|------|-------------|---------------|
| æ¢ç´¢æ–¹æ³• | ã™ã¹ã¦ã®çµ„ã¿åˆã‚ã› | ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |
| å®Ÿè¡Œæ™‚é–“ | é•·ã„ï¼ˆå…¨æ¢ç´¢ï¼‰ | çŸ­ã„ï¼ˆæŒ‡å®šå›æ•°ã®ã¿ï¼‰ |
| æœ€è‰¯è§£ã®ä¿è¨¼ | ã‚ã‚Šï¼ˆå…¨æ¢ç´¢ï¼‰ | ãªã—ï¼ˆç¢ºç‡çš„ï¼‰ |
| é©ç”¨å ´é¢ | å°è¦æ¨¡æ¢ç´¢ | å¤§è¦æ¨¡æ¢ç´¢ |

### 4.4 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åŠ¹æœå¯è¦–åŒ–

```python
# Grid Searchã®å…¨çµæœã‚’å–å¾—
results = pd.DataFrame(grid_search.cv_results_)

# n_estimatorsã®å½±éŸ¿ã‚’å¯è¦–åŒ–
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# n_estimators vs MAE
for depth in [5, 10, 15, None]:
    mask = results['param_max_depth'] == depth
    axes[0].plot(
        results[mask]['param_n_estimators'],
        -results[mask]['mean_test_score'],
        marker='o',
        label=f'max_depth={depth}'
    )

axes[0].set_xlabel('n_estimators', fontsize=12)
axes[0].set_ylabel('äº¤å·®æ¤œè¨¼MAE (K)', fontsize=12)
axes[0].set_title('n_estimatorsã®å½±éŸ¿', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# max_depth vs MAE
for n_est in [50, 100, 200]:
    mask = results['param_n_estimators'] == n_est
    axes[1].plot(
        results[mask]['param_max_depth'].apply(lambda x: 20 if x is None else x),
        -results[mask]['mean_test_score'],
        marker='o',
        label=f'n_estimators={n_est}'
    )

axes[1].set_xlabel('max_depth', fontsize=12)
axes[1].set_ylabel('äº¤å·®æ¤œè¨¼MAE (K)', fontsize=12)
axes[1].set_title('max_depthã®å½±éŸ¿', fontsize=14)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## 5. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆææ–™å‘ã‘ï¼‰

ææ–™ãƒ‡ãƒ¼ã‚¿ã«ç‰¹åŒ–ã—ãŸç‰¹å¾´é‡ã‚’ä½œæˆã—ã€äºˆæ¸¬æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚

### 5.1 ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¨ã¯

**å®šç¾©ï¼š**
ç”Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰äºˆæ¸¬ã«æœ‰åŠ¹ãªç‰¹å¾´é‡ã‚’ä½œæˆãƒ»é¸æŠã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã€‚

**é‡è¦æ€§ï¼š**
ã€Œè‰¯ã„ç‰¹å¾´é‡ > é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«ã€
- é©åˆ‡ãªç‰¹å¾´é‡ã§ã€å˜ç´”ãªãƒ¢ãƒ‡ãƒ«ã§ã‚‚é«˜ç²¾åº¦ã‚’é”æˆã§ãã‚‹
- ä¸é©åˆ‡ãªç‰¹å¾´é‡ã§ã¯ã€ã©ã‚“ãªãƒ¢ãƒ‡ãƒ«ã§ã‚‚æ€§èƒ½ã¯ä¸ŠãŒã‚‰ãªã„

### 5.2 Matminerã«ã‚ˆã‚‹è‡ªå‹•ç‰¹å¾´é‡æŠ½å‡º

**Matminerï¼š**
ææ–™ç§‘å­¦å‘ã‘ã®ç‰¹å¾´é‡æŠ½å‡ºãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚

```bash
# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆåˆå›ã®ã¿ï¼‰
pip install matminer
```

```python
from matminer.featurizers.composition import ElementProperty
from pymatgen.core import Composition

# çµ„æˆãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹ï¼šLi2Oï¼‰
compositions = ['Li2O', 'LiCoO2', 'LiFePO4', 'Li4Ti5O12']

# Compositionã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
comp_objects = [Composition(c) for c in compositions]

# ElementPropertyã§ç‰¹å¾´é‡æŠ½å‡º
featurizer = ElementProperty.from_preset('magpie')

# ç‰¹å¾´é‡ã‚’è¨ˆç®—
features = []
for comp in comp_objects:
    feat = featurizer.featurize(comp)
    features.append(feat)

# DataFrameã«å¤‰æ›
feature_names = featurizer.feature_labels()
df_features = pd.DataFrame(features, columns=feature_names)

print("===== Matminerã§æŠ½å‡ºã—ãŸç‰¹å¾´é‡ =====")
print(f"ç‰¹å¾´é‡æ•°: {len(feature_names)}")
print(f"\næœ€åˆã®5ã¤ã®ç‰¹å¾´é‡:")
print(df_features.head())
print(f"\nç‰¹å¾´é‡ã®ä¾‹:")
for i in range(min(5, len(feature_names))):
    print(f"  {feature_names[i]}")
```

**Matminerã§æŠ½å‡ºã•ã‚Œã‚‹ç‰¹å¾´é‡ä¾‹ï¼š**
- `MagpieData avg_dev MeltingT`ï¼šå¹³å‡èç‚¹ã®åå·®
- `MagpieData mean Electronegativity`ï¼šå¹³å‡é›»æ°—é™°æ€§åº¦
- `MagpieData mean AtomicWeight`ï¼šå¹³å‡åŸå­é‡
- `MagpieData range Number`ï¼šåŸå­ç•ªå·ã®ç¯„å›²
- åˆè¨ˆ130ä»¥ä¸Šã®ç‰¹å¾´é‡

### 5.3 æ‰‹å‹•ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

```python
# åŸºæœ¬ãƒ‡ãƒ¼ã‚¿
data_advanced = pd.DataFrame({
    'element_A': [0.5, 0.6, 0.7, 0.8],
    'element_B': [0.5, 0.4, 0.3, 0.2],
    'melting_point': [1200, 1250, 1300, 1350]
})

# æ–°ã—ã„ç‰¹å¾´é‡ã‚’ä½œæˆ
data_advanced['sum_AB'] = data_advanced['element_A'] + data_advanced['element_B']  # åˆè¨ˆï¼ˆå¸¸ã«1.0ï¼‰
data_advanced['diff_AB'] = abs(data_advanced['element_A'] - data_advanced['element_B'])  # å·®ã®çµ¶å¯¾å€¤
data_advanced['product_AB'] = data_advanced['element_A'] * data_advanced['element_B']  # ç©ï¼ˆç›¸äº’ä½œç”¨ï¼‰
data_advanced['ratio_AB'] = data_advanced['element_A'] / (data_advanced['element_B'] + 1e-10)  # æ¯”ç‡
data_advanced['A_squared'] = data_advanced['element_A'] ** 2  # äºŒä¹—é …ï¼ˆéç·šå½¢æ€§ï¼‰
data_advanced['B_squared'] = data_advanced['element_B'] ** 2

print("===== ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ =====")
print(data_advanced)
```

### 5.4 ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ

```python
# æ‹¡å¼µç‰¹å¾´é‡ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´
X_advanced = data_advanced.drop('melting_point', axis=1)
y_advanced = data_advanced['melting_point']

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã§è¨“ç·´
model_advanced = RandomForestRegressor(n_estimators=100, random_state=42)
model_advanced.fit(X_advanced, y_advanced)

# ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—
importances = pd.DataFrame({
    'ç‰¹å¾´é‡': X_advanced.columns,
    'é‡è¦åº¦': model_advanced.feature_importances_
}).sort_values('é‡è¦åº¦', ascending=False)

print("===== ç‰¹å¾´é‡é‡è¦åº¦ =====")
print(importances)

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.barh(importances['ç‰¹å¾´é‡'], importances['é‡è¦åº¦'])
plt.xlabel('é‡è¦åº¦', fontsize=12)
plt.title('ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼‰', fontsize=14)
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()
```

### 5.5 ç‰¹å¾´é‡é¸æŠ

**ç›®çš„ï¼š**
äºˆæ¸¬ã«å¯„ä¸ã—ãªã„ç‰¹å¾´é‡ã‚’å‰Šé™¤ï¼ˆéå­¦ç¿’é˜²æ­¢ã€è¨ˆç®—æ™‚é–“çŸ­ç¸®ï¼‰ã€‚

```python
from sklearn.feature_selection import SelectKBest, f_regression

# SelectKBest: ä¸Šä½Kå€‹ã®ç‰¹å¾´é‡ã‚’é¸æŠ
selector = SelectKBest(score_func=f_regression, k=3)  # ä¸Šä½3å€‹
X_selected = selector.fit_transform(X_advanced, y_advanced)

# é¸ã°ã‚ŒãŸç‰¹å¾´é‡
selected_features = X_advanced.columns[selector.get_support()]
print(f"é¸ã°ã‚ŒãŸç‰¹å¾´é‡: {list(selected_features)}")

# é¸æŠå¾Œã®ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model_selected = RandomForestRegressor(n_estimators=100, random_state=42)
model_selected.fit(X_selected, y_advanced)

print(f"ç‰¹å¾´é‡é¸æŠå‰: {X_advanced.shape[1]}å€‹")
print(f"ç‰¹å¾´é‡é¸æŠå¾Œ: {X_selected.shape[1]}å€‹")
```

---

## 6. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

å®Ÿè·µã§é­é‡ã—ã‚„ã™ã„ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºç­–ã€‚

### 6.1 ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ä¸€è¦§

| ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ | åŸå›  | è§£æ±ºæ–¹æ³• |
|------------------|------|----------|
| `ModuleNotFoundError: No module named 'sklearn'` | scikit-learnæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« | `pip install scikit-learn` |
| `MemoryError` | ãƒ¡ãƒ¢ãƒªä¸è¶³ | ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºå‰Šæ¸›ã€ãƒãƒƒãƒå‡¦ç†ã€Google Colabåˆ©ç”¨ |
| `ConvergenceWarning: lbfgs failed to converge` | MLPã®å­¦ç¿’ãŒåæŸã›ãš | `max_iter`ã‚’å¢—ã‚„ã™ï¼ˆä¾‹ï¼š1000ï¼‰ã€å­¦ç¿’ç‡èª¿æ•´ |
| `ValueError: Input contains NaN` | ãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ | `df.dropna()`ã§å‰Šé™¤ or `df.fillna()`ã§è£œå®Œ |
| `ValueError: could not convert string to float` | æ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹ | `pd.get_dummies()`ã§ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ– |
| `RÂ² is negative` | ãƒ¢ãƒ‡ãƒ«ãŒãƒ©ãƒ³ãƒ€ãƒ äºˆæ¸¬ã‚ˆã‚Šæ‚ªã„ | ç‰¹å¾´é‡ã‚’è¦‹ç›´ã™ã€ãƒ¢ãƒ‡ãƒ«å¤‰æ›´ |
| `ZeroDivisionError` | 0é™¤ç®— | åˆ†æ¯ã«å°ã•ã„å€¤ã‚’è¿½åŠ ï¼ˆä¾‹ï¼š`x / (y + 1e-10)`ï¼‰ |

### 6.2 ãƒ‡ãƒãƒƒã‚°ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

**ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª**
```python
# ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬çµ±è¨ˆ
print(df.describe())

# æ¬ æå€¤ã®ç¢ºèª
print(df.isnull().sum())

# ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèª
print(df.dtypes)

# ç„¡é™å¤§ãƒ»NaNã®ç¢ºèª
print(df.isin([np.inf, -np.inf]).sum())
```

**ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–**
```python
# åˆ†å¸ƒã‚’ç¢ºèª
df.hist(figsize=(12, 8), bins=30)
plt.tight_layout()
plt.show()

# ç›¸é–¢è¡Œåˆ—
import seaborn as sns
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('ç›¸é–¢è¡Œåˆ—')
plt.show()
```

**ã‚¹ãƒ†ãƒƒãƒ—3: å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ**
```python
# æœ€åˆã®10ä»¶ã ã‘ã§ãƒ†ã‚¹ãƒˆ
X_small = X[:10]
y_small = y[:10]

model_test = RandomForestRegressor(n_estimators=10)
model_test.fit(X_small, y_small)
print("å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®è¨“ç·´æˆåŠŸ")
```

**ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ¢ãƒ‡ãƒ«ã®ç°¡ç•¥åŒ–**
```python
# è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã§å¤±æ•—ã—ãŸã‚‰ã€ã¾ãšç·šå½¢å›å¸°ã§è©¦ã™
model_simple = LinearRegression()
model_simple.fit(X_train, y_train)
print(f"ç·šå½¢å›å¸°ã®RÂ²: {model_simple.score(X_test, y_test):.4f}")
```

**ã‚¹ãƒ†ãƒƒãƒ—5: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’èª­ã‚€**
```python
try:
    model.fit(X_train, y_train)
except Exception as e:
    print(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {type(e).__name__}")
    print(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {str(e)}")
    import traceback
    traceback.print_exc()
```

### 6.3 æ€§èƒ½ãŒä½ã„å ´åˆã®å¯¾å‡¦æ³•

| ç—‡çŠ¶ | è€ƒãˆã‚‰ã‚Œã‚‹åŸå›  | å¯¾å‡¦æ³• |
|------|----------------|--------|
| RÂ² < 0.5 | ç‰¹å¾´é‡ãŒä¸é©åˆ‡ | ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€Matmineråˆ©ç”¨ |
| è¨“ç·´èª¤å·®ã¯å°ã€ãƒ†ã‚¹ãƒˆèª¤å·®ã¯å¤§ | éå­¦ç¿’ | æ­£å‰‡åŒ–å¼·åŒ–ã€ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã€ãƒ¢ãƒ‡ãƒ«ç°¡ç•¥åŒ– |
| è¨“ç·´èª¤å·®ã‚‚ãƒ†ã‚¹ãƒˆèª¤å·®ã‚‚å¤§ | æœªå­¦ç¿’ | ãƒ¢ãƒ‡ãƒ«è¤‡é›‘åŒ–ã€ç‰¹å¾´é‡è¿½åŠ ã€å­¦ç¿’ç‡èª¿æ•´ |
| äºˆæ¸¬å€¤ãŒå…¨ã¦åŒã˜ | ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã§ãã¦ã„ãªã„ | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¦‹ç›´ã—ã€ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° |
| è¨“ç·´ãŒé…ã„ | ãƒ‡ãƒ¼ã‚¿é‡orãƒ¢ãƒ‡ãƒ«ãŒå¤§ãã„ | ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€ãƒ¢ãƒ‡ãƒ«ç°¡ç•¥åŒ–ã€ä¸¦åˆ—åŒ– |

---

## 7. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒ£ãƒ¬ãƒ³ã‚¸ï¼šãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬

å­¦ã‚“ã ã“ã¨ã‚’çµ±åˆã—ã€å®Ÿè·µçš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å–ã‚Šçµ„ã¿ã¾ã—ã‚‡ã†ã€‚

### 7.1 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

**ç›®æ¨™ï¼š**
çµ„æˆã‹ã‚‰ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ã‚’äºˆæ¸¬ã™ã‚‹MIãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰

**ç›®æ¨™æ€§èƒ½ï¼š**
- RÂ² > 0.7ï¼ˆèª¬æ˜åŠ›70%ä»¥ä¸Šï¼‰
- MAE < 0.5 eVï¼ˆèª¤å·®0.5 eVä»¥ä¸‹ï¼‰

**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼š**
Materials Project APIï¼ˆã¾ãŸã¯æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ï¼‰

### 7.2 ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰

**Step 1: ãƒ‡ãƒ¼ã‚¿åé›†**
```python
# Materials Project APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆæ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã§ä»£æ›¿å¯ï¼‰
# ç›®æ¨™ï¼š100ä»¶ä»¥ä¸Šã®é…¸åŒ–ç‰©ãƒ‡ãƒ¼ã‚¿

data_project = pd.DataFrame({
    'formula': ['Li2O', 'Na2O', 'MgO', 'Al2O3', 'SiO2'] * 20,
    'Li_ratio': [0.67, 0.0, 0.0, 0.0, 0.0] * 20,
    'O_ratio': [0.33, 0.67, 0.5, 0.6, 0.67] * 20,
    'band_gap': [7.5, 5.2, 7.8, 8.8, 9.0] * 20
})

# ãƒã‚¤ã‚ºè¿½åŠ ï¼ˆã‚ˆã‚Šç¾å®Ÿçš„ã«ï¼‰
np.random.seed(42)
data_project['band_gap'] += np.random.normal(0, 0.3, len(data_project))

print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(data_project)}")
```

**Step 2: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**
```python
# å…ƒç´ æ¯”ç‡ã‹ã‚‰è¿½åŠ ç‰¹å¾´é‡ã‚’ä½œæˆ
# ï¼ˆå®Ÿéš›ã«ã¯Matminerã§åŸå­ç‰¹æ€§ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ï¼‰

data_project['sum_elements'] = data_project['Li_ratio'] + data_project['O_ratio']
data_project['product_LiO'] = data_project['Li_ratio'] * data_project['O_ratio']
```

**Step 3: ãƒ‡ãƒ¼ã‚¿åˆ†å‰²**
```python
X_project = data_project[['Li_ratio', 'O_ratio', 'sum_elements', 'product_LiO']]
y_project = data_project['band_gap']

X_train_proj, X_test_proj, y_train_proj, y_test_proj = train_test_split(
    X_project, y_project, test_size=0.2, random_state=42
)
```

**Step 4: ãƒ¢ãƒ‡ãƒ«é¸æŠã¨è¨“ç·´**
```python
# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’ä½¿ç”¨
model_project = RandomForestRegressor(
    n_estimators=200,
    max_depth=15,
    random_state=42
)
model_project.fit(X_train_proj, y_train_proj)
```

**Step 5: è©•ä¾¡**
```python
y_pred_proj = model_project.predict(X_test_proj)
mae_proj = mean_absolute_error(y_test_proj, y_pred_proj)
r2_proj = r2_score(y_test_proj, y_pred_proj)

print(f"===== ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçµæœ =====")
print(f"MAE: {mae_proj:.2f} eV")
print(f"RÂ²: {r2_proj:.4f}")

if r2_proj > 0.7 and mae_proj < 0.5:
    print("ğŸ‰ ç›®æ¨™é”æˆï¼")
else:
    print("âŒ ç›®æ¨™æœªé”æˆã€‚ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
```

**Step 6: å¯è¦–åŒ–**
```python
plt.figure(figsize=(10, 6))
plt.scatter(y_test_proj, y_pred_proj, alpha=0.6, s=100)
plt.plot([y_test_proj.min(), y_test_proj.max()],
         [y_test_proj.min(), y_test_proj.max()],
         'r--', lw=2, label='å®Œå…¨ãªäºˆæ¸¬')
plt.xlabel('å®Ÿæ¸¬ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ— (eV)', fontsize=12)
plt.ylabel('äºˆæ¸¬ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ— (eV)', fontsize=12)
plt.title('ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.text(0.05, 0.95, f'RÂ² = {r2_proj:.3f}\nMAE = {mae_proj:.3f} eV',
         transform=plt.gca().transAxes, fontsize=12, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
plt.tight_layout()
plt.show()
```

### 7.3 ç™ºå±•èª²é¡Œ

**åˆç´šï¼š**
- åˆ¥ã®ææ–™ç‰¹æ€§ï¼ˆèç‚¹ã€å½¢æˆã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰ã§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰

**ä¸­ç´šï¼š**
- Matminerã§130ä»¥ä¸Šã®ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã€æ€§èƒ½å‘ä¸Šã‚’ç›®æŒ‡ã™
- äº¤å·®æ¤œè¨¼ã§ãƒ¢ãƒ‡ãƒ«ã®ä¿¡é ¼æ€§ã‚’è©•ä¾¡

**ä¸Šç´šï¼š**
- Materials Project APIã‹ã‚‰å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
- ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ï¼ˆè¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ï¼‰
- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆMLPï¼‰ã§äºˆæ¸¬

---

## 8. ã¾ã¨ã‚

### ã“ã®ç« ã§å­¦ã‚“ã ã“ã¨

1. **ç’°å¢ƒæ§‹ç¯‰**
   - Anacondaã€venvã€Google Colabã®3ã¤ã®é¸æŠè‚¢
   - çŠ¶æ³ã«å¿œã˜ãŸæœ€é©ãªç’°å¢ƒã®é¸ã³æ–¹

2. **6ã¤ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«**
   - ç·šå½¢å›å¸°ï¼ˆBaselineï¼‰
   - ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼ˆãƒãƒ©ãƒ³ã‚¹å‹ï¼‰
   - LightGBMï¼ˆé«˜ç²¾åº¦ï¼‰
   - SVRï¼ˆéç·šå½¢å¯¾å¿œï¼‰
   - MLPï¼ˆæ·±å±¤å­¦ç¿’ï¼‰
   - Materials Projectå®Ÿãƒ‡ãƒ¼ã‚¿çµ±åˆ

3. **ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³**
   - ãƒ‡ãƒ¼ã‚¿æ•°ã€è¨ˆç®—æ™‚é–“ã€è§£é‡ˆæ€§ã«å¿œã˜ãŸæœ€é©ãƒ¢ãƒ‡ãƒ«
   - æ€§èƒ½æ¯”è¼ƒè¡¨ã¨ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ

4. **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**
   - Grid Searchã¨Random Search
   - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åŠ¹æœå¯è¦–åŒ–

5. **ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**
   - Matminerã«ã‚ˆã‚‹è‡ªå‹•æŠ½å‡º
   - æ‰‹å‹•ç‰¹å¾´é‡ä½œæˆï¼ˆç›¸äº’ä½œç”¨é …ã€äºŒä¹—é …ï¼‰
   - ç‰¹å¾´é‡é‡è¦åº¦ã¨é¸æŠ

6. **ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**
   - ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºç­–
   - ãƒ‡ãƒãƒƒã‚°ã®5ã‚¹ãƒ†ãƒƒãƒ—

7. **å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**
   - ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬ã®å®Œå…¨ãªå®Ÿè£…
   - ç›®æ¨™é”æˆã®ãŸã‚ã®ã‚¹ãƒ†ãƒƒãƒ—

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’çµ‚ãˆãŸã‚ãªãŸã¯ï¼š**
- âœ… ææ–™ç‰¹æ€§äºˆæ¸¬ã®å®Ÿè£…ãŒã§ãã‚‹
- âœ… 5ã¤ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹
- âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒã§ãã‚‹
- âœ… ã‚¨ãƒ©ãƒ¼ã‚’è‡ªåŠ›ã§è§£æ±ºã§ãã‚‹

**æ¬¡ã«å­¦ã¶ã¹ãå†…å®¹ï¼š**
1. **æ·±å±¤å­¦ç¿’ã®å¿œç”¨**
   - Graph Neural Networksï¼ˆGNNï¼‰
   - Crystal Graph Convolutional Networksï¼ˆCGCNNï¼‰

2. **ãƒ™ã‚¤ã‚ºæœ€é©åŒ–**
   - å®Ÿé¨“å›æ•°ã‚’æœ€å°åŒ–ã™ã‚‹æ‰‹æ³•
   - Gaussian Processå›å¸°

3. **è»¢ç§»å­¦ç¿’**
   - å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§é«˜ç²¾åº¦ã‚’å®Ÿç¾
   - äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ´»ç”¨

---

## æ¼”ç¿’å•é¡Œ

### å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰

æœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§å®Ÿè£…ã—ãŸ6ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®ä¸­ã§ã€ãƒ‡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„å ´åˆï¼ˆ< 100ä»¶ï¼‰ã«æœ€ã‚‚é©ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸ã³ã€ç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

éå­¦ç¿’ã®ãƒªã‚¹ã‚¯ã¨ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ã‚’è€ƒæ…®ã—ã¾ã—ã‚‡ã†ã€‚

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

**ç­”ãˆï¼šç·šå½¢å›å¸°**

**ç†ç”±ï¼š**
1. **éå­¦ç¿’ã®ãƒªã‚¹ã‚¯ãŒä½ã„**ï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„ãŸã‚ã€å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§ã‚‚å®‰å®š
2. **è§£é‡ˆæ€§ãŒé«˜ã„**ï¼šä¿‚æ•°ã‚’è¦‹ã‚Œã°ç‰¹å¾´é‡ã®å½±éŸ¿ãŒåˆ†ã‹ã‚‹
3. **è¨“ç·´ãŒé«˜é€Ÿ**ï¼šè¨ˆç®—ã‚³ã‚¹ãƒˆãŒä½ã„

**ä»–ã®å€™è£œï¼šSVR**
- éç·šå½¢æ€§ãŒå¼·ã„å ´åˆã¯SVRã‚‚æœ‰åŠ¹
- ãŸã ã—ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒå¿…è¦

ãƒ‡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„å ´åˆã€è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€MLPï¼‰ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’æš—è¨˜ã—ã¦ã—ã¾ã„ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§æ€§èƒ½ãŒå¤§å¹…ã«ä½ä¸‹ã—ã¾ã™ï¼ˆéå­¦ç¿’ï¼‰ã€‚

</details>

---

### å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰

Grid Searchã¨Random Searchã‚’æ¯”è¼ƒã—ã€ã©ã®ã‚ˆã†ãªçŠ¶æ³ã§å„æ‰‹æ³•ã‚’ä½¿ã†ã¹ãã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

æ¢ç´¢ç©ºé–“ã®å¤§ãã•ã¨è¨ˆç®—æ™‚é–“ã®åˆ¶ç´„ã‚’è€ƒæ…®ã—ã¾ã—ã‚‡ã†ã€‚

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

**Grid Search ã‚’ä½¿ã†ã¹ãçŠ¶æ³ï¼š**
1. **æ¢ç´¢ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå°‘ãªã„**ï¼ˆ2-3å€‹ï¼‰
2. **å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€™è£œãŒå°‘ãªã„**ï¼ˆå„3-5å€‹ç¨‹åº¦ï¼‰
3. **è¨ˆç®—æ™‚é–“ã«ä½™è£•ãŒã‚ã‚‹**
4. **æœ€è‰¯è§£ã‚’ç¢ºå®Ÿã«è¦‹ã¤ã‘ãŸã„**

**ä¾‹ï¼š** n_estimators=[50, 100, 200] Ã— max_depth=[5, 10, 15] = 9é€šã‚Š

**Random Search ã‚’ä½¿ã†ã¹ãçŠ¶æ³ï¼š**
1. **æ¢ç´¢ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¤šã„**ï¼ˆ4å€‹ä»¥ä¸Šï¼‰
2. **å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€™è£œãŒå¤šã„/é€£ç¶šå€¤**
3. **è¨ˆç®—æ™‚é–“ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹**
4. **ã‚ã‚‹ç¨‹åº¦è‰¯ã„è§£ãŒè¦‹ã¤ã‹ã‚Œã°ååˆ†**

**ä¾‹ï¼š** 5å€‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€å„10å€™è£œ = 100,000é€šã‚Š â†’ Random Searchã§100å›ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

**ä¸€èˆ¬çš„ãªæˆ¦ç•¥ï¼š**
1. ã¾ãšRandom Searchã§å¤§ã¾ã‹ãªç¯„å›²ã‚’çµã‚‹ï¼ˆ100-200å›ï¼‰
2. æœ‰æœ›ãªç¯„å›²ã‚’Grid Searchã§è©³ç´°æ¢ç´¢

</details>

---

### å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰

ä»¥ä¸‹ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚åŸå› ã¨è§£æ±ºæ–¹æ³•ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

```
ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
```

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

MLPRegressor ã®è¨“ç·´ã§ç™ºç”Ÿã™ã‚‹ã‚¨ãƒ©ãƒ¼ã§ã™ã€‚

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

**åŸå› ï¼š**
MLPRegressorï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰ã®è¨“ç·´ãŒã€æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ï¼ˆmax_iterï¼‰ä»¥å†…ã«åæŸã—ãªã‹ã£ãŸã€‚

**è€ƒãˆã‚‰ã‚Œã‚‹è¦å› ï¼š**
1. max_iterãŒå°ã•ã™ãã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ200ï¼‰
2. å­¦ç¿’ç‡ãŒå°ã•ã™ãã‚‹ï¼ˆå­¦ç¿’ãŒé…ã„ï¼‰
3. ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒä¸é©åˆ‡ï¼ˆæ¨™æº–åŒ–ã—ã¦ã„ãªã„ï¼‰
4. ãƒ¢ãƒ‡ãƒ«ãŒè¤‡é›‘ã™ãã‚‹ï¼ˆå±¤æ•°ãŒå¤šã„ã€ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ãŒå¤šã„ï¼‰

**è§£æ±ºæ–¹æ³•ï¼š**

**æ–¹æ³•1: max_iterã‚’å¢—ã‚„ã™**
```python
model_mlp = MLPRegressor(max_iter=1000)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ200â†’1000
```

**æ–¹æ³•2: ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–åŒ–**
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

**æ–¹æ³•3: å­¦ç¿’ç‡ã‚’èª¿æ•´**
```python
model_mlp = MLPRegressor(
    learning_rate_init=0.01,  # å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹
    max_iter=500
)
```

**æ–¹æ³•4: Early Stoppingã‚’æœ‰åŠ¹åŒ–**
```python
model_mlp = MLPRegressor(
    early_stopping=True,  # æ¤œè¨¼èª¤å·®ãŒæ”¹å–„ã—ãªã‘ã‚Œã°åœæ­¢
    validation_fraction=0.2,
    max_iter=1000
)
```

**æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼š**
ã¾ãšæ–¹æ³•2ï¼ˆãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–ï¼‰ã‚’è©¦ã—ã€ãã‚Œã§ã‚‚åæŸã—ãªã‘ã‚Œã°æ–¹æ³•1ã¨4ã‚’ä½µç”¨ã€‚

</details>

---

### å•é¡Œ4ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰

Matminerã‚’ä½¿ã£ã¦ã€çµ„æˆ `"Li2O"` ã‹ã‚‰5ã¤ä»¥ä¸Šã®ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

`ElementProperty` featurizerã¨ `from_preset('magpie')` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

```python
from matminer.featurizers.composition import ElementProperty
from pymatgen.core import Composition
import pandas as pd

# çµ„æˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
comp = Composition("Li2O")

# Magpieãƒ—ãƒªã‚»ãƒƒãƒˆã§ç‰¹å¾´é‡æŠ½å‡ºå™¨ã‚’åˆæœŸåŒ–
featurizer = ElementProperty.from_preset('magpie')

# ç‰¹å¾´é‡ã‚’è¨ˆç®—
features = featurizer.featurize(comp)

# ç‰¹å¾´é‡åã‚’å–å¾—
feature_names = featurizer.feature_labels()

# DataFrameã«å¤‰æ›ï¼ˆè¦‹ã‚„ã™ãï¼‰
df = pd.DataFrame([features], columns=feature_names)

print(f"===== Li2Oã®ç‰¹å¾´é‡ï¼ˆæœ€åˆã®5ã¤ï¼‰ =====")
for i in range(5):
    print(f"{feature_names[i]}: {features[i]:.4f}")

print(f"\nåˆè¨ˆç‰¹å¾´é‡æ•°: {len(features)}")
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ï¼š**
```
===== Li2Oã®ç‰¹å¾´é‡ï¼ˆæœ€åˆã®5ã¤ï¼‰ =====
MagpieData minimum Number: 3.0000
MagpieData maximum Number: 8.0000
MagpieData range Number: 5.0000
MagpieData mean Number: 5.3333
MagpieData avg_dev Number: 1.5556

åˆè¨ˆç‰¹å¾´é‡æ•°: 132
```

**è§£èª¬ï¼š**
- `MagpieData minimum Number`: æœ€å°åŸå­ç•ªå·ï¼ˆLi: 3ï¼‰
- `MagpieData maximum Number`: æœ€å¤§åŸå­ç•ªå·ï¼ˆO: 8ï¼‰
- `MagpieData range Number`: åŸå­ç•ªå·ã®ç¯„å›²ï¼ˆ8-3=5ï¼‰
- `MagpieData mean Number`: å¹³å‡åŸå­ç•ªå·ï¼ˆ(3+3+8)/3=5.33ï¼‰
- `MagpieData avg_dev Number`: åŸå­ç•ªå·ã®å¹³å‡åå·®

Matminerã¯132å€‹ã®ç‰¹å¾´é‡ã‚’è‡ªå‹•æŠ½å‡ºã—ã¾ã™ï¼ˆé›»æ°—é™°æ€§åº¦ã€åŸå­åŠå¾„ã€èç‚¹ãªã©ï¼‰ã€‚

</details>

---

### å•é¡Œ5ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰

ãƒãƒ³ãƒ‰ã‚®ãƒ£ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§RÂ²ãŒ0.5ã—ã‹å‡ºã¾ã›ã‚“ã§ã—ãŸã€‚æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®3ã¤ã®å…·ä½“çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã€ãã‚Œãã‚Œã®å®Ÿè£…æ–¹æ³•ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

ç‰¹å¾´é‡ã€ãƒ¢ãƒ‡ãƒ«ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®3ã¤ã®è¦³ç‚¹ã‹ã‚‰è€ƒãˆã¾ã—ã‚‡ã†ã€‚

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆæœ€ã‚‚åŠ¹æœçš„ï¼‰**

**å®Ÿè£…æ–¹æ³•ï¼š**
```python
from matminer.featurizers.composition import ElementProperty
from pymatgen.core import Composition

# çµ„æˆã‹ã‚‰åŸå­ç‰¹æ€§ã‚’æŠ½å‡º
def extract_features(formula):
    comp = Composition(formula)
    featurizer = ElementProperty.from_preset('magpie')
    features = featurizer.featurize(comp)
    return features

# æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«ç‰¹å¾´é‡ã‚’è¿½åŠ 
data_project['features'] = data_project['formula'].apply(extract_features)
# DataFrameã«å±•é–‹ï¼ˆ132æ¬¡å…ƒã®ç‰¹å¾´é‡ï¼‰
features_df = pd.DataFrame(data_project['features'].tolist())
X_enhanced = features_df  # å…ƒã®2æ¬¡å…ƒ â†’ 132æ¬¡å…ƒã«æ‹¡å¼µ
```

**æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„ï¼š**
RÂ² 0.5 â†’ 0.75-0.85ï¼ˆç‰¹å¾´é‡ãŒå¤§å¹…ã«å¢—ãˆã‚‹ãŸã‚ï¼‰

---

**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ï¼ˆè¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ï¼‰**

**å®Ÿè£…æ–¹æ³•ï¼š**
```python
from sklearn.ensemble import VotingRegressor

# 3ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›
model_rf = RandomForestRegressor(n_estimators=200, random_state=42)
model_lgb = lgb.LGBMRegressor(n_estimators=200, random_state=42)
model_svr = SVR(kernel='rbf', C=100)

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆå¹³å‡äºˆæ¸¬ï¼‰
ensemble = VotingRegressor([
    ('rf', model_rf),
    ('lgb', model_lgb),
    ('svr', model_svr)
])

ensemble.fit(X_train, y_train)
y_pred_ensemble = ensemble.predict(X_test)
```

**æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„ï¼š**
RÂ² 0.5 â†’ 0.6-0.7ï¼ˆå˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šå®‰å®šï¼‰

---

**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ3: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**

**å®Ÿè£…æ–¹æ³•ï¼š**
```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_dist = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(10, 50),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10)
}

random_search = RandomizedSearchCV(
    RandomForestRegressor(random_state=42),
    param_distributions=param_dist,
    n_iter=100,  # 100é€šã‚Šè©¦ã™
    cv=5,
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    random_state=42
)

random_search.fit(X_train, y_train)
best_model = random_search.best_estimator_
```

**æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„ï¼š**
RÂ² 0.5 â†’ 0.55-0.65ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚ˆã‚Šæœ€é©åŒ–ï¼‰

---

**æœ€é©ãªæˆ¦ç•¥ï¼š**
1. ã¾ãš**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1**ï¼ˆç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼‰ã‚’å®Ÿæ–½ â†’ æœ€å¤§ã®åŠ¹æœ
2. æ¬¡ã«**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ3**ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ã§å¾®èª¿æ•´
3. æœ€å¾Œã«**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2**ï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰ã§æœ€çµ‚çš„ãªæ€§èƒ½å‘ä¸Š

ã“ã®é †åºã§ã€RÂ² 0.5 â†’ 0.8ä»¥ä¸Šã‚’ç›®æŒ‡ã›ã¾ã™ã€‚

</details>

---

## å‚è€ƒæ–‡çŒ®

1. Pedregosa, F., et al. (2011). "Scikit-learn: Machine Learning in Python." *Journal of Machine Learning Research*, 12, 2825-2830.
   URL: https://scikit-learn.org
   *scikit-learnå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€‚ã™ã¹ã¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è©³ç´°ãªè§£èª¬ã¨ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã€‚*

2. Ward, L., et al. (2018). "Matminer: An open source toolkit for materials data mining." *Computational Materials Science*, 152, 60-69.
   DOI: [10.1016/j.commatsci.2018.05.018](https://doi.org/10.1016/j.commatsci.2018.05.018)
   GitHub: https://github.com/hackingmaterials/matminer
   *ææ–™ç§‘å­¦å‘ã‘ç‰¹å¾´é‡æŠ½å‡ºãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚132ç¨®é¡ã®ææ–™è¨˜è¿°å­ã‚’è‡ªå‹•ç”Ÿæˆã€‚*

3. Jain, A., et al. (2013). "Commentary: The Materials Project: A materials genome approach to accelerating materials innovation." *APL Materials*, 1(1), 011002.
   DOI: [10.1063/1.4812323](https://doi.org/10.1063/1.4812323)
   URL: https://materialsproject.org
   *Materials Projectå…¬å¼è«–æ–‡ã€‚140,000ç¨®é¡ä»¥ä¸Šã®ææ–™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€‚*

4. Ke, G., et al. (2017). "LightGBM: A Highly Efficient Gradient Boosting Decision Tree." *Advances in Neural Information Processing Systems*, 30, 3146-3154.
   GitHub: https://github.com/microsoft/LightGBM
   *LightGBMå…¬å¼è«–æ–‡ã€‚å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã®é«˜é€Ÿå®Ÿè£…ã€‚*

5. Bergstra, J., & Bengio, Y. (2012). "Random Search for Hyper-Parameter Optimization." *Journal of Machine Learning Research*, 13, 281-305.
   URL: https://www.jmlr.org/papers/v13/bergstra12a.html
   *Random Searchã®ç†è«–çš„èƒŒæ™¯ã€‚Grid Searchã‚ˆã‚ŠåŠ¹ç‡çš„ãªæ¢ç´¢æ‰‹æ³•ã€‚*

6. Raschka, S., & Mirjalili, V. (2019). *Python Machine Learning, 3rd Edition*. Packt Publishing.
   *Pythonã«ã‚ˆã‚‹æ©Ÿæ¢°å­¦ç¿’ã®åŒ…æ‹¬çš„ãªæ•™ç§‘æ›¸ã€‚scikit-learnã®å®Ÿè·µçš„ãªä½¿ã„æ–¹ã‚’è©³èª¬ã€‚*

7. scikit-learn User Guide. (2024). "Hyperparameter tuning."
   URL: https://scikit-learn.org/stable/modules/grid_search.html
   *ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å…¬å¼ã‚¬ã‚¤ãƒ‰ã€‚Grid Searchã€Random Searchã®è©³ç´°ã€‚*

---

**ä½œæˆæ—¥**: 2025-10-16
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 3.0
**ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**: content_agent_prompts.py v1.0
**è‘—è€…**: MI Knowledge Hub ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
