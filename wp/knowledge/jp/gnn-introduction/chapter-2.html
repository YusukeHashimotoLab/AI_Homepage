<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬2ç« ï¼šGNNã®åŸºç¤ç†è«– - AI Terakoya</title>

    <style>
        :root {
            --color-primary: #2c3e50;
            --color-primary-dark: #1a252f;
            --color-accent: #7b2cbf;
            --color-accent-light: #9d4edd;
            --color-text: #2d3748;
            --color-text-light: #4a5568;
            --color-bg: #ffffff;
            --color-bg-alt: #f7fafc;
            --color-border: #e2e8f0;
            --color-code-bg: #f8f9fa;
            --color-link: #3182ce;
            --color-link-hover: #2c5aa0;

            --spacing-xs: 0.5rem;
            --spacing-sm: 1rem;
            --spacing-md: 1.5rem;
            --spacing-lg: 2rem;
            --spacing-xl: 3rem;

            --font-body: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;

            --border-radius: 8px;
            --box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-body);
            line-height: 1.7;
            color: var(--color-text);
            background-color: var(--color-bg);
            font-size: 16px;
        }

        header {
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            padding: var(--spacing-xl) var(--spacing-md);
            margin-bottom: var(--spacing-xl);
            box-shadow: var(--box-shadow);
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: var(--spacing-sm);
            line-height: 1.2;
        }

        .subtitle {
            font-size: 1.1rem;
            opacity: 0.95;
            font-weight: 400;
            margin-bottom: var(--spacing-md);
        }

        .meta {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-md);
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 var(--spacing-md) var(--spacing-xl);
        }

        h2 {
            font-size: 1.75rem;
            color: var(--color-primary);
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            padding-bottom: var(--spacing-xs);
            border-bottom: 3px solid var(--color-accent);
        }

        h3 {
            font-size: 1.4rem;
            color: var(--color-primary);
            margin-top: var(--spacing-lg);
            margin-bottom: var(--spacing-sm);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-top: var(--spacing-md);
            margin-bottom: var(--spacing-sm);
        }

        p {
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        a {
            color: var(--color-link);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--color-link-hover);
            text-decoration: underline;
        }

        ul, ol {
            margin-left: var(--spacing-lg);
            margin-bottom: var(--spacing-md);
        }

        li {
            margin-bottom: var(--spacing-xs);
            color: var(--color-text);
        }

        pre {
            background-color: var(--color-code-bg);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            overflow-x: auto;
            margin-bottom: var(--spacing-md);
            font-family: var(--font-mono);
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.9em;
            background-color: var(--color-code-bg);
            padding: 0.2em 0.4em;
            border-radius: 3px;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: var(--spacing-md);
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--color-border);
            padding: var(--spacing-sm);
            text-align: left;
        }

        th {
            background-color: var(--color-bg-alt);
            font-weight: 600;
            color: var(--color-primary);
        }

        blockquote {
            border-left: 4px solid var(--color-accent);
            padding-left: var(--spacing-md);
            margin: var(--spacing-md) 0;
            color: var(--color-text-light);
            font-style: italic;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        .mermaid {
            text-align: center;
            margin: var(--spacing-lg) 0;
            background-color: var(--color-bg-alt);
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        details {
            background-color: var(--color-bg-alt);
            border: 1px solid var(--color-border);
            border-radius: var(--border-radius);
            padding: var(--spacing-md);
            margin-bottom: var(--spacing-md);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--color-primary);
            user-select: none;
            padding: var(--spacing-xs);
            margin: calc(-1 * var(--spacing-md));
            padding: var(--spacing-md);
            border-radius: var(--border-radius);
        }

        summary:hover {
            background-color: rgba(123, 44, 191, 0.1);
        }

        details[open] summary {
            margin-bottom: var(--spacing-md);
            border-bottom: 1px solid var(--color-border);
        }

        .learning-objectives {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: var(--spacing-lg);
            border-radius: var(--border-radius);
            border-left: 4px solid var(--color-accent);
            margin-bottom: var(--spacing-xl);
        }

        .learning-objectives h2 {
            margin-top: 0;
            border-bottom: none;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            gap: var(--spacing-md);
            margin: var(--spacing-xl) 0;
            padding-top: var(--spacing-lg);
            border-top: 2px solid var(--color-border);
        }

        .nav-button {
            flex: 1;
            padding: var(--spacing-md);
            background: linear-gradient(135deg, var(--color-accent) 0%, var(--color-accent-light) 100%);
            color: white;
            border-radius: var(--border-radius);
            text-align: center;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: var(--box-shadow);
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            text-decoration: none;
        }

        footer {
            margin-top: var(--spacing-xl);
            padding: var(--spacing-lg) var(--spacing-md);
            background-color: var(--color-bg-alt);
            border-top: 1px solid var(--color-border);
            text-align: center;
            font-size: 0.9rem;
            color: var(--color-text-light);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            h3 {
                font-size: 1.2rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            .navigation {
                flex-direction: column;
            }

            table {
                font-size: 0.85rem;
            }

            th, td {
                padding: var(--spacing-xs);
            }
        }
    </style>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- MathJax for LaTeX equation rendering -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>ç¬¬2ç« ï¼šGNNã®åŸºç¤ç†è«–</h1>
            <p class="subtitle">ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã‹ã‚‰ææ–™ç§‘å­¦ç‰¹åŒ–GNNã¾ã§</p>
            <div class="meta">
                <span class="meta-item">ğŸ“– èª­äº†æ™‚é–“: 25-30åˆ†</span>
                <span class="meta-item">ğŸ“Š é›£æ˜“åº¦: ä¸­ç´š</span>
                <span class="meta-item">ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹: 10å€‹</span>
                <span class="meta-item">ğŸ“ æ¼”ç¿’å•é¡Œ: 3å•</span>
            </div>
        </div>
    </header>

    <main class="container">

<h1>ç¬¬2ç« ï¼šGNNã®åŸºç¤ç†è«–</h1>

<strong>ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã‹ã‚‰ææ–™ç§‘å­¦ç‰¹åŒ–GNNã¾ã§</strong>

<h2>å­¦ç¿’ç›®æ¨™</h2>

ã“ã®ç« ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š

- âœ… ã‚°ãƒ©ãƒ•ã®æ•°å­¦çš„å®šç¾©ã¨è¡¨ç¾æ–¹æ³•ã‚’ç†è§£ã™ã‚‹
- âœ… ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®3ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆé›†ç´„â†’æ›´æ–°â†’å‡ºåŠ›ï¼‰ã‚’èª¬æ˜ã§ãã‚‹
- âœ… GCNã€GATã€GraphSAGEã®åŸç†ã¨é•ã„ã‚’ç†è§£ã™ã‚‹
- âœ… ææ–™ç§‘å­¦ç‰¹åŒ–GNNï¼ˆSchNetã€DimeNetï¼‰ã®ç‰¹å¾´ã‚’çŸ¥ã‚‹
- âœ… ã‚·ãƒ³ãƒ—ãƒ«ãªGNNã‚’PyTorchã§å®Ÿè£…ã§ãã‚‹
- âœ… ç­‰å¤‰GNNã®é‡è¦æ€§ã‚’ç†è§£ã™ã‚‹

<strong>èª­äº†æ™‚é–“</strong>: 25-30åˆ†
<strong>ã‚³ãƒ¼ãƒ‰ä¾‹</strong>: 10å€‹
<strong>æ¼”ç¿’å•é¡Œ</strong>: 3å•

---

<h2>2.1 ã‚°ãƒ©ãƒ•ã®æ•°å­¦çš„å®šç¾©</h2>

<h3>ã‚°ãƒ©ãƒ•ã®åŸºæœ¬è¦ç´ </h3>

<strong>å®šç¾©</strong>:
> ã‚°ãƒ©ãƒ• $G = (V, E)$ ã¯ã€é ‚ç‚¹é›†åˆ $V$ ã¨è¾ºé›†åˆ $E \subseteq V \times V$ ã‹ã‚‰ãªã‚‹ã€‚

<strong>è¨˜æ³•</strong>:
- $n = |V|$: é ‚ç‚¹æ•°
- $m = |E|$: è¾ºæ•°
- $\mathcal{N}(v)$: é ‚ç‚¹ $v$ ã®éš£æ¥é ‚ç‚¹é›†åˆ

---

<h3>éš£æ¥è¡Œåˆ—ï¼ˆAdjacency Matrixï¼‰</h3>

<strong>å®šç¾©</strong>:
$$
A \in \{0, 1\}^{n \times n}, \quad A_{ij} = \begin{cases}
1 & \text{if } (v_i, v_j) \in E \\
0 & \text{otherwise}
\end{cases}
$$

<strong>Pythonã§ã®å®Ÿè£…</strong>:
<pre><code class="language-python">import numpy as np

<h1>ä¾‹ï¼šä¸‰è§’å½¢ã‚°ãƒ©ãƒ•ï¼ˆ3é ‚ç‚¹ã€3è¾ºï¼‰</h1>
n = 3
A = np.array([
    [0, 1, 1],  # é ‚ç‚¹0: 1, 2ã«æ¥ç¶š
    [1, 0, 1],  # é ‚ç‚¹1: 0, 2ã«æ¥ç¶š
    [1, 1, 0]   # é ‚ç‚¹2: 0, 1ã«æ¥ç¶š
])

print("éš£æ¥è¡Œåˆ—:")
print(A)
print(f"\né ‚ç‚¹æ•°: {n}")
print(f"è¾ºæ•°: {A.sum() // 2}")  # ç„¡å‘ã‚°ãƒ©ãƒ•ã¯2ã§å‰²ã‚‹</code></pre>

<strong>å‡ºåŠ›</strong>:
<pre><code>éš£æ¥è¡Œåˆ—:
[[0 1 1]
 [1 0 1]
 [1 1 0]]

é ‚ç‚¹æ•°: 3
è¾ºæ•°: 3</code></pre>

---

<h3>æ¬¡æ•°è¡Œåˆ—ï¼ˆDegree Matrixï¼‰</h3>

<strong>å®šç¾©</strong>:
$$
D \in \mathbb{R}^{n \times n}, \quad D_{ii} = \sum_{j=1}^{n} A_{ij}
$$

<strong>ç‰©ç†çš„æ„å‘³</strong>: å„é ‚ç‚¹ã®æ¥ç¶šæ•°ï¼ˆåŒ–å­¦ã§ã¯çµåˆæ•°ï¼‰

<pre><code class="language-python"><h1>æ¬¡æ•°è¡Œåˆ—</h1>
D = np.diag(A.sum(axis=1))
print("æ¬¡æ•°è¡Œåˆ—:")
print(D)
print(f"\nå„é ‚ç‚¹ã®æ¬¡æ•°: {np.diag(D)}")</code></pre>

<strong>å‡ºåŠ›</strong>:
<pre><code>æ¬¡æ•°è¡Œåˆ—:
[[2 0 0]
 [0 2 0]
 [0 0 2]]

å„é ‚ç‚¹ã®æ¬¡æ•°: [2 2 2]</code></pre>

---

<h3>ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³è¡Œåˆ—ï¼ˆLaplacian Matrixï¼‰</h3>

<strong>å®šç¾©</strong>:
$$
L = D - A
$$

<strong>æ­£è¦åŒ–ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³</strong>ï¼ˆGNNã§ã‚ˆãä½¿ç”¨ï¼‰:
$$
\tilde{L} = D^{-1/2} L D^{-1/2} = I - D^{-1/2} A D^{-1/2}
$$

<pre><code class="language-python"><h1>ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³è¡Œåˆ—</h1>
L = D - A
print("ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³è¡Œåˆ—:")
print(L)

<h1>æ­£è¦åŒ–ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³</h1>
D_inv_sqrt = np.diag(1 / np.sqrt(np.diag(D)))
L_norm = np.eye(n) - D_inv_sqrt @ A @ D_inv_sqrt
print("\næ­£è¦åŒ–ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³:")
print(L_norm)</code></pre>

<strong>å‡ºåŠ›</strong>:
<pre><code>ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³è¡Œåˆ—:
[[ 2 -1 -1]
 [-1  2 -1]
 [-1 -1  2]]

æ­£è¦åŒ–ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³:
[[ 1.  -0.5 -0.5]
 [-0.5  1.  -0.5]
 [-0.5 -0.5  1. ]]</code></pre>

<strong>ç”¨é€”</strong>:
- ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚°ãƒ©ãƒ•ç†è«–
- ã‚°ãƒ©ãƒ•ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›
- ã‚°ãƒ©ãƒ•ä¿¡å·å‡¦ç†

---

<h3>é ‚ç‚¹ç‰¹å¾´é‡ã¨è¾ºç‰¹å¾´é‡</h3>

<strong>é ‚ç‚¹ç‰¹å¾´è¡Œåˆ—</strong> $X \in \mathbb{R}^{n \times d}$:
- å„è¡Œ $x_i \in \mathbb{R}^d$: é ‚ç‚¹ $i$ ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«
- ææ–™ç§‘å­¦: åŸå­ç•ªå·ã€é›»æ°—é™°æ€§åº¦ã€ä¾¡é›»å­æ•°ãªã©

<strong>è¾ºç‰¹å¾´è¡Œåˆ—</strong> $E \in \mathbb{R}^{m \times d_e}$:
- å„è¡Œ $e_{ij} \in \mathbb{R}^{d_e}$: è¾º $(i, j)$ ã®ç‰¹å¾´
- ææ–™ç§‘å­¦: çµåˆé•·ã€çµåˆæ¬¡æ•°ã€çµåˆè§’ãªã©

<pre><code class="language-python"><h1>ä¾‹ï¼šæ°´åˆ†å­ï¼ˆHâ‚‚Oï¼‰ã®ç‰¹å¾´é‡</h1>
X = np.array([
    [8, 2.55, 6],   # O: åŸå­ç•ªå·8, é›»æ°—é™°æ€§åº¦2.55, ä¾¡é›»å­6
    [1, 2.20, 1],   # H1
    [1, 2.20, 1]    # H2
])

print("é ‚ç‚¹ç‰¹å¾´è¡Œåˆ— (3Ã—3):")
print(X)
print(f"å½¢çŠ¶: {X.shape}")</code></pre>

---

<h2>2.2 ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®ä»•çµ„ã¿</h2>

<h3>Message Passing Neural Network (MPNN)</h3>

GNNã®<strong>çµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</strong>ã§ã™ï¼ˆGilmer et al., 2017ï¼‰ã€‚

<strong>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </strong>:

<div class="mermaid">graph LR
    A[å…¥åŠ›: é ‚ç‚¹ç‰¹å¾´ X] --> B[ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ]
    B --> C[ã‚¹ãƒ†ãƒƒãƒ—2: é›†ç´„ Aggregation]
    C --> D[ã‚¹ãƒ†ãƒƒãƒ—3: æ›´æ–° Update]
    D --> E{ç¹°ã‚Šè¿”ã—?}
    E -->|Yes| B
    E -->|No| F[å‡ºåŠ›: æ–°ã—ã„ç‰¹å¾´]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style F fill:#ffebee</div>

---

<h3>ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆï¼ˆMessageï¼‰</h3>

<strong>å®šç¾©</strong>:
$$
m_{ij}^{(t)} = \text{Message}(h_i^{(t)}, h_j^{(t)}, e_{ij})
$$

- $h_i^{(t)}$: ãƒ¬ã‚¤ãƒ¤ãƒ¼ $t$ ã§ã®é ‚ç‚¹ $i$ ã®éš ã‚ŒçŠ¶æ…‹
- $h_j^{(t)}$: éš£æ¥é ‚ç‚¹ $j$ ã®éš ã‚ŒçŠ¶æ…‹
- $e_{ij}$: è¾º $(i, j)$ ã®ç‰¹å¾´é‡

<strong>æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªå½¢</strong>:
$$
m_{ij}^{(t)} = W \cdot h_j^{(t)}
$$

<pre><code class="language-python">import torch
import torch.nn as nn

class MessageFunction(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.W = nn.Linear(in_dim, out_dim)

    def forward(self, h_j):
        """
        éš£æ¥é ‚ç‚¹ã‹ã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ

        Parameters:
        -----------
        h_j : Tensor (num_neighbors, in_dim)
            éš£æ¥é ‚ç‚¹ã®ç‰¹å¾´é‡

        Returns:
        --------
        messages : Tensor (num_neighbors, out_dim)
            ç”Ÿæˆã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        return self.W(h_j)

<h1>ä¾‹</h1>
in_dim, out_dim = 16, 32
msg_fn = MessageFunction(in_dim, out_dim)

<h1>éš£æ¥é ‚ç‚¹ã®ç‰¹å¾´ï¼ˆ3å€‹ã®éš£æ¥é ‚ç‚¹ï¼‰</h1>
h_neighbors = torch.randn(3, in_dim)
messages = msg_fn(h_neighbors)
print(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢çŠ¶: {messages.shape}")
<h1>å‡ºåŠ›: torch.Size([3, 32])</h1></code></pre>

---

<h3>ã‚¹ãƒ†ãƒƒãƒ—2: é›†ç´„ï¼ˆAggregationï¼‰</h3>

<strong>å®šç¾©</strong>:
$$
m_i^{(t)} = \text{Aggregate}\left( \{m_{ij}^{(t)} : j \in \mathcal{N}(i)\} \right)
$$

<strong>ä»£è¡¨çš„ãªé›†ç´„é–¢æ•°</strong>:

| é›†ç´„æ–¹æ³• | æ•°å¼ | ç‰¹å¾´ |
|---------|------|------|
| <strong>Sum</strong> | $\sum_{j \in \mathcal{N}(i)} m_{ij}^{(t)}$ | é †åºä¸å¤‰ã€æ¬¡æ•°ã«æ•æ„Ÿ |
| <strong>Mean</strong> | $\frac{1}{|\mathcal{N}(i)|} \sum_{j \in \mathcal{N}(i)} m_{ij}^{(t)}$ | æ¬¡æ•°æ­£è¦åŒ– |
| <strong>Max</strong> | $\max_{j \in \mathcal{N}(i)} m_{ij}^{(t)}$ | æœ€ã‚‚å¼·ã„ç‰¹å¾´ã‚’ä¿æŒ |
| <strong>Attention</strong> | $\sum_{j \in \mathcal{N}(i)} \alpha_{ij} m_{ij}^{(t)}$ | é‡è¦åº¦ã§é‡ã¿ä»˜ã‘ |

<pre><code class="language-python">class AggregationFunction:
    @staticmethod
    def sum_agg(messages):
        """Sum aggregation"""
        return torch.sum(messages, dim=0)

    @staticmethod
    def mean_agg(messages):
        """Mean aggregation"""
        return torch.mean(messages, dim=0)

    @staticmethod
    def max_agg(messages):
        """Max aggregation"""
        return torch.max(messages, dim=0)[0]

<h1>ä¾‹</h1>
messages = torch.tensor([
    [1.0, 2.0, 3.0],
    [4.0, 5.0, 6.0],
    [7.0, 8.0, 9.0]
])

print("Sum:", AggregationFunction.sum_agg(messages))
<h1>å‡ºåŠ›: tensor([12., 15., 18.])</h1>

print("Mean:", AggregationFunction.mean_agg(messages))
<h1>å‡ºåŠ›: tensor([4., 5., 6.])</h1>

print("Max:", AggregationFunction.max_agg(messages))
<h1>å‡ºåŠ›: tensor([7., 8., 9.])</h1></code></pre>

---

<h3>ã‚¹ãƒ†ãƒƒãƒ—3: æ›´æ–°ï¼ˆUpdateï¼‰</h3>

<strong>å®šç¾©</strong>:
$$
h_i^{(t+1)} = \text{Update}\left( h_i^{(t)}, m_i^{(t)} \right)
$$

<strong>å…¸å‹çš„ãªæ›´æ–°å¼</strong>:
$$
h_i^{(t+1)} = \sigma\left( W_1 h_i^{(t)} + W_2 m_i^{(t)} \right)
$$

<pre><code class="language-python">class UpdateFunction(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.W1 = nn.Linear(hidden_dim, hidden_dim)
        self.W2 = nn.Linear(hidden_dim, hidden_dim)
        self.activation = nn.ReLU()

    def forward(self, h_i, m_i):
        """
        é ‚ç‚¹ç‰¹å¾´ã‚’æ›´æ–°

        Parameters:
        -----------
        h_i : Tensor (hidden_dim,)
            ç¾åœ¨ã®é ‚ç‚¹ç‰¹å¾´
        m_i : Tensor (hidden_dim,)
            é›†ç´„ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

        Returns:
        --------
        h_new : Tensor (hidden_dim,)
            æ›´æ–°ã•ã‚ŒãŸé ‚ç‚¹ç‰¹å¾´
        """
        return self.activation(self.W1(h_i) + self.W2(m_i))

<h1>ä¾‹</h1>
hidden_dim = 32
update_fn = UpdateFunction(hidden_dim)

h_current = torch.randn(hidden_dim)
m_aggregated = torch.randn(hidden_dim)
h_new = update_fn(h_current, m_aggregated)

print(f"æ›´æ–°å‰: {h_current[:5]}")
print(f"æ›´æ–°å¾Œ: {h_new[:5]}")</code></pre>

---

<h3>ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®å…¨ä½“åƒ</h3>

<pre><code class="language-python">class SimpleGNN(nn.Module):
    def __init__(self, in_dim, hidden_dim, num_layers):
        super().__init__()
        self.num_layers = num_layers

        # å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.message_fns = nn.ModuleList([
            MessageFunction(hidden_dim, hidden_dim)
            for _ in range(num_layers)
        ])
        self.update_fns = nn.ModuleList([
            UpdateFunction(hidden_dim)
            for _ in range(num_layers)
        ])

        # å…¥åŠ›å¤‰æ›
        self.input_proj = nn.Linear(in_dim, hidden_dim)

    def forward(self, x, edge_index):
        """
        Parameters:
        -----------
        x : Tensor (num_nodes, in_dim)
            é ‚ç‚¹ç‰¹å¾´è¡Œåˆ—
        edge_index : Tensor (2, num_edges)
            è¾ºã®ãƒªã‚¹ãƒˆ [[src], [dst]]

        Returns:
        --------
        h : Tensor (num_nodes, hidden_dim)
            æ›´æ–°ã•ã‚ŒãŸé ‚ç‚¹ç‰¹å¾´
        """
        # å…¥åŠ›å¤‰æ›
        h = self.input_proj(x)

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼
        for layer in range(self.num_layers):
            h_new = []

            # å„é ‚ç‚¹ã‚’æ›´æ–°
            for i in range(x.size(0)):
                # éš£æ¥é ‚ç‚¹ã‚’å–å¾—
                neighbors = edge_index[1][edge_index[0] == i]

                if len(neighbors) > 0:
                    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
                    messages = self.message_fns[layer](h[neighbors])

                    # ã‚¹ãƒ†ãƒƒãƒ—2: é›†ç´„
                    m_i = torch.mean(messages, dim=0)

                    # ã‚¹ãƒ†ãƒƒãƒ—3: æ›´æ–°
                    h_i_new = self.update_fns[layer](h[i], m_i)
                else:
                    # éš£æ¥é ‚ç‚¹ãŒãªã„å ´åˆ
                    h_i_new = h[i]

                h_new.append(h_i_new)

            h = torch.stack(h_new)

        return h

<h1>ä½¿ç”¨ä¾‹</h1>
model = SimpleGNN(in_dim=16, hidden_dim=32, num_layers=3)

<h1>ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸‰è§’å½¢ï¼‰</h1>
x = torch.randn(3, 16)  # 3é ‚ç‚¹ã€16æ¬¡å…ƒç‰¹å¾´
edge_index = torch.tensor([
    [0, 0, 1, 1, 2, 2],  # å§‹ç‚¹
    [1, 2, 0, 2, 0, 1]   # çµ‚ç‚¹
])

<h1>é †ä¼æ’­</h1>
h_out = model(x, edge_index)
print(f"å‡ºåŠ›å½¢çŠ¶: {h_out.shape}")
<h1>å‡ºåŠ›: torch.Size([3, 32])</h1></code></pre>

---

<h2>2.3 ä»£è¡¨çš„ãªGNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</h2>

<h3>Graph Convolutional Network (GCN)</h3>

<strong>è«–æ–‡</strong>: Kipf & Welling (2017), *ICLR*

<strong>æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢</strong>: ã‚°ãƒ©ãƒ•ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ç•³ã¿è¾¼ã¿

<strong>æ›´æ–°å¼</strong>:
$$
H^{(l+1)} = \sigma\left( \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^{(l)} W^{(l)} \right)
$$

- $\tilde{A} = A + I$: è‡ªå·±ãƒ«ãƒ¼ãƒ—ä»˜ãéš£æ¥è¡Œåˆ—
- $\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$: æ¬¡æ•°è¡Œåˆ—
- $H^{(l)} \in \mathbb{R}^{n \times d}$: ãƒ¬ã‚¤ãƒ¤ãƒ¼ $l$ ã®ç‰¹å¾´é‡
- $W^{(l)} \in \mathbb{R}^{d \times d'}$: å­¦ç¿’å¯èƒ½ãªé‡ã¿

<strong>Pythonã§ã®å®Ÿè£…</strong>:
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class GCNLayer(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)

    def forward(self, X, A):
        """
        Parameters:
        -----------
        X : Tensor (num_nodes, in_features)
            é ‚ç‚¹ç‰¹å¾´è¡Œåˆ—
        A : Tensor (num_nodes, num_nodes)
            éš£æ¥è¡Œåˆ—

        Returns:
        --------
        H : Tensor (num_nodes, out_features)
            æ›´æ–°ã•ã‚ŒãŸç‰¹å¾´é‡
        """
        # è‡ªå·±ãƒ«ãƒ¼ãƒ—ã®è¿½åŠ 
        A_tilde = A + torch.eye(A.size(0), device=A.device)

        # æ¬¡æ•°è¡Œåˆ—
        D_tilde = torch.diag(A_tilde.sum(dim=1))

        # æ­£è¦åŒ–: D^(-1/2) * A * D^(-1/2)
        D_inv_sqrt = torch.diag(1.0 / torch.sqrt(D_tilde.diagonal()))
        A_norm = D_inv_sqrt @ A_tilde @ D_inv_sqrt

        # ã‚°ãƒ©ãƒ•ç•³ã¿è¾¼ã¿
        H = A_norm @ X
        H = self.linear(H)
        return F.relu(H)

<h1>ä½¿ç”¨ä¾‹</h1>
gcn = GCNLayer(in_features=16, out_features=32)

<h1>ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿</h1>
X = torch.randn(5, 16)  # 5é ‚ç‚¹ã€16æ¬¡å…ƒ
A = torch.tensor([
    [0, 1, 1, 0, 0],
    [1, 0, 1, 1, 0],
    [1, 1, 0, 1, 1],
    [0, 1, 1, 0, 1],
    [0, 0, 1, 1, 0]
], dtype=torch.float32)

H = gcn(X, A)
print(f"GCNå‡ºåŠ›å½¢çŠ¶: {H.shape}")
<h1>å‡ºåŠ›: torch.Size([5, 32])</h1></code></pre>

<strong>ç‰¹å¾´</strong>:
- âœ… ã‚·ãƒ³ãƒ—ãƒ«ã§é«˜é€Ÿ
- âœ… éå‰°å¹³æ»‘åŒ–ï¼ˆover-smoothingï¼‰ã«æ³¨æ„
- âœ… å›ºå®šçš„ãªé‡ã¿ï¼ˆå…¨éš£æ¥é ‚ç‚¹ãŒåŒã˜æ‰±ã„ï¼‰

---

<h3>Graph Attention Network (GAT)</h3>

<strong>è«–æ–‡</strong>: VeliÄkoviÄ‡ et al. (2018), *ICLR*

<strong>æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢</strong>: Attentionã§é‡è¦ãªéš£æ¥é ‚ç‚¹ã‚’é‡è¦–

<strong>Attentionä¿‚æ•°</strong>:
$$
\alpha_{ij} = \frac{\exp\left( \text{LeakyReLU}(a^T [W h_i \| W h_j]) \right)}
{\sum_{k \in \mathcal{N}(i)} \exp\left( \text{LeakyReLU}(a^T [W h_i \| W h_k]) \right)}
$$

<strong>æ›´æ–°å¼</strong>:
$$
h_i^{(l+1)} = \sigma\left( \sum_{j \in \mathcal{N}(i)} \alpha_{ij} W^{(l)} h_j^{(l)} \right)
$$

<pre><code class="language-python">class GATLayer(nn.Module):
    def __init__(self, in_features, out_features, dropout=0.6,
                 alpha=0.2):
        super().__init__()
        self.W = nn.Linear(in_features, out_features, bias=False)
        self.a = nn.Parameter(torch.zeros(2 * out_features, 1))
        self.leakyrelu = nn.LeakyReLU(alpha)
        self.dropout = nn.Dropout(dropout)

        nn.init.xavier_uniform_(self.a.data, gain=1.414)

    def forward(self, X, A):
        """
        Parameters:
        -----------
        X : Tensor (num_nodes, in_features)
        A : Tensor (num_nodes, num_nodes)

        Returns:
        --------
        H : Tensor (num_nodes, out_features)
        """
        # ç·šå½¢å¤‰æ›
        Wh = self.W(X)  # (N, out_features)
        N = Wh.size(0)

        # Attentionè¨ˆç®—
        # [Wh_i || Wh_j] for all edges
        Wh_repeat_interleave = Wh.repeat_interleave(N, dim=0)
        Wh_repeat = Wh.repeat(N, 1)
        concat = torch.cat([Wh_repeat_interleave, Wh_repeat], dim=1)
        concat = concat.view(N, N, -1)

        # Attention score
        e = self.leakyrelu(concat @ self.a).squeeze(2)

        # ãƒã‚¹ã‚¯ï¼ˆè¾ºãŒãªã„å ´åˆã¯-infï¼‰
        zero_vec = -9e15 * torch.ones_like(e)
        attention = torch.where(A > 0, e, zero_vec)

        # Softmax
        attention = F.softmax(attention, dim=1)
        attention = self.dropout(attention)

        # Weighted sum
        H = torch.matmul(attention, Wh)
        return F.elu(H)

<h1>ä½¿ç”¨ä¾‹</h1>
gat = GATLayer(in_features=16, out_features=32)
H_gat = gat(X, A)
print(f"GATå‡ºåŠ›å½¢çŠ¶: {H_gat.shape}")
<h1>å‡ºåŠ›: torch.Size([5, 32])</h1></code></pre>

<strong>ç‰¹å¾´</strong>:
- âœ… å‹•çš„ãªé‡ã¿ï¼ˆé‡è¦ãªéš£æ¥é ‚ç‚¹ã‚’è‡ªå‹•å­¦ç¿’ï¼‰
- âœ… è§£é‡ˆå¯èƒ½æ€§ï¼ˆAttentionä¿‚æ•°ã®å¯è¦–åŒ–ï¼‰
- âŒ è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ï¼ˆGCNã®ç´„2å€ï¼‰

---

<h3>GraphSAGEï¼ˆSAmple and aggreGatEï¼‰</h3>

<strong>è«–æ–‡</strong>: Hamilton et al. (2017), *NeurIPS*

<strong>æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢</strong>: ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ã®ãŸã‚ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

<strong>æ›´æ–°å¼</strong>:
$$
h_i^{(l+1)} = \sigma\left( W \cdot \text{Concat}\left( h_i^{(l)}, \text{Aggregate}(\{h_j^{(l)} : j \in \mathcal{S}(i)\}) \right) \right)
$$

- $\mathcal{S}(i)$: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸéš£æ¥é ‚ç‚¹ï¼ˆå…¨ã¦ã§ã¯ãªã„ï¼‰

<pre><code class="language-python">class GraphSAGELayer(nn.Module):
    def __init__(self, in_features, out_features, num_samples=10):
        super().__init__()
        self.num_samples = num_samples
        # Concatç‰ˆ: å…¥åŠ›ã¯ in_features * 2
        self.linear = nn.Linear(in_features * 2, out_features)

    def forward(self, X, A):
        """
        Parameters:
        -----------
        X : Tensor (num_nodes, in_features)
        A : Tensor (num_nodes, num_nodes)

        Returns:
        --------
        H : Tensor (num_nodes, out_features)
        """
        N = X.size(0)
        H_new = []

        for i in range(N):
            # éš£æ¥é ‚ç‚¹ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            neighbors = torch.nonzero(A[i]).squeeze()
            if neighbors.numel() > self.num_samples:
                # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                perm = torch.randperm(neighbors.numel())
                sampled = neighbors[perm[:self.num_samples]]
            else:
                sampled = neighbors

            # é›†ç´„ï¼ˆMeanï¼‰
            if sampled.numel() > 0:
                h_neighbors = X[sampled]
                h_agg = torch.mean(h_neighbors, dim=0)
            else:
                h_agg = torch.zeros_like(X[i])

            # Concat
            h_concat = torch.cat([X[i], h_agg], dim=0)

            # ç·šå½¢å¤‰æ›
            h_new = self.linear(h_concat)
            H_new.append(h_new)

        H = torch.stack(H_new)
        return F.relu(H)

<h1>ä½¿ç”¨ä¾‹</h1>
sage = GraphSAGELayer(in_features=16, out_features=32,
                      num_samples=3)
H_sage = sage(X, A)
print(f"GraphSAGEå‡ºåŠ›å½¢çŠ¶: {H_sage.shape}")
<h1>å‡ºåŠ›: torch.Size([5, 32])</h1></code></pre>

<strong>ç‰¹å¾´</strong>:
- âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ï¼ˆå¤§è¦æ¨¡ã‚°ãƒ©ãƒ•ã«å¯¾å¿œï¼‰
- âœ… ãƒŸãƒ‹ãƒãƒƒãƒè¨“ç·´ãŒå¯èƒ½
- âœ… å¸°ç´çš„å­¦ç¿’ï¼ˆæ–°ã—ã„é ‚ç‚¹ã¸ã®æ±åŒ–ï¼‰

---

<h3>3ã¤ã®GNNã®æ¯”è¼ƒ</h3>

<div class="mermaid">flowchart TD
    A[GNNé¸æŠ] --> B{ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º}
    B -->|å°è¦æ¨¡<br/>10ké ‚ç‚¹| C[GCN]
    B -->|ä¸­è¦æ¨¡<br/>10k-100k| D[GAT]
    B -->|å¤§è¦æ¨¡<br/>100k+| E[GraphSAGE]

    C --> F[ã‚·ãƒ³ãƒ—ãƒ«ã€é«˜é€Ÿ]
    D --> G[é«˜ç²¾åº¦ã€è§£é‡ˆæ€§]
    E --> H[ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«]

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style E fill:#e8f5e9</div>

| æ‰‹æ³• | è¨ˆç®—é‡ | ç²¾åº¦ | ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ | è§£é‡ˆæ€§ | æ¨å¥¨ç”¨é€” |
|------|-------|------|---------------|-------|---------|
| <strong>GCN</strong> | $O(m \cdot d^2)$ | ä¸­ | ä½ | ä¸­ | å°è¦æ¨¡ã€ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚° |
| <strong>GAT</strong> | $O(m \cdot d^2 + n \cdot d)$ | é«˜ | ä¸­ | é«˜ | ä¸­è¦æ¨¡ã€é«˜ç²¾åº¦è¦æ±‚ |
| <strong>GraphSAGE</strong> | $O(k \cdot s \cdot d^2)$ | ä¸­ã€œé«˜ | é«˜ | ä¸­ | å¤§è¦æ¨¡ã€å®Ÿæ™‚é–“äºˆæ¸¬ |

- $m$: è¾ºæ•°
- $n$: é ‚ç‚¹æ•°
- $d$: ç‰¹å¾´æ¬¡å…ƒ
- $k$: ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
- $s$: ã‚µãƒ³ãƒ—ãƒ«æ•°

---

<h2>2.4 ææ–™ç§‘å­¦ç‰¹åŒ–GNN</h2>

<h3>SchNetï¼ˆContinuous-filter Convolutional NNï¼‰</h3>

<strong>è«–æ–‡</strong>: SchÃ¼tt et al. (2017), *NeurIPS*

<strong>å¯¾è±¡</strong>: åˆ†å­ãƒ»ææ–™ã®<strong>é‡å­åŒ–å­¦ç‰¹æ€§</strong>äºˆæ¸¬

<strong>æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢</strong>:
1. <strong>é€£ç¶šãƒ•ã‚£ãƒ«ã‚¿</strong>: é›¢æ•£ã‚°ãƒ©ãƒ•ã§ã¯ãªã3Dç©ºé–“ã§ã®ç•³ã¿è¾¼ã¿
2. <strong>è·é›¢ä¾å­˜</strong>: åŸå­é–“è·é›¢ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–

<strong>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</strong>:

<div class="mermaid">graph LR
    A[åŸå­ç‰¹å¾´] --> B[åŸ‹ã‚è¾¼ã¿å±¤]
    B --> C[ç›¸äº’ä½œç”¨ãƒ–ãƒ­ãƒƒã‚¯ 1]
    C --> D[ç›¸äº’ä½œç”¨ãƒ–ãƒ­ãƒƒã‚¯ 2]
    D --> E[ç›¸äº’ä½œç”¨ãƒ–ãƒ­ãƒƒã‚¯ 3]
    E --> F[å‡ºåŠ›å±¤]

    G[åŸå­é–“è·é›¢] --> C
    G --> D
    G --> E

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#ffebee
    style F fill:#fff9c4
    style G fill:#e1bee7</div>

<strong>æ•°å¼</strong>:
$$
h_i^{(l+1)} = h_i^{(l)} + \sum_{j \in \mathcal{N}(i)} h_j^{(l)} \odot \phi\left( \|r_i - r_j\| \right)
$$

- $\phi(d)$: <strong>é€£ç¶šãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°</strong>ï¼ˆè·é›¢ $d$ ã«ä¾å­˜ï¼‰
- $r_i, r_j$: åŸå­ã®3Dåº§æ¨™

<strong>ãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°</strong>:
$$
\phi(d) = \sum_{k=1}^{K} w_k \exp\left( -\gamma (d - \mu_k)^2 \right)
$$

- ã‚¬ã‚¦ã‚¹åŸºåº•å±•é–‹ï¼ˆRBF: Radial Basis Functionï¼‰

<pre><code class="language-python">import torch
import torch.nn as nn

class GaussianBasis(nn.Module):
    def __init__(self, start=0.0, stop=5.0, num_gaussians=50):
        super().__init__()
        self.mu = nn.Parameter(
            torch.linspace(start, stop, num_gaussians),
            requires_grad=False
        )
        self.gamma = nn.Parameter(
            torch.tensor(10.0),
            requires_grad=True
        )

    def forward(self, distances):
        """
        Parameters:
        -----------
        distances : Tensor (num_edges,)
            åŸå­é–“è·é›¢

        Returns:
        --------
        rbf : Tensor (num_edges, num_gaussians)
            ã‚¬ã‚¦ã‚¹åŸºåº•å±•é–‹
        """
        # (num_edges, 1) - (1, num_gaussians)
        diff = distances.unsqueeze(-1) - self.mu.unsqueeze(0)
        rbf = torch.exp(-self.gamma * diff ** 2)
        return rbf

class SchNetInteraction(nn.Module):
    def __init__(self, hidden_dim, num_gaussians):
        super().__init__()
        self.rbf_layer = GaussianBasis(num_gaussians=num_gaussians)
        self.filter_net = nn.Sequential(
            nn.Linear(num_gaussians, hidden_dim),
            nn.Softplus(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        self.linear = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, h, edge_index, distances):
        """
        Parameters:
        -----------
        h : Tensor (num_atoms, hidden_dim)
            åŸå­ç‰¹å¾´
        edge_index : Tensor (2, num_edges)
            è¾ºã®ãƒªã‚¹ãƒˆ
        distances : Tensor (num_edges,)
            åŸå­é–“è·é›¢

        Returns:
        --------
        h_new : Tensor (num_atoms, hidden_dim)
            æ›´æ–°ã•ã‚ŒãŸç‰¹å¾´
        """
        # RBFå±•é–‹
        rbf = self.rbf_layer(distances)

        # ãƒ•ã‚£ãƒ«ã‚¿ç”Ÿæˆ
        W = self.filter_net(rbf)

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°
        src, dst = edge_index
        messages = h[dst] * W  # è¦ç´ ç©

        # é›†ç´„
        h_agg = torch.zeros_like(h)
        h_agg.index_add_(0, src, messages)

        # æ›´æ–°
        h_new = h + self.linear(h_agg)
        return h_new

<h1>ä½¿ç”¨ä¾‹</h1>
schnet_layer = SchNetInteraction(hidden_dim=128,
                                 num_gaussians=50)

<h1>ãƒ‡ãƒ¼ã‚¿</h1>
num_atoms = 5
h = torch.randn(num_atoms, 128)
edge_index = torch.tensor([[0, 1, 2, 3], [1, 2, 3, 4]])
distances = torch.tensor([1.5, 1.8, 2.0, 1.6])

h_new = schnet_layer(h, edge_index, distances)
print(f"SchNetå‡ºåŠ›å½¢çŠ¶: {h_new.shape}")
<h1>å‡ºåŠ›: torch.Size([5, 128])</h1></code></pre>

<strong>é©ç”¨ä¾‹</strong>:
- QM9ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆåˆ†å­ç‰¹æ€§äºˆæ¸¬ï¼‰
- MD17ï¼ˆåˆ†å­å‹•åŠ›å­¦ï¼‰
- OC20ï¼ˆè§¦åª’å¸ç€ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰

<strong>æ€§èƒ½</strong>:
<pre><code>QM9 HOMO-LUMO gap:
- DFTè¨ˆç®—: 24æ™‚é–“/åˆ†å­
- SchNet: 0.01ç§’/åˆ†å­ï¼ˆMAE=0.04 eVï¼‰</code></pre>

---

<h3>DimeNetï¼ˆDirectional Message Passing NNï¼‰</h3>

<strong>è«–æ–‡</strong>: Klicpera et al. (2020), *ICLR*

<strong>æ‹¡å¼µ</strong>: <strong>çµåˆè§’</strong>ã‚‚è€ƒæ…®

<strong>æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢</strong>:
- è·é›¢ã ã‘ã§ãªã<strong>è§’åº¦æƒ…å ±</strong>ã‚‚åˆ©ç”¨
- 3ä½“ç›¸äº’ä½œç”¨ï¼ˆtriplet interactionï¼‰

<strong>æ›´æ–°å¼</strong>:
$$
m_{ij} = \sum_{k \in \mathcal{N}(j) \setminus \{i\}} W\left( d_{ij}, d_{jk}, \theta_{ijk} \right) h_k
$$

- $\theta_{ijk}$: è§’åº¦ $\angle i-j-k$

<div class="mermaid">graph TD
    A[åŸå­ i] ---|d_ij| B[åŸå­ j]
    B ---|d_jk| C[åŸå­ k]
    A -.è§’åº¦Î¸_ijk.-> C

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5</div>

<strong>è§’åº¦ã®è¨ˆç®—</strong>:
<pre><code class="language-python">import torch

def compute_angle(pos_i, pos_j, pos_k):
    """
    3åŸå­é–“ã®è§’åº¦ã‚’è¨ˆç®—

    Parameters:
    -----------
    pos_i, pos_j, pos_k : Tensor (3,)
        åŸå­ã®3Dåº§æ¨™

    Returns:
    --------
    angle : Tensor (1,)
        è§’åº¦ï¼ˆãƒ©ã‚¸ã‚¢ãƒ³ï¼‰
    """
    # ãƒ™ã‚¯ãƒˆãƒ«
    v_ij = pos_j - pos_i
    v_jk = pos_k - pos_j

    # å†…ç©
    cos_angle = torch.dot(v_ij, v_jk) / (
        torch.norm(v_ij) * torch.norm(v_jk) + 1e-8
    )

    # è§’åº¦
    angle = torch.acos(torch.clamp(cos_angle, -1.0, 1.0))
    return angle

<h1>ä¾‹ï¼šæ°´åˆ†å­ã®çµåˆè§’ï¼ˆH-O-Hï¼‰</h1>
pos_O = torch.tensor([0.0, 0.0, 0.0])
pos_H1 = torch.tensor([0.96, 0.0, 0.0])
pos_H2 = torch.tensor([0.24, 0.93, 0.0])

angle = compute_angle(pos_H1, pos_O, pos_H2)
print(f"H-O-Hè§’åº¦: {torch.rad2deg(angle):.1f}Â°")
<h1>å‡ºåŠ›: 104.5Â°ï¼ˆå®Ÿæ¸¬å€¤ã¨ã»ã¼ä¸€è‡´ï¼‰</h1></code></pre>

<strong>æ€§èƒ½</strong>:
<pre><code>QM9ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:
- SchNet: MAE=0.041 eV
- DimeNet: MAE=0.033 eVï¼ˆ20%æ”¹å–„ï¼‰

è¨ˆç®—æ™‚é–“:
- SchNet: 0.01ç§’/åˆ†å­
- DimeNet: 0.05ç§’/åˆ†å­ï¼ˆ5å€é…ã„ï¼‰</code></pre>

---

<h3>GemNetï¼ˆGeometric Message Passing NNï¼‰</h3>

<strong>è«–æ–‡</strong>: Gasteiger et al. (2021), *NeurIPS*

<strong>ã•ã‚‰ãªã‚‹æ‹¡å¼µ</strong>: <strong>4ä½“ç›¸äº’ä½œç”¨</strong>ï¼ˆäºŒé¢è§’ï¼‰

<strong>å¯¾è±¡</strong>: çµæ™¶æ§‹é€ ã€è¤‡é›‘ãªåˆ†å­

<strong>æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢</strong>:
- äºŒé¢è§’ï¼ˆtorsion angleï¼‰ã®è€ƒæ…®
- ã‚ˆã‚Šé«˜æ¬¡ã®å¹¾ä½•å­¦çš„æƒ…å ±

<div class="mermaid">graph LR
    A[åŸå­ i] --- B[åŸå­ j]
    B --- C[åŸå­ k]
    C --- D[åŸå­ l]

    A -.äºŒé¢è§’Ï†.-> D

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9</div>

<strong>æ€§èƒ½</strong>:
<pre><code>OC20ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆè§¦åª’ï¼‰:
- SchNet: MAE=0.61 eV
- DimeNet++: MAE=0.49 eV
- GemNet: MAE=0.43 eVï¼ˆæœ€é«˜ç²¾åº¦ï¼‰</code></pre>

---

<h3>ææ–™ç§‘å­¦GNNã®æ¯”è¼ƒ</h3>

| æ‰‹æ³• | è€ƒæ…®ã™ã‚‹æƒ…å ± | ç²¾åº¦ | é€Ÿåº¦ | æ¨å¥¨ç”¨é€” |
|------|------------|------|------|---------|
| <strong>SchNet</strong> | è·é›¢ | ä¸­ | é€Ÿã„ | åˆ†å­ç‰¹æ€§äºˆæ¸¬ |
| <strong>DimeNet</strong> | è·é›¢ + è§’åº¦ | é«˜ | ä¸­ | è§¦åª’ã€è¤‡é›‘ãªåˆ†å­ |
| <strong>GemNet</strong> | è·é›¢ + è§’åº¦ + äºŒé¢è§’ | æœ€é«˜ | é…ã„ | çµæ™¶ã€é«˜ç²¾åº¦è¦æ±‚ |

---

<h2>2.5 ç­‰å¤‰æ€§ï¼ˆEquivarianceï¼‰ã®é‡è¦æ€§</h2>

<h3>ç­‰å¤‰æ€§ã¨ã¯</h3>

<strong>å®šç¾©</strong>:
> é–¢æ•° $f$ ãŒå¤‰æ› $T$ ã«å¯¾ã—ã¦<strong>ç­‰å¤‰</strong>ï¼ˆequivariantï¼‰ã§ã‚ã‚‹ã¨ã¯ã€
> $$f(T(x)) = T(f(x))$$
> ãŒæˆã‚Šç«‹ã¤ã“ã¨ã€‚

<strong>ææ–™ç§‘å­¦ã§ã®æ„å‘³</strong>:
- åˆ†å­ã‚’å›è»¢ãƒ»ä¸¦é€²ã—ã¦ã‚‚ã€äºˆæ¸¬ã¯åŒã˜ï¼ˆã¾ãŸã¯å¯¾å¿œã™ã‚‹å¤‰æ›ï¼‰

---

<h3>E(3)ç­‰å¤‰æ€§</h3>

<strong>E(3)ç¾¤</strong>: 3æ¬¡å…ƒãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ç©ºé–“ã®ç­‰é•·å¤‰æ›
- å›è»¢ï¼ˆRotationï¼‰
- ä¸¦é€²ï¼ˆTranslationï¼‰
- åè»¢ï¼ˆInversionï¼‰

<strong>é‡è¦æ€§</strong>:
- ç‰©ç†æ³•å‰‡ã¯åº§æ¨™ç³»ã«ä¾å­˜ã—ãªã„
- GNNã‚‚åŒæ§˜ã§ã‚ã‚‹ã¹ã

---

<h3>ç­‰å¤‰GNNã®ä¾‹ï¼šNequIPã€MACE</h3>

<strong>NequIP</strong> (Batzner et al., 2022):
- <strong>E(3)ç­‰å¤‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°</strong>
- çƒé¢èª¿å’Œé–¢æ•°ï¼ˆSpherical Harmonicsï¼‰ã®åˆ©ç”¨

<strong>æ›´æ–°å¼</strong>:
$$
m_{ij} = \phi\left( \|r_i - r_j\| \right) \otimes Y_l(r_{ij})
$$

- $Y_l$: çƒé¢èª¿å’Œé–¢æ•°ï¼ˆè§’åº¦æƒ…å ±ã‚’ä¿æŒï¼‰
- $\otimes$: ãƒ†ãƒ³ã‚½ãƒ«ç©

<strong>MACE</strong> (Batatia et al., 2022):
- <strong>é«˜æ¬¡ã®ç­‰å¤‰æ€§</strong>
- ã‚ˆã‚Šæ­£ç¢ºãªåŠ›å ´ï¼ˆforce fieldï¼‰äºˆæ¸¬

<strong>æ€§èƒ½</strong>:
<pre><code>MD17ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆåˆ†å­å‹•åŠ›å­¦ï¼‰:
- SchNet: MAE(åŠ›) = 0.21 kcal/mol/Ã…
- NequIP: MAE(åŠ›) = 0.05 kcal/mol/Ã…ï¼ˆ76%æ”¹å–„ï¼‰</code></pre>

---

<h3>ç­‰å¤‰æ€§ã®ãƒ†ã‚¹ãƒˆ</h3>

<pre><code class="language-python">import torch
import torch.nn as nn

def test_equivariance(model, pos, edge_index):
    """
    ãƒ¢ãƒ‡ãƒ«ã®ç­‰å¤‰æ€§ã‚’ãƒ†ã‚¹ãƒˆ
    """
    # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®äºˆæ¸¬
    pred_original = model(pos, edge_index)

    # å›è»¢è¡Œåˆ—ï¼ˆ90åº¦å›è»¢ï¼‰
    angle = torch.tensor(torch.pi / 2)
    rotation = torch.tensor([
        [torch.cos(angle), -torch.sin(angle), 0],
        [torch.sin(angle), torch.cos(angle), 0],
        [0, 0, 1]
    ])

    # åº§æ¨™ã‚’å›è»¢
    pos_rotated = pos @ rotation.T

    # å›è»¢å¾Œã®äºˆæ¸¬
    pred_rotated = model(pos_rotated, edge_index)

    # äºˆæ¸¬ã‚’å›è»¢
    pred_original_rotated = pred_original @ rotation.T

    # èª¤å·®ã‚’è¨ˆç®—
    error = torch.abs(pred_rotated - pred_original_rotated).mean()
    print(f"ç­‰å¤‰æ€§èª¤å·®: {error.item():.6f}")

    if error < 1e-5:
        print("âœ… ãƒ¢ãƒ‡ãƒ«ã¯ç­‰å¤‰ã§ã™")
    else:
        print("âŒ ãƒ¢ãƒ‡ãƒ«ã¯ç­‰å¤‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“")

<h1>ä½¿ç”¨ä¾‹ï¼ˆç°¡ç•¥ç‰ˆï¼‰</h1>
class SimpleEquivariantModel(nn.Module):
    def forward(self, pos, edge_index):
        # ç°¡ç•¥åŒ–: åº§æ¨™ã®å·®åˆ†ã‚’è¨ˆç®—ï¼ˆç­‰å¤‰ï¼‰
        src, dst = edge_index
        diff = pos[dst] - pos[src]
        return diff

model = SimpleEquivariantModel()
pos = torch.randn(5, 3)
edge_index = torch.tensor([[0, 1, 2], [1, 2, 3]])

test_equivariance(model, pos, edge_index)</code></pre>

---

<h2>2.6 ã‚³ãƒ©ãƒ ï¼šãªãœæ·±ã„GNNã¯é›£ã—ã„ã‹</h2>

<h3>éå‰°å¹³æ»‘åŒ–ï¼ˆOver-smoothingï¼‰</h3>

<strong>å•é¡Œ</strong>: ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’æ·±ãã™ã‚‹ã¨ã€<strong>å…¨ã¦ã®é ‚ç‚¹ãŒåŒã˜ç‰¹å¾´</strong>ã«ãªã‚‹

<strong>åŸå› </strong>: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®ç¹°ã‚Šè¿”ã—ã§æƒ…å ±ãŒæ‹¡æ•£

<pre><code class="language-python"><h1>éå‰°å¹³æ»‘åŒ–ã®ãƒ‡ãƒ¢</h1>
import torch
import torch.nn.functional as F

def demonstrate_oversmoothing(X, A, num_layers=10):
    """
    éå‰°å¹³æ»‘åŒ–ã®å¯è¦–åŒ–
    """
    H = X
    smoothness = []

    for layer in range(num_layers):
        # ç°¡å˜ãªGCNå±¤
        D = torch.diag(A.sum(dim=1))
        D_inv_sqrt = torch.diag(1.0 / torch.sqrt(D.diagonal()))
        A_norm = D_inv_sqrt @ A @ D_inv_sqrt

        H = A_norm @ H
        H = F.relu(H)

        # å¹³æ»‘åº¦ï¼ˆé ‚ç‚¹é–“ã®é¡ä¼¼åº¦ï¼‰
        similarity = F.cosine_similarity(
            H.unsqueeze(1), H.unsqueeze(0), dim=2
        )
        avg_similarity = similarity[torch.triu_indices(
            H.size(0), H.size(0), offset=1
        )[0], torch.triu_indices(
            H.size(0), H.size(0), offset=1
        )[1]].mean()

        smoothness.append(avg_similarity.item())
        print(f"Layer {layer+1}: å¹³å‡é¡ä¼¼åº¦ = {avg_similarity:.4f}")

    return smoothness

<h1>å®Ÿè¡Œ</h1>
X = torch.randn(5, 16)
A = torch.eye(5) + torch.rand(5, 5) > 0.7
smoothness = demonstrate_oversmoothing(X, A.float(), num_layers=10)</code></pre>

<strong>å‡ºåŠ›ä¾‹</strong>:
<pre><code>Layer 1: å¹³å‡é¡ä¼¼åº¦ = 0.2341
Layer 2: å¹³å‡é¡ä¼¼åº¦ = 0.4523
Layer 3: å¹³å‡é¡ä¼¼åº¦ = 0.6789
...
Layer 10: å¹³å‡é¡ä¼¼åº¦ = 0.9876</code></pre>

â†’ ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒæ·±ããªã‚‹ã«ã¤ã‚Œã€å…¨é ‚ç‚¹ãŒä¼¼ã¦ãã‚‹

---

<h3>å¯¾ç­–</h3>

1. <strong>Residual Connectionï¼ˆæ®‹å·®æ¥ç¶šï¼‰</strong>:
   $$h_i^{(l+1)} = h_i^{(l)} + \text{GNN}(h_i^{(l)})$$

2. <strong>Jumping Knowledge Network</strong>:
   - å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å‡ºåŠ›ã‚’çµåˆ

3. <strong>PairNorm</strong>:
   - ç‰¹å¾´é‡ã®æ­£è¦åŒ–

<pre><code class="language-python">class GNNWithResidual(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.conv = GCNLayer(hidden_dim, hidden_dim)

    def forward(self, X, A):
        # Residual connection
        H = self.conv(X, A)
        return X + H  # ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆ</code></pre>

---

<h2>2.7 æœ¬ç« ã®ã¾ã¨ã‚</h2>

<h3>å­¦ã‚“ã ã“ã¨</h3>

1. <strong>ã‚°ãƒ©ãƒ•ã®æ•°å­¦çš„å®šç¾©</strong>
   - éš£æ¥è¡Œåˆ—ã€æ¬¡æ•°è¡Œåˆ—ã€ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³è¡Œåˆ—
   - é ‚ç‚¹ç‰¹å¾´é‡ã¨è¾ºç‰¹å¾´é‡

2. <strong>ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°</strong>
   - 3ã‚¹ãƒ†ãƒƒãƒ—: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ â†’ é›†ç´„ â†’ æ›´æ–°
   - é›†ç´„é–¢æ•°: Sumã€Meanã€Maxã€Attention

3. <strong>ä»£è¡¨çš„GNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</strong>
   - GCN: ã‚·ãƒ³ãƒ—ãƒ«ã€é«˜é€Ÿ
   - GAT: Attentionã€é«˜ç²¾åº¦
   - GraphSAGE: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã€ãƒŸãƒ‹ãƒãƒƒãƒ

4. <strong>ææ–™ç§‘å­¦ç‰¹åŒ–GNN</strong>
   - SchNet: è·é›¢ä¾å­˜ã€é€£ç¶šãƒ•ã‚£ãƒ«ã‚¿
   - DimeNet: è§’åº¦æƒ…å ±ã‚‚è€ƒæ…®
   - GemNet: äºŒé¢è§’ã¾ã§è€ƒæ…®

5. <strong>ç­‰å¤‰æ€§</strong>
   - E(3)ç­‰å¤‰æ€§ã®é‡è¦æ€§
   - NequIPã€MACEãªã©æœ€æ–°æ‰‹æ³•

<h3>é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ</h3>

- âœ… ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã¯GNNã®<strong>çµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</strong>
- âœ… é›†ç´„é–¢æ•°ã®é¸æŠãŒæ€§èƒ½ã«å¤§ããå½±éŸ¿
- âœ… ææ–™ç§‘å­¦ã§ã¯<strong>å¹¾ä½•å­¦çš„æƒ…å ±</strong>ï¼ˆè·é›¢ã€è§’åº¦ï¼‰ãŒé‡è¦
- âœ… ç­‰å¤‰æ€§ã«ã‚ˆã‚Š<strong>ç‰©ç†æ³•å‰‡ã‚’ä¿è¨¼</strong>
- âœ… éå‰°å¹³æ»‘åŒ–ã«æ³¨æ„ï¼ˆResidual Connectionã§å¯¾ç­–ï¼‰

<h3>æ¬¡ã®ç« ã¸</h3>

ç¬¬3ç« ã§ã¯ã€<strong>PyTorch Geometricå®Ÿè·µ</strong>ã‚’å­¦ã³ã¾ã™ï¼š
- ç’°å¢ƒæ§‹ç¯‰ï¼ˆPyGã€RDKitã€ASEï¼‰
- QM9ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§åˆ†å­ç‰¹æ€§äºˆæ¸¬
- Materials Projectãƒ‡ãƒ¼ã‚¿ã§çµæ™¶ç‰¹æ€§äºˆæ¸¬
- ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

<strong>[ç¬¬3ç« ï¼šPyTorch Geometricå®Ÿè·µ â†’](./chapter-3.md)</strong>

---

<h2>æ¼”ç¿’å•é¡Œ</h2>

<h3>å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰</h3>

æ¬¡ã®æ–‡ç« ã®æ­£èª¤ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

1. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã¯ã€é›†ç´„ï¼ˆAggregationï¼‰â†’ æ›´æ–°ï¼ˆUpdateï¼‰â†’ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆã®é †ã§è¡Œã‚ã‚Œã‚‹
2. GATã¯Attentionã‚’ä½¿ã†ãŸã‚ã€å…¨ã¦ã®éš£æ¥é ‚ç‚¹ã‚’åŒã˜é‡ã¿ã§æ‰±ã†
3. SchNetã¯åŸå­é–“è·é›¢ã‚’æ˜ç¤ºçš„ã«è€ƒæ…®ã™ã‚‹

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®3ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ€ã„å‡ºã—ã¾ã—ã‚‡ã†
- GATã®æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã€Œé‡è¦ãªéš£æ¥é ‚ç‚¹ã‚’é‡è¦–ã€ã§ã™
- SchNetã®ç‰¹å¾´ã¯ã€Œé€£ç¶šãƒ•ã‚£ãƒ«ã‚¿ã€ã§ã™

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

<strong>è§£ç­”</strong>:
1. <strong>èª¤</strong> - æ­£ã—ã„é †ç•ªã¯ï¼šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ â†’ é›†ç´„ â†’ æ›´æ–°
2. <strong>èª¤</strong> - GATã¯ Attentionã§<strong>ç•°ãªã‚‹é‡ã¿</strong>ã‚’å‰²ã‚Šå½“ã¦ã‚‹
3. <strong>æ­£</strong> - SchNetã¯RBFï¼ˆã‚¬ã‚¦ã‚¹åŸºåº•ï¼‰ã§è·é›¢ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰

<strong>è§£èª¬</strong>:

1ã«ã¤ã„ã¦ï¼š
<pre><code class="language-python"><h1>æ­£ã—ã„é †åº</h1>
for layer in range(num_layers):
    # Step 1: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
    messages = message_function(h_neighbors)

    # Step 2: é›†ç´„
    m_i = aggregate(messages)

    # Step 3: æ›´æ–°
    h_i = update_function(h_i, m_i)</code></pre>

2ã«ã¤ã„ã¦ï¼š
- GAT ã® Attention ä¿‚æ•° $\alpha_{ij}$ ã¯éš£æ¥é ‚ç‚¹ã”ã¨ã«ç•°ãªã‚‹
- é‡è¦ãªéš£æ¥é ‚ç‚¹ã«ã¯å¤§ããªé‡ã¿ã€ãã†ã§ãªã„ã‚‚ã®ã«ã¯å°ã•ãªé‡ã¿

3ã«ã¤ã„ã¦ï¼š
- SchNet ã® ãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°: $\phi(d) = \sum_k w_k \exp(-\gamma (d - \mu_k)^2)$
- è·é›¢ $d$ ãŒç•°ãªã‚Œã°ã€ãƒ•ã‚£ãƒ«ã‚¿ã®å€¤ã‚‚ç•°ãªã‚‹

</details>

---

<h3>å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰</h3>

ä»¥ä¸‹ã®ã‚°ãƒ©ãƒ•ã«å¯¾ã—ã¦ã€GCNã®1å±¤ã®é †ä¼æ’­ã‚’æ‰‹è¨ˆç®—ã§æ±‚ã‚ã¦ãã ã•ã„ã€‚

<strong>ã‚°ãƒ©ãƒ•</strong>:
<pre><code>é ‚ç‚¹: 3å€‹ï¼ˆv0, v1, v2ï¼‰
è¾º: v0-v1, v1-v2ï¼ˆç·šå½¢ã‚°ãƒ©ãƒ•ï¼‰

é ‚ç‚¹ç‰¹å¾´:
X = [[1, 0],
     [0, 1],
     [1, 1]]

éš£æ¥è¡Œåˆ—:
A = [[0, 1, 0],
     [1, 0, 1],
     [0, 1, 0]]

é‡ã¿è¡Œåˆ—ï¼ˆç°¡ç•¥åŒ–ï¼‰:
W = [[1, 0],
     [0, 1]]  ï¼ˆæ’ç­‰è¡Œåˆ—ï¼‰</code></pre>

<strong>è¦æ±‚äº‹é …</strong>:
1. $\tilde{A} = A + I$ ã‚’è¨ˆç®—
2. æ­£è¦åŒ–éš£æ¥è¡Œåˆ— $\hat{A} = \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}$ ã‚’è¨ˆç®—
3. GCNå‡ºåŠ› $H = \hat{A} X W$ ã‚’è¨ˆç®—ï¼ˆæ´»æ€§åŒ–é–¢æ•°ãªã—ï¼‰

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

<strong>æ‰‹é †</strong>:
1. è‡ªå·±ãƒ«ãƒ¼ãƒ—ã‚’è¿½åŠ : $\tilde{A}_{ii} = 1$
2. æ¬¡æ•°è¡Œåˆ—: $\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$
3. $\tilde{D}^{-1/2}$ ã‚’è¨ˆç®—ï¼ˆå¯¾è§’è¦ç´ ã®é€†æ•°ã®å¹³æ–¹æ ¹ï¼‰
4. è¡Œåˆ—ç©ã‚’è¨ˆç®—

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

<strong>Step 1: è‡ªå·±ãƒ«ãƒ¼ãƒ—ä»˜ãéš£æ¥è¡Œåˆ—</strong>
$$
\tilde{A} = A + I = \begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 0
\end{bmatrix} + \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} = \begin{bmatrix}
1 & 1 & 0 \\
1 & 1 & 1 \\
0 & 1 & 1
\end{bmatrix}
$$

<strong>Step 2: æ¬¡æ•°è¡Œåˆ—</strong>
$$
\tilde{D} = \begin{bmatrix}
2 & 0 & 0 \\
0 & 3 & 0 \\
0 & 0 & 2
\end{bmatrix}
$$

ï¼ˆå„è¡Œã®å’Œï¼‰

<strong>Step 3: $\tilde{D}^{-1/2}$</strong>
$$
\tilde{D}^{-1/2} = \begin{bmatrix}
1/\sqrt{2} & 0 & 0 \\
0 & 1/\sqrt{3} & 0 \\
0 & 0 & 1/\sqrt{2}
\end{bmatrix} \approx \begin{bmatrix}
0.707 & 0 & 0 \\
0 & 0.577 & 0 \\
0 & 0 & 0.707
\end{bmatrix}
$$

<strong>Step 4: æ­£è¦åŒ–éš£æ¥è¡Œåˆ—</strong>
$$
\hat{A} = \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}
$$

è¨ˆç®—éç¨‹:
<pre><code class="language-python">import numpy as np

A_tilde = np.array([
    [1, 1, 0],
    [1, 1, 1],
    [0, 1, 1]
], dtype=float)

D_tilde = np.diag([2, 3, 2])
D_inv_sqrt = np.diag([1/np.sqrt(2), 1/np.sqrt(3), 1/np.sqrt(2)])

A_hat = D_inv_sqrt @ A_tilde @ D_inv_sqrt
print("æ­£è¦åŒ–éš£æ¥è¡Œåˆ—:")
print(A_hat)</code></pre>

$$
\hat{A} \approx \begin{bmatrix}
0.500 & 0.408 & 0 \\
0.408 & 0.333 & 0.408 \\
0 & 0.408 & 0.500
\end{bmatrix}
$$

<strong>Step 5: GCNå‡ºåŠ›</strong>
$$
H = \hat{A} X W
$$

ï¼ˆ$W = I$ ãªã®ã§ $H = \hat{A} X$ï¼‰

<pre><code class="language-python">X = np.array([
    [1, 0],
    [0, 1],
    [1, 1]
], dtype=float)

H = A_hat @ X
print("GCNå‡ºåŠ›:")
print(H)</code></pre>

$$
H \approx \begin{bmatrix}
0.500 & 0.408 \\
0.816 & 0.741 \\
0.408 & 0.908
\end{bmatrix}
$$

<strong>è§£é‡ˆ</strong>:
- é ‚ç‚¹1ï¼ˆä¸­å¿ƒï¼‰: ä¸¡å´ã®éš£æ¥é ‚ç‚¹ã®æƒ…å ±ã‚’é›†ç´„
- é ‚ç‚¹0,2ï¼ˆç«¯ç‚¹ï¼‰: éš£æ¥é ‚ç‚¹1ã®æƒ…å ±ã‚’ä¸»ã«å–ã‚Šè¾¼ã‚€

<strong>Pythonã§ã®æ¤œè¨¼</strong>:
<pre><code class="language-python"><h1>å®Œå…¨ãªã‚³ãƒ¼ãƒ‰</h1>
A = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]], dtype=float)
X = np.array([[1, 0], [0, 1], [1, 1]], dtype=float)

<h1>GCN</h1>
A_tilde = A + np.eye(3)
D_tilde = np.diag(A_tilde.sum(axis=1))
D_inv_sqrt = np.diag(1.0 / np.sqrt(D_tilde.diagonal()))
A_hat = D_inv_sqrt @ A_tilde @ D_inv_sqrt

H = A_hat @ X
print("æœ€çµ‚å‡ºåŠ›:")
print(H)</code></pre>

</details>

---

<h3>å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰</h3>

SchNetã®é€£ç¶šãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°ã‚’å®Ÿè£…ã—ã€ç•°ãªã‚‹åŸå­é–“è·é›¢ã«å¯¾ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ã®å¿œç­”ã‚’å¯è¦–åŒ–ã—ã¦ãã ã•ã„ã€‚

<strong>è¦æ±‚äº‹é …</strong>:
1. ã‚¬ã‚¦ã‚¹åŸºåº•ï¼ˆRBFï¼‰é–¢æ•°ã‚’å®Ÿè£…
2. è·é›¢0.5Ã…ã€œ5.0Ã…ã«å¯¾ã™ã‚‹RBFå¿œç­”ã‚’è¨ˆç®—
3. ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–
4. ãƒ•ã‚£ãƒ«ã‚¿ã®ç‰©ç†çš„æ„å‘³ã‚’è€ƒå¯Ÿ

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

<strong>RBF ã®å¼</strong>:
$$\phi_k(d) = \exp\left( -\gamma (d - \mu_k)^2 \right)$$

- $\mu_k$: ã‚¬ã‚¦ã‚¹é–¢æ•°ã®ä¸­å¿ƒï¼ˆ0ã€œ5Ã…ã«å‡ç­‰é…ç½®ï¼‰
- $\gamma$: åºƒãŒã‚Šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ10ç¨‹åº¦ï¼‰

<strong>å¯è¦–åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ</strong>:
- Xè»¸: è·é›¢ (0.5ã€œ5.0Ã…)
- Yè»¸: RBF ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (0ã€œ49)
- è‰²: RBF å¿œç­”å€¤ (0ã€œ1)

</details>

<details>
<summary>è§£ç­”ä¾‹</summary>

<pre><code class="language-python">import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

<h1>===== å®Ÿè£… =====</h1>
class GaussianBasisFunction:
    def __init__(self, start=0.0, stop=5.0, num_gaussians=50,
                 gamma=10.0):
        """
        ã‚¬ã‚¦ã‚¹åŸºåº•é–¢æ•°ï¼ˆRBFï¼‰

        Parameters:
        -----------
        start, stop : float
            è·é›¢ã®ç¯„å›²
        num_gaussians : int
            ã‚¬ã‚¦ã‚¹é–¢æ•°ã®æ•°
        gamma : float
            åºƒãŒã‚Šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        self.mu = torch.linspace(start, stop, num_gaussians)
        self.gamma = gamma

    def __call__(self, distances):
        """
        RBF å¿œç­”ã‚’è¨ˆç®—

        Parameters:
        -----------
        distances : Tensor (num_distances,)

        Returns:
        --------
        rbf : Tensor (num_distances, num_gaussians)
        """
        # (num_distances, 1) - (1, num_gaussians)
        diff = distances.unsqueeze(-1) - self.mu.unsqueeze(0)
        rbf = torch.exp(-self.gamma * diff ** 2)
        return rbf

<h1>===== å¯è¦–åŒ– =====</h1>
<h1>RBFç”Ÿæˆ</h1>
rbf_layer = GaussianBasisFunction(
    start=0.0, stop=5.0,
    num_gaussians=50, gamma=10.0
)

<h1>è·é›¢ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ0.5ã€œ5.0Ã…ï¼‰</h1>
distances = torch.linspace(0.5, 5.0, 100)

<h1>RBF å¿œç­”</h1>
rbf_response = rbf_layer(distances)  # (100, 50)

<h1>ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—</h1>
plt.figure(figsize=(12, 6))
sns.heatmap(
    rbf_response.T.numpy(),  # è»¢ç½®ï¼ˆRBF x è·é›¢ï¼‰
    cmap='viridis',
    xticklabels=10,
    yticklabels=10,
    cbar_kws={'label': 'RBF Response'}
)
plt.xlabel('Distance (Ã…)')
plt.ylabel('RBF Index')
plt.title('SchNet Continuous Filter: RBF Response')

<h1>Xè»¸ãƒ©ãƒ™ãƒ«ã‚’å®Ÿéš›ã®è·é›¢ã«</h1>
xticks = np.linspace(0, len(distances)-1, 10).astype(int)
xticklabels = [f'{distances[i]:.1f}' for i in xticks]
plt.xticks(xticks, xticklabels)

plt.tight_layout()
plt.savefig('schnet_rbf_heatmap.png', dpi=150)
plt.show()

<h1>===== ç‰¹å®šè·é›¢ã®RBFå¿œç­” =====</h1>
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
example_distances = [1.0, 1.5, 2.0, 3.0]  # Ã…

for ax, d in zip(axes.flatten(), example_distances):
    d_tensor = torch.tensor([d])
    rbf = rbf_layer(d_tensor).squeeze()

    ax.plot(rbf_layer.mu.numpy(), rbf.numpy(),
            marker='o', linewidth=2)
    ax.axvline(d, color='red', linestyle='--',
               label=f'Distance = {d}Ã…')
    ax.set_xlabel('RBF Center Î¼ (Ã…)')
    ax.set_ylabel('RBF Response')
    ax.set_title(f'RBF Response at d = {d}Ã…')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('schnet_rbf_profiles.png', dpi=150)
plt.show()

<h1>===== ç‰©ç†çš„æ„å‘³ã®è€ƒå¯Ÿ =====</h1>
print("\n===== ç‰©ç†çš„æ„å‘³ =====")
print("1. çŸ­è·é›¢ï¼ˆ0.5-2.0Ã…ï¼‰: å…±æœ‰çµåˆé ˜åŸŸ")
print("   - C-C: 1.54Ã…, C=C: 1.34Ã…, C-H: 1.09Ã…")
print("   - RBFã¯æ€¥å³»ã«åå¿œï¼ˆçµåˆã®æœ‰ç„¡ã‚’è­˜åˆ¥ï¼‰")

print("\n2. ä¸­è·é›¢ï¼ˆ2.0-3.5Ã…ï¼‰: éå…±æœ‰çµåˆç›¸äº’ä½œç”¨")
print("   - æ°´ç´ çµåˆ: 2.8Ã…, ãƒ•ã‚¡ãƒ³ãƒ‡ãƒ«ãƒ¯ãƒ¼ãƒ«ã‚¹åŠ›")
print("   - RBFã¯ãªã ã‚‰ã‹ã«åå¿œ")

print("\n3. é•·è·é›¢ï¼ˆ3.5-5.0Ã…ï¼‰: å¼±ã„ç›¸äº’ä½œç”¨")
print("   - é™é›»ç›¸äº’ä½œç”¨ã€åˆ†æ•£åŠ›")
print("   - RBFã®å¿œç­”ã¯å°ã•ã„")

print("\n4. ã‚¬ã‚¦ã‚¹åŸºåº•ã®å½¹å‰²:")
print("   - é€£ç¶šçš„ãªè·é›¢è¡¨ç¾ï¼ˆé›¢æ•£åŒ–ãªã—ï¼‰")
print("   - ä»»æ„ã®è·é›¢ã«å¯¾ã—ã¦å¾®åˆ†å¯èƒ½")
print("   - æ©Ÿæ¢°å­¦ç¿’ã§æœ€é©åŒ–å¯èƒ½ï¼ˆÎ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰")</code></pre>

<strong>å‡ºåŠ›ã®è§£é‡ˆ</strong>:

1. <strong>ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—</strong>:
   - å¯¾è§’ç·šçŠ¶ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå„RBFãŒç‰¹å®šè·é›¢ã§æœ€å¤§å¿œç­”ï¼‰
   - æ»‘ã‚‰ã‹ãªé·ç§»ï¼ˆã‚¬ã‚¦ã‚¹é–¢æ•°ã®é‡ãªã‚Šï¼‰

2. <strong>RBFãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«</strong>:
   - è·é›¢1.0Ã…: RBF #10ä»˜è¿‘ãŒå¼·ãåå¿œ
   - è·é›¢2.0Ã…: RBF #20ä»˜è¿‘ãŒå¼·ãåå¿œ
   - ã‚¬ã‚¦ã‚¹å½¢çŠ¶ã«ã‚ˆã‚Šã€éš£æ¥RBFã‚‚å¼±ãåå¿œ

3. <strong>ç‰©ç†çš„æ„å‘³</strong>:
   - <strong>SchNetã¯è·é›¢ã‚’ã€Œåˆ†å¸ƒã€ã¨ã—ã¦è¡¨ç¾</strong>
   - é›¢æ•£çš„ãªãƒ“ãƒ³åˆ†ã‘ã§ã¯ãªãã€é€£ç¶šçš„ãªé‡ãªã‚Š
   - ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒè·é›¢ä¾å­˜æ€§ã‚’å­¦ç¿’

<strong>æ‹¡å¼µèª²é¡Œ</strong>:
1. $\gamma$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰ãˆã¦ã€RBFã®åºƒãŒã‚Šã‚’èª¿æ•´
2. éå¯¾ç§°ãªã‚¬ã‚¦ã‚¹åŸºåº•ï¼ˆçŸ­è·é›¢ã‚’å¯†ã«ã€é•·è·é›¢ã‚’ç–ã«ï¼‰
3. å®Ÿéš›ã®åˆ†å­ã§RBFãƒ•ã‚£ãƒ«ã‚¿ã‚’å¯è¦–åŒ–

</details>

---

<h2>å‚è€ƒæ–‡çŒ®</h2>

1. Kipf, T. N. & Welling, M. (2017). "Semi-Supervised Classification with Graph Convolutional Networks." *ICLR*.
   DOI: [https://arxiv.org/abs/1609.02907](https://arxiv.org/abs/1609.02907)

2. VeliÄkoviÄ‡, P. et al. (2018). "Graph Attention Networks." *ICLR*.
   DOI: [https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903)

3. Hamilton, W. L. et al. (2017). "Inductive Representation Learning on Large Graphs." *NeurIPS*.
   DOI: [https://arxiv.org/abs/1706.02216](https://arxiv.org/abs/1706.02216)

4. SchÃ¼tt, K. T. et al. (2017). "SchNet: A continuous-filter convolutional neural network for modeling quantum interactions." *NeurIPS*.
   DOI: [https://arxiv.org/abs/1706.08566](https://arxiv.org/abs/1706.08566)

5. Klicpera, J. et al. (2020). "Directional Message Passing for Molecular Graphs." *ICLR*.
   DOI: [https://arxiv.org/abs/2003.03123](https://arxiv.org/abs/2003.03123)

6. Gasteiger, J. et al. (2021). "GemNet: Universal Directional Graph Neural Networks for Molecules." *NeurIPS*.
   DOI: [https://arxiv.org/abs/2106.08903](https://arxiv.org/abs/2106.08903)

7. Batzner, S. et al. (2022). "E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials." *Nature Communications*, 13, 2453.
   DOI: [https://doi.org/10.1038/s41467-022-29939-5](https://doi.org/10.1038/s41467-022-29939-5)

---

<h2>ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³</h2>

<h3>å‰ã®ç« </h3>
<strong>[ç¬¬1ç« ï¼šãªãœææ–™ç§‘å­¦ã«GNNãŒå¿…è¦ã‹ â†](./chapter-1.md)</strong>

<h3>æ¬¡ã®ç« </h3>
<strong>[ç¬¬3ç« ï¼šPyTorch Geometricå®Ÿè·µ â†’](./chapter-3.md)</strong>

<h3>ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡</h3>
<strong>[â† ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹](./index.md)</strong>

---

<h2>è‘—è€…æƒ…å ±</h2>

<strong>ä½œæˆè€…</strong>: AI Terakoya Content Team
<strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰
<strong>ä½œæˆæ—¥</strong>: 2025-10-17
<strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0

<strong>æ›´æ–°å±¥æ­´</strong>:
- 2025-10-17: v1.0 åˆç‰ˆå…¬é–‹

<strong>ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯</strong>:
- GitHub Issues: [ãƒªãƒã‚¸ãƒˆãƒªURL]/issues
- Email: yusuke.hashimoto.b8@tohoku.ac.jp

<strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0

---

<strong>ç¬¬3ç« ã§ã€å®Ÿéš›ã«GNNã‚’å‹•ã‹ã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼</strong>
<div class="navigation">
    <a href="chapter-1.html" class="nav-button">â† ç¬¬1ç« </a>
    <a href="index.html" class="nav-button">ã‚·ãƒªãƒ¼ã‚ºç›®æ¬¡ã«æˆ»ã‚‹</a>
    <a href="chapter-3.html" class="nav-button">ç¬¬3ç«  â†’</a>
</div>
    </main>

    <footer>
        <p><strong>ä½œæˆè€…</strong>: AI Terakoya Content Team</p>
        <p><strong>ç›£ä¿®</strong>: Dr. Yusuke Hashimotoï¼ˆæ±åŒ—å¤§å­¦ï¼‰</p>
        <p><strong>ãƒãƒ¼ã‚¸ãƒ§ãƒ³</strong>: 1.0 | <strong>ä½œæˆæ—¥</strong>: 2025-10-17</p>
        <p><strong>ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</strong>: Creative Commons BY 4.0</p>
        <p>Â© 2025 AI Terakoya. All rights reserved.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const mermaidCodeBlocks = document.querySelectorAll('pre.codehilite code.language-mermaid, pre code.language-mermaid');

            mermaidCodeBlocks.forEach(function(codeBlock) {
                const pre = codeBlock.parentElement;
                const mermaidCode = codeBlock.textContent;

                const mermaidDiv = document.createElement('div');
                mermaidDiv.className = 'mermaid';
                mermaidDiv.textContent = mermaidCode.trim();

                pre.parentNode.replaceChild(mermaidDiv, pre);
            });

            if (typeof mermaid !== 'undefined') {
                mermaid.initialize({
                    startOnLoad: true,
                    theme: 'default'
                });
                mermaid.init(undefined, document.querySelectorAll('.mermaid'));
            }
        });
    </script>
</body>
</html>
