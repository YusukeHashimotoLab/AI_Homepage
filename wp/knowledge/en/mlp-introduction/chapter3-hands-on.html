<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="From Environment Setup to Training and MLP-MD">
    <title>Chapter 3: Hands-On MLP with Python - SchNetPack Tutorial - MI Knowledge Hub</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Helvetica Neue", Arial, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 0.9rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        tbody {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: #666;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Details/Summary (for exercises) */
        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            padding: 0.5rem;
        }

        summary:hover {
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }

        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }

            table {
                font-size: 0.9rem;
            }
        }
    </style>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="container">
            <h1>Chapter 3: Hands-On MLP with Python - SchNetPack Tutorial</h1>
            <div class="meta">
                <span>📖 Reading Time: 30-35 minutes</span>
                <span>📊 Level: beginner-intermediate</span>
            </div>
        </div>
    </header>

    <main class="container">
        <h1 id="chapter-3-hands-on-mlp-with-python-schnetpack-tutorial">Chapter 3: Hands-On MLP with Python - SchNetPack Tutorial</h1>
<h2 id="learning-objectives">Learning Objectives</h2>
<p>By completing this chapter, you will be able to:<br />
- Install SchNetPack in a Python environment and set up the development environment<br />
- Train MLP models using a small dataset (aspirin molecule from MD17)<br />
- Evaluate trained model accuracy and verify energy/force prediction errors<br />
- Execute MLP-MD simulations and analyze trajectories<br />
- Understand common errors and troubleshooting strategies</p>
<hr />
<h2 id="31-environment-setup-installing-required-tools">3.1 Environment Setup: Installing Required Tools</h2>
<p>To practice with MLPs, you need to set up a Python environment and install SchNetPack.</p>
<h3 id="required-software">Required Software</h3>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Version</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Python</strong></td>
<td>3.9-3.11</td>
<td>Base language</td>
</tr>
<tr>
<td><strong>PyTorch</strong></td>
<td>2.0+</td>
<td>Deep learning framework</td>
</tr>
<tr>
<td><strong>SchNetPack</strong></td>
<td>2.0+</td>
<td>MLP training and inference</td>
</tr>
<tr>
<td><strong>ASE</strong></td>
<td>3.22+</td>
<td>Atomic structure manipulation, MD execution</td>
</tr>
<tr>
<td><strong>NumPy/Matplotlib</strong></td>
<td>Latest</td>
<td>Data analysis and visualization</td>
</tr>
</tbody>
</table>
<h3 id="installation-steps">Installation Steps</h3>
<p><strong>Step 1: Create Conda Environment</strong></p>
<pre class="codehilite"><code class="language-bash"># Create new Conda environment (Python 3.10)
conda create -n mlp-tutorial python=3.10 -y
conda activate mlp-tutorial
</code></pre>

<p><strong>Step 2: Install PyTorch</strong></p>
<pre class="codehilite"><code class="language-bash"># CPU version (local machine, lightweight)
conda install pytorch cpuonly -c pytorch

# GPU version (if CUDA is available)
conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia
</code></pre>

<p><strong>Step 3: Install SchNetPack and ASE</strong></p>
<pre class="codehilite"><code class="language-bash"># SchNetPack (pip recommended)
pip install schnetpack

# ASE (Atomic Simulation Environment)
pip install ase

# Visualization tools
pip install matplotlib seaborn
</code></pre>

<p><strong>Step 4: Verify Installation</strong></p>
<pre class="codehilite"><code class="language-python"># Example 1: Environment verification script (5 lines)
import torch
import schnetpack as spk
print(f&quot;PyTorch: {torch.__version__}&quot;)
print(f&quot;SchNetPack: {spk.__version__}&quot;)
print(f&quot;GPU available: {torch.cuda.is_available()}&quot;)
</code></pre>

<p><strong>Expected output</strong>:</p>
<pre class="codehilite"><code>PyTorch: 2.1.0
SchNetPack: 2.0.3
GPU available: False  # For CPU
</code></pre>

<hr />
<h2 id="32-data-preparation-obtaining-the-md17-dataset">3.2 Data Preparation: Obtaining the MD17 Dataset</h2>
<p>SchNetPack includes the <strong>MD17</strong> benchmark dataset for small molecules.</p>
<h3 id="about-the-md17-dataset">About the MD17 Dataset</h3>
<ul>
<li><strong>Content</strong>: Molecular dynamics trajectories from DFT calculations</li>
<li><strong>Molecules</strong>: 10 types including aspirin, benzene, ethanol</li>
<li><strong>Data size</strong>: ~100,000 configurations per molecule</li>
<li><strong>Accuracy</strong>: PBE/def2-SVP level (DFT)</li>
<li><strong>Use case</strong>: Benchmarking MLP methods</li>
</ul>
<h3 id="downloading-and-loading-data">Downloading and Loading Data</h3>
<p><strong>Example 2: Loading MD17 dataset (10 lines)</strong></p>
<pre class="codehilite"><code class="language-python">from schnetpack.datasets import MD17
from schnetpack.data import AtomsDataModule

# Download aspirin molecule dataset (~100k configurations)
dataset = MD17(
    datapath='./data',
    molecule='aspirin',
    download=True
)

print(f&quot;Total samples: {len(dataset)}&quot;)
print(f&quot;Properties: {dataset.available_properties}&quot;)
print(f&quot;First sample: {dataset[0]}&quot;)
</code></pre>

<p><strong>Output</strong>:</p>
<pre class="codehilite"><code>Total samples: 211762
Properties: ['energy', 'forces']
First sample: {'_atomic_numbers': tensor([...]), 'energy': tensor(-1234.5), 'forces': tensor([...])}
</code></pre>

<h3 id="data-splitting">Data Splitting</h3>
<p><strong>Example 3: Train/validation/test split (10 lines)</strong></p>
<pre class="codehilite"><code class="language-python"># Split data into train:validation:test = 70%:15%:15%
data_module = AtomsDataModule(
    datapath='./data',
    dataset=dataset,
    batch_size=32,
    num_train=100000,      # Number of training samples
    num_val=10000,          # Number of validation samples
    num_test=10000,         # Number of test samples
    split_file='split.npz', # Save split information
)
data_module.prepare_data()
data_module.setup()
</code></pre>

<p><strong>Explanation</strong>:<br />
- <code>batch_size=32</code>: Process 32 configurations at a time (memory efficiency)<br />
- <code>num_train=100000</code>: Large dataset improves generalization<br />
- <code>split_file</code>: Save split to file (ensures reproducibility)</p>
<hr />
<h2 id="33-model-training-with-schnetpack">3.3 Model Training with SchNetPack</h2>
<p>Train a SchNet model to learn energies and forces.</p>
<h3 id="configuring-schnet-architecture">Configuring SchNet Architecture</h3>
<p><strong>Example 4: Defining SchNet model (15 lines)</strong></p>
<pre class="codehilite"><code class="language-python">import schnetpack.transform as trn
from schnetpack.representation import SchNet
from schnetpack.model import AtomisticModel
from schnetpack.task import ModelOutput

# 1. SchNet representation layer (atomic configuration → feature vectors)
representation = SchNet(
    n_atom_basis=128,      # Dimension of atomic feature vectors
    n_interactions=6,      # Number of message passing layers
    cutoff=5.0,            # Cutoff radius (Å)
    n_filters=128          # Number of filters
)

# 2. Output layer (energy prediction)
output = ModelOutput(
    name='energy',
    loss_fn=torch.nn.MSELoss(),
    metrics={'MAE': spk.metrics.MeanAbsoluteError()}
)
</code></pre>

<p><strong>Parameter explanation</strong>:<br />
- <code>n_atom_basis=128</code>: Each atom's feature vector is 128-dimensional (typical value)<br />
- <code>n_interactions=6</code>: 6 message passing layers (deeper captures long-range interactions)<br />
- <code>cutoff=5.0Å</code>: Ignore atomic interactions beyond this distance (computational efficiency)</p>
<h3 id="executing-training">Executing Training</h3>
<p><strong>Example 5: Training loop setup (15 lines)</strong></p>
<pre class="codehilite"><code class="language-python">import pytorch_lightning as pl
from schnetpack.task import AtomisticTask

# Define training task
task = AtomisticTask(
    model=AtomisticModel(representation, [output]),
    outputs=[output],
    optimizer_cls=torch.optim.AdamW,
    optimizer_args={'lr': 1e-4}  # Learning rate
)

# Configure Trainer
trainer = pl.Trainer(
    max_epochs=50,               # Maximum 50 epochs
    accelerator='cpu',           # Use CPU (GPU: 'gpu')
    devices=1,
    default_root_dir='./training'
)

# Start training
trainer.fit(task, datamodule=data_module)
</code></pre>

<p><strong>Training time estimates</strong>:<br />
- CPU (4 cores): ~2-3 hours (100k configurations)<br />
- GPU (RTX 3090): ~15-20 minutes</p>
<h3 id="monitoring-training-progress">Monitoring Training Progress</h3>
<p><strong>Example 6: Visualization with TensorBoard (10 lines)</strong></p>
<pre class="codehilite"><code class="language-python"># Launch TensorBoard (in separate terminal)
# tensorboard --logdir=./training/lightning_logs

# Check logs from Python
import pandas as pd

metrics = pd.read_csv('./training/lightning_logs/version_0/metrics.csv')
print(metrics[['epoch', 'train_loss', 'val_loss']].tail(10))
</code></pre>

<p><strong>Expected output</strong>:</p>
<pre class="codehilite"><code>   epoch  train_loss  val_loss
40    40      0.0023    0.0031
41    41      0.0022    0.0030
42    42      0.0021    0.0029
...
</code></pre>

<p><strong>Key observations</strong>:<br />
- Both <code>train_loss</code> and <code>val_loss</code> decreasing → Normal learning<br />
- <code>val_loss</code> starts increasing → <strong>Overfitting</strong> sign → Consider Early Stopping</p>
<hr />
<h2 id="34-accuracy-validation-energy-and-force-prediction-accuracy">3.4 Accuracy Validation: Energy and Force Prediction Accuracy</h2>
<p>Evaluate whether the trained model achieves DFT-level accuracy.</p>
<h3 id="evaluation-on-test-set">Evaluation on Test Set</h3>
<p><strong>Example 7: Test set evaluation (12 lines)</strong></p>
<pre class="codehilite"><code class="language-python"># Evaluate on test set
test_results = trainer.test(task, datamodule=data_module)

# Display results
print(f&quot;Energy MAE: {test_results[0]['test_energy_MAE']:.4f} eV&quot;)
print(f&quot;Energy RMSE: {test_results[0]['test_energy_RMSE']:.4f} eV&quot;)

# Force evaluation (requires separate calculation)
from schnetpack.metrics import MeanAbsoluteError
force_mae = MeanAbsoluteError(target='forces')
# ... Force evaluation code
</code></pre>

<p><strong>Good accuracy benchmarks</strong> (aspirin molecule, 21 atoms):<br />
- <strong>Energy MAE</strong>: &lt; 1 kcal/mol (&lt; 0.043 eV)<br />
- <strong>Force MAE</strong>: &lt; 1 kcal/mol/Å (&lt; 0.043 eV/Å)</p>
<h3 id="correlation-plot-of-predictions-vs-true-values">Correlation Plot of Predictions vs. True Values</h3>
<p><strong>Example 8: Visualizing prediction accuracy (15 lines)</strong></p>
<pre class="codehilite"><code class="language-python">import matplotlib.pyplot as plt
import numpy as np

# Make predictions on test data
model = task.model
predictions, targets = [], []

for batch in data_module.test_dataloader():
    pred = model(batch)['energy'].detach().numpy()
    true = batch['energy'].numpy()
    predictions.extend(pred)
    targets.extend(true)

# Create scatter plot
plt.scatter(targets, predictions, alpha=0.5, s=1)
plt.plot([min(targets), max(targets)], [min(targets), max(targets)], 'r--')
plt.xlabel('DFT Energy (eV)')
plt.ylabel('MLP Predicted Energy (eV)')
plt.title('Energy Prediction Accuracy')
plt.show()
</code></pre>

<p><strong>Ideal result</strong>:<br />
- Points densely clustered on red diagonal line (y=x)<br />
- R² &gt; 0.99 (coefficient of determination)</p>
<hr />
<h2 id="35-mlp-md-simulation-running-molecular-dynamics">3.5 MLP-MD Simulation: Running Molecular Dynamics</h2>
<p>Use the trained MLP to run MD simulations 10⁴ times faster than DFT.</p>
<h3 id="setting-up-mlp-md-with-ase">Setting Up MLP-MD with ASE</h3>
<p><strong>Example 9: Preparing MLP-MD calculation (10 lines)</strong></p>
<pre class="codehilite"><code class="language-python">from ase import units
from ase.md.velocitydistribution import MaxwellBoltzmannDistribution
from ase.md.verlet import VelocityVerlet
import schnetpack.interfaces.ase_interface as spk_ase

# Wrap MLP as ASE Calculator
calculator = spk_ase.SpkCalculator(
    model_file='./training/best_model.ckpt',
    device='cpu'
)

# Prepare initial structure (first configuration from MD17)
atoms = dataset.get_atoms(0)
atoms.calc = calculator
</code></pre>

<h3 id="setting-initial-velocities-and-equilibration">Setting Initial Velocities and Equilibration</h3>
<p><strong>Example 10: Temperature initialization (10 lines)</strong></p>
<pre class="codehilite"><code class="language-python"># Set velocity distribution at 300K
temperature = 300  # K
MaxwellBoltzmannDistribution(atoms, temperature_K=temperature)

# Remove total momentum (eliminate system translation)
from ase.md.velocitydistribution import Stationary
Stationary(atoms)

print(f&quot;Initial kinetic energy: {atoms.get_kinetic_energy():.3f} eV&quot;)
print(f&quot;Initial potential energy: {atoms.get_potential_energy():.3f} eV&quot;)
</code></pre>

<h3 id="running-md-simulation">Running MD Simulation</h3>
<p><strong>Example 11: MD execution and trajectory saving (12 lines)</strong></p>
<pre class="codehilite"><code class="language-python">from ase.io.trajectory import Trajectory

# Configure MD simulator
timestep = 0.5 * units.fs  # 0.5 femtoseconds
dyn = VelocityVerlet(atoms, timestep=timestep)

# Output trajectory file
traj = Trajectory('aspirin_md.traj', 'w', atoms)
dyn.attach(traj.write, interval=10)  # Save every 10 steps

# Run 10,000 steps (5 picoseconds) of MD
dyn.run(10000)
print(&quot;MD simulation completed!&quot;)
</code></pre>

<p><strong>Computation time estimates</strong>:<br />
- CPU (4 cores): ~5 minutes (10,000 steps)<br />
- DFT would take: ~1 week (10,000 steps)<br />
- <strong>Achieved 10⁴× speedup!</strong></p>
<h3 id="trajectory-analysis">Trajectory Analysis</h3>
<p><strong>Example 12: Energy conservation and RDF calculation (15 lines)</strong></p>
<pre class="codehilite"><code class="language-python">from ase.io import read
import numpy as np

# Load trajectory
traj_data = read('aspirin_md.traj', index=':')

# Check energy conservation
energies = [a.get_total_energy() for a in traj_data]
plt.plot(energies)
plt.xlabel('Time step')
plt.ylabel('Total Energy (eV)')
plt.title('Energy Conservation Check')
plt.show()

# Calculate energy drift (monotonic increase/decrease)
drift = (energies[-1] - energies[0]) / len(energies)
print(f&quot;Energy drift: {drift:.6f} eV/step&quot;)
</code></pre>

<p><strong>Good simulation indicators</strong>:<br />
- Energy drift: &lt; 0.001 eV/step<br />
- Total energy oscillates over time (conservation law)</p>
<hr />
<h2 id="36-property-calculations-vibrational-spectra-and-diffusion-coefficients">3.6 Property Calculations: Vibrational Spectra and Diffusion Coefficients</h2>
<p>Calculate physical properties from MLP-MD.</p>
<h3 id="vibrational-spectrum-power-spectrum">Vibrational Spectrum (Power Spectrum)</h3>
<p><strong>Example 13: Vibrational spectrum calculation (15 lines)</strong></p>
<pre class="codehilite"><code class="language-python">from scipy.fft import fft, fftfreq

# Extract velocity time series for one atom
atom_idx = 0  # First atom
velocities = np.array([a.get_velocities()[atom_idx] for a in traj_data])

# Fourier transform of x-direction velocity
vx = velocities[:, 0]
freq = fftfreq(len(vx), d=timestep)
spectrum = np.abs(fft(vx))**2

# Plot only positive frequencies
mask = freq &gt; 0
plt.plot(freq[mask] * 1e15 / (2 * np.pi), spectrum[mask])  # Hz → THz conversion
plt.xlabel('Frequency (THz)')
plt.ylabel('Power Spectrum')
plt.title('Vibrational Spectrum')
plt.xlim(0, 100)
plt.show()
</code></pre>

<p><strong>Interpretation</strong>:<br />
- Peaks correspond to molecular vibrational modes<br />
- Compare with DFT-calculated vibrational spectrum for accuracy validation</p>
<h3 id="mean-square-displacement-msd-and-diffusion-coefficient">Mean Square Displacement (MSD) and Diffusion Coefficient</h3>
<p><strong>Example 14: MSD calculation (15 lines)</strong></p>
<pre class="codehilite"><code class="language-python">def calculate_msd(traj, atom_idx=0):
    &quot;&quot;&quot;Calculate mean square displacement&quot;&quot;&quot;
    positions = np.array([a.positions[atom_idx] for a in traj])
    msd = np.zeros(len(positions))

    for t in range(len(positions)):
        displacement = positions[t:] - positions[:-t or None]
        msd[t] = np.mean(np.sum(displacement**2, axis=1))

    return msd

# Calculate and plot MSD
msd = calculate_msd(traj_data)
time_ps = np.arange(len(msd)) * timestep / units.fs * 1e-3  # Picoseconds

plt.plot(time_ps, msd)
plt.xlabel('Time (ps)')
plt.ylabel('MSD (Ų)')
plt.title('Mean Square Displacement')
plt.show()
</code></pre>

<p><strong>Calculating diffusion coefficient</strong>:</p>
<pre class="codehilite"><code class="language-python"># Calculate diffusion coefficient from linear region of MSD (Einstein relation)
# D = lim_{t→∞} MSD(t) / (6t)
linear_region = slice(100, 500)
fit = np.polyfit(time_ps[linear_region], msd[linear_region], deg=1)
D = fit[0] / 6  # Ų/ps → cm²/s conversion needed
print(f&quot;Diffusion coefficient: {D:.6f} Ų/ps&quot;)
</code></pre>

<hr />
<h2 id="37-active-learning-efficient-data-addition">3.7 Active Learning: Efficient Data Addition</h2>
<p>Automatically detect configurations where the model is uncertain and add DFT calculations.</p>
<h3 id="evaluating-ensemble-uncertainty">Evaluating Ensemble Uncertainty</h3>
<p><strong>Example 15: Prediction uncertainty (15 lines)</strong></p>
<pre class="codehilite"><code class="language-python"># Train multiple independent models (abbreviated: run Example 5 three times)
models = [model1, model2, model3]  # 3 independent models

def predict_with_uncertainty(atoms, models):
    &quot;&quot;&quot;Ensemble prediction with uncertainty&quot;&quot;&quot;
    predictions = []
    for model in models:
        atoms.calc = spk_ase.SpkCalculator(model_file=model, device='cpu')
        predictions.append(atoms.get_potential_energy())

    mean = np.mean(predictions)
    std = np.std(predictions)
    return mean, std

# Evaluate uncertainty for each configuration in MD trajectory
uncertainties = []
for atoms in traj_data[::100]:  # Every 100 frames
    _, std = predict_with_uncertainty(atoms, models)
    uncertainties.append(std)

# Identify configurations with high uncertainty
threshold = np.percentile(uncertainties, 95)
high_uncertainty_frames = np.where(np.array(uncertainties) &gt; threshold)[0]
print(f&quot;High uncertainty frames: {high_uncertainty_frames}&quot;)
</code></pre>

<p><strong>Next steps</strong>:<br />
- Add high-uncertainty configurations to DFT calculations<br />
- Update dataset and retrain model<br />
- Verify accuracy improvement</p>
<hr />
<h2 id="38-troubleshooting-common-errors-and-solutions">3.8 Troubleshooting: Common Errors and Solutions</h2>
<p>Common problems encountered in practice and their solutions.</p>
<table>
<thead>
<tr>
<th>Error</th>
<th>Cause</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Out of Memory (OOM)</strong></td>
<td>Batch size too large</td>
<td>Reduce <code>batch_size</code> 32→16→8</td>
</tr>
<tr>
<td><strong>Loss becomes NaN</strong></td>
<td>Learning rate too high</td>
<td>Lower <code>lr=1e-4</code>→<code>1e-5</code></td>
</tr>
<tr>
<td><strong>Energy drift in MD</strong></td>
<td>Timestep too large</td>
<td>Reduce <code>timestep=0.5fs</code>→<code>0.25fs</code></td>
</tr>
<tr>
<td><strong>Poor generalization</strong></td>
<td>Biased training data</td>
<td>Diversify data with Active Learning</td>
</tr>
<tr>
<td><strong>CUDA error</strong></td>
<td>GPU compatibility issue</td>
<td>Check PyTorch and CUDA versions</td>
</tr>
</tbody>
</table>
<h3 id="debugging-best-practices">Debugging Best Practices</h3>
<pre class="codehilite"><code class="language-python"># 1. Test with small dataset
data_module.num_train = 1000  # Quick test with 1,000 configurations

# 2. Check overfitting on single batch
trainer = pl.Trainer(max_epochs=100, overfit_batches=1)
# If training error approaches 0, model has learning capability

# 3. Gradient clipping
task = AtomisticTask(..., gradient_clip_val=1.0)  # Prevent gradient explosion
</code></pre>

<hr />
<h2 id="39-chapter-summary">3.9 Chapter Summary</h2>
<h3 id="what-you-learned">What You Learned</h3>
<ol>
<li>
<p><strong>Environment Setup</strong><br />
   - Installing Conda environment, PyTorch, SchNetPack<br />
   - Choosing GPU/CPU environment</p>
</li>
<li>
<p><strong>Data Preparation</strong><br />
   - Downloading and loading MD17 dataset<br />
   - Splitting into train/validation/test sets</p>
</li>
<li>
<p><strong>Model Training</strong><br />
   - Configuring SchNet architecture (6 layers, 128 dimensions)<br />
   - Training for 50 epochs (CPU: 2-3 hours)<br />
   - Monitoring progress with TensorBoard</p>
</li>
<li>
<p><strong>Accuracy Validation</strong><br />
   - Confirming energy MAE &lt; 1 kcal/mol achievement<br />
   - Correlation plot of predictions vs. true values<br />
   - High accuracy with R² &gt; 0.99</p>
</li>
<li>
<p><strong>MLP-MD Execution</strong><br />
   - Integration as ASE Calculator<br />
   - Running 10,000 steps (5 picoseconds) of MD<br />
   - Experiencing 10⁴× speedup over DFT</p>
</li>
<li>
<p><strong>Property Calculations</strong><br />
   - Vibrational spectrum (Fourier transform)<br />
   - Diffusion coefficient (calculated from mean square displacement)</p>
</li>
<li>
<p><strong>Active Learning</strong><br />
   - Configuration selection using ensemble uncertainty<br />
   - Automated data addition strategy</p>
</li>
</ol>
<h3 id="key-points">Key Points</h3>
<ul>
<li><strong>SchNetPack is easy to implement</strong>: MLP training with just a few dozen lines of code</li>
<li><strong>Practical accuracy with small data (100k configurations)</strong>: MD17 is an excellent benchmark</li>
<li><strong>MLP-MD is practical</strong>: 10⁴× faster than DFT, executable on personal PCs</li>
<li><strong>Active Learning improves efficiency</strong>: Automatically discover important configurations, reduce data collection costs</li>
</ul>
<h3 id="moving-to-the-next-chapter">Moving to the Next Chapter</h3>
<p>Chapter 4 will cover state-of-the-art MLP methods (NequIP, MACE) and real research applications:<br />
- Theory of E(3)-equivariant graph neural networks<br />
- Dramatic improvement in data efficiency (100k→3,000 configurations)<br />
- Application cases in catalysis, battery materials<br />
- Realizing large-scale simulations (1 million atoms)</p>
<hr />
<h2 id="practice-problems">Practice Problems</h2>
<h3 id="problem-1-difficulty-easy">Problem 1 (Difficulty: easy)</h3>
<p>Using the SchNet configuration in Example 4, change <code>n_interactions</code> (number of message passing layers) to 3, 6, and 9, train the models, and predict how the test MAE will change.</p>
<details>
<summary>Hint</summary>

Deeper layers can capture long-range atomic interactions. However, too deep risks overfitting.

</details>

<details>
<summary>Sample Answer</summary>

**Predicted results**:

| `n_interactions` | Predicted Test MAE | Training Time | Characteristics |
|-----------------|-------------------|---------------|-----------------|
| **3** | 0.8-1.2 kcal/mol | 1 hour | Shallow, cannot fully capture long-range interactions |
| **6** | 0.5-0.8 kcal/mol | 2-3 hours | Well-balanced (recommended) |
| **9** | 0.6-1.0 kcal/mol | 4-5 hours | Overfitting risk, accuracy drops with insufficient training data |

**Experimental method**:

<pre class="codehilite"><code class="language-python">for n in [3, 6, 9]:
    representation = SchNet(n_interactions=n, ...)
    task = AtomisticTask(...)
    trainer.fit(task, datamodule=data_module)
    results = trainer.test(task, datamodule=data_module)
    print(f&quot;n={n}: MAE={results[0]['test_energy_MAE']:.4f} eV&quot;)
</code></pre>



**Conclusion**: For small molecules (aspirin 21 atoms), `n_interactions=6` is optimal. For large systems (100+ atoms), 9-12 layers may be effective.

</details>

<h3 id="problem-2-difficulty-medium">Problem 2 (Difficulty: medium)</h3>
<p>In the MLP-MD from Example 11, if energy drift exceeds acceptable range (e.g., 0.01 eV/step), what countermeasures can be considered? List three.</p>
<details>
<summary>Hint</summary>

Consider from three perspectives: timestep, training accuracy, and MD algorithm.

</details>

<details>
<summary>Sample Answer</summary>

**Countermeasure 1: Reduce timestep**

<pre class="codehilite"><code class="language-python">timestep = 0.25 * units.fs  # Halve 0.5fs → 0.25fs
dyn = VelocityVerlet(atoms, timestep=timestep)
</code></pre>


- **Reason**: Smaller timestep reduces numerical integration error
- **Downside**: 2× computation time

**Countermeasure 2: Improve model training accuracy**

<pre class="codehilite"><code class="language-python"># Train with more data
data_module.num_train = 200000  # Increase 100k→200k configurations

# Or increase weight of force loss function
task = AtomisticTask(..., loss_weights={'energy': 1.0, 'forces': 1000})
</code></pre>


- **Reason**: Low force prediction accuracy destabilizes MD
- **Target**: Force MAE < 0.05 eV/Å

**Countermeasure 3: Switch to Langevin dynamics (heat bath coupling)**

<pre class="codehilite"><code class="language-python">from ase.md.langevin import Langevin
dyn = Langevin(atoms, timestep=0.5*units.fs,
               temperature_K=300, friction=0.01)
</code></pre>


- **Reason**: Heat bath absorbs energy drift
- **Note**: No longer strict microcanonical ensemble (NVE)

**Priority**: Countermeasure 2 (improve accuracy) → Countermeasure 1 (timestep) → Countermeasure 3 (Langevin)

</details>

<hr />
<h2 id="references">References</h2>
<ol>
<li>
<p>Schütt, K. T., et al. (2019). "SchNetPack: A Deep Learning Toolbox For Atomistic Systems." <em>Journal of Chemical Theory and Computation</em>, 15(1), 448-455.<br />
   DOI: <a href="https://doi.org/10.1021/acs.jctc.8b00908">10.1021/acs.jctc.8b00908</a></p>
</li>
<li>
<p>Chmiela, S., et al. (2017). "Machine learning of accurate energy-conserving molecular force fields." <em>Science Advances</em>, 3(5), e1603015.<br />
   DOI: <a href="https://doi.org/10.1126/sciadv.1603015">10.1126/sciadv.1603015</a></p>
</li>
<li>
<p>Larsen, A. H., et al. (2017). "The atomic simulation environment—a Python library for working with atoms." <em>Journal of Physics: Condensed Matter</em>, 29(27), 273002.<br />
   DOI: <a href="https://doi.org/10.1088/1361-648X/aa680e">10.1088/1361-648X/aa680e</a></p>
</li>
<li>
<p>Paszke, A., et al. (2019). "PyTorch: An imperative style, high-performance deep learning library." <em>Advances in Neural Information Processing Systems</em>, 32.<br />
   arXiv: <a href="https://arxiv.org/abs/1912.01703">1912.01703</a></p>
</li>
<li>
<p>Zhang, L., et al. (2020). "Active learning of uniformly accurate interatomic potentials for materials simulation." <em>Physical Review Materials</em>, 3(2), 023804.<br />
   DOI: <a href="https://doi.org/10.1103/PhysRevMaterials.3.023804">10.1103/PhysRevMaterials.3.023804</a></p>
</li>
<li>
<p>Schütt, K. T., et al. (2017). "Quantum-chemical insights from deep tensor neural networks." <em>Nature Communications</em>, 8(1), 13890.<br />
   DOI: <a href="https://doi.org/10.1038/ncomms13890">10.1038/ncomms13890</a></p>
</li>
</ol>
<hr />
<h2 id="author-information">Author Information</h2>
<p><strong>Created by</strong>: MI Knowledge Hub Content Team<br />
<strong>Supervised by</strong>: Dr. Yusuke Hashimoto (Tohoku University)<br />
<strong>Created on</strong>: 2025-10-17<br />
<strong>Version</strong>: 1.0 (Chapter 3 initial version)<br />
<strong>Series</strong>: MLP Introduction Series</p>
<p><strong>Update History</strong>:<br />
- 2025-10-17: v1.0 Chapter 3 initial version created<br />
  - Python environment setup (Conda, PyTorch, SchNetPack)<br />
  - MD17 dataset preparation and splitting<br />
  - SchNet model training (15 code examples)<br />
  - MLP-MD execution and analysis (trajectory, vibrational spectrum, MSD)<br />
  - Active Learning uncertainty evaluation<br />
  - Troubleshooting table (5 items)<br />
  - 2 practice problems (easy, medium)<br />
  - 6 references</p>
<p><strong>License</strong>: Creative Commons BY-NC-SA 4.0</p>

        <div class="nav-buttons">
            <a href="index.html" class="nav-button">← Back to Series Index</a>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 MI Knowledge Hub - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
