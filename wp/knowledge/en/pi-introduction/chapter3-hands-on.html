<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Implementation and Best Practices for Chemical Process Optimization">
    <title>Chapter 3: Experiencing PI with Python - Process Optimization Practice - MI Knowledge Hub</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Helvetica Neue", Arial, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 0.9rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        tbody {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: #666;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Details/Summary (for exercises) */
        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            padding: 0.5rem;
        }

        summary:hover {
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }

        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }

            table {
                font-size: 0.9rem;
            }
        }
    </style>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="container">
            <h1>Chapter 3: Experiencing PI with Python - Process Optimization Practice</h1>
            <div class="meta">
                <span>üìñ Reading Time: 35-40 minutes</span>
                <span>üìä Level: intermediate</span>
            </div>
        </div>
    </header>

    <main class="container">
        <h1 id="chapter-3-experiencing-pi-with-python-process-optimization-practice">Chapter 3: Experiencing PI with Python - Process Optimization Practice</h1>
<h2 id="learning-objectives">Learning Objectives</h2>
<p>By reading this article, you will acquire:<br />
- Ability to set up Python environment and install PI libraries<br />
- Skills in process data preprocessing and visualization<br />
- Implementation of 5+ regression models for process property prediction<br />
- Optimization of process conditions using Bayesian optimization<br />
- Analysis of trade-offs with multi-objective optimization<br />
- Independent troubleshooting of errors</p>
<hr />
<h2 id="1-environment-setup-3-options">1. Environment Setup: 3 Options</h2>
<p>There are three ways to set up a Python environment for chemical process optimization, depending on your situation.</p>
<h3 id="11-option-1-anaconda-recommended-for-beginners">1.1 Option 1: Anaconda (Recommended for Beginners)</h3>
<p><strong>Features:</strong><br />
- Scientific computing libraries included from the start<br />
- Easy environment management (GUI available)<br />
- Windows/Mac/Linux support</p>
<p><strong>Installation Steps:</strong></p>
<pre class="codehilite"><code class="language-bash"># 1. Download Anaconda
# Official site: https://www.anaconda.com/download
# Select Python 3.11 or higher

# 2. After installation, launch Anaconda Prompt

# 3. Create virtual environment (PI-dedicated environment)
conda create -n pi-env python=3.11 numpy pandas matplotlib scikit-learn jupyter scipy

# 4. Activate environment
conda activate pi-env

# 5. Install additional libraries
conda install -c conda-forge lightgbm scikit-optimize pymoo

# 6. Verify installation
python --version
# Output: Python 3.11.x
</code></pre>

<p><strong>Advantages of Anaconda:</strong><br />
- ‚úÖ NumPy, SciPy included from the start<br />
- ‚úÖ Fewer dependency issues<br />
- ‚úÖ Visual management with Anaconda Navigator<br />
- ‚ùå Large file size (3GB+)</p>
<h3 id="12-option-2-venv-python-standard">1.2 Option 2: venv (Python Standard)</h3>
<p><strong>Features:</strong><br />
- Python standard tool (no additional installation required)<br />
- Lightweight (install only what you need)<br />
- Project-specific environment isolation</p>
<p><strong>Installation Steps:</strong></p>
<pre class="codehilite"><code class="language-bash"># 1. Check if Python 3.11 or higher is installed
python3 --version
# Output: Python 3.11.x or higher required

# 2. Create virtual environment
python3 -m venv pi-env

# 3. Activate environment
# macOS/Linux:
source pi-env/bin/activate

# Windows (PowerShell):
pi-env\Scripts\Activate.ps1

# Windows (Command Prompt):
pi-env\Scripts\activate.bat

# 4. Upgrade pip
pip install --upgrade pip

# 5. Install required libraries
pip install numpy pandas matplotlib scikit-learn scipy jupyter
pip install lightgbm scikit-optimize pymoo

# 6. Verify installation
pip list
</code></pre>

<p><strong>Advantages of venv:</strong><br />
- ‚úÖ Lightweight (tens of MB)<br />
- ‚úÖ Python standard tool (no additional installation)<br />
- ‚úÖ Independent per project<br />
- ‚ùå Need to resolve dependencies manually</p>
<h3 id="13-option-3-google-colab-no-installation-required">1.3 Option 3: Google Colab (No Installation Required)</h3>
<p><strong>Features:</strong><br />
- Executable in browser only<br />
- No installation required (cloud execution)<br />
- Free GPU/TPU access</p>
<p><strong>Usage:</strong></p>
<pre class="codehilite"><code>1. Access Google Colab: https://colab.research.google.com
2. Create new notebook
3. Run the following code (required libraries already installed)
</code></pre>

<pre class="codehilite"><code class="language-python"># Google Colab has the following pre-installed
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

# Additional libraries require installation
!pip install scikit-optimize pymoo lightgbm

print(&quot;Library import successful!&quot;)
print(f&quot;NumPy version: {np.__version__}&quot;)
print(f&quot;Pandas version: {pd.__version__}&quot;)
</code></pre>

<p><strong>Advantages of Google Colab:</strong><br />
- ‚úÖ No installation required (immediate start)<br />
- ‚úÖ Free GPU access<br />
- ‚úÖ Google Drive integration (easy data storage)<br />
- ‚ùå Internet connection required<br />
- ‚ùå Session resets after 12 hours</p>
<h3 id="14-environment-selection-guide">1.4 Environment Selection Guide</h3>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Option</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>First-time Python environment</td>
<td>Anaconda</td>
<td>Easy setup, fewer issues</td>
</tr>
<tr>
<td>Already have Python environment</td>
<td>venv</td>
<td>Lightweight, project independence</td>
</tr>
<tr>
<td>Want to try immediately</td>
<td>Google Colab</td>
<td>No installation, instant start</td>
</tr>
<tr>
<td>Large-scale optimization needed</td>
<td>Anaconda or venv</td>
<td>Local execution, no resource limits</td>
</tr>
<tr>
<td>Offline environment</td>
<td>Anaconda or venv</td>
<td>Local execution, no internet required</td>
</tr>
</tbody>
</table>
<h3 id="15-installation-verification-and-troubleshooting">1.5 Installation Verification and Troubleshooting</h3>
<p><strong>Verification Commands:</strong></p>
<pre class="codehilite"><code class="language-python"># Executable in all environments
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn
import scipy

print(&quot;===== Environment Check =====&quot;)
print(f&quot;Python version: {sys.version}&quot;)
print(f&quot;NumPy version: {np.__version__}&quot;)
print(f&quot;Pandas version: {pd.__version__}&quot;)
print(f&quot;Matplotlib version: {plt.matplotlib.__version__}&quot;)
print(f&quot;scikit-learn version: {sklearn.__version__}&quot;)
print(f&quot;SciPy version: {scipy.__version__}&quot;)

# PI-specific library check
try:
    import skopt
    print(f&quot;scikit-optimize version: {skopt.__version__}&quot;)
except ImportError:
    print(&quot;‚ö†Ô∏è scikit-optimize not installed (pip install scikit-optimize)&quot;)

try:
    import pymoo
    print(f&quot;pymoo version: {pymoo.__version__}&quot;)
except ImportError:
    print(&quot;‚ö†Ô∏è pymoo not installed (pip install pymoo)&quot;)

print(&quot;\n‚úÖ Basic libraries are properly installed!&quot;)
</code></pre>

<p><strong>Common Errors and Solutions:</strong></p>
<table>
<thead>
<tr>
<th>Error Message</th>
<th>Cause</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ModuleNotFoundError: No module named 'skopt'</code></td>
<td>scikit-optimize not installed</td>
<td>Run <code>pip install scikit-optimize</code></td>
</tr>
<tr>
<td><code>ImportError: DLL load failed</code> (Windows)</td>
<td>Missing C++ redistributable package</td>
<td>Install Microsoft Visual C++ Redistributable</td>
</tr>
<tr>
<td><code>SSL: CERTIFICATE_VERIFY_FAILED</code></td>
<td>SSL certificate error</td>
<td><code>pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org &lt;package&gt;</code></td>
</tr>
<tr>
<td><code>MemoryError</code></td>
<td>Out of memory</td>
<td>Reduce data size or use Google Colab</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="2-process-data-preparation-and-visualization">2. Process Data Preparation and Visualization</h2>
<p>We will simulate real chemical process data and perform preprocessing and visualization.</p>
<h3 id="21-example-1-process-data-generation-and-loading">2.1 Example 1: Process Data Generation and Loading</h3>
<p><strong>Overview:</strong><br />
Generate simulated data for chemical reaction process (temperature, pressure, catalyst amount ‚Üí yield).</p>
<pre class="codehilite"><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import time

# Generate simulated chemical reaction process data
np.random.seed(42)
n_samples = 200

# Process conditions (input variables)
temperature = np.random.uniform(300, 500, n_samples)  # Temperature [K]
pressure = np.random.uniform(1, 10, n_samples)  # Pressure [bar]
catalyst = np.random.uniform(0.1, 5.0, n_samples)  # Catalyst amount [wt%]

# Yield model (nonlinear relationship + noise)
# Yield = f(temperature, pressure, catalyst amount) + noise
yield_percentage = (
    20  # Base yield
    + 0.15 * temperature  # Temperature effect (positive correlation)
    - 0.0002 * temperature**2  # Temperature quadratic term (optimal temperature exists)
    + 5.0 * pressure  # Pressure effect (positive correlation)
    + 3.0 * catalyst  # Catalyst effect (positive correlation)
    - 0.3 * catalyst**2  # Catalyst quadratic term (effect decreases with excess)
    + 0.01 * temperature * pressure  # Temperature-pressure interaction
    + np.random.normal(0, 3, n_samples)  # Noise (measurement error)
)

# Store data in DataFrame
process_data = pd.DataFrame({
    'temperature_K': temperature,
    'pressure_bar': pressure,
    'catalyst_wt%': catalyst,
    'yield_%': yield_percentage
})

print(&quot;===== Process Data Overview =====&quot;)
print(process_data.head(10))
print(f&quot;\nNumber of data points: {len(process_data)}&quot;)
print(f&quot;\nBasic statistics:&quot;)
print(process_data.describe())

# Save in CSV format (actual process data provided in this format)
process_data.to_csv('process_data.csv', index=False)
print(&quot;\n‚úÖ Data saved to process_data.csv&quot;)
</code></pre>

<p><strong>Code Explanation:</strong><br />
1. <strong>Process Conditions</strong>: Temperature (300-500 K), Pressure (1-10 bar), Catalyst amount (0.1-5.0 wt%)<br />
2. <strong>Yield Model</strong>: Nonlinear relationship (quadratic terms, interaction terms) + noise<br />
3. <strong>Real Data Simulation</strong>: Typical chemical reaction behavior (optimal conditions exist, effect decreases with excess)</p>
<h3 id="22-example-2-data-visualization-pairplot">2.2 Example 2: Data Visualization (Pairplot)</h3>
<pre class="codehilite"><code class="language-python">import seaborn as sns

# Check relationships between variables with pairplot
fig = plt.figure(figsize=(12, 10))
sns.pairplot(
    process_data,
    diag_kind='hist',  # Histogram on diagonal
    plot_kws={'alpha': 0.6, 's': 50},  # Scatter plot settings
    diag_kws={'bins': 20, 'edgecolor': 'black'}  # Histogram settings
)
plt.suptitle('Process Data Pairplot', y=1.01, fontsize=16)
plt.tight_layout()
plt.show()

print(&quot;===== Correlation Matrix =====&quot;)
correlation_matrix = process_data.corr()
print(correlation_matrix)

# Visualize correlation with heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(
    correlation_matrix,
    annot=True,  # Display numbers
    fmt='.3f',  # 3 decimal places
    cmap='coolwarm',  # Colormap
    center=0,  # Center at 0
    square=True,  # Square cells
    linewidths=1,  # Cell borders
    cbar_kws={'label': 'Correlation Coefficient'}
)
plt.title('Process Variable Correlation Heatmap', fontsize=14)
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>Interpretation Points:</strong><br />
- Temperature vs Yield: Curved relationship (optimal temperature exists)<br />
- Pressure vs Yield: Positive correlation (pressure‚Üë ‚Üí yield‚Üë)<br />
- Catalyst amount vs Yield: Curved relationship (effect decreases with excess)</p>
<h3 id="23-example-3-data-preprocessing-missing-values-outliers">2.3 Example 3: Data Preprocessing (Missing Values &amp; Outliers)</h3>
<pre class="codehilite"><code class="language-python"># Check and handle missing values
print(&quot;===== Missing Values Check =====&quot;)
print(process_data.isnull().sum())

# Add missing values artificially (frequent in real data)
process_data_with_missing = process_data.copy()
missing_indices = np.random.choice(process_data.index, size=10, replace=False)
process_data_with_missing.loc[missing_indices, 'catalyst_wt%'] = np.nan

print(f&quot;\nAfter adding missing values: {process_data_with_missing.isnull().sum()['catalyst_wt%']} cases&quot;)

# Impute missing values (fill with mean)
process_data_filled = process_data_with_missing.copy()
process_data_filled['catalyst_wt%'].fillna(
    process_data_filled['catalyst_wt%'].mean(),
    inplace=True
)

print(&quot;‚úÖ Missing values imputed with mean&quot;)

# Outlier detection (Z-score method)
from scipy import stats

z_scores = np.abs(stats.zscore(process_data[['yield_%']]))
outliers = (z_scores &gt; 3).any(axis=1)

print(f&quot;\n===== Outlier Detection =====&quot;)
print(f&quot;Number of outliers: {outliers.sum()}&quot;)
print(f&quot;Outlier percentage: {outliers.sum() / len(process_data) * 100:.1f}%&quot;)

# Remove outliers
process_data_clean = process_data[~outliers].copy()
print(f&quot;Data count after removal: {len(process_data_clean)}&quot;)
</code></pre>

<h3 id="24-example-4-feature-engineering-interaction-terms">2.4 Example 4: Feature Engineering (Interaction Terms)</h3>
<pre class="codehilite"><code class="language-python"># Create process-specific features
process_data_enhanced = process_data_clean.copy()

# Interaction terms (temperature√ópressure, temperature√ócatalyst)
process_data_enhanced['temp_pressure'] = (
    process_data_enhanced['temperature_K'] * process_data_enhanced['pressure_bar']
)
process_data_enhanced['temp_catalyst'] = (
    process_data_enhanced['temperature_K'] * process_data_enhanced['catalyst_wt%']
)

# Quadratic terms (temperature¬≤, catalyst¬≤)
process_data_enhanced['temp_squared'] = process_data_enhanced['temperature_K'] ** 2
process_data_enhanced['catalyst_squared'] = process_data_enhanced['catalyst_wt%'] ** 2

# Ratio (catalyst/pressure)
process_data_enhanced['catalyst_pressure_ratio'] = (
    process_data_enhanced['catalyst_wt%'] / (process_data_enhanced['pressure_bar'] + 1e-10)
)

print(&quot;===== Data After Feature Engineering =====&quot;)
print(process_data_enhanced.head())
print(f&quot;\nNumber of features: {len(process_data_enhanced.columns) - 1} (original 3 ‚Üí 8)&quot;)
</code></pre>

<h3 id="25-example-5-data-splitting-traintest">2.5 Example 5: Data Splitting (Train/Test)</h3>
<pre class="codehilite"><code class="language-python"># Separate features and target variable
X = process_data_enhanced.drop('yield_%', axis=1)  # Input: Process conditions
y = process_data_enhanced['yield_%']  # Output: Yield

# Split into training and test data (80% vs 20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(&quot;===== Data Split =====&quot;)
print(f&quot;Training data: {len(X_train)} samples&quot;)
print(f&quot;Test data: {len(X_test)} samples&quot;)
print(f&quot;\nFeature column names:&quot;)
print(list(X.columns))
</code></pre>

<h3 id="26-example-6-data-standardization">2.6 Example 6: Data Standardization</h3>
<pre class="codehilite"><code class="language-python">from sklearn.preprocessing import StandardScaler

# Create standardizer (convert to mean 0, standard deviation 1)
scaler = StandardScaler()

# Learn standardization parameters from training data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Transform test data with same parameters

print(&quot;===== Standardization =====&quot;)
print(&quot;Training data (before standardization):&quot;)
print(f&quot;  Temperature mean: {X_train['temperature_K'].mean():.1f} K&quot;)
print(f&quot;  Temperature std: {X_train['temperature_K'].std():.1f} K&quot;)

print(&quot;\nTraining data (after standardization):&quot;)
print(f&quot;  Temperature mean: {X_train_scaled[:, 0].mean():.3f}&quot;)
print(f&quot;  Temperature std: {X_train_scaled[:, 0].std():.3f}&quot;)

print(&quot;\n‚úÖ All features standardized to mean 0, std 1&quot;)
</code></pre>

<h3 id="27-example-7-time-series-process-data-visualization">2.7 Example 7: Time Series Process Data Visualization</h3>
<pre class="codehilite"><code class="language-python"># Generate time series process data (batch process example)
np.random.seed(42)
time_hours = np.arange(0, 24, 0.5)  # 24 hours, 0.5 hour intervals
n_points = len(time_hours)

# Time series changes in process variables
temp_time = 350 + 50 * np.sin(2 * np.pi * time_hours / 24) + np.random.normal(0, 2, n_points)
pressure_time = 5 + 2 * np.sin(2 * np.pi * time_hours / 12 + np.pi/4) + np.random.normal(0, 0.3, n_points)
yield_time = 60 + 10 * np.sin(2 * np.pi * time_hours / 24 - np.pi/2) + np.random.normal(0, 1.5, n_points)

# Visualization
fig, axes = plt.subplots(3, 1, figsize=(12, 10))

axes[0].plot(time_hours, temp_time, 'r-', linewidth=2, label='Temperature')
axes[0].set_ylabel('Temperature [K]', fontsize=12)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

axes[1].plot(time_hours, pressure_time, 'b-', linewidth=2, label='Pressure')
axes[1].set_ylabel('Pressure [bar]', fontsize=12)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

axes[2].plot(time_hours, yield_time, 'g-', linewidth=2, label='Yield')
axes[2].set_xlabel('Time [h]', fontsize=12)
axes[2].set_ylabel('Yield [%]', fontsize=12)
axes[2].legend()
axes[2].grid(True, alpha=0.3)

plt.suptitle('Batch Process Time Series Data', fontsize=16)
plt.tight_layout()
plt.show()
</code></pre>

<hr />
<h2 id="3-yield-prediction-with-regression-models">3. Yield Prediction with Regression Models</h2>
<p>We implement 5 machine learning models to predict yield from process conditions.</p>
<h3 id="31-example-8-linear-regression-baseline">3.1 Example 8: Linear Regression (Baseline)</h3>
<pre class="codehilite"><code class="language-python">from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Build linear regression model
start_time = time.time()
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)
training_time_lr = time.time() - start_time

# Prediction
y_pred_lr = model_lr.predict(X_test)

# Evaluation
mae_lr = mean_absolute_error(y_test, y_pred_lr)
rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))
r2_lr = r2_score(y_test, y_pred_lr)

print(&quot;===== Linear Regression Model Performance =====&quot;)
print(f&quot;Training time: {training_time_lr:.4f} seconds&quot;)
print(f&quot;Mean Absolute Error (MAE): {mae_lr:.2f} %&quot;)
print(f&quot;Root Mean Squared Error (RMSE): {rmse_lr:.2f} %&quot;)
print(f&quot;R-squared (R¬≤): {r2_lr:.4f}&quot;)

# Display learned coefficients
print(&quot;\n===== Learned Coefficients (Top 3) =====&quot;)
coefficients = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': model_lr.coef_
}).sort_values('Coefficient', key=abs, ascending=False)
print(coefficients.head(3))
</code></pre>

<h3 id="32-example-9-random-forest-regression">3.2 Example 9: Random Forest Regression</h3>
<pre class="codehilite"><code class="language-python">from sklearn.ensemble import RandomForestRegressor

# Build Random Forest model
start_time = time.time()
model_rf = RandomForestRegressor(
    n_estimators=100,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)
model_rf.fit(X_train, y_train)
training_time_rf = time.time() - start_time

# Prediction and evaluation
y_pred_rf = model_rf.predict(X_test)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)

print(&quot;\n===== Random Forest Performance =====&quot;)
print(f&quot;Training time: {training_time_rf:.4f} seconds&quot;)
print(f&quot;Mean Absolute Error (MAE): {mae_rf:.2f} %&quot;)
print(f&quot;Root Mean Squared Error (RMSE): {rmse_rf:.2f} %&quot;)
print(f&quot;R-squared (R¬≤): {r2_rf:.4f}&quot;)

# Feature importance
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model_rf.feature_importances_
}).sort_values('Importance', ascending=False)

print(&quot;\n===== Feature Importance (Top 3) =====&quot;)
print(feature_importance.head(3))
</code></pre>

<h3 id="33-example-10-lightgbm-regression">3.3 Example 10: LightGBM Regression</h3>
<pre class="codehilite"><code class="language-python">import lightgbm as lgb

# Build LightGBM model
start_time = time.time()
model_lgb = lgb.LGBMRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=10,
    num_leaves=31,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbose=-1
)
model_lgb.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    eval_metric='rmse',
    callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]
)
training_time_lgb = time.time() - start_time

# Prediction and evaluation
y_pred_lgb = model_lgb.predict(X_test)
mae_lgb = mean_absolute_error(y_test, y_pred_lgb)
rmse_lgb = np.sqrt(mean_squared_error(y_test, y_pred_lgb))
r2_lgb = r2_score(y_test, y_pred_lgb)

print(&quot;\n===== LightGBM Performance =====&quot;)
print(f&quot;Training time: {training_time_lgb:.4f} seconds&quot;)
print(f&quot;Mean Absolute Error (MAE): {mae_lgb:.2f} %&quot;)
print(f&quot;Root Mean Squared Error (RMSE): {rmse_lgb:.2f} %&quot;)
print(f&quot;R-squared (R¬≤): {r2_lgb:.4f}&quot;)
</code></pre>

<h3 id="34-example-11-support-vector-regression-svr">3.4 Example 11: Support Vector Regression (SVR)</h3>
<pre class="codehilite"><code class="language-python">from sklearn.svm import SVR

# Build SVR model (use standardized data)
start_time = time.time()
model_svr = SVR(
    kernel='rbf',
    C=100,
    gamma='scale',
    epsilon=0.1
)
model_svr.fit(X_train_scaled, y_train)
training_time_svr = time.time() - start_time

# Prediction and evaluation
y_pred_svr = model_svr.predict(X_test_scaled)
mae_svr = mean_absolute_error(y_test, y_pred_svr)
rmse_svr = np.sqrt(mean_squared_error(y_test, y_pred_svr))
r2_svr = r2_score(y_test, y_pred_svr)

print(&quot;\n===== SVR Performance =====&quot;)
print(f&quot;Training time: {training_time_svr:.4f} seconds&quot;)
print(f&quot;Mean Absolute Error (MAE): {mae_svr:.2f} %&quot;)
print(f&quot;Root Mean Squared Error (RMSE): {rmse_svr:.2f} %&quot;)
print(f&quot;R-squared (R¬≤): {r2_svr:.4f}&quot;)
</code></pre>

<h3 id="35-example-12-neural-network-mlp">3.5 Example 12: Neural Network (MLP)</h3>
<pre class="codehilite"><code class="language-python">from sklearn.neural_network import MLPRegressor

# Build MLP model
start_time = time.time()
model_mlp = MLPRegressor(
    hidden_layer_sizes=(64, 32, 16),
    activation='relu',
    solver='adam',
    alpha=0.001,
    learning_rate_init=0.01,
    max_iter=500,
    random_state=42,
    early_stopping=True,
    validation_fraction=0.2,
    verbose=False
)
model_mlp.fit(X_train_scaled, y_train)
training_time_mlp = time.time() - start_time

# Prediction and evaluation
y_pred_mlp = model_mlp.predict(X_test_scaled)
mae_mlp = mean_absolute_error(y_test, y_pred_mlp)
rmse_mlp = np.sqrt(mean_squared_error(y_test, y_pred_mlp))
r2_mlp = r2_score(y_test, y_pred_mlp)

print(&quot;\n===== MLP Performance =====&quot;)
print(f&quot;Training time: {training_time_mlp:.4f} seconds&quot;)
print(f&quot;Mean Absolute Error (MAE): {mae_mlp:.2f} %&quot;)
print(f&quot;Root Mean Squared Error (RMSE): {rmse_mlp:.2f} %&quot;)
print(f&quot;R-squared (R¬≤): {r2_mlp:.4f}&quot;)
print(f&quot;Number of iterations: {model_mlp.n_iter_}&quot;)
</code></pre>

<h3 id="36-example-13-model-performance-comparison">3.6 Example 13: Model Performance Comparison</h3>
<pre class="codehilite"><code class="language-python"># Model performance comparison table
comparison = pd.DataFrame({
    'Model': ['Linear Regression', 'Random Forest', 'LightGBM', 'SVR', 'MLP'],
    'MAE (%)': [mae_lr, mae_rf, mae_lgb, mae_svr, mae_mlp],
    'RMSE (%)': [rmse_lr, rmse_rf, rmse_lgb, rmse_svr, rmse_mlp],
    'R¬≤': [r2_lr, r2_rf, r2_lgb, r2_svr, r2_mlp],
    'Training Time (s)': [training_time_lr, training_time_rf, training_time_lgb,
                  training_time_svr, training_time_mlp]
})

print(&quot;\n===== Comprehensive Model Performance Comparison =====&quot;)
print(comparison.to_string(index=False))

# Visualization
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# MAE comparison
axes[0].bar(comparison['Model'], comparison['MAE (%)'],
            color=['blue', 'green', 'orange', 'purple', 'red'])
axes[0].set_ylabel('MAE (%)', fontsize=12)
axes[0].set_title('Mean Absolute Error (Lower is Better)', fontsize=14)
axes[0].tick_params(axis='x', rotation=45)
axes[0].grid(True, alpha=0.3, axis='y')

# R¬≤ comparison
axes[1].bar(comparison['Model'], comparison['R¬≤'],
            color=['blue', 'green', 'orange', 'purple', 'red'])
axes[1].set_ylabel('R¬≤', fontsize=12)
axes[1].set_title('R-squared (Closer to 1 is Better)', fontsize=14)
axes[1].tick_params(axis='x', rotation=45)
axes[1].grid(True, alpha=0.3, axis='y')

# Training time comparison
axes[2].bar(comparison['Model'], comparison['Training Time (s)'],
            color=['blue', 'green', 'orange', 'purple', 'red'])
axes[2].set_ylabel('Training Time (s)', fontsize=12)
axes[2].set_title('Training Time (Shorter is Better)', fontsize=14)
axes[2].tick_params(axis='x', rotation=45)
axes[2].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
</code></pre>

<h3 id="37-example-14-predicted-vs-actual-plot">3.7 Example 14: Predicted vs Actual Plot</h3>
<pre class="codehilite"><code class="language-python"># Visualize best model (LightGBM) prediction results
plt.figure(figsize=(10, 8))
plt.scatter(y_test, y_pred_lgb, alpha=0.6, s=100, c='green', edgecolors='k', linewidth=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
         'r--', lw=2, label='Perfect Prediction')
plt.xlabel('Actual Yield (%)', fontsize=14)
plt.ylabel('Predicted Yield (%)', fontsize=14)
plt.title('LightGBM: Yield Prediction Accuracy', fontsize=16)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)

# Add performance metrics as text
textstr = f'R¬≤ = {r2_lgb:.3f}\nMAE = {mae_lgb:.2f} %\nRMSE = {rmse_lgb:.2f} %'
plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes,
         fontsize=12, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.show()
</code></pre>

<hr />
<h2 id="4-process-optimization">4. Process Optimization</h2>
<h3 id="41-example-15-grid-search-for-optimal-conditions">4.1 Example 15: Grid Search for Optimal Conditions</h3>
<pre class="codehilite"><code class="language-python">from scipy.optimize import minimize

# Objective function: Maximize yield (minimize negative yield)
def objective_yield(params):
    &quot;&quot;&quot;
    params = [temperature, pressure, catalyst]
    &quot;&quot;&quot;
    # Unpack parameters
    temp = params[0]
    press = params[1]
    cat = params[2]

    # Build features (same order as training)
    features = np.array([[
        temp, press, cat,
        temp * press,  # temp_pressure
        temp * cat,    # temp_catalyst
        temp**2,       # temp_squared
        cat**2,        # catalyst_squared
        cat / (press + 1e-10)  # catalyst_pressure_ratio
    ]])

    # Predict with model (using LightGBM)
    predicted_yield = model_lgb.predict(features)[0]

    # Return negative value for maximization
    return -predicted_yield

# Constraints (process operating range)
bounds = [
    (300, 500),  # Temperature [K]
    (1, 10),     # Pressure [bar]
    (0.1, 5.0)   # Catalyst amount [wt%]
]

# Initial guess
x0 = [400, 5, 2.5]

# Run optimization
result = minimize(
    objective_yield,
    x0,
    method='L-BFGS-B',
    bounds=bounds
)

print(&quot;===== Grid Search Optimization Results =====&quot;)
print(f&quot;Optimal conditions:&quot;)
print(f&quot;  Temperature: {result.x[0]:.1f} K&quot;)
print(f&quot;  Pressure: {result.x[1]:.2f} bar&quot;)
print(f&quot;  Catalyst amount: {result.x[2]:.2f} wt%&quot;)
print(f&quot;\nMaximum predicted yield: {-result.fun:.2f} %&quot;)
print(f&quot;Optimization success: {result.success}&quot;)
print(f&quot;Number of iterations: {result.nit}&quot;)
</code></pre>

<h3 id="42-example-16-bayesian-optimization-efficient-search">4.2 Example 16: Bayesian Optimization (Efficient Search)</h3>
<pre class="codehilite"><code class="language-python">from skopt import gp_minimize
from skopt.space import Real
from skopt.utils import use_named_args

# Define search space
space = [
    Real(300, 500, name='temperature'),
    Real(1, 10, name='pressure'),
    Real(0.1, 5.0, name='catalyst')
]

# Objective function (for Bayesian optimization)
@use_named_args(space)
def objective_bayes(**params):
    temp = params['temperature']
    press = params['pressure']
    cat = params['catalyst']

    # Build features
    features = np.array([[
        temp, press, cat,
        temp * press,
        temp * cat,
        temp**2,
        cat**2,
        cat / (press + 1e-10)
    ]])

    # Predicted yield (negative value for maximization)
    predicted_yield = model_lgb.predict(features)[0]
    return -predicted_yield

# Run Bayesian optimization
result_bayes = gp_minimize(
    objective_bayes,
    space,
    n_calls=30,  # 30 evaluations
    random_state=42,
    verbose=False
)

print(&quot;\n===== Bayesian Optimization Results =====&quot;)
print(f&quot;Optimal conditions:&quot;)
print(f&quot;  Temperature: {result_bayes.x[0]:.1f} K&quot;)
print(f&quot;  Pressure: {result_bayes.x[1]:.2f} bar&quot;)
print(f&quot;  Catalyst amount: {result_bayes.x[2]:.2f} wt%&quot;)
print(f&quot;\nMaximum predicted yield: {-result_bayes.fun:.2f} %&quot;)

# Optimization convergence history
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(result_bayes.func_vals) + 1),
         -result_bayes.func_vals, 'b-o', linewidth=2, markersize=6)
plt.axhline(y=-result_bayes.fun, color='r', linestyle='--',
            label=f'Best value: {-result_bayes.fun:.2f}%')
plt.xlabel('Number of Evaluations', fontsize=12)
plt.ylabel('Predicted Yield (%)', fontsize=12)
plt.title('Bayesian Optimization Convergence History', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<h3 id="43-example-17-design-of-experiments-doe">4.3 Example 17: Design of Experiments (DoE)</h3>
<pre class="codehilite"><code class="language-python">from itertools import product

# 2-level experimental design (2^3 = 8 experiments)
levels = {
    'temperature': [350, 450],  # Low level, high level
    'pressure': [3, 8],
    'catalyst': [1.0, 4.0]
}

# Generate all combinations
experiments = list(product(levels['temperature'], levels['pressure'], levels['catalyst']))

# Calculate predicted yield for each experiment
results_doe = []
for temp, press, cat in experiments:
    features = np.array([[
        temp, press, cat,
        temp * press,
        temp * cat,
        temp**2,
        cat**2,
        cat / (press + 1e-10)
    ]])
    predicted_yield = model_lgb.predict(features)[0]
    results_doe.append({
        'Temperature [K]': temp,
        'Pressure [bar]': press,
        'Catalyst [wt%]': cat,
        'Predicted Yield [%]': predicted_yield
    })

# Convert results to DataFrame
df_doe = pd.DataFrame(results_doe).sort_values('Predicted Yield [%]', ascending=False)

print(&quot;\n===== Design of Experiments (2^3 DoE) Results =====&quot;)
print(df_doe.to_string(index=False))

# Extract best condition
best_condition = df_doe.iloc[0]
print(f&quot;\nBest condition:&quot;)
print(f&quot;  Temperature: {best_condition['Temperature [K]']:.0f} K&quot;)
print(f&quot;  Pressure: {best_condition['Pressure [bar]']:.0f} bar&quot;)
print(f&quot;  Catalyst amount: {best_condition['Catalyst [wt%]']:.1f} wt%&quot;)
print(f&quot;  Predicted yield: {best_condition['Predicted Yield [%]']:.2f} %&quot;)
</code></pre>

<h3 id="44-example-18-response-surface-method">4.4 Example 18: Response Surface Method</h3>
<pre class="codehilite"><code class="language-python">from scipy.interpolate import griddata

# Create grid for temperature and pressure range (catalyst fixed)
temp_range = np.linspace(300, 500, 50)
press_range = np.linspace(1, 10, 50)
temp_grid, press_grid = np.meshgrid(temp_range, press_range)

# Predict yield at each grid point (catalyst amount fixed to optimal value)
catalyst_fixed = result_bayes.x[2]
yield_grid = np.zeros_like(temp_grid)

for i in range(len(temp_range)):
    for j in range(len(press_range)):
        temp = temp_grid[j, i]
        press = press_grid[j, i]
        cat = catalyst_fixed

        features = np.array([[
            temp, press, cat,
            temp * press,
            temp * cat,
            temp**2,
            cat**2,
            cat / (press + 1e-10)
        ]])

        yield_grid[j, i] = model_lgb.predict(features)[0]

# Visualize response surface
fig = plt.figure(figsize=(14, 6))

# Contour plot
ax1 = fig.add_subplot(1, 2, 1)
contour = ax1.contourf(temp_grid, press_grid, yield_grid, levels=20, cmap='viridis')
ax1.set_xlabel('Temperature [K]', fontsize=12)
ax1.set_ylabel('Pressure [bar]', fontsize=12)
ax1.set_title(f'Response Surface (Catalyst = {catalyst_fixed:.2f} wt%)', fontsize=14)
plt.colorbar(contour, ax=ax1, label='Predicted Yield [%]')

# 3D surface
ax2 = fig.add_subplot(1, 2, 2, projection='3d')
surf = ax2.plot_surface(temp_grid, press_grid, yield_grid,
                        cmap='viridis', alpha=0.8)
ax2.set_xlabel('Temperature [K]', fontsize=10)
ax2.set_ylabel('Pressure [bar]', fontsize=10)
ax2.set_zlabel('Predicted Yield [%]', fontsize=10)
ax2.set_title('3D Response Surface', fontsize=14)
plt.colorbar(surf, ax=ax2, label='Predicted Yield [%]', shrink=0.5)

plt.tight_layout()
plt.show()
</code></pre>

<h3 id="45-example-19-constrained-optimization">4.5 Example 19: Constrained Optimization</h3>
<pre class="codehilite"><code class="language-python">from scipy.optimize import NonlinearConstraint

# Objective function (maximize yield)
def objective_constrained(params):
    temp, press, cat = params
    features = np.array([[
        temp, press, cat,
        temp * press,
        temp * cat,
        temp**2,
        cat**2,
        cat / (press + 1e-10)
    ]])
    predicted_yield = model_lgb.predict(features)[0]
    return -predicted_yield

# Constraint function: Energy cost &lt; 100 [arbitrary units]
# Cost = 0.1 * temperature + 2.0 * pressure
def energy_cost_constraint(params):
    temp, press, cat = params
    cost = 0.1 * temp + 2.0 * press
    return cost

# Constraint: Energy cost &lt;= 100
constraint = NonlinearConstraint(energy_cost_constraint, -np.inf, 100)

# Run optimization
result_constrained = minimize(
    objective_constrained,
    x0=[400, 5, 2.5],
    method='SLSQP',
    bounds=bounds,
    constraints=constraint
)

print(&quot;\n===== Constrained Optimization Results =====&quot;)
print(f&quot;Optimal conditions (under energy cost constraint):&quot;)
print(f&quot;  Temperature: {result_constrained.x[0]:.1f} K&quot;)
print(f&quot;  Pressure: {result_constrained.x[1]:.2f} bar&quot;)
print(f&quot;  Catalyst amount: {result_constrained.x[2]:.2f} wt%&quot;)
print(f&quot;\nMaximum predicted yield: {-result_constrained.fun:.2f} %&quot;)
print(f&quot;Energy cost: {energy_cost_constraint(result_constrained.x):.2f}&quot;)
print(f&quot;Constraint satisfied: {energy_cost_constraint(result_constrained.x) &lt;= 100}&quot;)
</code></pre>

<h3 id="46-example-20-multi-objective-optimization-yield-vs-cost">4.6 Example 20: Multi-objective Optimization (Yield vs Cost)</h3>
<pre class="codehilite"><code class="language-python">from pymoo.algorithms.moo.nsga2 import NSGA2
from pymoo.core.problem import Problem
from pymoo.optimize import minimize as pymoo_minimize

# Define multi-objective optimization problem
class ProcessOptimizationProblem(Problem):
    def __init__(self):
        super().__init__(
            n_var=3,  # Number of variables (temperature, pressure, catalyst)
            n_obj=2,  # Number of objectives (yield, cost)
            xl=np.array([300, 1, 0.1]),  # Lower bounds
            xu=np.array([500, 10, 5.0])  # Upper bounds
        )

    def _evaluate(self, X, out, *args, **kwargs):
        # X: (n_samples, 3) array
        n_samples = X.shape[0]
        f1 = np.zeros(n_samples)  # Objective 1: -yield (minimize)
        f2 = np.zeros(n_samples)  # Objective 2: cost (minimize)

        for i in range(n_samples):
            temp, press, cat = X[i]

            # Predict yield
            features = np.array([[
                temp, press, cat,
                temp * press,
                temp * cat,
                temp**2,
                cat**2,
                cat / (press + 1e-10)
            ]])
            predicted_yield = model_lgb.predict(features)[0]

            # Objective 1: Maximize yield ‚Üí Minimize -yield
            f1[i] = -predicted_yield

            # Objective 2: Minimize cost
            # Cost = Energy cost + Catalyst cost
            energy_cost = 0.1 * temp + 2.0 * press
            catalyst_cost = 5.0 * cat
            f2[i] = energy_cost + catalyst_cost

        out[&quot;F&quot;] = np.column_stack([f1, f2])

# Optimize with NSGA-II algorithm
problem = ProcessOptimizationProblem()
algorithm = NSGA2(pop_size=50)

result_nsga2 = pymoo_minimize(
    problem,
    algorithm,
    ('n_gen', 100),  # 100 generations
    verbose=False
)

# Get Pareto optimal solutions
pareto_front = result_nsga2.F
pareto_solutions = result_nsga2.X

print(&quot;\n===== Multi-objective Optimization (NSGA-II) Results =====&quot;)
print(f&quot;Number of Pareto optimal solutions: {len(pareto_solutions)}&quot;)
print(f&quot;\nPareto optimal solution examples (first 3):&quot;)
for i in range(min(3, len(pareto_solutions))):
    temp, press, cat = pareto_solutions[i]
    yield_val = -pareto_front[i, 0]
    cost_val = pareto_front[i, 1]
    print(f&quot;\nSolution {i+1}:&quot;)
    print(f&quot;  Temperature: {temp:.1f} K, Pressure: {press:.2f} bar, Catalyst: {cat:.2f} wt%&quot;)
    print(f&quot;  Yield: {yield_val:.2f} %, Cost: {cost_val:.2f}&quot;)

# Visualize Pareto front
plt.figure(figsize=(10, 6))
plt.scatter(-pareto_front[:, 0], pareto_front[:, 1],
            c='blue', s=50, alpha=0.6, edgecolors='k', linewidth=0.5)
plt.xlabel('Yield [%]', fontsize=12)
plt.ylabel('Cost [arbitrary units]', fontsize=12)
plt.title('Pareto Front (Yield vs Cost)', fontsize=14)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<h3 id="47-example-21-optimization-results-comparison">4.7 Example 21: Optimization Results Comparison</h3>
<pre class="codehilite"><code class="language-python"># Compare results of all optimization methods
optimization_results = pd.DataFrame({
    'Method': [
        'Grid Search',
        'Bayesian Optimization',
        'DoE (2^3)',
        'Constrained Optimization'
    ],
    'Temperature [K]': [
        result.x[0],
        result_bayes.x[0],
        best_condition['Temperature [K]'],
        result_constrained.x[0]
    ],
    'Pressure [bar]': [
        result.x[1],
        result_bayes.x[1],
        best_condition['Pressure [bar]'],
        result_constrained.x[1]
    ],
    'Catalyst [wt%]': [
        result.x[2],
        result_bayes.x[2],
        best_condition['Catalyst [wt%]'],
        result_constrained.x[2]
    ],
    'Predicted Yield [%]': [
        -result.fun,
        -result_bayes.fun,
        best_condition['Predicted Yield [%]'],
        -result_constrained.fun
    ]
})

print(&quot;\n===== Optimization Method Comparison =====&quot;)
print(optimization_results.to_string(index=False))
</code></pre>

<h3 id="48-example-22-optimization-method-flowchart">4.8 Example 22: Optimization Method Flowchart</h3>
<pre class="codehilite"><code class="language-python"># Mermaid flowchart (displayed in Markdown)
print(&quot;&quot;&quot;
```mermaid
graph TD
    A[Process Optimization Task] --&gt; B{Number of Objectives?}
    B --&gt;|Single Objective| C{Constraints?}
    B --&gt;|Multi-objective| D[NSGA-II/Genetic Algorithm]

    C --&gt;|None| E{Evaluation Cost?}
    C --&gt;|Present| F[Constrained Optimization&lt;br/&gt;SLSQP/COBYLA]

    E --&gt;|Low| G[Grid Search&lt;br/&gt;or DoE]
    E --&gt;|High| H[Bayesian Optimization&lt;br/&gt;Gaussian Process]

    D --&gt; I[Get Pareto Front]
    F --&gt; J[Get Optimal Condition]
    G --&gt; J
    H --&gt; J
    I --&gt; K[Trade-off Analysis]
    J --&gt; L[Experimental Verification]
    K --&gt; L

    style A fill:#e3f2fd
    style D fill:#c8e6c9
    style F fill:#fff9c4
    style H fill:#ffccbc
    style I fill:#f3e5f5
</code></pre>

<p>""")</p>
<pre class="codehilite"><code>---

## 5. Advanced Techniques

### 5.9 Example 23: Hyperparameter Tuning (Grid Search)

```python
from sklearn.model_selection import GridSearchCV

# Random Forest hyperparameter candidates
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 15, 20],
    'min_samples_split': [2, 5, 10]
}

# Grid Search settings
grid_search = GridSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_grid=param_grid,
    cv=5,  # 5-fold cross-validation
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    verbose=1
)

# Run Grid Search
print(&quot;===== Grid Search Started =====&quot;)
grid_search.fit(X_train, y_train)

print(f&quot;\n===== Best Hyperparameters =====&quot;)
for param, value in grid_search.best_params_.items():
    print(f&quot;{param}: {value}&quot;)

print(f&quot;\nCross-validation MAE: {-grid_search.best_score_:.2f} %&quot;)

# Evaluate best model on test data
best_model_gs = grid_search.best_estimator_
y_pred_gs = best_model_gs.predict(X_test)
mae_gs = mean_absolute_error(y_test, y_pred_gs)
r2_gs = r2_score(y_test, y_pred_gs)

print(f&quot;\nTest data performance:&quot;)
print(f&quot;  MAE: {mae_gs:.2f} %&quot;)
print(f&quot;  R¬≤: {r2_gs:.4f}&quot;)
</code></pre>

<h3 id="510-example-24-time-series-process-anomaly-detection">5.10 Example 24: Time Series Process Anomaly Detection</h3>
<pre class="codehilite"><code class="language-python">from sklearn.ensemble import IsolationForest

# Time series process data (using data generated in Example 7)
process_time_series = pd.DataFrame({
    'time_h': time_hours,
    'temperature_K': temp_time,
    'pressure_bar': pressure_time,
    'yield_%': yield_time
})

# Anomaly detection with Isolation Forest
iso_forest = IsolationForest(
    contamination=0.1,  # Assume 10% anomaly rate
    random_state=42
)

# Features (temperature, pressure, yield)
X_anomaly = process_time_series[['temperature_K', 'pressure_bar', 'yield_%']]

# Calculate anomaly scores
anomaly_scores = iso_forest.fit_predict(X_anomaly)
process_time_series['anomaly'] = anomaly_scores

# Extract anomalous data
anomalies = process_time_series[process_time_series['anomaly'] == -1]

print(f&quot;\n===== Anomaly Detection Results =====&quot;)
print(f&quot;Anomalous data count: {len(anomalies)} / {len(process_time_series)}&quot;)
print(f&quot;Anomaly rate: {len(anomalies) / len(process_time_series) * 100:.1f}%&quot;)

# Visualization
plt.figure(figsize=(14, 5))
plt.plot(process_time_series['time_h'], process_time_series['yield_%'],
         'b-', linewidth=1.5, label='Normal Data')
plt.scatter(anomalies['time_h'], anomalies['yield_%'],
            c='red', s=100, marker='x', linewidth=2, label='Anomalous Data')
plt.xlabel('Time [h]', fontsize=12)
plt.ylabel('Yield [%]', fontsize=12)
plt.title('Time Series Process Anomaly Detection', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<h3 id="511-example-25-shap-value-interpretability-analysis">5.11 Example 25: SHAP Value Interpretability Analysis</h3>
<pre class="codehilite"><code class="language-python"># Analyze feature influence with SHAP (SHapley Additive exPlanations)
try:
    import shap

    # Create SHAP Explainer (for LightGBM)
    explainer = shap.TreeExplainer(model_lgb)
    shap_values = explainer.shap_values(X_test)

    # SHAP value summary plot
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X_test, feature_names=X.columns, show=False)
    plt.title('SHAP Value Summary Plot (Feature Influence)', fontsize=14)
    plt.tight_layout()
    plt.show()

    print(&quot;\n‚úÖ SHAP value analysis completed&quot;)
    print(&quot;Visualized how each feature influences predictions.&quot;)

except ImportError:
    print(&quot;\n‚ö†Ô∏è SHAP library not installed&quot;)
    print(&quot;Install with: pip install shap&quot;)
</code></pre>

<h3 id="512-example-26-process-simulation-pid-control">5.12 Example 26: Process Simulation (PID Control)</h3>
<pre class="codehilite"><code class="language-python"># Simple PID controller simulation
class PIDController:
    def __init__(self, Kp, Ki, Kd, setpoint):
        self.Kp = Kp  # Proportional gain
        self.Ki = Ki  # Integral gain
        self.Kd = Kd  # Derivative gain
        self.setpoint = setpoint  # Target value
        self.integral = 0
        self.prev_error = 0

    def update(self, measured_value, dt):
        # Calculate error
        error = self.setpoint - measured_value

        # Integral term
        self.integral += error * dt

        # Derivative term
        derivative = (error - self.prev_error) / dt

        # PID output
        output = (
            self.Kp * error +
            self.Ki * self.integral +
            self.Kd * derivative
        )

        # Save error for next step
        self.prev_error = error

        return output

# Process model (first-order lag system)
def process_model(input_val, current_temp, tau=5.0, K=1.0, dt=0.1):
    &quot;&quot;&quot;
    First-order lag system process model
    tau: time constant, K: gain
    &quot;&quot;&quot;
    dT = (K * input_val - current_temp) / tau
    new_temp = current_temp + dT * dt
    return new_temp

# Simulation setup
dt = 0.1  # Time step [seconds]
t_end = 50  # Simulation time [seconds]
time_sim = np.arange(0, t_end, dt)

# Initialize PID controller (target temperature: 400 K)
pid = PIDController(Kp=2.0, Ki=0.5, Kd=1.0, setpoint=400)

# Run simulation
temperature = 350  # Initial temperature [K]
temperatures = []
inputs = []

for t in time_sim:
    # Calculate PID control input
    control_input = pid.update(temperature, dt)

    # Update temperature with process model
    temperature = process_model(control_input, temperature, dt=dt)

    # Record
    temperatures.append(temperature)
    inputs.append(control_input)

# Visualization
fig, axes = plt.subplots(2, 1, figsize=(12, 8))

# Temperature trajectory
axes[0].plot(time_sim, temperatures, 'b-', linewidth=2, label='Process Temperature')
axes[0].axhline(y=400, color='r', linestyle='--', linewidth=1.5, label='Target Temperature')
axes[0].set_ylabel('Temperature [K]', fontsize=12)
axes[0].set_title('PID Control Simulation', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Control input trajectory
axes[1].plot(time_sim, inputs, 'g-', linewidth=2, label='Control Input')
axes[1].set_xlabel('Time [seconds]', fontsize=12)
axes[1].set_ylabel('Control Input', fontsize=12)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f&quot;\n===== PID Control Simulation Results =====&quot;)
print(f&quot;Final temperature: {temperatures[-1]:.2f} K (Target: 400 K)&quot;)
print(f&quot;Steady-state error: {abs(400 - temperatures[-1]):.2f} K&quot;)
</code></pre>

<hr />
<h2 id="6-troubleshooting-guide">6. Troubleshooting Guide</h2>
<h3 id="61-common-error-list">6.1 Common Error List</h3>
<table>
<thead>
<tr>
<th>Error Message</th>
<th>Cause</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ModuleNotFoundError: No module named 'skopt'</code></td>
<td>scikit-optimize not installed</td>
<td><code>pip install scikit-optimize</code></td>
</tr>
<tr>
<td><code>ValueError: Input contains NaN</code></td>
<td>Data contains missing values</td>
<td>Remove with <code>df.dropna()</code> or impute with <code>df.fillna()</code></td>
</tr>
<tr>
<td><code>ConvergenceWarning</code></td>
<td>Optimization not converged</td>
<td>Increase <code>max_iter</code>, adjust learning rate</td>
</tr>
<tr>
<td><code>MemoryError</code></td>
<td>Out of memory</td>
<td>Reduce data size, use batch processing</td>
</tr>
<tr>
<td><code>LinAlgError: Singular matrix</code></td>
<td>Matrix is singular</td>
<td>Check multicollinearity, add regularization</td>
</tr>
</tbody>
</table>
<h3 id="62-debug-checklist">6.2 Debug Checklist</h3>
<p><strong>Step 1: Check Data</strong></p>
<pre class="codehilite"><code class="language-python"># Basic statistics
print(process_data.describe())

# Check missing values
print(process_data.isnull().sum())

# Check data types
print(process_data.dtypes)
</code></pre>

<p><strong>Step 2: Simplify Model</strong></p>
<pre class="codehilite"><code class="language-python"># If complex model fails, try linear regression first
model_simple = LinearRegression()
model_simple.fit(X_train, y_train)
print(f&quot;Linear regression R¬≤: {model_simple.score(X_test, y_test):.4f}&quot;)
</code></pre>

<p><strong>Step 3: Check Scaling</strong></p>
<pre class="codehilite"><code class="language-python"># Standardization required for SVR and MLP
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
</code></pre>

<hr />
<h2 id="7-project-challenge-chemical-reactor-optimization">7. Project Challenge: Chemical Reactor Optimization</h2>
<p>Apply what you've learned to a practical project.</p>
<h3 id="71-project-overview">7.1 Project Overview</h3>
<p><strong>Goal:</strong><br />
Optimize chemical reactor operating conditions to maximize yield</p>
<p><strong>Target Performance:</strong><br />
- Prediction model: R¬≤ &gt; 0.85<br />
- Optimization: Yield &gt; 90%</p>
<h3 id="72-step-by-step-guide">7.2 Step-by-Step Guide</h3>
<p><strong>Step 1: Data Generation (More Realistic Data)</strong></p>
<pre class="codehilite"><code class="language-python"># Generate more complex reactor data
np.random.seed(42)
n_reactor = 300

temp_reactor = np.random.uniform(320, 480, n_reactor)
press_reactor = np.random.uniform(2, 12, n_reactor)
cat_reactor = np.random.uniform(0.5, 6.0, n_reactor)
residence_time = np.random.uniform(5, 30, n_reactor)  # Residence time [min]

# More complex yield model (4 variables, interactions, optimal values)
yield_reactor = (
    25
    + 0.18 * temp_reactor
    - 0.00025 * temp_reactor**2
    + 6.0 * press_reactor
    - 0.3 * press_reactor**2
    + 4.0 * cat_reactor
    - 0.4 * cat_reactor**2
    + 1.5 * residence_time
    - 0.03 * residence_time**2
    + 0.015 * temp_reactor * press_reactor
    + 0.008 * cat_reactor * residence_time
    + np.random.normal(0, 2.5, n_reactor)
)

reactor_data = pd.DataFrame({
    'temperature': temp_reactor,
    'pressure': press_reactor,
    'catalyst': cat_reactor,
    'residence_time': residence_time,
    'yield': yield_reactor
})

print(&quot;===== Reactor Data =====&quot;)
print(reactor_data.describe())
</code></pre>

<p><strong>Step 2: Feature Engineering</strong></p>
<pre class="codehilite"><code class="language-python"># Add features
reactor_data['temp_press'] = reactor_data['temperature'] * reactor_data['pressure']
reactor_data['cat_time'] = reactor_data['catalyst'] * reactor_data['residence_time']
reactor_data['temp_sq'] = reactor_data['temperature'] ** 2
reactor_data['press_sq'] = reactor_data['pressure'] ** 2

X_reactor = reactor_data.drop('yield', axis=1)
y_reactor = reactor_data['yield']

X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
    X_reactor, y_reactor, test_size=0.2, random_state=42
)
</code></pre>

<p><strong>Step 3: Model Training (LightGBM)</strong></p>
<pre class="codehilite"><code class="language-python">model_reactor = lgb.LGBMRegressor(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=15,
    random_state=42,
    verbose=-1
)
model_reactor.fit(X_train_r, y_train_r)

y_pred_r = model_reactor.predict(X_test_r)
r2_reactor = r2_score(y_test_r, y_pred_r)
mae_reactor = mean_absolute_error(y_test_r, y_pred_r)

print(f&quot;\n===== Reactor Model Performance =====&quot;)
print(f&quot;R¬≤: {r2_reactor:.3f}&quot;)
print(f&quot;MAE: {mae_reactor:.2f}%&quot;)

if r2_reactor &gt; 0.85:
    print(&quot;üéâ Goal achieved! (R¬≤ &gt; 0.85)&quot;)
</code></pre>

<p><strong>Step 4: Bayesian Optimization for Condition Search</strong></p>
<pre class="codehilite"><code class="language-python"># Search for optimal conditions
space_reactor = [
    Real(320, 480, name='temperature'),
    Real(2, 12, name='pressure'),
    Real(0.5, 6.0, name='catalyst'),
    Real(5, 30, name='residence_time')
]

@use_named_args(space_reactor)
def objective_reactor(**params):
    temp = params['temperature']
    press = params['pressure']
    cat = params['catalyst']
    res_time = params['residence_time']

    features = np.array([[
        temp, press, cat, res_time,
        temp * press,
        cat * res_time,
        temp**2,
        press**2
    ]])

    predicted_yield = model_reactor.predict(features)[0]
    return -predicted_yield

result_reactor = gp_minimize(
    objective_reactor,
    space_reactor,
    n_calls=50,
    random_state=42,
    verbose=False
)

print(f&quot;\n===== Optimal Conditions =====&quot;)
print(f&quot;Temperature: {result_reactor.x[0]:.1f} K&quot;)
print(f&quot;Pressure: {result_reactor.x[1]:.2f} bar&quot;)
print(f&quot;Catalyst amount: {result_reactor.x[2]:.2f} wt%&quot;)
print(f&quot;Residence time: {result_reactor.x[3]:.1f} min&quot;)
print(f&quot;\nMaximum predicted yield: {-result_reactor.fun:.2f}%&quot;)

if -result_reactor.fun &gt; 90:
    print(&quot;üéâ Goal achieved! (Yield &gt; 90%)&quot;)
</code></pre>

<hr />
<h2 id="8-summary">8. Summary</h2>
<h3 id="what-you-learned-in-this-chapter">What You Learned in This Chapter</h3>
<ol>
<li>
<p><strong>Environment Setup</strong><br />
   - Three options: Anaconda, venv, Google Colab<br />
   - Installation of PI-specific libraries (scikit-optimize, pymoo)</p>
</li>
<li>
<p><strong>Process Data Processing</strong><br />
   - Data generation and visualization (pairplot, heatmap)<br />
   - Preprocessing (missing value imputation, outlier removal, standardization)<br />
   - Feature engineering (interaction terms, quadratic terms)</p>
</li>
<li>
<p><strong>5 Regression Models</strong><br />
   - Linear regression, Random Forest, LightGBM, SVR, MLP<br />
   - Model performance comparison (MAE, RMSE, R¬≤)</p>
</li>
<li>
<p><strong>Process Optimization Methods</strong><br />
   - Grid search, Bayesian optimization, DoE, response surface method<br />
   - Constrained optimization, multi-objective optimization (NSGA-II)</p>
</li>
<li>
<p><strong>Advanced Techniques</strong><br />
   - Hyperparameter tuning<br />
   - Anomaly detection (Isolation Forest)<br />
   - Interpretability analysis (SHAP values)<br />
   - PID control simulation</p>
</li>
</ol>
<h3 id="next-steps">Next Steps</h3>
<p><strong>After completing this tutorial, you can:</strong><br />
- ‚úÖ Preprocess and visualize process data<br />
- ‚úÖ Use 5+ regression models appropriately<br />
- ‚úÖ Optimize process conditions with Bayesian optimization<br />
- ‚úÖ Analyze trade-offs with multi-objective optimization</p>
<p><strong>What to learn next:</strong><br />
1. <strong>Application to Real Processes</strong><br />
   - Data acquisition from DCS (Distributed Control System)<br />
   - Real-time optimization</p>
<ol start="2">
<li>
<p><strong>Deep Learning Applications</strong><br />
   - LSTM (time series prediction)<br />
   - Autoencoder (anomaly detection)</p>
</li>
<li>
<p><strong>Autonomous Process Control</strong><br />
   - Reinforcement learning control<br />
   - Model Predictive Control (MPC)</p>
</li>
</ol>
<hr />
<h2 id="exercise-problems">Exercise Problems</h2>
<h3 id="problem-1-difficulty-easy">Problem 1 (Difficulty: Easy)</h3>
<p>List three reasons why Bayesian optimization is superior to grid search for process optimization.</p>
<details>
<summary>Answer</summary>

**Advantages of Bayesian Optimization:**

1. **Fewer Evaluations Required**
   - Grid search: Try all combinations (e.g., 10√ó10√ó10 = 1000 times)
   - Bayesian optimization: Reach optimal solution in ~30-50 evaluations

2. **Intelligent Search**
   - Utilizes past evaluation results to prioritize promising regions
   - Grid search blindly explores all combinations

3. **Reduced Experimental Cost**
   - Chemical process experiments take hours to days per run
   - Fewer evaluations = significant reduction in total experiment time

**Example:**
- Grid search: 1000 experiments √ó 3 hours = 3000 hours (125 days)
- Bayesian optimization: 50 experiments √ó 3 hours = 150 hours (6.25 days)

**Approximately 20√ó time reduction!**

</details>

<h3 id="problem-2-difficulty-medium">Problem 2 (Difficulty: Medium)</h3>
<p>Explain what a Pareto front obtained from multi-objective optimization (NSGA-II) is and provide one application example in chemical processes.</p>
<details>
<summary>Answer</summary>

**What is Pareto Front:**

In simultaneous optimization of multiple objectives, there exists a trade-off relationship where improving one objective worsens another. The Pareto front is "the set of solutions where no objective can be improved without worsening others."

**Characteristics:**
- All solutions on Pareto front are "optimal solutions"
- Which solution to choose depends on decision maker's priorities

**Chemical Process Application Example: Distillation Column Optimization**

**Objective 1**: Minimize energy cost
**Objective 2**: Maximize product purity

**Pareto Front Example:**

| Solution | Energy Cost | Product Purity |
|----------|------------|----------------|
| A  | Low (100 $/kg)   | Low (95%) |
| B  | Medium (150 $/kg)   | Medium (98%) |
| C  | High (200 $/kg)   | High (99.5%) |

**Selection Criteria:**
- Cost-focused ‚Üí Solution A (minimum energy cost)
- Quality-focused ‚Üí Solution C (maximum purity)
- Balanced ‚Üí Solution B (middle ground)

NSGA-II automatically discovers such Pareto fronts.

</details>

<h3 id="problem-3-difficulty-hard">Problem 3 (Difficulty: Hard)</h3>
<p>Explain the impact of PID control parameters (Kp, Ki, Kd) on temperature control behavior, describing advantages and disadvantages when increasing each parameter.</p>
<details>
<summary>Answer</summary>

**PID Parameter Impacts:**

**1. Increasing Kp (Proportional Gain)**

**Advantages:**
- Faster response (reach target faster)
- Reduced steady-state error

**Disadvantages:**
- Larger overshoot (oscillation beyond target)
- Reduced stability (oscillatory behavior)

**2. Increasing Ki (Integral Gain)**

**Advantages:**
- Complete elimination of steady-state error
- Improved long-term accuracy

**Disadvantages:**
- Slower response (takes time to accumulate integral term)
- Wind-up phenomenon (integral term becomes excessively large)
- Increased overshoot

**3. Increasing Kd (Derivative Gain)**

**Advantages:**
- Overshoot suppression (anticipatory control)
- Improved stability
- Oscillation damping

**Disadvantages:**
- Sensitive to noise (amplifies small measurement fluctuations)
- Potential high-frequency oscillations

**Optimal Tuning Methods:**

1. **Ziegler-Nichols method** (classical)
2. **Auto-tuning** (modern)
3. **Simulation-based optimization** (applying Bayesian optimization learned in this chapter)

**Implementation Example:**

<pre class="codehilite"><code class="language-python"># Auto-tune PID parameters with Bayesian optimization
space_pid = [
    Real(0.1, 10.0, name='Kp'),
    Real(0.01, 1.0, name='Ki'),
    Real(0.01, 5.0, name='Kd')
]

@use_named_args(space_pid)
def objective_pid(**params):
    # Run PID simulation
    # Goal: Minimize overshoot + settling time
    overshoot, settling_time = simulate_pid(
        Kp=params['Kp'],
        Ki=params['Ki'],
        Kd=params['Kd']
    )
    return overshoot + 0.1 * settling_time

# Run optimization
result_pid = gp_minimize(objective_pid, space_pid, n_calls=50)
</code></pre>



</details>

<hr />
<h2 id="references">References</h2>
<ol>
<li>
<p>Pedregosa, F., et al. (2011). "Scikit-learn: Machine Learning in Python." <em>Journal of Machine Learning Research</em>, 12, 2825-2830.<br />
   URL: https://scikit-learn.org</p>
</li>
<li>
<p>Brochu, E., Cora, V. M., &amp; de Freitas, N. (2010). "A Tutorial on Bayesian Optimization of Expensive Cost Functions." arXiv:1012.2599.<br />
   URL: https://arxiv.org/abs/1012.2599</p>
</li>
<li>
<p>Deb, K., et al. (2002). "A fast and elitist multiobjective genetic algorithm: NSGA-II." <em>IEEE Transactions on Evolutionary Computation</em>, 6(2), 182-197.<br />
   DOI: <a href="https://doi.org/10.1109/4235.996017">10.1109/4235.996017</a></p>
</li>
<li>
<p>Shahriari, B., et al. (2016). "Taking the Human Out of the Loop: A Review of Bayesian Optimization." <em>Proceedings of the IEEE</em>, 104(1), 148-175.<br />
   DOI: <a href="https://doi.org/10.1109/JPROC.2015.2494218">10.1109/JPROC.2015.2494218</a></p>
</li>
<li>
<p>Lundberg, S. M., &amp; Lee, S. I. (2017). "A Unified Approach to Interpreting Model Predictions." <em>Advances in Neural Information Processing Systems</em>, 30.<br />
   URL: https://github.com/slundberg/shap</p>
</li>
<li>
<p>√Östr√∂m, K. J., &amp; H√§gglund, T. (2006). <em>Advanced PID Control</em>. ISA-The Instrumentation, Systems, and Automation Society.<br />
   ISBN: 978-1556179426</p>
</li>
<li>
<p>scikit-optimize Documentation. (2024). "Bayesian Optimization."<br />
   URL: https://scikit-optimize.github.io/stable/</p>
</li>
<li>
<p>pymoo Documentation. (2024). "Multi-objective Optimization."<br />
   URL: https://pymoo.org/</p>
</li>
</ol>
<hr />
<p><strong>Created</strong>: 2025-10-16<br />
<strong>Version</strong>: 1.0<br />
<strong>Series</strong>: PI Introduction Series v1.0<br />
<strong>Author</strong>: MI Knowledge Hub Project</p>

        <div class="nav-buttons">
            <a href="index.html" class="nav-button">‚Üê Back to Series Index</a>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 MI Knowledge Hub - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
