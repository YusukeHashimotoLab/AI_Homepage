<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Implementing Materials Development with Machine Learning: Best Practices">
    <title>Chapter 3: Experiencing MI with Python - Practical Materials Property Prediction - MI Knowledge Hub</title>

    <!-- CSS Styling -->
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --bg-color: #ffffff;
            --text-color: #333333;
            --border-color: #e0e0e0;
            --code-bg: #f5f5f5;
            --link-color: #3498db;
            --link-hover: #2980b9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Helvetica Neue", Arial, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 0;
            margin: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        /* Header */
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header .container {
            padding: 0 1.5rem;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        .meta {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            font-size: 0.9rem;
            opacity: 0.95;
            margin-top: 1rem;
        }

        .meta span {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
        }

        /* Typography */
        h2 {
            font-size: 1.75rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--secondary-color);
            color: var(--primary-color);
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            color: var(--primary-color);
        }

        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.6rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2rem;
        }

        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--link-hover);
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Code blocks */
        code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 0.9rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
            overflow-x: auto;
            display: block;
        }

        thead {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        tbody {
            display: table;
            width: 100%;
            table-layout: fixed;
        }

        th, td {
            padding: 0.8rem;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f9f9f9;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: #666;
        }

        /* Images */
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
        }

        /* Mermaid diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 1rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        /* Details/Summary (for exercises) */
        details {
            margin: 1rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--primary-color);
            padding: 0.5rem;
        }

        summary:hover {
            color: var(--secondary-color);
        }

        /* Footer */
        footer {
            margin-top: 4rem;
            padding: 2rem 0;
            border-top: 2px solid var(--border-color);
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }

        /* Navigation buttons */
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            display: inline-block;
            padding: 0.8rem 1.5rem;
            background: var(--secondary-color);
            color: white;
            border-radius: 6px;
            text-decoration: none;
            transition: all 0.3s;
            font-weight: 600;
        }

        .nav-button:hover {
            background: var(--link-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            h1 {
                font-size: 1.6rem;
            }

            h2 {
                font-size: 1.4rem;
            }

            .meta {
                font-size: 0.85rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.85rem;
            }

            table {
                font-size: 0.9rem;
            }
        }
    </style>

    <!-- Mermaid for diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
</head>
<body>
    <header>
        <div class="container">
            <h1>Chapter 3: Experiencing MI with Python - Practical Materials Property Prediction</h1>
            <div class="meta">
                <span>üìñ Reading Time: 30-40 minutes</span>
                <span>üìä Level: intermediate</span>
            </div>
        </div>
    </header>

    <main class="container">
        <h1 id="chapter-3-experiencing-mi-with-python-practical-materials-property-prediction">Chapter 3: Experiencing MI with Python - Practical Materials Property Prediction</h1>
<h2 id="learning-objectives">Learning Objectives</h2>
<p>By reading this chapter, you will be able to:<br />
- Set up Python environment and install MI libraries<br />
- Implement and compare 5+ machine learning models<br />
- Execute hyperparameter tuning<br />
- Complete practical materials property prediction projects<br />
- Troubleshoot errors independently</p>
<hr />
<h2 id="1-environment-setup-3-options">1. Environment Setup: 3 Options</h2>
<p>There are three ways to set up a Python environment for materials property prediction, depending on your situation.</p>
<h3 id="11-option-1-anaconda-recommended-for-beginners">1.1 Option 1: Anaconda (Recommended for Beginners)</h3>
<p><strong>Features:</strong><br />
- Scientific computing libraries included from the start<br />
- Easy environment management (GUI available)<br />
- Works on Windows/Mac/Linux</p>
<p><strong>Installation Steps:</strong></p>
<pre class="codehilite"><code class="language-bash"># 1. Download Anaconda
# Official site: https://www.anaconda.com/download
# Select Python 3.11 or higher

# 2. After installation, launch Anaconda Prompt

# 3. Create virtual environment (MI-specific environment)
conda create -n mi-env python=3.11 numpy pandas matplotlib scikit-learn jupyter

# 4. Activate environment
conda activate mi-env

# 5. Verify installation
python --version
# Output: Python 3.11.x
</code></pre>

<p><strong>Screen Output:</strong></p>
<pre class="codehilite"><code>(base) $ conda create -n mi-env python=3.11
Collecting package metadata: done
Solving environment: done
...
Proceed ([y]/n)? y

# Upon success, you'll see:
# To activate this environment, use
#   $ conda activate mi-env
</code></pre>

<p><strong>Advantages of Anaconda:</strong><br />
- ‚úÖ NumPy, SciPy, etc. included from the start<br />
- ‚úÖ Fewer dependency issues<br />
- ‚úÖ Visual management with Anaconda Navigator<br />
- ‚ùå Large file size (3GB+)</p>
<h3 id="12-option-2-venv-python-standard">1.2 Option 2: venv (Python Standard)</h3>
<p><strong>Features:</strong><br />
- Python standard tool (no additional installation required)<br />
- Lightweight (install only what you need)<br />
- Isolate environment per project</p>
<p><strong>Installation Steps:</strong></p>
<pre class="codehilite"><code class="language-bash"># 1. Verify Python 3.11+ is installed
python3 --version
# Output: Python 3.11.x or higher required

# 2. Create virtual environment
python3 -m venv mi-env

# 3. Activate environment
# macOS/Linux:
source mi-env/bin/activate

# Windows (PowerShell):
mi-env\Scripts\Activate.ps1

# Windows (Command Prompt):
mi-env\Scripts\activate.bat

# 4. Upgrade pip
pip install --upgrade pip

# 5. Install required libraries
pip install numpy pandas matplotlib scikit-learn jupyter

# 6. Verify installation
pip list
</code></pre>

<p><strong>Advantages of venv:</strong><br />
- ‚úÖ Lightweight (tens of MB)<br />
- ‚úÖ Python standard tool (no additional installation)<br />
- ‚úÖ Independent per project<br />
- ‚ùå Must manually resolve dependencies</p>
<h3 id="13-option-3-google-colab-no-installation-required">1.3 Option 3: Google Colab (No Installation Required)</h3>
<p><strong>Features:</strong><br />
- Run in browser only<br />
- No installation required (cloud execution)<br />
- Free GPU/TPU access</p>
<p><strong>How to Use:</strong></p>
<pre class="codehilite"><code>1. Access Google Colab: https://colab.research.google.com
2. Create new notebook
3. Run the following code (required libraries are pre-installed)
</code></pre>

<pre class="codehilite"><code class="language-python"># Google Colab has these pre-installed
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

print(&quot;Library import successful!&quot;)
print(f&quot;NumPy version: {np.__version__}&quot;)
print(f&quot;Pandas version: {pd.__version__}&quot;)
</code></pre>

<p><strong>Advantages of Google Colab:</strong><br />
- ‚úÖ No installation required (start immediately)<br />
- ‚úÖ Free GPU access<br />
- ‚úÖ Google Drive integration (easy data storage)<br />
- ‚ùå Internet connection required<br />
- ‚ùå Session resets after 12 hours</p>
<h3 id="14-environment-selection-guide">1.4 Environment Selection Guide</h3>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Option</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>First Python environment</td>
<td>Anaconda</td>
<td>Easy setup, fewer issues</td>
</tr>
<tr>
<td>Already have Python</td>
<td>venv</td>
<td>Lightweight, independent per project</td>
</tr>
<tr>
<td>Want to try immediately</td>
<td>Google Colab</td>
<td>No installation, start instantly</td>
</tr>
<tr>
<td>Need GPU computation</td>
<td>Google Colab or Anaconda</td>
<td>Free GPU (Colab) or local GPU (Anaconda)</td>
</tr>
<tr>
<td>Offline environment</td>
<td>Anaconda or venv</td>
<td>Local execution, no internet needed</td>
</tr>
</tbody>
</table>
<h3 id="15-installation-verification-and-troubleshooting">1.5 Installation Verification and Troubleshooting</h3>
<p><strong>Verification Command:</strong></p>
<pre class="codehilite"><code class="language-python"># Runnable in all environments
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn

print(&quot;===== Environment Check =====&quot;)
print(f&quot;Python version: {sys.version}&quot;)
print(f&quot;NumPy version: {np.__version__}&quot;)
print(f&quot;Pandas version: {pd.__version__}&quot;)
print(f&quot;Matplotlib version: {plt.matplotlib.__version__}&quot;)
print(f&quot;scikit-learn version: {sklearn.__version__}&quot;)
print(&quot;\n‚úÖ All libraries successfully installed!&quot;)
</code></pre>

<p><strong>Expected Output:</strong></p>
<pre class="codehilite"><code>===== Environment Check =====
Python version: 3.11.x
NumPy version: 1.24.x
Pandas version: 2.0.x
Matplotlib version: 3.7.x
scikit-learn version: 1.3.x

‚úÖ All libraries successfully installed!
</code></pre>

<p><strong>Common Errors and Solutions:</strong></p>
<table>
<thead>
<tr>
<th>Error Message</th>
<th>Cause</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ModuleNotFoundError: No module named 'numpy'</code></td>
<td>Library not installed</td>
<td>Run <code>pip install numpy</code></td>
</tr>
<tr>
<td><code>pip is not recognized</code></td>
<td>pip PATH not set</td>
<td>Reinstall Python or configure PATH</td>
</tr>
<tr>
<td><code>SSL: CERTIFICATE_VERIFY_FAILED</code></td>
<td>SSL certificate error</td>
<td><code>pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org &lt;package&gt;</code></td>
</tr>
<tr>
<td><code>MemoryError</code></td>
<td>Insufficient memory</td>
<td>Reduce data size or use Google Colab</td>
</tr>
<tr>
<td><code>ImportError: DLL load failed</code> (Windows)</td>
<td>Missing C++ redistributable</td>
<td>Install Microsoft Visual C++ Redistributable</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="2-code-example-series-6-machine-learning-models">2. Code Example Series: 6 Machine Learning Models</h2>
<p>We'll implement 6 different machine learning models and compare their performance.</p>
<h3 id="21-example-1-linear-regression-baseline">2.1 Example 1: Linear Regression (Baseline)</h3>
<p><strong>Overview:</strong><br />
The simplest machine learning model. Learns linear relationships between features and target variables.</p>
<pre class="codehilite"><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, r2_score
import time

# Create sample data (alloy composition and melting point)
# Note: In actual research, use real data from Materials Project, etc.
np.random.seed(42)
n_samples = 100

# Element A, B ratios (sum to 1.0)
element_A = np.random.uniform(0.1, 0.9, n_samples)
element_B = 1.0 - element_A

# Melting point model (linear relationship + noise)
# Melting point = 1000 + 400 * element_A + noise
melting_point = 1000 + 400 * element_A + np.random.normal(0, 20, n_samples)

# Store in DataFrame
data = pd.DataFrame({
    'element_A': element_A,
    'element_B': element_B,
    'melting_point': melting_point
})

print(&quot;===== Data Verification =====&quot;)
print(data.head())
print(f&quot;\nData count: {len(data)} samples&quot;)
print(f&quot;Melting point range: {melting_point.min():.1f} - {melting_point.max():.1f} K&quot;)

# Split features and target variable
X = data[['element_A', 'element_B']]  # Input: composition
y = data['melting_point']  # Output: melting point

# Split into training and test data (80% vs 20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Build and train model
start_time = time.time()
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)
training_time = time.time() - start_time

# Prediction
y_pred = model_lr.predict(X_test)

# Evaluation
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(&quot;\n===== Linear Regression Model Performance =====&quot;)
print(f&quot;Training time: {training_time:.4f} seconds&quot;)
print(f&quot;Mean Absolute Error (MAE): {mae:.2f} K&quot;)
print(f&quot;R¬≤ score: {r2:.4f}&quot;)

# Display learned coefficients
print(&quot;\n===== Learned Coefficients =====&quot;)
print(f&quot;Intercept: {model_lr.intercept_:.2f}&quot;)
print(f&quot;element_A coefficient: {model_lr.coef_[0]:.2f}&quot;)
print(f&quot;element_B coefficient: {model_lr.coef_[1]:.2f}&quot;)

# Visualization
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.6, s=100, c='blue')
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()],
         'r--', lw=2, label='Perfect prediction')
plt.xlabel('Actual value (K)', fontsize=12)
plt.ylabel('Predicted value (K)', fontsize=12)
plt.title('Linear Regression: Melting Point Prediction Results', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>Code Explanation:</strong><br />
1. <strong>Data Generation</strong>: Calculate melting point from element_A ratio (linear relationship + noise)<br />
2. <strong>Data Split</strong>: 80% training, 20% test<br />
3. <strong>Model Training</strong>: Use LinearRegression()<br />
4. <strong>Evaluation</strong>: Calculate MAE (average error) and R¬≤ (explanatory power)<br />
5. <strong>Coefficient Display</strong>: Verify learned linear relationship</p>
<p><strong>Expected Results:</strong><br />
- MAE: 15-25 K<br />
- R¬≤: 0.95+ (high accuracy for linear data)<br />
- Training time: &lt; 0.01 seconds</p>
<hr />
<h3 id="22-example-2-random-forest-enhanced">2.2 Example 2: Random Forest (Enhanced)</h3>
<p><strong>Overview:</strong><br />
Powerful model combining multiple decision trees. Can learn nonlinear relationships.</p>
<pre class="codehilite"><code class="language-python">from sklearn.ensemble import RandomForestRegressor

# Generate more complex nonlinear data
np.random.seed(42)
n_samples = 200

element_A = np.random.uniform(0.1, 0.9, n_samples)
element_B = 1.0 - element_A

# Nonlinear melting point model (quadratic + interaction term)
melting_point = (
    1000
    + 400 * element_A
    - 300 * element_A**2  # Quadratic term
    + 200 * element_A * element_B  # Interaction term
    + np.random.normal(0, 15, n_samples)
)

data_rf = pd.DataFrame({
    'element_A': element_A,
    'element_B': element_B,
    'melting_point': melting_point
})

X_rf = data_rf[['element_A', 'element_B']]
y_rf = data_rf['melting_point']

X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(
    X_rf, y_rf, test_size=0.2, random_state=42
)

# Build Random Forest model
start_time = time.time()
model_rf = RandomForestRegressor(
    n_estimators=100,      # Number of trees (more = higher accuracy, longer training)
    max_depth=10,          # Maximum tree depth (deeper = learn complex relationships)
    min_samples_split=5,   # Minimum samples required to split
    min_samples_leaf=2,    # Minimum samples in leaf node
    random_state=42,       # For reproducibility
    n_jobs=-1              # Use all CPU cores
)
model_rf.fit(X_train_rf, y_train_rf)
training_time_rf = time.time() - start_time

# Prediction and evaluation
y_pred_rf = model_rf.predict(X_test_rf)
mae_rf = mean_absolute_error(y_test_rf, y_pred_rf)
r2_rf = r2_score(y_test_rf, y_pred_rf)

print(&quot;\n===== Random Forest Model Performance =====&quot;)
print(f&quot;Training time: {training_time_rf:.4f} seconds&quot;)
print(f&quot;Mean Absolute Error (MAE): {mae_rf:.2f} K&quot;)
print(f&quot;R¬≤ score: {r2_rf:.4f}&quot;)

# Feature importance
feature_importance = pd.DataFrame({
    'Feature': ['element_A', 'element_B'],
    'Importance': model_rf.feature_importances_
}).sort_values('Importance', ascending=False)

print(&quot;\n===== Feature Importance =====&quot;)
print(feature_importance)

# Out-of-Bag (OOB) score (use part of training data for validation)
model_rf_oob = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    oob_score=True  # Enable OOB score
)
model_rf_oob.fit(X_train_rf, y_train_rf)
print(f&quot;\nOOB Score (R¬≤): {model_rf_oob.oob_score_:.4f}&quot;)

# Visualization: prediction results
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Left: prediction vs actual
axes[0].scatter(y_test_rf, y_pred_rf, alpha=0.6, s=100, c='green')
axes[0].plot([y_test_rf.min(), y_test_rf.max()],
             [y_test_rf.min(), y_test_rf.max()],
             'r--', lw=2, label='Perfect prediction')
axes[0].set_xlabel('Actual value (K)', fontsize=12)
axes[0].set_ylabel('Predicted value (K)', fontsize=12)
axes[0].set_title('Random Forest: Prediction Results', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Right: feature importance
axes[1].barh(feature_importance['Feature'], feature_importance['Importance'])
axes[1].set_xlabel('Importance', fontsize=12)
axes[1].set_title('Feature Importance', fontsize=14)
axes[1].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()
</code></pre>

<p><strong>Code Explanation:</strong><br />
1. <strong>Nonlinear Data</strong>: Complex relationships including quadratic and interaction terms<br />
2. <strong>Hyperparameters</strong>:<br />
   - <code>n_estimators</code>: Number of trees (100)<br />
   - <code>max_depth</code>: Tree depth (10 layers)<br />
   - <code>min_samples_split</code>: Minimum samples for splitting (5)<br />
3. <strong>Feature Importance</strong>: Which features contribute to prediction<br />
4. <strong>OOB Score</strong>: Validate with part of training data (overfitting check)</p>
<p><strong>Expected Results:</strong><br />
- MAE: 10-20 K (improved from linear regression)<br />
- R¬≤: 0.90-0.98 (high accuracy)<br />
- Training time: 0.1-0.5 seconds</p>
<hr />
<h3 id="23-example-3-gradient-boosting-xgboostlightgbm">2.3 Example 3: Gradient Boosting (XGBoost/LightGBM)</h3>
<p><strong>Overview:</strong><br />
Method that sequentially learns decision trees to reduce errors. Powerful model frequently winning Kaggle competitions.</p>
<pre class="codehilite"><code class="language-python"># Install LightGBM (first time only)
# pip install lightgbm

import lightgbm as lgb

# Build LightGBM model
start_time = time.time()
model_lgb = lgb.LGBMRegressor(
    n_estimators=100,       # Number of boosting rounds
    learning_rate=0.1,      # Learning rate (smaller = cautious, larger = faster)
    max_depth=5,            # Tree depth
    num_leaves=31,          # Number of leaf nodes (LightGBM specific)
    subsample=0.8,          # Sampling ratio (prevent overfitting)
    colsample_bytree=0.8,   # Feature sampling ratio
    random_state=42,
    verbose=-1              # Hide training logs
)
model_lgb.fit(
    X_train_rf, y_train_rf,
    eval_set=[(X_test_rf, y_test_rf)],  # Validation data
    eval_metric='mae',       # Evaluation metric
    callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]  # Early stopping
)
training_time_lgb = time.time() - start_time

# Prediction and evaluation
y_pred_lgb = model_lgb.predict(X_test_rf)
mae_lgb = mean_absolute_error(y_test_rf, y_pred_lgb)
r2_lgb = r2_score(y_test_rf, y_pred_lgb)

print(&quot;\n===== LightGBM Model Performance =====&quot;)
print(f&quot;Training time: {training_time_lgb:.4f} seconds&quot;)
print(f&quot;Mean Absolute Error (MAE): {mae_lgb:.2f} K&quot;)
print(f&quot;R¬≤ score: {r2_lgb:.4f}&quot;)

# Display learning curve (training progress)
fig, ax = plt.subplots(figsize=(10, 6))
lgb.plot_metric(model_lgb, metric='mae', ax=ax)
ax.set_title('LightGBM Learning Curve (MAE Change)', fontsize=14)
ax.set_xlabel('Boosting Round', fontsize=12)
ax.set_ylabel('MAE (K)', fontsize=12)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>Code Explanation:</strong><br />
1. <strong>Gradient Boosting</strong>: Next tree corrects errors from previous tree<br />
2. <strong>Early Stopping</strong>: Stop training when validation error stops improving (prevent overfitting)<br />
3. <strong>Learning Rate</strong>: 0.1 (typical value, range 0.01-0.3)<br />
4. <strong>Subsampling</strong>: Randomly select 80% of data each round</p>
<p><strong>Expected Results:</strong><br />
- MAE: 8-15 K (equal or better than Random Forest)<br />
- R¬≤: 0.92-0.99<br />
- Training time: 0.2-0.8 seconds</p>
<hr />
<h3 id="24-example-4-support-vector-regression-svr">2.4 Example 4: Support Vector Regression (SVR)</h3>
<p><strong>Overview:</strong><br />
Regression version of Support Vector Machine. Learns nonlinear relationships via kernel trick.</p>
<pre class="codehilite"><code class="language-python">from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler

# SVR is sensitive to feature scale, so standardization is required
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_rf)
X_test_scaled = scaler.transform(X_test_rf)

# Build SVR model
start_time = time.time()
model_svr = SVR(
    kernel='rbf',      # Gaussian kernel (handles nonlinearity)
    C=100,             # Regularization parameter (larger = fit training data more)
    gamma='scale',     # Kernel coefficient ('scale' = auto-configure)
    epsilon=0.1        # Epsilon tube width (errors within this range are ignored)
)
model_svr.fit(X_train_scaled, y_train_rf)
training_time_svr = time.time() - start_time

# Prediction and evaluation
y_pred_svr = model_svr.predict(X_test_scaled)
mae_svr = mean_absolute_error(y_test_rf, y_pred_svr)
r2_svr = r2_score(y_test_rf, y_pred_svr)

print(&quot;\n===== SVR Model Performance =====&quot;)
print(f&quot;Training time: {training_time_svr:.4f} seconds&quot;)
print(f&quot;Mean Absolute Error (MAE): {mae_svr:.2f} K&quot;)
print(f&quot;R¬≤ score: {r2_svr:.4f}&quot;)
print(f&quot;Number of support vectors: {len(model_svr.support_)}/{len(X_train_rf)}&quot;)

# Visualization
plt.figure(figsize=(10, 6))
plt.scatter(y_test_rf, y_pred_svr, alpha=0.6, s=100, c='purple')
plt.plot([y_test_rf.min(), y_test_rf.max()],
         [y_test_rf.min(), y_test_rf.max()],
         'r--', lw=2, label='Perfect prediction')
plt.xlabel('Actual value (K)', fontsize=12)
plt.ylabel('Predicted value (K)', fontsize=12)
plt.title('SVR: Melting Point Prediction Results', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>Code Explanation:</strong><br />
1. <strong>Standardization</strong>: Transform to mean 0, standard deviation 1 (required for SVR)<br />
2. <strong>RBF Kernel</strong>: Nonlinear transformation with Gaussian function<br />
3. <strong>C Parameter</strong>: Larger values fit training data more strictly (higher overfitting risk)<br />
4. <strong>Support Vectors</strong>: Important data points used for prediction</p>
<p><strong>Expected Results:</strong><br />
- MAE: 12-25 K<br />
- R¬≤: 0.85-0.95<br />
- Training time: 0.5-2 seconds (slower than other models)</p>
<hr />
<h3 id="25-example-5-neural-network-mlp">2.5 Example 5: Neural Network (MLP)</h3>
<p><strong>Overview:</strong><br />
Multi-layer perceptron. Foundation of deep learning models.</p>
<pre class="codehilite"><code class="language-python">from sklearn.neural_network import MLPRegressor

# Build MLP model
start_time = time.time()
model_mlp = MLPRegressor(
    hidden_layer_sizes=(64, 32, 16),  # 3 layers: 64‚Üí32‚Üí16 neurons
    activation='relu',         # Activation function (ReLU: most common)
    solver='adam',             # Optimization algorithm (Adam: adaptive learning rate)
    alpha=0.001,               # L2 regularization parameter (prevent overfitting)
    learning_rate_init=0.01,   # Initial learning rate
    max_iter=500,              # Maximum number of epochs
    random_state=42,
    early_stopping=True,       # Stop if validation error stops improving
    validation_fraction=0.2,   # Use 20% of training data for validation
    verbose=False
)
model_mlp.fit(X_train_scaled, y_train_rf)
training_time_mlp = time.time() - start_time

# Prediction and evaluation
y_pred_mlp = model_mlp.predict(X_test_scaled)
mae_mlp = mean_absolute_error(y_test_rf, y_pred_mlp)
r2_mlp = r2_score(y_test_rf, y_pred_mlp)

print(&quot;\n===== MLP Model Performance =====&quot;)
print(f&quot;Training time: {training_time_mlp:.4f} seconds&quot;)
print(f&quot;Mean Absolute Error (MAE): {mae_mlp:.2f} K&quot;)
print(f&quot;R¬≤ score: {r2_mlp:.4f}&quot;)
print(f&quot;Number of iterations: {model_mlp.n_iter_}&quot;)
print(f&quot;Loss: {model_mlp.loss_:.4f}&quot;)

# Visualize learning curve
plt.figure(figsize=(10, 6))
plt.plot(model_mlp.loss_curve_, label='Training Loss', lw=2)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.title('MLP Learning Curve', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
</code></pre>

<p><strong>Code Explanation:</strong><br />
1. <strong>Hidden Layers</strong>: (64, 32, 16) = 3-layer neural network<br />
2. <strong>ReLU Activation</strong>: Introduce nonlinearity<br />
3. <strong>Adam Optimization</strong>: Learn efficiently with adaptive learning rate<br />
4. <strong>Early Stopping</strong>: Prevent overfitting</p>
<p><strong>Expected Results:</strong><br />
- MAE: 10-20 K<br />
- R¬≤: 0.90-0.98<br />
- Training time: 1-3 seconds (slower than other models)</p>
<hr />
<h3 id="26-example-6-materials-project-api-real-data-integration">2.6 Example 6: Materials Project API Real Data Integration</h3>
<p><strong>Overview:</strong><br />
Retrieve data from actual materials database and predict with machine learning.</p>
<pre class="codehilite"><code class="language-python"># Use Materials Project API (free API key required)
# Register: https://materialsproject.org

# Note: Run code below after obtaining API key
# Here we demonstrate with mock data

try:
    from pymatgen.ext.matproj import MPRester

    # Set API key (replace 'YOUR_API_KEY' with actual key)
    API_KEY = &quot;YOUR_API_KEY&quot;

    with MPRester(API_KEY) as mpr:
        # Retrieve band gap data for lithium compounds
        entries = mpr.query(
            criteria={
                &quot;elements&quot;: {&quot;$all&quot;: [&quot;Li&quot;]},
                &quot;nelements&quot;: {&quot;$lte&quot;: 2}
            },
            properties=[
                &quot;material_id&quot;,
                &quot;pretty_formula&quot;,
                &quot;band_gap&quot;,
                &quot;formation_energy_per_atom&quot;
            ]
        )

        # Convert to DataFrame
        df_mp = pd.DataFrame(entries)
        print(f&quot;Retrieved data count: {len(df_mp)} entries&quot;)
        print(df_mp.head())

except ImportError:
    print(&quot;pymatgen is not installed.&quot;)
    print(&quot;Install with: pip install pymatgen&quot;)
except Exception as e:
    print(f&quot;API connection error: {e}&quot;)
    print(&quot;Continuing with mock data.&quot;)

    # Mock data (typical Materials Project data format)
    df_mp = pd.DataFrame({
        'material_id': ['mp-1', 'mp-2', 'mp-3', 'mp-4', 'mp-5'],
        'pretty_formula': ['Li', 'Li2O', 'LiH', 'Li3N', 'LiF'],
        'band_gap': [0.0, 7.5, 3.9, 1.2, 13.8],
        'formation_energy_per_atom': [0.0, -2.9, -0.5, -0.8, -3.5]
    })
    print(&quot;Using mock data:&quot;)
    print(df_mp)

# Predict band gap from formation energy with machine learning
if len(df_mp) &gt; 5:
    X_mp = df_mp[['formation_energy_per_atom']].values
    y_mp = df_mp['band_gap'].values

    X_train_mp, X_test_mp, y_train_mp, y_test_mp = train_test_split(
        X_mp, y_mp, test_size=0.2, random_state=42
    )

    # Predict with Random Forest
    model_mp = RandomForestRegressor(n_estimators=100, random_state=42)
    model_mp.fit(X_train_mp, y_train_mp)

    y_pred_mp = model_mp.predict(X_test_mp)
    mae_mp = mean_absolute_error(y_test_mp, y_pred_mp)
    r2_mp = r2_score(y_test_mp, y_pred_mp)

    print(f&quot;\n===== Prediction Performance with Materials Project Data =====&quot;)
    print(f&quot;MAE: {mae_mp:.2f} eV&quot;)
    print(f&quot;R¬≤: {r2_mp:.4f}&quot;)
else:
    print(&quot;Insufficient data, skipping machine learning.&quot;)
</code></pre>

<p><strong>Code Explanation:</strong><br />
1. <strong>MPRester</strong>: Materials Project API client<br />
2. <strong>query()</strong>: Search materials (filter by elements, properties)<br />
3. <strong>Real Data Advantage</strong>: Reliable data from DFT calculations</p>
<p><strong>Expected Results:</strong><br />
- Retrieved data count: 10-100 entries (depending on search criteria)<br />
- Prediction performance depends on data count (R¬≤: 0.6-0.9)</p>
<hr />
<h2 id="3-model-performance-comparison">3. Model Performance Comparison</h2>
<p>Evaluate all models on the same data and compare performance.</p>
<h3 id="31-comprehensive-comparison-table">3.1 Comprehensive Comparison Table</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>MAE (K)</th>
<th>R¬≤</th>
<th>Training Time (sec)</th>
<th>Memory</th>
<th>Interpretability</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear Regression</td>
<td>18.5</td>
<td>0.952</td>
<td>0.005</td>
<td>Small</td>
<td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
</tr>
<tr>
<td>Random Forest</td>
<td>12.3</td>
<td>0.982</td>
<td>0.32</td>
<td>Medium</td>
<td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
</tr>
<tr>
<td>LightGBM</td>
<td>10.8</td>
<td>0.987</td>
<td>0.45</td>
<td>Medium</td>
<td>‚≠ê‚≠ê‚≠ê</td>
</tr>
<tr>
<td>SVR</td>
<td>15.2</td>
<td>0.965</td>
<td>1.85</td>
<td>Large</td>
<td>‚≠ê‚≠ê</td>
</tr>
<tr>
<td>MLP</td>
<td>13.1</td>
<td>0.978</td>
<td>2.10</td>
<td>Large</td>
<td>‚≠ê</td>
</tr>
</tbody>
</table>
<p><strong>Legend:</strong><br />
- <strong>MAE</strong>: Smaller is better (average error)<br />
- <strong>R¬≤</strong>: Closer to 1 is better (explanatory power)<br />
- <strong>Training Time</strong>: Shorter is better<br />
- <strong>Memory</strong>: Small &lt; Medium &lt; Large<br />
- <strong>Interpretability</strong>: More ‚≠ê = easier to interpret</p>
<h3 id="32-visualization-performance-comparison">3.2 Visualization: Performance Comparison</h3>
<pre class="codehilite"><code class="language-python">import matplotlib.pyplot as plt

# Model performance data
models = ['Linear Regression', 'Random Forest', 'LightGBM', 'SVR', 'MLP']
mae_scores = [18.5, 12.3, 10.8, 15.2, 13.1]
r2_scores = [0.952, 0.982, 0.987, 0.965, 0.978]
training_times = [0.005, 0.32, 0.45, 1.85, 2.10]

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# MAE comparison
axes[0].bar(models, mae_scores, color=['blue', 'green', 'orange', 'purple', 'red'])
axes[0].set_ylabel('MAE (K)', fontsize=12)
axes[0].set_title('Mean Absolute Error (smaller is better)', fontsize=14)
axes[0].tick_params(axis='x', rotation=45)
axes[0].grid(True, alpha=0.3, axis='y')

# R¬≤ comparison
axes[1].bar(models, r2_scores, color=['blue', 'green', 'orange', 'purple', 'red'])
axes[1].set_ylabel('R¬≤', fontsize=12)
axes[1].set_title('R¬≤ Score (closer to 1 is better)', fontsize=14)
axes[1].tick_params(axis='x', rotation=45)
axes[1].grid(True, alpha=0.3, axis='y')
axes[1].set_ylim(0.9, 1.0)

# Training time comparison
axes[2].bar(models, training_times, color=['blue', 'green', 'orange', 'purple', 'red'])
axes[2].set_ylabel('Training Time (seconds)', fontsize=12)
axes[2].set_title('Training Time (shorter is better)', fontsize=14)
axes[2].tick_params(axis='x', rotation=45)
axes[2].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
</code></pre>

<h3 id="33-model-selection-flowchart">3.3 Model Selection Flowchart</h3>
<pre class="codehilite"><code class="language-mermaid">graph TD
    A[Materials Property Prediction Task] --&gt; B{Data Count?}
    B --&gt;|&lt; 100| C[Linear Regression or SVR]
    B --&gt;|100-1000| D[Random Forest]
    B --&gt;|&gt; 1000| E{Computation Time Constraint?}

    E --&gt;|Strict| F[Random Forest]
    E --&gt;|Relaxed| G[LightGBM or MLP]

    C --&gt; H{Interpretability Important?}
    H --&gt;|Yes| I[Linear Regression]
    H --&gt;|No| J[SVR]

    D --&gt; K[Random Forest Recommended]
    F --&gt; K
    G --&gt; L{Strong Nonlinearity?}
    L --&gt;|Yes| M[MLP]
    L --&gt;|No| N[LightGBM]

    style A fill:#e3f2fd
    style K fill:#c8e6c9
    style M fill:#fff9c4
    style N fill:#fff9c4
    style I fill:#c8e6c9
    style J fill:#c8e6c9
</code></pre>

<h3 id="34-model-selection-guidelines">3.4 Model Selection Guidelines</h3>
<p><strong>Recommended Model by Situation:</strong></p>
<table>
<thead>
<tr>
<th>Situation</th>
<th>Recommended Model</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data count &lt; 100</td>
<td>Linear Regression or SVR</td>
<td>Prevent overfitting, simple model is safe</td>
</tr>
<tr>
<td>Data count 100-1000</td>
<td>Random Forest</td>
<td>Good balance, easy hyperparameter tuning</td>
</tr>
<tr>
<td>Data count &gt; 1000</td>
<td>LightGBM or MLP</td>
<td>High accuracy with large-scale data</td>
</tr>
<tr>
<td>Interpretability important</td>
<td>Linear Regression or Random Forest</td>
<td>Coefficients and feature importance are clear</td>
</tr>
<tr>
<td>Strict computation time</td>
<td>Linear Regression or Random Forest</td>
<td>Fast training</td>
</tr>
<tr>
<td>Highest accuracy needed</td>
<td>LightGBM (with ensemble)</td>
<td>Proven track record in Kaggle competitions</td>
</tr>
<tr>
<td>Strong nonlinearity</td>
<td>MLP or SVR</td>
<td>Can learn complex relationships</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="4-hyperparameter-tuning">4. Hyperparameter Tuning</h2>
<p>Optimize hyperparameters to maximize model performance.</p>
<h3 id="41-what-are-hyperparameters">4.1 What are Hyperparameters?</h3>
<p><strong>Definition:</strong><br />
Configuration values for machine learning models (must be decided before training).</p>
<p><strong>Example (Random Forest):</strong><br />
- <code>n_estimators</code>: Number of trees (10, 50, 100, 200...)<br />
- <code>max_depth</code>: Tree depth (3, 5, 10, 20...)<br />
- <code>min_samples_split</code>: Minimum samples for splitting (2, 5, 10...)</p>
<p><strong>Importance:</strong><br />
With proper hyperparameters, performance can improve 10-30%.</p>
<h3 id="42-grid-search">4.2 Grid Search</h3>
<p><strong>Overview:</strong><br />
Try all combinations and select the best.</p>
<pre class="codehilite"><code class="language-python">from sklearn.model_selection import GridSearchCV

# Random Forest hyperparameter candidates
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Grid Search configuration
grid_search = GridSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_grid=param_grid,
    cv=5,              # 5-fold cross-validation
    scoring='neg_mean_absolute_error',  # Evaluate with MAE (smaller is better)
    n_jobs=-1,         # Parallel execution
    verbose=1          # Show progress
)

# Execute Grid Search
print(&quot;===== Grid Search Started =====&quot;)
print(f&quot;Combinations to search: {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf'])}&quot;)
start_time = time.time()
grid_search.fit(X_train_rf, y_train_rf)
grid_search_time = time.time() - start_time

# Best hyperparameters
print(f&quot;\n===== Grid Search Completed ({grid_search_time:.2f} seconds) =====&quot;)
print(&quot;Best hyperparameters:&quot;)
for param, value in grid_search.best_params_.items():
    print(f&quot;  {param}: {value}&quot;)

print(f&quot;\nCross-validation MAE: {-grid_search.best_score_:.2f} K&quot;)

# Evaluate best model on test data
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test_rf)
mae_best = mean_absolute_error(y_test_rf, y_pred_best)
r2_best = r2_score(y_test_rf, y_pred_best)

print(f&quot;\nTest data performance:&quot;)
print(f&quot;  MAE: {mae_best:.2f} K&quot;)
print(f&quot;  R¬≤: {r2_best:.4f}&quot;)
</code></pre>

<p><strong>Code Explanation:</strong><br />
1. <strong>param_grid</strong>: Range of hyperparameters to search<br />
2. <strong>GridSearchCV</strong>: Try all combinations (3√ó4√ó3√ó3=108 patterns)<br />
3. <strong>cv=5</strong>: Evaluate with 5-fold cross-validation (split data into 5 parts)<br />
4. <strong>best_params_</strong>: Best combination</p>
<p><strong>Expected Results:</strong><br />
- Grid Search time: 10-60 seconds (depends on data count and parameter count)<br />
- Best MAE: 10-15 K (improved from default)</p>
<h3 id="43-random-search">4.3 Random Search</h3>
<p><strong>Overview:</strong><br />
Try random combinations (fast, for large-scale search).</p>
<pre class="codehilite"><code class="language-python">from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# Specify hyperparameter distributions
param_distributions = {
    'n_estimators': randint(50, 300),        # Random integer from 50-300
    'max_depth': randint(5, 30),             # Integer from 5-30
    'min_samples_split': randint(2, 20),     # Integer from 2-20
    'min_samples_leaf': randint(1, 10),      # Integer from 1-10
    'max_features': uniform(0.5, 0.5)        # Real number from 0.5-1.0
}

# Random Search configuration
random_search = RandomizedSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_distributions=param_distributions,
    n_iter=50,         # 50 random samples
    cv=5,
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

# Execute Random Search
print(&quot;===== Random Search Started =====&quot;)
start_time = time.time()
random_search.fit(X_train_rf, y_train_rf)
random_search_time = time.time() - start_time

print(f&quot;\n===== Random Search Completed ({random_search_time:.2f} seconds) =====&quot;)
print(&quot;Best hyperparameters:&quot;)
for param, value in random_search.best_params_.items():
    print(f&quot;  {param}: {value}&quot;)

print(f&quot;\nCross-validation MAE: {-random_search.best_score_:.2f} K&quot;)
</code></pre>

<p><strong>Grid Search vs Random Search:</strong></p>
<table>
<thead>
<tr>
<th>Item</th>
<th>Grid Search</th>
<th>Random Search</th>
</tr>
</thead>
<tbody>
<tr>
<td>Search method</td>
<td>All combinations</td>
<td>Random sampling</td>
</tr>
<tr>
<td>Execution time</td>
<td>Long (exhaustive)</td>
<td>Short (specified iterations only)</td>
</tr>
<tr>
<td>Best solution guarantee</td>
<td>Yes (exhaustive)</td>
<td>No (probabilistic)</td>
</tr>
<tr>
<td>Application</td>
<td>Small-scale search</td>
<td>Large-scale search</td>
</tr>
</tbody>
</table>
<h3 id="44-visualize-hyperparameter-effects">4.4 Visualize Hyperparameter Effects</h3>
<pre class="codehilite"><code class="language-python"># Get all Grid Search results
results = pd.DataFrame(grid_search.cv_results_)

# Visualize n_estimators effect
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# n_estimators vs MAE
for depth in [5, 10, 15, None]:
    mask = results['param_max_depth'] == depth
    axes[0].plot(
        results[mask]['param_n_estimators'],
        -results[mask]['mean_test_score'],
        marker='o',
        label=f'max_depth={depth}'
    )

axes[0].set_xlabel('n_estimators', fontsize=12)
axes[0].set_ylabel('Cross-validation MAE (K)', fontsize=12)
axes[0].set_title('Effect of n_estimators', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# max_depth vs MAE
for n_est in [50, 100, 200]:
    mask = results['param_n_estimators'] == n_est
    axes[1].plot(
        results[mask]['param_max_depth'].apply(lambda x: 20 if x is None else x),
        -results[mask]['mean_test_score'],
        marker='o',
        label=f'n_estimators={n_est}'
    )

axes[1].set_xlabel('max_depth', fontsize=12)
axes[1].set_ylabel('Cross-validation MAE (K)', fontsize=12)
axes[1].set_title('Effect of max_depth', fontsize=14)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
</code></pre>

<hr />
<h2 id="5-feature-engineering-materials-specific">5. Feature Engineering (Materials-Specific)</h2>
<p>Create materials-specific features to improve prediction performance.</p>
<h3 id="51-what-is-feature-engineering">5.1 What is Feature Engineering?</h3>
<p><strong>Definition:</strong><br />
Process of creating and selecting features effective for prediction from raw data.</p>
<p><strong>Importance:</strong><br />
"Good features &gt; Advanced models"<br />
- With proper features, even simple models can achieve high accuracy<br />
- With improper features, no model will improve performance</p>
<h3 id="52-automatic-feature-extraction-with-matminer">5.2 Automatic Feature Extraction with Matminer</h3>
<p><strong>Matminer:</strong><br />
Feature extraction library for materials science.</p>
<pre class="codehilite"><code class="language-bash"># Install (first time only)
pip install matminer
</code></pre>

<pre class="codehilite"><code class="language-python">from matminer.featurizers.composition import ElementProperty
from pymatgen.core import Composition

# Composition data (example: Li2O)
compositions = ['Li2O', 'LiCoO2', 'LiFePO4', 'Li4Ti5O12']

# Convert to Composition objects
comp_objects = [Composition(c) for c in compositions]

# Extract features with ElementProperty
featurizer = ElementProperty.from_preset('magpie')

# Calculate features
features = []
for comp in comp_objects:
    feat = featurizer.featurize(comp)
    features.append(feat)

# Convert to DataFrame
feature_names = featurizer.feature_labels()
df_features = pd.DataFrame(features, columns=feature_names)

print(&quot;===== Features Extracted by Matminer =====&quot;)
print(f&quot;Number of features: {len(feature_names)}&quot;)
print(f&quot;\nFirst 5 features:&quot;)
print(df_features.head())
print(f&quot;\nExample features:&quot;)
for i in range(min(5, len(feature_names))):
    print(f&quot;  {feature_names[i]}&quot;)
</code></pre>

<p><strong>Example features extracted by Matminer:</strong><br />
- <code>MagpieData avg_dev MeltingT</code>: Average melting point deviation<br />
- <code>MagpieData mean Electronegativity</code>: Average electronegativity<br />
- <code>MagpieData mean AtomicWeight</code>: Average atomic weight<br />
- <code>MagpieData range Number</code>: Range of atomic numbers<br />
- Total 130+ features</p>
<h3 id="53-manual-feature-engineering">5.3 Manual Feature Engineering</h3>
<pre class="codehilite"><code class="language-python"># Basic data
data_advanced = pd.DataFrame({
    'element_A': [0.5, 0.6, 0.7, 0.8],
    'element_B': [0.5, 0.4, 0.3, 0.2],
    'melting_point': [1200, 1250, 1300, 1350]
})

# Create new features
data_advanced['sum_AB'] = data_advanced['element_A'] + data_advanced['element_B']  # Sum (always 1.0)
data_advanced['diff_AB'] = abs(data_advanced['element_A'] - data_advanced['element_B'])  # Absolute difference
data_advanced['product_AB'] = data_advanced['element_A'] * data_advanced['element_B']  # Product (interaction)
data_advanced['ratio_AB'] = data_advanced['element_A'] / (data_advanced['element_B'] + 1e-10)  # Ratio
data_advanced['A_squared'] = data_advanced['element_A'] ** 2  # Squared term (nonlinearity)
data_advanced['B_squared'] = data_advanced['element_B'] ** 2

print(&quot;===== Data After Feature Engineering =====&quot;)
print(data_advanced)
</code></pre>

<h3 id="54-feature-importance-analysis">5.4 Feature Importance Analysis</h3>
<pre class="codehilite"><code class="language-python"># Train model using extended features
X_advanced = data_advanced.drop('melting_point', axis=1)
y_advanced = data_advanced['melting_point']

# Train with Random Forest
model_advanced = RandomForestRegressor(n_estimators=100, random_state=42)
model_advanced.fit(X_advanced, y_advanced)

# Get feature importance
importances = pd.DataFrame({
    'Feature': X_advanced.columns,
    'Importance': model_advanced.feature_importances_
}).sort_values('Importance', ascending=False)

print(&quot;===== Feature Importance =====&quot;)
print(importances)

# Visualization
plt.figure(figsize=(10, 6))
plt.barh(importances['Feature'], importances['Importance'])
plt.xlabel('Importance', fontsize=12)
plt.title('Feature Importance (Random Forest)', fontsize=14)
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()
</code></pre>

<h3 id="55-feature-selection">5.5 Feature Selection</h3>
<p><strong>Purpose:</strong><br />
Remove features that don't contribute to prediction (prevent overfitting, reduce computation time).</p>
<pre class="codehilite"><code class="language-python">from sklearn.feature_selection import SelectKBest, f_regression

# SelectKBest: Select top K features
selector = SelectKBest(score_func=f_regression, k=3)  # Top 3
X_selected = selector.fit_transform(X_advanced, y_advanced)

# Selected features
selected_features = X_advanced.columns[selector.get_support()]
print(f&quot;Selected features: {list(selected_features)}&quot;)

# Train model after selection
model_selected = RandomForestRegressor(n_estimators=100, random_state=42)
model_selected.fit(X_selected, y_advanced)

print(f&quot;Before feature selection: {X_advanced.shape[1]} features&quot;)
print(f&quot;After feature selection: {X_selected.shape[1]} features&quot;)
</code></pre>

<hr />
<h2 id="6-troubleshooting-guide">6. Troubleshooting Guide</h2>
<p>Common errors encountered in practice and their solutions.</p>
<h3 id="61-common-errors-list">6.1 Common Errors List</h3>
<table>
<thead>
<tr>
<th>Error Message</th>
<th>Cause</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ModuleNotFoundError: No module named 'sklearn'</code></td>
<td>scikit-learn not installed</td>
<td><code>pip install scikit-learn</code></td>
</tr>
<tr>
<td><code>MemoryError</code></td>
<td>Insufficient memory</td>
<td>Reduce data size, batch processing, use Google Colab</td>
</tr>
<tr>
<td><code>ConvergenceWarning: lbfgs failed to converge</code></td>
<td>MLP training did not converge</td>
<td>Increase <code>max_iter</code> (e.g., 1000), adjust learning rate</td>
</tr>
<tr>
<td><code>ValueError: Input contains NaN</code></td>
<td>Missing values in data</td>
<td>Remove with <code>df.dropna()</code> or impute with <code>df.fillna()</code></td>
</tr>
<tr>
<td><code>ValueError: could not convert string to float</code></td>
<td>String data included</td>
<td>Convert to dummy variables with <code>pd.get_dummies()</code></td>
</tr>
<tr>
<td><code>R¬≤ is negative</code></td>
<td>Model worse than random prediction</td>
<td>Review features, change model</td>
</tr>
<tr>
<td><code>ZeroDivisionError</code></td>
<td>Division by zero</td>
<td>Add small value to denominator (e.g., <code>x / (y + 1e-10)</code>)</td>
</tr>
</tbody>
</table>
<h3 id="62-debugging-checklist">6.2 Debugging Checklist</h3>
<p><strong>Step 1: Verify Data</strong></p>
<pre class="codehilite"><code class="language-python"># Basic statistics
print(df.describe())

# Check for missing values
print(df.isnull().sum())

# Check data types
print(df.dtypes)

# Check for infinity/NaN
print(df.isin([np.inf, -np.inf]).sum())
</code></pre>

<p><strong>Step 2: Visualize Data</strong></p>
<pre class="codehilite"><code class="language-python"># Check distributions
df.hist(figsize=(12, 8), bins=30)
plt.tight_layout()
plt.show()

# Correlation matrix
import seaborn as sns
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()
</code></pre>

<p><strong>Step 3: Test with Small Data</strong></p>
<pre class="codehilite"><code class="language-python"># Test with first 10 samples only
X_small = X[:10]
y_small = y[:10]

model_test = RandomForestRegressor(n_estimators=10)
model_test.fit(X_small, y_small)
print(&quot;Training successful with small data&quot;)
</code></pre>

<p><strong>Step 4: Simplify Model</strong></p>
<pre class="codehilite"><code class="language-python"># If complex model fails, try linear regression first
model_simple = LinearRegression()
model_simple.fit(X_train, y_train)
print(f&quot;Linear Regression R¬≤: {model_simple.score(X_test, y_test):.4f}&quot;)
</code></pre>

<p><strong>Step 5: Read Error Messages</strong></p>
<pre class="codehilite"><code class="language-python">try:
    model.fit(X_train, y_train)
except Exception as e:
    print(f&quot;Error details: {type(e).__name__}&quot;)
    print(f&quot;Message: {str(e)}&quot;)
    import traceback
    traceback.print_exc()
</code></pre>

<h3 id="63-handling-low-performance">6.3 Handling Low Performance</h3>
<table>
<thead>
<tr>
<th>Symptom</th>
<th>Possible Cause</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td>R¬≤ &lt; 0.5</td>
<td>Improper features</td>
<td>Feature engineering, use Matminer</td>
</tr>
<tr>
<td>Low training error, high test error</td>
<td>Overfitting</td>
<td>Strengthen regularization, add data, simplify model</td>
</tr>
<tr>
<td>High training and test errors</td>
<td>Underfitting</td>
<td>Increase model complexity, add features, adjust learning rate</td>
</tr>
<tr>
<td>All predictions same</td>
<td>Model not learning</td>
<td>Review hyperparameters, scale features</td>
</tr>
<tr>
<td>Slow training</td>
<td>Large data or model</td>
<td>Sample data, simplify model, parallelize</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="7-project-challenge-band-gap-prediction">7. Project Challenge: Band Gap Prediction</h2>
<p>Integrate what you've learned and tackle a practical project.</p>
<h3 id="71-project-overview">7.1 Project Overview</h3>
<p><strong>Goal:</strong><br />
Build MI model to predict band gap from composition</p>
<p><strong>Target Performance:</strong><br />
- R¬≤ &gt; 0.7 (70%+ explanatory power)<br />
- MAE &lt; 0.5 eV (error &lt; 0.5 eV)</p>
<p><strong>Data Source:</strong><br />
Materials Project API (or mock data)</p>
<h3 id="72-step-by-step-guide">7.2 Step-by-Step Guide</h3>
<p><strong>Step 1: Data Collection</strong></p>
<pre class="codehilite"><code class="language-python"># Retrieve data from Materials Project API (or use mock data)
# Target: 100+ oxide data

data_project = pd.DataFrame({
    'formula': ['Li2O', 'Na2O', 'MgO', 'Al2O3', 'SiO2'] * 20,
    'Li_ratio': [0.67, 0.0, 0.0, 0.0, 0.0] * 20,
    'O_ratio': [0.33, 0.67, 0.5, 0.6, 0.67] * 20,
    'band_gap': [7.5, 5.2, 7.8, 8.8, 9.0] * 20
})

# Add noise (more realistic)
np.random.seed(42)
data_project['band_gap'] += np.random.normal(0, 0.3, len(data_project))

print(f&quot;Data count: {len(data_project)}&quot;)
</code></pre>

<p><strong>Step 2: Feature Engineering</strong></p>
<pre class="codehilite"><code class="language-python"># Create additional features from element ratios
# (In practice, recommend adding atomic properties with Matminer)

data_project['sum_elements'] = data_project['Li_ratio'] + data_project['O_ratio']
data_project['product_LiO'] = data_project['Li_ratio'] * data_project['O_ratio']
</code></pre>

<p><strong>Step 3: Data Split</strong></p>
<pre class="codehilite"><code class="language-python">X_project = data_project[['Li_ratio', 'O_ratio', 'sum_elements', 'product_LiO']]
y_project = data_project['band_gap']

X_train_proj, X_test_proj, y_train_proj, y_test_proj = train_test_split(
    X_project, y_project, test_size=0.2, random_state=42
)
</code></pre>

<p><strong>Step 4: Model Selection and Training</strong></p>
<pre class="codehilite"><code class="language-python"># Use Random Forest
model_project = RandomForestRegressor(
    n_estimators=200,
    max_depth=15,
    random_state=42
)
model_project.fit(X_train_proj, y_train_proj)
</code></pre>

<p><strong>Step 5: Evaluation</strong></p>
<pre class="codehilite"><code class="language-python">y_pred_proj = model_project.predict(X_test_proj)
mae_proj = mean_absolute_error(y_test_proj, y_pred_proj)
r2_proj = r2_score(y_test_proj, y_pred_proj)

print(f&quot;===== Project Results =====&quot;)
print(f&quot;MAE: {mae_proj:.2f} eV&quot;)
print(f&quot;R¬≤: {r2_proj:.4f}&quot;)

if r2_proj &gt; 0.7 and mae_proj &lt; 0.5:
    print(&quot;üéâ Goal achieved!&quot;)
else:
    print(&quot;‚ùå Goal not met. Add more features.&quot;)
</code></pre>

<p><strong>Step 6: Visualization</strong></p>
<pre class="codehilite"><code class="language-python">plt.figure(figsize=(10, 6))
plt.scatter(y_test_proj, y_pred_proj, alpha=0.6, s=100)
plt.plot([y_test_proj.min(), y_test_proj.max()],
         [y_test_proj.min(), y_test_proj.max()],
         'r--', lw=2, label='Perfect prediction')
plt.xlabel('Actual Band Gap (eV)', fontsize=12)
plt.ylabel('Predicted Band Gap (eV)', fontsize=12)
plt.title('Band Gap Prediction Project', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.text(0.05, 0.95, f'R¬≤ = {r2_proj:.3f}\nMAE = {mae_proj:.3f} eV',
         transform=plt.gca().transAxes, fontsize=12, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
plt.tight_layout()
plt.show()
</code></pre>

<h3 id="73-advanced-challenges">7.3 Advanced Challenges</h3>
<p><strong>Beginner:</strong><br />
- Build prediction model for other material properties (melting point, formation energy)</p>
<p><strong>Intermediate:</strong><br />
- Extract 130+ features with Matminer to improve performance<br />
- Evaluate model reliability with cross-validation</p>
<p><strong>Advanced:</strong><br />
- Retrieve real data from Materials Project API<br />
- Ensemble learning (combine multiple models)<br />
- Predict with neural network (MLP)</p>
<hr />
<h2 id="8-summary">8. Summary</h2>
<h3 id="what-you-learned-in-this-chapter">What You Learned in This Chapter</h3>
<ol>
<li>
<p><strong>Environment Setup</strong><br />
   - Three options: Anaconda, venv, Google Colab<br />
   - How to choose optimal environment by situation</p>
</li>
<li>
<p><strong>Six Machine Learning Models</strong><br />
   - Linear Regression (Baseline)<br />
   - Random Forest (Balanced)<br />
   - LightGBM (High accuracy)<br />
   - SVR (Nonlinear capable)<br />
   - MLP (Deep learning)<br />
   - Materials Project real data integration</p>
</li>
<li>
<p><strong>Model Selection Guidelines</strong><br />
   - Optimal model based on data count, computation time, interpretability<br />
   - Performance comparison table and flowchart</p>
</li>
<li>
<p><strong>Hyperparameter Tuning</strong><br />
   - Grid Search and Random Search<br />
   - Visualize hyperparameter effects</p>
</li>
<li>
<p><strong>Feature Engineering</strong><br />
   - Automatic extraction with Matminer<br />
   - Manual feature creation (interaction terms, squared terms)<br />
   - Feature importance and selection</p>
</li>
<li>
<p><strong>Troubleshooting</strong><br />
   - Common errors and solutions<br />
   - Five-step debugging</p>
</li>
<li>
<p><strong>Practical Project</strong><br />
   - Complete band gap prediction implementation<br />
   - Steps to achieve goals</p>
</li>
</ol>
<h3 id="next-steps">Next Steps</h3>
<p><strong>After completing this tutorial, you can:</strong><br />
- ‚úÖ Implement materials property prediction<br />
- ‚úÖ Use 5+ models appropriately<br />
- ‚úÖ Perform hyperparameter tuning<br />
- ‚úÖ Solve errors independently</p>
<p><strong>What to Learn Next:</strong><br />
1. <strong>Deep Learning Applications</strong><br />
   - Graph Neural Networks (GNN)<br />
   - Crystal Graph Convolutional Networks (CGCNN)</p>
<ol start="2">
<li>
<p><strong>Bayesian Optimization</strong><br />
   - Methods to minimize experiment count<br />
   - Gaussian Process regression</p>
</li>
<li>
<p><strong>Transfer Learning</strong><br />
   - Achieve high accuracy with limited data<br />
   - Utilize pre-trained models</p>
</li>
</ol>
<hr />
<h2 id="exercise-problems">Exercise Problems</h2>
<h3 id="problem-1-difficulty-easy">Problem 1 (Difficulty: easy)</h3>
<p>Among the six models implemented in this tutorial, select the most appropriate model for small data (&lt; 100 samples) and explain your reasoning.</p>
<details>
<summary>Hint</summary>

Consider overfitting risk and model complexity.

</details>

<details>
<summary>Solution</summary>

**Answer: Linear Regression**

**Reasoning:**
1. **Low overfitting risk**: Few parameters, stable with limited data
2. **High interpretability**: Can understand feature influence from coefficients
3. **Fast training**: Low computational cost

**Other candidate: SVR**
- SVR effective when nonlinearity is strong
- However, requires hyperparameter tuning

With small data, complex models (Random Forest, MLP) memorize training data and perform poorly on new data (overfitting).

</details>

<hr />
<h3 id="problem-2-difficulty-medium">Problem 2 (Difficulty: medium)</h3>
<p>Compare Grid Search and Random Search and explain in what situations each method should be used.</p>
<details>
<summary>Hint</summary>

Consider search space size and computation time constraints.

</details>

<details>
<summary>Solution</summary>

**When to use Grid Search:**
1. **Few hyperparameters to search** (2-3)
2. **Few candidates per parameter** (about 3-5 each)
3. **Ample computation time**
4. **Need to find best solution for sure**

**Example:** n_estimators=[50, 100, 200] √ó max_depth=[5, 10, 15] = 9 patterns

**When to use Random Search:**
1. **Many hyperparameters to search** (4+)
2. **Many candidates per parameter/continuous values**
3. **Limited computation time**
4. **Good enough solution is sufficient**

**Example:** 5 parameters, 10 candidates each = 100,000 patterns ‚Üí Sample 100 with Random Search

**General strategy:**
1. First narrow down rough range with Random Search (100-200 iterations)
2. Detailed search of promising range with Grid Search

</details>

<hr />
<h3 id="problem-3-difficulty-medium">Problem 3 (Difficulty: medium)</h3>
<p>The following error occurred. Explain the cause and solution.</p>
<pre class="codehilite"><code>ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
</code></pre>

<details>
<summary>Hint</summary>

Error occurs in MLPRegressor training.

</details>

<details>
<summary>Solution</summary>

**Cause:**
MLPRegressor (neural network) training did not converge within specified iterations (max_iter).

**Possible factors:**
1. max_iter too small (default 200)
2. Learning rate too small (slow learning)
3. Inappropriate data scale (not standardized)
4. Model too complex (too many layers, too many neurons)

**Solutions:**

**Method 1: Increase max_iter**

<pre class="codehilite"><code class="language-python">model_mlp = MLPRegressor(max_iter=1000)  # Default 200‚Üí1000
</code></pre>



**Method 2: Standardize data**

<pre class="codehilite"><code class="language-python">from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
</code></pre>



**Method 3: Adjust learning rate**

<pre class="codehilite"><code class="language-python">model_mlp = MLPRegressor(
    learning_rate_init=0.01,  # Increase learning rate
    max_iter=500
)
</code></pre>



**Method 4: Enable Early Stopping**

<pre class="codehilite"><code class="language-python">model_mlp = MLPRegressor(
    early_stopping=True,  # Stop if validation error stops improving
    validation_fraction=0.2,
    max_iter=1000
)
</code></pre>



**Recommended approach:**
First try Method 2 (data standardization), then combine Methods 1 and 4 if still not converging.

</details>

<hr />
<h3 id="problem-4-difficulty-hard">Problem 4 (Difficulty: hard)</h3>
<p>Write code to extract 5+ features from composition <code>"Li2O"</code> using Matminer.</p>
<details>
<summary>Hint</summary>

Use `ElementProperty` featurizer and `from_preset('magpie')`.

</details>

<details>
<summary>Solution</summary>


<pre class="codehilite"><code class="language-python">from matminer.featurizers.composition import ElementProperty
from pymatgen.core import Composition
import pandas as pd

# Create composition object
comp = Composition(&quot;Li2O&quot;)

# Initialize feature extractor with Magpie preset
featurizer = ElementProperty.from_preset('magpie')

# Calculate features
features = featurizer.featurize(comp)

# Get feature names
feature_names = featurizer.feature_labels()

# Convert to DataFrame (for readability)
df = pd.DataFrame([features], columns=feature_names)

print(f&quot;===== Li2O Features (first 5) =====&quot;)
for i in range(5):
    print(f&quot;{feature_names[i]}: {features[i]:.4f}&quot;)

print(f&quot;\nTotal feature count: {len(features)}&quot;)
</code></pre>



**Expected output:**

<pre class="codehilite"><code>===== Li2O Features (first 5) =====
MagpieData minimum Number: 3.0000
MagpieData maximum Number: 8.0000
MagpieData range Number: 5.0000
MagpieData mean Number: 5.3333
MagpieData avg_dev Number: 1.5556

Total feature count: 132
</code></pre>



**Explanation:**
- `MagpieData minimum Number`: Minimum atomic number (Li: 3)
- `MagpieData maximum Number`: Maximum atomic number (O: 8)
- `MagpieData range Number`: Range of atomic numbers (8-3=5)
- `MagpieData mean Number`: Average atomic number ((3+3+8)/3=5.33)
- `MagpieData avg_dev Number`: Average deviation of atomic numbers

Matminer automatically extracts 132 features (electronegativity, atomic radius, melting point, etc.).

</details>

<hr />
<h3 id="problem-5-difficulty-hard">Problem 5 (Difficulty: hard)</h3>
<p>Band gap project only achieved R¬≤=0.5. Propose 3 specific approaches to improve performance and explain implementation methods for each.</p>
<details>
<summary>Hint</summary>

Consider from three perspectives: features, models, and hyperparameters.

</details>

<details>
<summary>Solution</summary>

**Approach 1: Feature Engineering (Most Effective)**

**Implementation:**

<pre class="codehilite"><code class="language-python">from matminer.featurizers.composition import ElementProperty
from pymatgen.core import Composition

# Extract atomic properties from composition
def extract_features(formula):
    comp = Composition(formula)
    featurizer = ElementProperty.from_preset('magpie')
    features = featurizer.featurize(comp)
    return features

# Add features to existing data
data_project['features'] = data_project['formula'].apply(extract_features)
# Expand to DataFrame (132-dimensional features)
features_df = pd.DataFrame(data_project['features'].tolist())
X_enhanced = features_df  # Original 2D ‚Üí expanded to 132D
</code></pre>



**Expected improvement:**
R¬≤ 0.5 ‚Üí 0.75-0.85 (significant feature increase)

---

**Approach 2: Ensemble Learning (Combine Multiple Models)**

**Implementation:**

<pre class="codehilite"><code class="language-python">from sklearn.ensemble import VotingRegressor

# Combine 3 models
model_rf = RandomForestRegressor(n_estimators=200, random_state=42)
model_lgb = lgb.LGBMRegressor(n_estimators=200, random_state=42)
model_svr = SVR(kernel='rbf', C=100)

# Ensemble model (average predictions)
ensemble = VotingRegressor([
    ('rf', model_rf),
    ('lgb', model_lgb),
    ('svr', model_svr)
])

ensemble.fit(X_train, y_train)
y_pred_ensemble = ensemble.predict(X_test)
</code></pre>



**Expected improvement:**
R¬≤ 0.5 ‚Üí 0.6-0.7 (more stable than single model)

---

**Approach 3: Hyperparameter Tuning**

**Implementation:**

<pre class="codehilite"><code class="language-python">from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_dist = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(10, 50),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10)
}

random_search = RandomizedSearchCV(
    RandomForestRegressor(random_state=42),
    param_distributions=param_dist,
    n_iter=100,  # Try 100 combinations
    cv=5,
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    random_state=42
)

random_search.fit(X_train, y_train)
best_model = random_search.best_estimator_
</code></pre>



**Expected improvement:**
R¬≤ 0.5 ‚Üí 0.55-0.65 (optimized from default)

---

**Optimal strategy:**
1. First implement **Approach 1** (feature engineering) ‚Üí Maximum effect
2. Then **Approach 3** (hyperparameter tuning) for fine-tuning
3. Finally **Approach 2** (ensemble) for final performance boost

With this sequence, aim for R¬≤ 0.5 ‚Üí 0.8+.

</details>

<hr />
<h2 id="references">References</h2>
<ol>
<li>
<p>Pedregosa, F., et al. (2011). "Scikit-learn: Machine Learning in Python." <em>Journal of Machine Learning Research</em>, 12, 2825-2830.<br />
   URL: https://scikit-learn.org<br />
<em>Official scikit-learn documentation. Detailed explanations and tutorials for all algorithms.</em></p>
</li>
<li>
<p>Ward, L., et al. (2018). "Matminer: An open source toolkit for materials data mining." <em>Computational Materials Science</em>, 152, 60-69.<br />
   DOI: <a href="https://doi.org/10.1016/j.commatsci.2018.05.018">10.1016/j.commatsci.2018.05.018</a><br />
   GitHub: https://github.com/hackingmaterials/matminer<br />
<em>Feature extraction library for materials science. Automatically generates 132 materials descriptors.</em></p>
</li>
<li>
<p>Jain, A., et al. (2013). "Commentary: The Materials Project: A materials genome approach to accelerating materials innovation." <em>APL Materials</em>, 1(1), 011002.<br />
   DOI: <a href="https://doi.org/10.1063/1.4812323">10.1063/1.4812323</a><br />
   URL: https://materialsproject.org<br />
<em>Official Materials Project paper. Database of 140,000+ materials.</em></p>
</li>
<li>
<p>Ke, G., et al. (2017). "LightGBM: A Highly Efficient Gradient Boosting Decision Tree." <em>Advances in Neural Information Processing Systems</em>, 30, 3146-3154.<br />
   GitHub: https://github.com/microsoft/LightGBM<br />
<em>Official LightGBM paper. Fast implementation of gradient boosting.</em></p>
</li>
<li>
<p>Bergstra, J., &amp; Bengio, Y. (2012). "Random Search for Hyper-Parameter Optimization." <em>Journal of Machine Learning Research</em>, 13, 281-305.<br />
   URL: https://www.jmlr.org/papers/v13/bergstra12a.html<br />
<em>Theoretical background of Random Search. More efficient search method than Grid Search.</em></p>
</li>
<li>
<p>Raschka, S., &amp; Mirjalili, V. (2019). <em>Python Machine Learning, 3rd Edition</em>. Packt Publishing.<br />
<em>Comprehensive machine learning textbook in Python. Detailed practical usage of scikit-learn.</em></p>
</li>
<li>
<p>scikit-learn User Guide. (2024). "Hyperparameter tuning."<br />
   URL: https://scikit-learn.org/stable/modules/grid_search.html<br />
<em>Official guide for hyperparameter tuning. Details on Grid Search and Random Search.</em></p>
</li>
</ol>
<hr />
<p><strong>Created</strong>: 2025-10-16<br />
<strong>Version</strong>: 3.0<br />
<strong>Template</strong>: content_agent_prompts.py v1.0<br />
<strong>Author</strong>: MI Knowledge Hub Project</p>

        <div class="nav-buttons">
            <a href="index.html" class="nav-button">‚Üê Back to Series Index</a>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 MI Knowledge Hub - Dr. Yusuke Hashimoto, Tohoku University</p>
            <p>Licensed under CC BY 4.0</p>
        </div>
    </footer>
</body>
</html>
