---
title: "ç¬¬3ç« ï¼šPythonã§ä½“é¨“ã™ã‚‹PI - ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–å®Ÿè·µ"
subtitle: "åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–ã®å®Ÿè£…ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹"
level: "intermediate"
difficulty: "ä¸­ç´š"
target_audience: "undergraduate-graduate-students"
estimated_time: "35-40åˆ†"
learning_objectives:
  - Pythonç’°å¢ƒã‚’æ§‹ç¯‰ã—ã€PIç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã‚‹
  - ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨å¯è¦–åŒ–ãŒã§ãã‚‹
  - 5ç¨®é¡ä»¥ä¸Šã®å›å¸°ãƒ¢ãƒ‡ãƒ«ã§ãƒ—ãƒ­ã‚»ã‚¹ç‰¹æ€§ã‚’äºˆæ¸¬ã§ãã‚‹
  - ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚Šãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶ã‚’æœ€é©åŒ–ã§ãã‚‹
  - å¤šç›®çš„æœ€é©åŒ–ã§è¤‡æ•°ã®ç›®æ¨™ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è§£æã§ãã‚‹
topics: ["python", "process-optimization", "bayesian-optimization", "multi-objective", "hands-on"]
prerequisites: ["åŸºç¤Python", "NumPy/PandasåŸºç¤", "åŸºç¤çµ±è¨ˆå­¦"]
series: "PIå…¥é–€ã‚·ãƒªãƒ¼ã‚º v1.0"
series_order: 3
version: "1.0"
created_at: "2025-10-16"
---

# ç¬¬3ç« ï¼šPythonã§ä½“é¨“ã™ã‚‹PI - ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–å®Ÿè·µ

## å­¦ç¿’ç›®æ¨™

ã“ã®è¨˜äº‹ã‚’èª­ã‚€ã“ã¨ã§ã€ä»¥ä¸‹ã‚’ç¿’å¾—ã§ãã¾ã™ï¼š
- Pythonç’°å¢ƒã‚’æ§‹ç¯‰ã—ã€PIç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã‚‹
- ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨å¯è¦–åŒ–ãŒã§ãã‚‹
- 5ç¨®é¡ä»¥ä¸Šã®å›å¸°ãƒ¢ãƒ‡ãƒ«ã§ãƒ—ãƒ­ã‚»ã‚¹ç‰¹æ€§ã‚’äºˆæ¸¬ã§ãã‚‹
- ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚Šãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶ã‚’æœ€é©åŒ–ã§ãã‚‹
- å¤šç›®çš„æœ€é©åŒ–ã§è¤‡æ•°ã®ç›®æ¨™ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è§£æã§ãã‚‹
- ã‚¨ãƒ©ãƒ¼ã‚’è‡ªåŠ›ã§ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã§ãã‚‹

---

## 1. ç’°å¢ƒæ§‹ç¯‰ï¼š3ã¤ã®é¸æŠè‚¢

åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–ã®Pythonç’°å¢ƒã‚’æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã¯ã€çŠ¶æ³ã«å¿œã˜ã¦3ã¤ã‚ã‚Šã¾ã™ã€‚

### 1.1 Option 1: Anacondaï¼ˆæ¨å¥¨åˆå¿ƒè€…ï¼‰

**ç‰¹å¾´ï¼š**
- ç§‘å­¦è¨ˆç®—ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæœ€åˆã‹ã‚‰æƒã£ã¦ã„ã‚‹
- ç’°å¢ƒç®¡ç†ãŒç°¡å˜ï¼ˆGUIåˆ©ç”¨å¯èƒ½ï¼‰
- Windows/Mac/Linuxå¯¾å¿œ

**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †ï¼š**

```bash
# 1. Anacondaã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# å…¬å¼ã‚µã‚¤ãƒˆ: https://www.anaconda.com/download
# Python 3.11ä»¥ä¸Šã‚’é¸æŠ

# 2. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã€Anaconda Promptã‚’èµ·å‹•

# 3. ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆï¼ˆPIå°‚ç”¨ç’°å¢ƒï¼‰
conda create -n pi-env python=3.11 numpy pandas matplotlib scikit-learn jupyter scipy

# 4. ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–
conda activate pi-env

# 5. è¿½åŠ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
conda install -c conda-forge lightgbm scikit-optimize pymoo

# 6. å‹•ä½œç¢ºèª
python --version
# å‡ºåŠ›: Python 3.11.x
```

**Anacondaã®åˆ©ç‚¹ï¼š**
- âœ… NumPyã€SciPyãªã©ãŒæœ€åˆã‹ã‚‰å«ã¾ã‚Œã‚‹
- âœ… ä¾å­˜é–¢ä¿‚ã®å•é¡ŒãŒå°‘ãªã„
- âœ… Anaconda Navigatorã§è¦–è¦šçš„ã«ç®¡ç†å¯èƒ½
- âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„ï¼ˆ3GBä»¥ä¸Šï¼‰

### 1.2 Option 2: venvï¼ˆPythonæ¨™æº–ï¼‰

**ç‰¹å¾´ï¼š**
- Pythonæ¨™æº–ãƒ„ãƒ¼ãƒ«ï¼ˆè¿½åŠ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼‰
- è»½é‡ï¼ˆå¿…è¦ãªã‚‚ã®ã ã‘ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã«ç’°å¢ƒã‚’åˆ†é›¢

**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †ï¼š**

```bash
# 1. Python 3.11ä»¥ä¸ŠãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
python3 --version
# å‡ºåŠ›: Python 3.11.x ä»¥ä¸ŠãŒå¿…è¦

# 2. ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆ
python3 -m venv pi-env

# 3. ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–
# macOS/Linux:
source pi-env/bin/activate

# Windows (PowerShell):
pi-env\Scripts\Activate.ps1

# Windows (Command Prompt):
pi-env\Scripts\activate.bat

# 4. pipã‚’ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
pip install --upgrade pip

# 5. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install numpy pandas matplotlib scikit-learn scipy jupyter
pip install lightgbm scikit-optimize pymoo

# 6. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
pip list
```

**venvã®åˆ©ç‚¹ï¼š**
- âœ… è»½é‡ï¼ˆæ•°åMBï¼‰
- âœ… Pythonæ¨™æº–ãƒ„ãƒ¼ãƒ«ï¼ˆè¿½åŠ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼‰
- âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã«ç‹¬ç«‹
- âŒ ä¾å­˜é–¢ä¿‚ã‚’æ‰‹å‹•ã§è§£æ±ºã™ã‚‹å¿…è¦ãŒã‚ã‚‹

### 1.3 Option 3: Google Colabï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼‰

**ç‰¹å¾´ï¼š**
- ãƒ–ãƒ©ã‚¦ã‚¶ã ã‘ã§å®Ÿè¡Œå¯èƒ½
- ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰å®Ÿè¡Œï¼‰
- GPU/TPUãŒç„¡æ–™ã§ä½¿ãˆã‚‹

**ä½¿ç”¨æ–¹æ³•ï¼š**

```
1. Google Colabã«ã‚¢ã‚¯ã‚»ã‚¹: https://colab.research.google.com
2. æ–°ã—ã„ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½œæˆ
3. ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œï¼ˆå¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯è‡ªå‹•ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ï¼‰
```

```python
# Google Colabã§ã¯æœ€åˆã‹ã‚‰ä»¥ä¸‹ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

# è¿½åŠ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª
!pip install scikit-optimize pymoo lightgbm

print("ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
print(f"NumPy version: {np.__version__}")
print(f"Pandas version: {pd.__version__}")
```

**Google Colabã®åˆ©ç‚¹ï¼š**
- âœ… ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼ˆã™ãé–‹å§‹å¯èƒ½ï¼‰
- âœ… ç„¡æ–™ã§GPUåˆ©ç”¨å¯èƒ½
- âœ… Google Driveã¨é€£æºï¼ˆãƒ‡ãƒ¼ã‚¿ä¿å­˜ãŒç°¡å˜ï¼‰
- âŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šãŒå¿…é ˆ
- âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒ12æ™‚é–“ã§ãƒªã‚»ãƒƒãƒˆã•ã‚Œã‚‹

### 1.4 ç’°å¢ƒé¸æŠã‚¬ã‚¤ãƒ‰

| çŠ¶æ³ | æ¨å¥¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ | ç†ç”± |
|------|----------------|------|
| åˆã‚ã¦ã®Pythonç’°å¢ƒ | Anaconda | ç’°å¢ƒæ§‹ç¯‰ãŒç°¡å˜ã€ãƒˆãƒ©ãƒ–ãƒ«ãŒå°‘ãªã„ |
| æ—¢ã«Pythonç’°å¢ƒãŒã‚ã‚‹ | venv | è»½é‡ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã«ç‹¬ç«‹ |
| ä»Šã™ãè©¦ã—ãŸã„ | Google Colab | ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ã€å³åº§ã«é–‹å§‹å¯èƒ½ |
| å¤§è¦æ¨¡æœ€é©åŒ–ãŒå¿…è¦ | Anaconda or venv | ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ãªã— |
| ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç’°å¢ƒ | Anaconda or venv | ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¸è¦ |

### 1.5 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¤œè¨¼ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

**æ¤œè¨¼ã‚³ãƒãƒ³ãƒ‰ï¼š**

```python
# ã™ã¹ã¦ã®ç’°å¢ƒã§å®Ÿè¡Œå¯èƒ½
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn
import scipy

print("===== ç’°å¢ƒç¢ºèª =====")
print(f"Python version: {sys.version}")
print(f"NumPy version: {np.__version__}")
print(f"Pandas version: {pd.__version__}")
print(f"Matplotlib version: {plt.matplotlib.__version__}")
print(f"scikit-learn version: {sklearn.__version__}")
print(f"SciPy version: {scipy.__version__}")

# PIç‰¹æœ‰ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç¢ºèª
try:
    import skopt
    print(f"scikit-optimize version: {skopt.__version__}")
except ImportError:
    print("âš ï¸ scikit-optimizeæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆpip install scikit-optimizeï¼‰")

try:
    import pymoo
    print(f"pymoo version: {pymoo.__version__}")
except ImportError:
    print("âš ï¸ pymooæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆpip install pymooï¼‰")

print("\nâœ… åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæ­£å¸¸ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™ï¼")
```

**ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºæ–¹æ³•ï¼š**

| ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ | åŸå›  | è§£æ±ºæ–¹æ³• |
|------------------|------|----------|
| `ModuleNotFoundError: No module named 'skopt'` | scikit-optimizeæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« | `pip install scikit-optimize` ã‚’å®Ÿè¡Œ |
| `ImportError: DLL load failed` (Windows) | C++å†é ’å¸ƒå¯èƒ½ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¸è¶³ | Microsoft Visual C++ Redistributableã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« |
| `SSL: CERTIFICATE_VERIFY_FAILED` | SSLè¨¼æ˜æ›¸ã‚¨ãƒ©ãƒ¼ | `pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org <package>` |
| `MemoryError` | ãƒ¡ãƒ¢ãƒªä¸è¶³ | ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’å‰Šæ¸› or Google Colabåˆ©ç”¨ |

---

## 2. ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã¨å¯è¦–åŒ–

åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ã®å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’æ¨¡æ“¬ã—ã€å‰å‡¦ç†ã¨å¯è¦–åŒ–ã‚’è¡Œã„ã¾ã™ã€‚

### 2.1 Example 1: ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã¨èª­ã¿è¾¼ã¿

**æ¦‚è¦ï¼š**
åŒ–å­¦åå¿œãƒ—ãƒ­ã‚»ã‚¹ã®æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™ï¼ˆæ¸©åº¦ã€åœ§åŠ›ã€è§¦åª’é‡ â†’ åç‡ï¼‰ã€‚

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import time

# åŒ–å­¦åå¿œãƒ—ãƒ­ã‚»ã‚¹ã®æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
n_samples = 200

# ãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶ï¼ˆå…¥åŠ›å¤‰æ•°ï¼‰
temperature = np.random.uniform(300, 500, n_samples)  # æ¸©åº¦ [K]
pressure = np.random.uniform(1, 10, n_samples)  # åœ§åŠ› [bar]
catalyst = np.random.uniform(0.1, 5.0, n_samples)  # è§¦åª’é‡ [wt%]

# åç‡ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆéç·šå½¢é–¢ä¿‚ + ãƒã‚¤ã‚ºï¼‰
# åç‡ = f(æ¸©åº¦, åœ§åŠ›, è§¦åª’é‡) + ãƒã‚¤ã‚º
yield_percentage = (
    20  # ãƒ™ãƒ¼ã‚¹åç‡
    + 0.15 * temperature  # æ¸©åº¦ã®åŠ¹æœï¼ˆæ­£ã®ç›¸é–¢ï¼‰
    - 0.0002 * temperature**2  # æ¸©åº¦ã®äºŒæ¬¡é …ï¼ˆæœ€é©æ¸©åº¦ãŒå­˜åœ¨ï¼‰
    + 5.0 * pressure  # åœ§åŠ›ã®åŠ¹æœï¼ˆæ­£ã®ç›¸é–¢ï¼‰
    + 3.0 * catalyst  # è§¦åª’é‡ã®åŠ¹æœï¼ˆæ­£ã®ç›¸é–¢ï¼‰
    - 0.3 * catalyst**2  # è§¦åª’é‡ã®äºŒæ¬¡é …ï¼ˆéå‰°æ·»åŠ ã§åŠ¹æœæ¸›å°‘ï¼‰
    + 0.01 * temperature * pressure  # æ¸©åº¦-åœ§åŠ›ã®ç›¸äº’ä½œç”¨
    + np.random.normal(0, 3, n_samples)  # ãƒã‚¤ã‚ºï¼ˆæ¸¬å®šèª¤å·®ï¼‰
)

# ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«æ ¼ç´
process_data = pd.DataFrame({
    'temperature_K': temperature,
    'pressure_bar': pressure,
    'catalyst_wt%': catalyst,
    'yield_%': yield_percentage
})

print("===== ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª =====")
print(process_data.head(10))
print(f"\nãƒ‡ãƒ¼ã‚¿æ•°: {len(process_data)}ä»¶")
print(f"\nåŸºæœ¬çµ±è¨ˆé‡:")
print(process_data.describe())

# CSVå½¢å¼ã§ä¿å­˜ï¼ˆå®Ÿéš›ã®ãƒ—ãƒ­ã‚»ã‚¹ã§ã¯ã“ã®ã‚ˆã†ãªå½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ãŒæä¾›ã•ã‚Œã‚‹ï¼‰
process_data.to_csv('process_data.csv', index=False)
print("\nâœ… ãƒ‡ãƒ¼ã‚¿ã‚’ process_data.csv ã«ä¿å­˜ã—ã¾ã—ãŸ")
```

**ã‚³ãƒ¼ãƒ‰è§£èª¬ï¼š**
1. **ãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶**ï¼šæ¸©åº¦ï¼ˆ300-500 Kï¼‰ã€åœ§åŠ›ï¼ˆ1-10 barï¼‰ã€è§¦åª’é‡ï¼ˆ0.1-5.0 wt%ï¼‰
2. **åç‡ãƒ¢ãƒ‡ãƒ«**ï¼šéç·šå½¢é–¢ä¿‚ï¼ˆäºŒæ¬¡é …ã€ç›¸äº’ä½œç”¨é …ï¼‰+ ãƒã‚¤ã‚º
3. **å®Ÿãƒ‡ãƒ¼ã‚¿æ¨¡æ“¬**ï¼šåŒ–å­¦åå¿œã®å…¸å‹çš„ãªæŒ™å‹•ï¼ˆæœ€é©æ¡ä»¶ãŒå­˜åœ¨ã€éå‰°æ·»åŠ ã§åŠ¹æœæ¸›å°‘ï¼‰

### 2.2 Example 2: ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ï¼ˆæ•£å¸ƒå›³è¡Œåˆ—ï¼‰

```python
import seaborn as sns

# æ•£å¸ƒå›³è¡Œåˆ—ã§å¤‰æ•°é–“ã®é–¢ä¿‚ã‚’ç¢ºèª
fig = plt.figure(figsize=(12, 10))
sns.pairplot(
    process_data,
    diag_kind='hist',  # å¯¾è§’ç·šã«ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    plot_kws={'alpha': 0.6, 's': 50},  # æ•£å¸ƒå›³ã®è¨­å®š
    diag_kws={'bins': 20, 'edgecolor': 'black'}  # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®è¨­å®š
)
plt.suptitle('ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã®æ•£å¸ƒå›³è¡Œåˆ—', y=1.01, fontsize=16)
plt.tight_layout()
plt.show()

print("===== ç›¸é–¢ä¿‚æ•°è¡Œåˆ— =====")
correlation_matrix = process_data.corr()
print(correlation_matrix)

# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§ç›¸é–¢ã‚’å¯è¦–åŒ–
plt.figure(figsize=(8, 6))
sns.heatmap(
    correlation_matrix,
    annot=True,  # æ•°å€¤ã‚’è¡¨ç¤º
    fmt='.3f',  # å°æ•°ç‚¹3æ¡
    cmap='coolwarm',  # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—
    center=0,  # 0ã‚’ä¸­å¿ƒã«ã™ã‚‹
    square=True,  # æ­£æ–¹å½¢ã®ã‚»ãƒ«
    linewidths=1,  # ã‚»ãƒ«ã®å¢ƒç•Œç·š
    cbar_kws={'label': 'ç›¸é–¢ä¿‚æ•°'}
)
plt.title('ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ã®ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—', fontsize=14)
plt.tight_layout()
plt.show()
```

**è§£é‡ˆã®ãƒã‚¤ãƒ³ãƒˆï¼š**
- æ¸©åº¦ã¨åç‡ã®é–¢ä¿‚ï¼šæ›²ç·šçš„ï¼ˆæœ€é©æ¸©åº¦ãŒå­˜åœ¨ï¼‰
- åœ§åŠ›ã¨åç‡ã®é–¢ä¿‚ï¼šæ­£ã®ç›¸é–¢ï¼ˆåœ§åŠ›â†‘ â†’ åç‡â†‘ï¼‰
- è§¦åª’é‡ã¨åç‡ã®é–¢ä¿‚ï¼šæ›²ç·šçš„ï¼ˆéå‰°æ·»åŠ ã§åŠ¹æœæ¸›å°‘ï¼‰

### 2.3 Example 3: ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ï¼ˆæ¬ æå€¤ãƒ»å¤–ã‚Œå€¤å‡¦ç†ï¼‰

```python
# æ¬ æå€¤ã®ç¢ºèªã¨å‡¦ç†
print("===== æ¬ æå€¤ã®ç¢ºèª =====")
print(process_data.isnull().sum())

# äººç‚ºçš„ã«æ¬ æå€¤ã‚’è¿½åŠ ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã§ã¯é »ç¹ã«ç™ºç”Ÿï¼‰
process_data_with_missing = process_data.copy()
missing_indices = np.random.choice(process_data.index, size=10, replace=False)
process_data_with_missing.loc[missing_indices, 'catalyst_wt%'] = np.nan

print(f"\næ¬ æå€¤ã‚’è¿½åŠ å¾Œ: {process_data_with_missing.isnull().sum()['catalyst_wt%']}ä»¶")

# æ¬ æå€¤ã®è£œå®Œï¼ˆå¹³å‡å€¤ã§è£œå®Œï¼‰
process_data_filled = process_data_with_missing.copy()
process_data_filled['catalyst_wt%'].fillna(
    process_data_filled['catalyst_wt%'].mean(),
    inplace=True
)

print("âœ… æ¬ æå€¤ã‚’å¹³å‡å€¤ã§è£œå®Œã—ã¾ã—ãŸ")

# å¤–ã‚Œå€¤ã®æ¤œå‡ºï¼ˆZã‚¹ã‚³ã‚¢æ³•ï¼‰
from scipy import stats

z_scores = np.abs(stats.zscore(process_data[['yield_%']]))
outliers = (z_scores > 3).any(axis=1)

print(f"\n===== å¤–ã‚Œå€¤ã®æ¤œå‡º =====")
print(f"å¤–ã‚Œå€¤ã®æ•°: {outliers.sum()}ä»¶")
print(f"å¤–ã‚Œå€¤ã®å‰²åˆ: {outliers.sum() / len(process_data) * 100:.1f}%")

# å¤–ã‚Œå€¤ã‚’é™¤å»
process_data_clean = process_data[~outliers].copy()
print(f"é™¤å»å¾Œã®ãƒ‡ãƒ¼ã‚¿æ•°: {len(process_data_clean)}ä»¶")
```

### 2.4 Example 4: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆç›¸äº’ä½œç”¨é …ï¼‰

```python
# ãƒ—ãƒ­ã‚»ã‚¹ç‰¹æœ‰ã®ç‰¹å¾´é‡ã‚’ä½œæˆ
process_data_enhanced = process_data_clean.copy()

# ç›¸äº’ä½œç”¨é …ï¼ˆæ¸©åº¦Ã—åœ§åŠ›ã€æ¸©åº¦Ã—è§¦åª’é‡ï¼‰
process_data_enhanced['temp_pressure'] = (
    process_data_enhanced['temperature_K'] * process_data_enhanced['pressure_bar']
)
process_data_enhanced['temp_catalyst'] = (
    process_data_enhanced['temperature_K'] * process_data_enhanced['catalyst_wt%']
)

# äºŒæ¬¡é …ï¼ˆæ¸©åº¦Â², è§¦åª’é‡Â²ï¼‰
process_data_enhanced['temp_squared'] = process_data_enhanced['temperature_K'] ** 2
process_data_enhanced['catalyst_squared'] = process_data_enhanced['catalyst_wt%'] ** 2

# æ¯”ç‡ï¼ˆè§¦åª’/åœ§åŠ›ï¼‰
process_data_enhanced['catalyst_pressure_ratio'] = (
    process_data_enhanced['catalyst_wt%'] / (process_data_enhanced['pressure_bar'] + 1e-10)
)

print("===== ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ =====")
print(process_data_enhanced.head())
print(f"\nç‰¹å¾´é‡æ•°: {len(process_data_enhanced.columns) - 1}å€‹ï¼ˆå…ƒã®3å€‹ â†’ 8å€‹ï¼‰")
```

### 2.5 Example 5: ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆè¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆï¼‰

```python
# ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®åˆ†é›¢
X = process_data_enhanced.drop('yield_%', axis=1)  # å…¥åŠ›ï¼šãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶
y = process_data_enhanced['yield_%']  # å‡ºåŠ›ï¼šåç‡

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²ï¼ˆ80% vs 20%ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("===== ãƒ‡ãƒ¼ã‚¿åˆ†å‰² =====")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}ä»¶")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}ä»¶")
print(f"\nç‰¹å¾´é‡ã®åˆ—å:")
print(list(X.columns))
```

### 2.6 Example 6: ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–

```python
from sklearn.preprocessing import StandardScaler

# æ¨™æº–åŒ–å™¨ã®ä½œæˆï¼ˆå¹³å‡0ã€æ¨™æº–åå·®1ã«å¤‰æ›ï¼‰
scaler = StandardScaler()

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§æ¨™æº–åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›

print("===== æ¨™æº–åŒ– =====")
print("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¨™æº–åŒ–å‰ï¼‰:")
print(f"  æ¸©åº¦ã®å¹³å‡: {X_train['temperature_K'].mean():.1f} K")
print(f"  æ¸©åº¦ã®æ¨™æº–åå·®: {X_train['temperature_K'].std():.1f} K")

print("\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¨™æº–åŒ–å¾Œï¼‰:")
print(f"  æ¸©åº¦ã®å¹³å‡: {X_train_scaled[:, 0].mean():.3f}")
print(f"  æ¸©åº¦ã®æ¨™æº–åå·®: {X_train_scaled[:, 0].std():.3f}")

print("\nâœ… æ¨™æº–åŒ–ã«ã‚ˆã‚Šã€ã™ã¹ã¦ã®ç‰¹å¾´é‡ãŒå¹³å‡0ã€æ¨™æº–åå·®1ã«ãªã‚Šã¾ã—ãŸ")
```

### 2.7 Example 7: æ™‚ç³»åˆ—ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–

```python
# æ™‚ç³»åˆ—ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆãƒãƒƒãƒãƒ—ãƒ­ã‚»ã‚¹ã®ä¾‹ï¼‰
np.random.seed(42)
time_hours = np.arange(0, 24, 0.5)  # 24æ™‚é–“ã€0.5æ™‚é–“é–“éš”
n_points = len(time_hours)

# ãƒ—ãƒ­ã‚»ã‚¹å¤‰æ•°ã®æ™‚ç³»åˆ—å¤‰åŒ–
temp_time = 350 + 50 * np.sin(2 * np.pi * time_hours / 24) + np.random.normal(0, 2, n_points)
pressure_time = 5 + 2 * np.sin(2 * np.pi * time_hours / 12 + np.pi/4) + np.random.normal(0, 0.3, n_points)
yield_time = 60 + 10 * np.sin(2 * np.pi * time_hours / 24 - np.pi/2) + np.random.normal(0, 1.5, n_points)

# å¯è¦–åŒ–
fig, axes = plt.subplots(3, 1, figsize=(12, 10))

axes[0].plot(time_hours, temp_time, 'r-', linewidth=2, label='æ¸©åº¦')
axes[0].set_ylabel('æ¸©åº¦ [K]', fontsize=12)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

axes[1].plot(time_hours, pressure_time, 'b-', linewidth=2, label='åœ§åŠ›')
axes[1].set_ylabel('åœ§åŠ› [bar]', fontsize=12)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

axes[2].plot(time_hours, yield_time, 'g-', linewidth=2, label='åç‡')
axes[2].set_xlabel('æ™‚é–“ [h]', fontsize=12)
axes[2].set_ylabel('åç‡ [%]', fontsize=12)
axes[2].legend()
axes[2].grid(True, alpha=0.3)

plt.suptitle('ãƒãƒƒãƒãƒ—ãƒ­ã‚»ã‚¹ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿', fontsize=16)
plt.tight_layout()
plt.show()
```

---

## 3. å›å¸°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹åç‡äºˆæ¸¬

ãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶ã‹ã‚‰åç‡ã‚’äºˆæ¸¬ã™ã‚‹5ã¤ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

### 3.1 Example 8: ç·šå½¢å›å¸°ï¼ˆBaselineï¼‰

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
start_time = time.time()
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)
training_time_lr = time.time() - start_time

# äºˆæ¸¬
y_pred_lr = model_lr.predict(X_test)

# è©•ä¾¡
mae_lr = mean_absolute_error(y_test, y_pred_lr)
rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))
r2_lr = r2_score(y_test, y_pred_lr)

print("===== ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ =====")
print(f"è¨“ç·´æ™‚é–“: {training_time_lr:.4f} ç§’")
print(f"å¹³å‡çµ¶å¯¾èª¤å·® (MAE): {mae_lr:.2f} %")
print(f"äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·® (RMSE): {rmse_lr:.2f} %")
print(f"æ±ºå®šä¿‚æ•° (RÂ²): {r2_lr:.4f}")

# å­¦ç¿’ã—ãŸä¿‚æ•°ã‚’è¡¨ç¤º
print("\n===== å­¦ç¿’ã—ãŸä¿‚æ•°ï¼ˆãƒˆãƒƒãƒ—3ï¼‰ =====")
coefficients = pd.DataFrame({
    'ç‰¹å¾´é‡': X.columns,
    'ä¿‚æ•°': model_lr.coef_
}).sort_values('ä¿‚æ•°', key=abs, ascending=False)
print(coefficients.head(3))
```

### 3.2 Example 9: ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°

```python
from sklearn.ensemble import RandomForestRegressor

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
start_time = time.time()
model_rf = RandomForestRegressor(
    n_estimators=100,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)
model_rf.fit(X_train, y_train)
training_time_rf = time.time() - start_time

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred_rf = model_rf.predict(X_test)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)

print("\n===== ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®æ€§èƒ½ =====")
print(f"è¨“ç·´æ™‚é–“: {training_time_rf:.4f} ç§’")
print(f"å¹³å‡çµ¶å¯¾èª¤å·® (MAE): {mae_rf:.2f} %")
print(f"äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·® (RMSE): {rmse_rf:.2f} %")
print(f"æ±ºå®šä¿‚æ•° (RÂ²): {r2_rf:.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦
feature_importance = pd.DataFrame({
    'ç‰¹å¾´é‡': X.columns,
    'é‡è¦åº¦': model_rf.feature_importances_
}).sort_values('é‡è¦åº¦', ascending=False)

print("\n===== ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆãƒˆãƒƒãƒ—3ï¼‰ =====")
print(feature_importance.head(3))
```

### 3.3 Example 10: LightGBMå›å¸°

```python
import lightgbm as lgb

# LightGBMãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
start_time = time.time()
model_lgb = lgb.LGBMRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=10,
    num_leaves=31,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbose=-1
)
model_lgb.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    eval_metric='rmse',
    callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]
)
training_time_lgb = time.time() - start_time

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred_lgb = model_lgb.predict(X_test)
mae_lgb = mean_absolute_error(y_test, y_pred_lgb)
rmse_lgb = np.sqrt(mean_squared_error(y_test, y_pred_lgb))
r2_lgb = r2_score(y_test, y_pred_lgb)

print("\n===== LightGBMã®æ€§èƒ½ =====")
print(f"è¨“ç·´æ™‚é–“: {training_time_lgb:.4f} ç§’")
print(f"å¹³å‡çµ¶å¯¾èª¤å·® (MAE): {mae_lgb:.2f} %")
print(f"äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·® (RMSE): {rmse_lgb:.2f} %")
print(f"æ±ºå®šä¿‚æ•° (RÂ²): {r2_lgb:.4f}")
```

### 3.4 Example 11: ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼å›å¸°ï¼ˆSVRï¼‰

```python
from sklearn.svm import SVR

# SVRãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ï¼ˆæ¨™æº–åŒ–æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
start_time = time.time()
model_svr = SVR(
    kernel='rbf',
    C=100,
    gamma='scale',
    epsilon=0.1
)
model_svr.fit(X_train_scaled, y_train)
training_time_svr = time.time() - start_time

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred_svr = model_svr.predict(X_test_scaled)
mae_svr = mean_absolute_error(y_test, y_pred_svr)
rmse_svr = np.sqrt(mean_squared_error(y_test, y_pred_svr))
r2_svr = r2_score(y_test, y_pred_svr)

print("\n===== SVRã®æ€§èƒ½ =====")
print(f"è¨“ç·´æ™‚é–“: {training_time_svr:.4f} ç§’")
print(f"å¹³å‡çµ¶å¯¾èª¤å·® (MAE): {mae_svr:.2f} %")
print(f"äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·® (RMSE): {rmse_svr:.2f} %")
print(f"æ±ºå®šä¿‚æ•° (RÂ²): {r2_svr:.4f}")
```

### 3.5 Example 12: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆMLPï¼‰

```python
from sklearn.neural_network import MLPRegressor

# MLPãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
start_time = time.time()
model_mlp = MLPRegressor(
    hidden_layer_sizes=(64, 32, 16),
    activation='relu',
    solver='adam',
    alpha=0.001,
    learning_rate_init=0.01,
    max_iter=500,
    random_state=42,
    early_stopping=True,
    validation_fraction=0.2,
    verbose=False
)
model_mlp.fit(X_train_scaled, y_train)
training_time_mlp = time.time() - start_time

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred_mlp = model_mlp.predict(X_test_scaled)
mae_mlp = mean_absolute_error(y_test, y_pred_mlp)
rmse_mlp = np.sqrt(mean_squared_error(y_test, y_pred_mlp))
r2_mlp = r2_score(y_test, y_pred_mlp)

print("\n===== MLPã®æ€§èƒ½ =====")
print(f"è¨“ç·´æ™‚é–“: {training_time_mlp:.4f} ç§’")
print(f"å¹³å‡çµ¶å¯¾èª¤å·® (MAE): {mae_mlp:.2f} %")
print(f"äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·® (RMSE): {rmse_mlp:.2f} %")
print(f"æ±ºå®šä¿‚æ•° (RÂ²): {r2_mlp:.4f}")
print(f"ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°: {model_mlp.n_iter_}")
```

### 3.6 Example 13: ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®æ¯”è¼ƒ

```python
# ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®æ¯”è¼ƒè¡¨
comparison = pd.DataFrame({
    'ãƒ¢ãƒ‡ãƒ«': ['ç·šå½¢å›å¸°', 'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ', 'LightGBM', 'SVR', 'MLP'],
    'MAE (%)': [mae_lr, mae_rf, mae_lgb, mae_svr, mae_mlp],
    'RMSE (%)': [rmse_lr, rmse_rf, rmse_lgb, rmse_svr, rmse_mlp],
    'RÂ²': [r2_lr, r2_rf, r2_lgb, r2_svr, r2_mlp],
    'è¨“ç·´æ™‚é–“ (ç§’)': [training_time_lr, training_time_rf, training_time_lgb,
                  training_time_svr, training_time_mlp]
})

print("\n===== ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ç·åˆæ¯”è¼ƒ =====")
print(comparison.to_string(index=False))

# å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# MAEæ¯”è¼ƒ
axes[0].bar(comparison['ãƒ¢ãƒ‡ãƒ«'], comparison['MAE (%)'],
            color=['blue', 'green', 'orange', 'purple', 'red'])
axes[0].set_ylabel('MAE (%)', fontsize=12)
axes[0].set_title('å¹³å‡çµ¶å¯¾èª¤å·®ï¼ˆå°ã•ã„ã»ã©è‰¯ã„ï¼‰', fontsize=14)
axes[0].tick_params(axis='x', rotation=45)
axes[0].grid(True, alpha=0.3, axis='y')

# RÂ²æ¯”è¼ƒ
axes[1].bar(comparison['ãƒ¢ãƒ‡ãƒ«'], comparison['RÂ²'],
            color=['blue', 'green', 'orange', 'purple', 'red'])
axes[1].set_ylabel('RÂ²', fontsize=12)
axes[1].set_title('æ±ºå®šä¿‚æ•°ï¼ˆ1ã«è¿‘ã„ã»ã©è‰¯ã„ï¼‰', fontsize=14)
axes[1].tick_params(axis='x', rotation=45)
axes[1].grid(True, alpha=0.3, axis='y')

# è¨“ç·´æ™‚é–“æ¯”è¼ƒ
axes[2].bar(comparison['ãƒ¢ãƒ‡ãƒ«'], comparison['è¨“ç·´æ™‚é–“ (ç§’)'],
            color=['blue', 'green', 'orange', 'purple', 'red'])
axes[2].set_ylabel('è¨“ç·´æ™‚é–“ (ç§’)', fontsize=12)
axes[2].set_title('è¨“ç·´æ™‚é–“ï¼ˆçŸ­ã„ã»ã©è‰¯ã„ï¼‰', fontsize=14)
axes[2].tick_params(axis='x', rotation=45)
axes[2].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

### 3.7 Example 14: äºˆæ¸¬vså®Ÿæ¸¬ã®ãƒ—ãƒ­ãƒƒãƒˆ

```python
# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBMï¼‰ã®äºˆæ¸¬çµæœã‚’å¯è¦–åŒ–
plt.figure(figsize=(10, 8))
plt.scatter(y_test, y_pred_lgb, alpha=0.6, s=100, c='green', edgecolors='k', linewidth=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
         'r--', lw=2, label='å®Œå…¨ãªäºˆæ¸¬')
plt.xlabel('å®Ÿæ¸¬åç‡ (%)', fontsize=14)
plt.ylabel('äºˆæ¸¬åç‡ (%)', fontsize=14)
plt.title('LightGBM: åç‡äºˆæ¸¬ã®ç²¾åº¦', fontsize=16)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)

# æ€§èƒ½æŒ‡æ¨™ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¿½åŠ 
textstr = f'RÂ² = {r2_lgb:.3f}\nMAE = {mae_lgb:.2f} %\nRMSE = {rmse_lgb:.2f} %'
plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes,
         fontsize=12, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.show()
```

---

## 4. ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–

### 4.1 Example 15: ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ã‚ˆã‚‹æœ€é©æ¡ä»¶æ¢ç´¢

```python
from scipy.optimize import minimize

# ç›®çš„é–¢æ•°ï¼šåç‡ã‚’æœ€å¤§åŒ–ï¼ˆè² ã®åç‡ã‚’æœ€å°åŒ–ï¼‰
def objective_yield(params):
    """
    params = [temperature, pressure, catalyst]
    """
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å±•é–‹
    temp = params[0]
    press = params[1]
    cat = params[2]

    # ç‰¹å¾´é‡ã‚’æ§‹ç¯‰ï¼ˆè¨“ç·´æ™‚ã¨åŒã˜é †åºï¼‰
    features = np.array([[
        temp, press, cat,
        temp * press,  # temp_pressure
        temp * cat,    # temp_catalyst
        temp**2,       # temp_squared
        cat**2,        # catalyst_squared
        cat / (press + 1e-10)  # catalyst_pressure_ratio
    ]])

    # ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ï¼ˆLightGBMã‚’ä½¿ç”¨ï¼‰
    predicted_yield = model_lgb.predict(features)[0]

    # æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã€è² ã®å€¤ã‚’è¿”ã™
    return -predicted_yield

# åˆ¶ç´„æ¡ä»¶ï¼ˆãƒ—ãƒ­ã‚»ã‚¹ã®æ“ä½œç¯„å›²ï¼‰
bounds = [
    (300, 500),  # æ¸©åº¦ [K]
    (1, 10),     # åœ§åŠ› [bar]
    (0.1, 5.0)   # è§¦åª’é‡ [wt%]
]

# åˆæœŸæ¨æ¸¬å€¤
x0 = [400, 5, 2.5]

# æœ€é©åŒ–å®Ÿè¡Œ
result = minimize(
    objective_yield,
    x0,
    method='L-BFGS-B',
    bounds=bounds
)

print("===== ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ã‚ˆã‚‹æœ€é©åŒ–çµæœ =====")
print(f"æœ€é©æ¡ä»¶:")
print(f"  æ¸©åº¦: {result.x[0]:.1f} K")
print(f"  åœ§åŠ›: {result.x[1]:.2f} bar")
print(f"  è§¦åª’é‡: {result.x[2]:.2f} wt%")
print(f"\næœ€å¤§äºˆæ¸¬åç‡: {-result.fun:.2f} %")
print(f"æœ€é©åŒ–æˆåŠŸ: {result.success}")
print(f"ç¹°ã‚Šè¿”ã—å›æ•°: {result.nit}")
```

### 4.2 Example 16: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼ˆåŠ¹ç‡çš„ãªæ¢ç´¢ï¼‰

```python
from skopt import gp_minimize
from skopt.space import Real
from skopt.utils import use_named_args

# æ¢ç´¢ç©ºé–“ã‚’å®šç¾©
space = [
    Real(300, 500, name='temperature'),
    Real(1, 10, name='pressure'),
    Real(0.1, 5.0, name='catalyst')
]

# ç›®çš„é–¢æ•°ï¼ˆãƒ™ã‚¤ã‚ºæœ€é©åŒ–ç”¨ï¼‰
@use_named_args(space)
def objective_bayes(**params):
    temp = params['temperature']
    press = params['pressure']
    cat = params['catalyst']

    # ç‰¹å¾´é‡ã‚’æ§‹ç¯‰
    features = np.array([[
        temp, press, cat,
        temp * press,
        temp * cat,
        temp**2,
        cat**2,
        cat / (press + 1e-10)
    ]])

    # äºˆæ¸¬åç‡ï¼ˆæœ€å¤§åŒ–ã®ãŸã‚è² ã®å€¤ï¼‰
    predicted_yield = model_lgb.predict(features)[0]
    return -predicted_yield

# ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’å®Ÿè¡Œ
result_bayes = gp_minimize(
    objective_bayes,
    space,
    n_calls=30,  # 30å›ã®è©•ä¾¡
    random_state=42,
    verbose=False
)

print("\n===== ãƒ™ã‚¤ã‚ºæœ€é©åŒ–çµæœ =====")
print(f"æœ€é©æ¡ä»¶:")
print(f"  æ¸©åº¦: {result_bayes.x[0]:.1f} K")
print(f"  åœ§åŠ›: {result_bayes.x[1]:.2f} bar")
print(f"  è§¦åª’é‡: {result_bayes.x[2]:.2f} wt%")
print(f"\næœ€å¤§äºˆæ¸¬åç‡: {-result_bayes.fun:.2f} %")

# æœ€é©åŒ–ã®åæŸå±¥æ­´
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(result_bayes.func_vals) + 1),
         -result_bayes.func_vals, 'b-o', linewidth=2, markersize=6)
plt.axhline(y=-result_bayes.fun, color='r', linestyle='--',
            label=f'æœ€è‰¯å€¤: {-result_bayes.fun:.2f}%')
plt.xlabel('è©•ä¾¡å›æ•°', fontsize=12)
plt.ylabel('äºˆæ¸¬åç‡ (%)', fontsize=12)
plt.title('ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®åæŸå±¥æ­´', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### 4.3 Example 17: å®Ÿé¨“è¨ˆç”»æ³•ï¼ˆDoEï¼‰

```python
from itertools import product

# 2æ°´æº–å®Ÿé¨“è¨ˆç”»ï¼ˆ2^3 = 8å®Ÿé¨“ï¼‰
levels = {
    'temperature': [350, 450],  # ä½æ°´æº–ã€é«˜æ°´æº–
    'pressure': [3, 8],
    'catalyst': [1.0, 4.0]
}

# ã™ã¹ã¦ã®çµ„ã¿åˆã‚ã›ã‚’ç”Ÿæˆ
experiments = list(product(levels['temperature'], levels['pressure'], levels['catalyst']))

# å„å®Ÿé¨“ã®äºˆæ¸¬åç‡ã‚’è¨ˆç®—
results_doe = []
for temp, press, cat in experiments:
    features = np.array([[
        temp, press, cat,
        temp * press,
        temp * cat,
        temp**2,
        cat**2,
        cat / (press + 1e-10)
    ]])
    predicted_yield = model_lgb.predict(features)[0]
    results_doe.append({
        'æ¸©åº¦ [K]': temp,
        'åœ§åŠ› [bar]': press,
        'è§¦åª’é‡ [wt%]': cat,
        'äºˆæ¸¬åç‡ [%]': predicted_yield
    })

# çµæœã‚’DataFrameã«å¤‰æ›
df_doe = pd.DataFrame(results_doe).sort_values('äºˆæ¸¬åç‡ [%]', ascending=False)

print("\n===== å®Ÿé¨“è¨ˆç”»æ³•ï¼ˆ2^3 DoEï¼‰çµæœ =====")
print(df_doe.to_string(index=False))

# æœ€è‰¯æ¡ä»¶ã‚’æŠ½å‡º
best_condition = df_doe.iloc[0]
print(f"\næœ€è‰¯æ¡ä»¶:")
print(f"  æ¸©åº¦: {best_condition['æ¸©åº¦ [K]']:.0f} K")
print(f"  åœ§åŠ›: {best_condition['åœ§åŠ› [bar]']:.0f} bar")
print(f"  è§¦åª’é‡: {best_condition['è§¦åª’é‡ [wt%]']:.1f} wt%")
print(f"  äºˆæ¸¬åç‡: {best_condition['äºˆæ¸¬åç‡ [%]']:.2f} %")
```

### 4.4 Example 18: å¿œç­”æ›²é¢æ³•ï¼ˆResponse Surfaceï¼‰

```python
from scipy.interpolate import griddata

# æ¸©åº¦ã¨åœ§åŠ›ã®ç¯„å›²ã§ã‚°ãƒªãƒƒãƒ‰ã‚’ä½œæˆï¼ˆè§¦åª’é‡ã¯å›ºå®šï¼‰
temp_range = np.linspace(300, 500, 50)
press_range = np.linspace(1, 10, 50)
temp_grid, press_grid = np.meshgrid(temp_range, press_range)

# å„ã‚°ãƒªãƒƒãƒ‰ãƒã‚¤ãƒ³ãƒˆã§åç‡ã‚’äºˆæ¸¬ï¼ˆè§¦åª’é‡ã¯æœ€é©å€¤ã«å›ºå®šï¼‰
catalyst_fixed = result_bayes.x[2]
yield_grid = np.zeros_like(temp_grid)

for i in range(len(temp_range)):
    for j in range(len(press_range)):
        temp = temp_grid[j, i]
        press = press_grid[j, i]
        cat = catalyst_fixed

        features = np.array([[
            temp, press, cat,
            temp * press,
            temp * cat,
            temp**2,
            cat**2,
            cat / (press + 1e-10)
        ]])

        yield_grid[j, i] = model_lgb.predict(features)[0]

# å¿œç­”æ›²é¢ã‚’å¯è¦–åŒ–
fig = plt.figure(figsize=(14, 6))

# ç­‰é«˜ç·šå›³
ax1 = fig.add_subplot(1, 2, 1)
contour = ax1.contourf(temp_grid, press_grid, yield_grid, levels=20, cmap='viridis')
ax1.set_xlabel('æ¸©åº¦ [K]', fontsize=12)
ax1.set_ylabel('åœ§åŠ› [bar]', fontsize=12)
ax1.set_title(f'å¿œç­”æ›²é¢ï¼ˆè§¦åª’é‡ = {catalyst_fixed:.2f} wt%ï¼‰', fontsize=14)
plt.colorbar(contour, ax=ax1, label='äºˆæ¸¬åç‡ [%]')

# 3Dè¡¨é¢
ax2 = fig.add_subplot(1, 2, 2, projection='3d')
surf = ax2.plot_surface(temp_grid, press_grid, yield_grid,
                        cmap='viridis', alpha=0.8)
ax2.set_xlabel('æ¸©åº¦ [K]', fontsize=10)
ax2.set_ylabel('åœ§åŠ› [bar]', fontsize=10)
ax2.set_zlabel('äºˆæ¸¬åç‡ [%]', fontsize=10)
ax2.set_title('3Då¿œç­”æ›²é¢', fontsize=14)
plt.colorbar(surf, ax=ax2, label='äºˆæ¸¬åç‡ [%]', shrink=0.5)

plt.tight_layout()
plt.show()
```

### 4.5 Example 19: åˆ¶ç´„ä»˜ãæœ€é©åŒ–

```python
from scipy.optimize import NonlinearConstraint

# ç›®çš„é–¢æ•°ï¼ˆåç‡ã‚’æœ€å¤§åŒ–ï¼‰
def objective_constrained(params):
    temp, press, cat = params
    features = np.array([[
        temp, press, cat,
        temp * press,
        temp * cat,
        temp**2,
        cat**2,
        cat / (press + 1e-10)
    ]])
    predicted_yield = model_lgb.predict(features)[0]
    return -predicted_yield

# åˆ¶ç´„é–¢æ•°ï¼šã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚³ã‚¹ãƒˆ < 100 [ä»»æ„å˜ä½]
# ã‚³ã‚¹ãƒˆ = 0.1 * æ¸©åº¦ + 2.0 * åœ§åŠ›
def energy_cost_constraint(params):
    temp, press, cat = params
    cost = 0.1 * temp + 2.0 * press
    return cost

# åˆ¶ç´„: ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚³ã‚¹ãƒˆ <= 100
constraint = NonlinearConstraint(energy_cost_constraint, -np.inf, 100)

# æœ€é©åŒ–å®Ÿè¡Œ
result_constrained = minimize(
    objective_constrained,
    x0=[400, 5, 2.5],
    method='SLSQP',
    bounds=bounds,
    constraints=constraint
)

print("\n===== åˆ¶ç´„ä»˜ãæœ€é©åŒ–çµæœ =====")
print(f"æœ€é©æ¡ä»¶ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚³ã‚¹ãƒˆåˆ¶ç´„ä¸‹ï¼‰:")
print(f"  æ¸©åº¦: {result_constrained.x[0]:.1f} K")
print(f"  åœ§åŠ›: {result_constrained.x[1]:.2f} bar")
print(f"  è§¦åª’é‡: {result_constrained.x[2]:.2f} wt%")
print(f"\næœ€å¤§äºˆæ¸¬åç‡: {-result_constrained.fun:.2f} %")
print(f"ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚³ã‚¹ãƒˆ: {energy_cost_constraint(result_constrained.x):.2f}")
print(f"åˆ¶ç´„æº€è¶³: {energy_cost_constraint(result_constrained.x) <= 100}")
```

### 4.6 Example 20: å¤šç›®çš„æœ€é©åŒ–ï¼ˆåç‡ vs ã‚³ã‚¹ãƒˆï¼‰

```python
from pymoo.algorithms.moo.nsga2 import NSGA2
from pymoo.core.problem import Problem
from pymoo.optimize import minimize as pymoo_minimize

# å¤šç›®çš„æœ€é©åŒ–å•é¡Œã®å®šç¾©
class ProcessOptimizationProblem(Problem):
    def __init__(self):
        super().__init__(
            n_var=3,  # å¤‰æ•°ã®æ•°ï¼ˆæ¸©åº¦ã€åœ§åŠ›ã€è§¦åª’é‡ï¼‰
            n_obj=2,  # ç›®çš„é–¢æ•°ã®æ•°ï¼ˆåç‡ã€ã‚³ã‚¹ãƒˆï¼‰
            xl=np.array([300, 1, 0.1]),  # ä¸‹é™
            xu=np.array([500, 10, 5.0])  # ä¸Šé™
        )

    def _evaluate(self, X, out, *args, **kwargs):
        # X: (n_samples, 3) ã®é…åˆ—
        n_samples = X.shape[0]
        f1 = np.zeros(n_samples)  # ç›®çš„1: -åç‡ï¼ˆæœ€å°åŒ–ï¼‰
        f2 = np.zeros(n_samples)  # ç›®çš„2: ã‚³ã‚¹ãƒˆï¼ˆæœ€å°åŒ–ï¼‰

        for i in range(n_samples):
            temp, press, cat = X[i]

            # åç‡ã‚’äºˆæ¸¬
            features = np.array([[
                temp, press, cat,
                temp * press,
                temp * cat,
                temp**2,
                cat**2,
                cat / (press + 1e-10)
            ]])
            predicted_yield = model_lgb.predict(features)[0]

            # ç›®çš„1: åç‡ã‚’æœ€å¤§åŒ– â†’ -åç‡ã‚’æœ€å°åŒ–
            f1[i] = -predicted_yield

            # ç›®çš„2: ã‚³ã‚¹ãƒˆã‚’æœ€å°åŒ–
            # ã‚³ã‚¹ãƒˆ = ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚³ã‚¹ãƒˆ + è§¦åª’ã‚³ã‚¹ãƒˆ
            energy_cost = 0.1 * temp + 2.0 * press
            catalyst_cost = 5.0 * cat
            f2[i] = energy_cost + catalyst_cost

        out["F"] = np.column_stack([f1, f2])

# NSGA-IIã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æœ€é©åŒ–
problem = ProcessOptimizationProblem()
algorithm = NSGA2(pop_size=50)

result_nsga2 = pymoo_minimize(
    problem,
    algorithm,
    ('n_gen', 100),  # 100ä¸–ä»£
    verbose=False
)

# ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã‚’å–å¾—
pareto_front = result_nsga2.F
pareto_solutions = result_nsga2.X

print("\n===== å¤šç›®çš„æœ€é©åŒ–ï¼ˆNSGA-IIï¼‰çµæœ =====")
print(f"ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã®æ•°: {len(pareto_solutions)}")
print(f"\nãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã®ä¾‹ï¼ˆæœ€åˆã®3ã¤ï¼‰:")
for i in range(min(3, len(pareto_solutions))):
    temp, press, cat = pareto_solutions[i]
    yield_val = -pareto_front[i, 0]
    cost_val = pareto_front[i, 1]
    print(f"\nè§£ {i+1}:")
    print(f"  æ¸©åº¦: {temp:.1f} K, åœ§åŠ›: {press:.2f} bar, è§¦åª’é‡: {cat:.2f} wt%")
    print(f"  åç‡: {yield_val:.2f} %, ã‚³ã‚¹ãƒˆ: {cost_val:.2f}")

# ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆã‚’å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(-pareto_front[:, 0], pareto_front[:, 1],
            c='blue', s=50, alpha=0.6, edgecolors='k', linewidth=0.5)
plt.xlabel('åç‡ [%]', fontsize=12)
plt.ylabel('ã‚³ã‚¹ãƒˆ [ä»»æ„å˜ä½]', fontsize=12)
plt.title('ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆï¼ˆåç‡ vs ã‚³ã‚¹ãƒˆï¼‰', fontsize=14)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### 4.7 Example 21: æœ€é©åŒ–çµæœã®æ¯”è¼ƒ

```python
# ã™ã¹ã¦ã®æœ€é©åŒ–æ‰‹æ³•ã®çµæœã‚’æ¯”è¼ƒ
optimization_results = pd.DataFrame({
    'æ‰‹æ³•': [
        'ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ',
        'ãƒ™ã‚¤ã‚ºæœ€é©åŒ–',
        'DoEï¼ˆ2^3ï¼‰',
        'åˆ¶ç´„ä»˜ãæœ€é©åŒ–'
    ],
    'æ¸©åº¦ [K]': [
        result.x[0],
        result_bayes.x[0],
        best_condition['æ¸©åº¦ [K]'],
        result_constrained.x[0]
    ],
    'åœ§åŠ› [bar]': [
        result.x[1],
        result_bayes.x[1],
        best_condition['åœ§åŠ› [bar]'],
        result_constrained.x[1]
    ],
    'è§¦åª’é‡ [wt%]': [
        result.x[2],
        result_bayes.x[2],
        best_condition['è§¦åª’é‡ [wt%]'],
        result_constrained.x[2]
    ],
    'äºˆæ¸¬åç‡ [%]': [
        -result.fun,
        -result_bayes.fun,
        best_condition['äºˆæ¸¬åç‡ [%]'],
        -result_constrained.fun
    ]
})

print("\n===== æœ€é©åŒ–æ‰‹æ³•ã®æ¯”è¼ƒ =====")
print(optimization_results.to_string(index=False))
```

### 4.8 Example 22: æœ€é©åŒ–æ‰‹æ³•ã®ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ

```python
# Mermaidãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆMarkdownã§è¡¨ç¤ºï¼‰
print("""
```mermaid
graph TD
    A[ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–ã‚¿ã‚¹ã‚¯] --> B{ç›®çš„ã®æ•°ã¯?}
    B -->|å˜ä¸€ç›®çš„| C{åˆ¶ç´„æ¡ä»¶ã¯?}
    B -->|å¤šç›®çš„| D[NSGA-II/éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ]

    C -->|ãªã—| E{è©•ä¾¡ã‚³ã‚¹ãƒˆã¯?}
    C -->|ã‚ã‚Š| F[åˆ¶ç´„ä»˜ãæœ€é©åŒ–<br/>SLSQP/COBYLA]

    E -->|ä½ã„| G[ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ<br/>or DoE]
    E -->|é«˜ã„| H[ãƒ™ã‚¤ã‚ºæœ€é©åŒ–<br/>Gaussian Process]

    D --> I[ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆå–å¾—]
    F --> J[æœ€é©æ¡ä»¶å–å¾—]
    G --> J
    H --> J
    I --> K[ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•è§£æ]
    J --> L[å®Ÿé¨“æ¤œè¨¼]
    K --> L

    style A fill:#e3f2fd
    style D fill:#c8e6c9
    style F fill:#fff9c4
    style H fill:#ffccbc
    style I fill:#f3e5f5
```
""")
```

---

## 5. é«˜åº¦ãªæ‰‹æ³•

### 5.9 Example 23: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆGrid Searchï¼‰

```python
from sklearn.model_selection import GridSearchCV

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€™è£œ
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 15, 20],
    'min_samples_split': [2, 5, 10]
}

# Grid Searchã®è¨­å®š
grid_search = GridSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_grid=param_grid,
    cv=5,  # 5-foldäº¤å·®æ¤œè¨¼
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    verbose=1
)

# Grid Searchå®Ÿè¡Œ
print("===== Grid Searché–‹å§‹ =====")
grid_search.fit(X_train, y_train)

print(f"\n===== æœ€è‰¯ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ =====")
for param, value in grid_search.best_params_.items():
    print(f"{param}: {value}")

print(f"\näº¤å·®æ¤œè¨¼MAE: {-grid_search.best_score_:.2f} %")

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è©•ä¾¡
best_model_gs = grid_search.best_estimator_
y_pred_gs = best_model_gs.predict(X_test)
mae_gs = mean_absolute_error(y_test, y_pred_gs)
r2_gs = r2_score(y_test, y_pred_gs)

print(f"\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½:")
print(f"  MAE: {mae_gs:.2f} %")
print(f"  RÂ²: {r2_gs:.4f}")
```

### 5.10 Example 24: æ™‚ç³»åˆ—ãƒ—ãƒ­ã‚»ã‚¹ã®ç•°å¸¸æ¤œçŸ¥

```python
from sklearn.ensemble import IsolationForest

# æ™‚ç³»åˆ—ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆExample 7ã§ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
process_time_series = pd.DataFrame({
    'time_h': time_hours,
    'temperature_K': temp_time,
    'pressure_bar': pressure_time,
    'yield_%': yield_time
})

# Isolation Forestã§ç•°å¸¸æ¤œçŸ¥
iso_forest = IsolationForest(
    contamination=0.1,  # ç•°å¸¸ã®å‰²åˆã‚’10%ã¨ä»®å®š
    random_state=42
)

# ç‰¹å¾´é‡ï¼ˆæ¸©åº¦ã€åœ§åŠ›ã€åç‡ï¼‰
X_anomaly = process_time_series[['temperature_K', 'pressure_bar', 'yield_%']]

# ç•°å¸¸ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
anomaly_scores = iso_forest.fit_predict(X_anomaly)
process_time_series['anomaly'] = anomaly_scores

# ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
anomalies = process_time_series[process_time_series['anomaly'] == -1]

print(f"\n===== ç•°å¸¸æ¤œçŸ¥çµæœ =====")
print(f"ç•°å¸¸ãƒ‡ãƒ¼ã‚¿æ•°: {len(anomalies)}ä»¶ / {len(process_time_series)}ä»¶")
print(f"ç•°å¸¸å‰²åˆ: {len(anomalies) / len(process_time_series) * 100:.1f}%")

# å¯è¦–åŒ–
plt.figure(figsize=(14, 5))
plt.plot(process_time_series['time_h'], process_time_series['yield_%'],
         'b-', linewidth=1.5, label='æ­£å¸¸ãƒ‡ãƒ¼ã‚¿')
plt.scatter(anomalies['time_h'], anomalies['yield_%'],
            c='red', s=100, marker='x', linewidth=2, label='ç•°å¸¸ãƒ‡ãƒ¼ã‚¿')
plt.xlabel('æ™‚é–“ [h]', fontsize=12)
plt.ylabel('åç‡ [%]', fontsize=12)
plt.title('æ™‚ç³»åˆ—ãƒ—ãƒ­ã‚»ã‚¹ã®ç•°å¸¸æ¤œçŸ¥', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### 5.11 Example 25: SHAPå€¤ã«ã‚ˆã‚‹è§£é‡ˆæ€§åˆ†æ

```python
# SHAPï¼ˆSHapley Additive exPlanationsï¼‰ã§ç‰¹å¾´é‡ã®å½±éŸ¿ã‚’åˆ†æ
try:
    import shap

    # SHAPã®Explainerã‚’ä½œæˆï¼ˆLightGBMç”¨ï¼‰
    explainer = shap.TreeExplainer(model_lgb)
    shap_values = explainer.shap_values(X_test)

    # SHAPå€¤ã®ã‚µãƒãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X_test, feature_names=X.columns, show=False)
    plt.title('SHAPå€¤ã‚µãƒãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆï¼ˆç‰¹å¾´é‡ã®å½±éŸ¿åº¦ï¼‰', fontsize=14)
    plt.tight_layout()
    plt.show()

    print("\nâœ… SHAPå€¤åˆ†æå®Œäº†")
    print("å„ç‰¹å¾´é‡ãŒäºˆæ¸¬ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹ã‚’å¯è¦–åŒ–ã—ã¾ã—ãŸã€‚")

except ImportError:
    print("\nâš ï¸ SHAPãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
    print("pip install shap ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
```

### 5.12 Example 26: ãƒ—ãƒ­ã‚»ã‚¹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆPIDåˆ¶å¾¡ï¼‰

```python
# ç°¡æ˜“PIDã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
class PIDController:
    def __init__(self, Kp, Ki, Kd, setpoint):
        self.Kp = Kp  # æ¯”ä¾‹ã‚²ã‚¤ãƒ³
        self.Ki = Ki  # ç©åˆ†ã‚²ã‚¤ãƒ³
        self.Kd = Kd  # å¾®åˆ†ã‚²ã‚¤ãƒ³
        self.setpoint = setpoint  # ç›®æ¨™å€¤
        self.integral = 0
        self.prev_error = 0

    def update(self, measured_value, dt):
        # èª¤å·®ã‚’è¨ˆç®—
        error = self.setpoint - measured_value

        # ç©åˆ†é …
        self.integral += error * dt

        # å¾®åˆ†é …
        derivative = (error - self.prev_error) / dt

        # PIDå‡ºåŠ›
        output = (
            self.Kp * error +
            self.Ki * self.integral +
            self.Kd * derivative
        )

        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ãŸã‚ã«èª¤å·®ã‚’ä¿å­˜
        self.prev_error = error

        return output

# ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆ1æ¬¡é…ã‚Œç³»ï¼‰
def process_model(input_val, current_temp, tau=5.0, K=1.0, dt=0.1):
    """
    1æ¬¡é…ã‚Œç³»ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«
    tau: æ™‚å®šæ•°, K: ã‚²ã‚¤ãƒ³
    """
    dT = (K * input_val - current_temp) / tau
    new_temp = current_temp + dT * dt
    return new_temp

# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
dt = 0.1  # æ™‚é–“åˆ»ã¿ [ç§’]
t_end = 50  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚é–“ [ç§’]
time_sim = np.arange(0, t_end, dt)

# PIDã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ã®åˆæœŸåŒ–ï¼ˆç›®æ¨™æ¸©åº¦: 400 Kï¼‰
pid = PIDController(Kp=2.0, Ki=0.5, Kd=1.0, setpoint=400)

# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
temperature = 350  # åˆæœŸæ¸©åº¦ [K]
temperatures = []
inputs = []

for t in time_sim:
    # PIDåˆ¶å¾¡å…¥åŠ›ã‚’è¨ˆç®—
    control_input = pid.update(temperature, dt)

    # ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«ã§æ¸©åº¦ã‚’æ›´æ–°
    temperature = process_model(control_input, temperature, dt=dt)

    # è¨˜éŒ²
    temperatures.append(temperature)
    inputs.append(control_input)

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 1, figsize=(12, 8))

# æ¸©åº¦ã®æ¨ç§»
axes[0].plot(time_sim, temperatures, 'b-', linewidth=2, label='ãƒ—ãƒ­ã‚»ã‚¹æ¸©åº¦')
axes[0].axhline(y=400, color='r', linestyle='--', linewidth=1.5, label='ç›®æ¨™æ¸©åº¦')
axes[0].set_ylabel('æ¸©åº¦ [K]', fontsize=12)
axes[0].set_title('PIDåˆ¶å¾¡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³', fontsize=14)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# åˆ¶å¾¡å…¥åŠ›ã®æ¨ç§»
axes[1].plot(time_sim, inputs, 'g-', linewidth=2, label='åˆ¶å¾¡å…¥åŠ›')
axes[1].set_xlabel('æ™‚é–“ [ç§’]', fontsize=12)
axes[1].set_ylabel('åˆ¶å¾¡å…¥åŠ›', fontsize=12)
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\n===== PIDåˆ¶å¾¡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœ =====")
print(f"æœ€çµ‚æ¸©åº¦: {temperatures[-1]:.2f} Kï¼ˆç›®æ¨™: 400 Kï¼‰")
print(f"å®šå¸¸åå·®: {abs(400 - temperatures[-1]):.2f} K")
```

---

## 6. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

### 6.1 ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ä¸€è¦§

| ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ | åŸå›  | è§£æ±ºæ–¹æ³• |
|------------------|------|----------|
| `ModuleNotFoundError: No module named 'skopt'` | scikit-optimizeæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« | `pip install scikit-optimize` |
| `ValueError: Input contains NaN` | ãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ | `df.dropna()` ã§å‰Šé™¤ or `df.fillna()` ã§è£œå®Œ |
| `ConvergenceWarning` | æœ€é©åŒ–ãŒåæŸã›ãš | `max_iter`ã‚’å¢—ã‚„ã™ã€å­¦ç¿’ç‡èª¿æ•´ |
| `MemoryError` | ãƒ¡ãƒ¢ãƒªä¸è¶³ | ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºå‰Šæ¸›ã€ãƒãƒƒãƒå‡¦ç† |
| `LinAlgError: Singular matrix` | è¡Œåˆ—ãŒç‰¹ç•° | ç‰¹å¾´é‡ã®å¤šé‡å…±ç·šæ€§ã‚’ç¢ºèªã€æ­£å‰‡åŒ–è¿½åŠ  |

### 6.2 ãƒ‡ãƒãƒƒã‚°ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

**ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª**
```python
# ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬çµ±è¨ˆ
print(process_data.describe())

# æ¬ æå€¤ã®ç¢ºèª
print(process_data.isnull().sum())

# ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèª
print(process_data.dtypes)
```

**ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ¢ãƒ‡ãƒ«ã®ç°¡ç•¥åŒ–**
```python
# è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã§å¤±æ•—ã—ãŸã‚‰ã€ã¾ãšç·šå½¢å›å¸°ã§è©¦ã™
model_simple = LinearRegression()
model_simple.fit(X_train, y_train)
print(f"ç·šå½¢å›å¸°ã®RÂ²: {model_simple.score(X_test, y_test):.4f}")
```

**ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ç¢ºèª**
```python
# SVRã‚„MLPã§ã¯æ¨™æº–åŒ–ãŒå¿…é ˆ
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

---

## 7. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒ£ãƒ¬ãƒ³ã‚¸ï¼šåŒ–å­¦åå¿œå™¨ã®æœ€é©åŒ–

å­¦ã‚“ã ã“ã¨ã‚’çµ±åˆã—ã€å®Ÿè·µçš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å–ã‚Šçµ„ã¿ã¾ã—ã‚‡ã†ã€‚

### 7.1 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

**ç›®æ¨™ï¼š**
åŒ–å­¦åå¿œå™¨ã®æ“ä½œæ¡ä»¶ã‚’æœ€é©åŒ–ã—ã€åç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹

**ç›®æ¨™æ€§èƒ½ï¼š**
- äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«: RÂ² > 0.85
- æœ€é©åŒ–: åç‡ > 90%

### 7.2 ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰

**Step 1: ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆã‚ˆã‚Šç¾å®Ÿçš„ãªãƒ‡ãƒ¼ã‚¿ï¼‰**
```python
# ã‚ˆã‚Šè¤‡é›‘ãªåå¿œå™¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
np.random.seed(42)
n_reactor = 300

temp_reactor = np.random.uniform(320, 480, n_reactor)
press_reactor = np.random.uniform(2, 12, n_reactor)
cat_reactor = np.random.uniform(0.5, 6.0, n_reactor)
residence_time = np.random.uniform(5, 30, n_reactor)  # æ»ç•™æ™‚é–“ [min]

# ã‚ˆã‚Šè¤‡é›‘ãªåç‡ãƒ¢ãƒ‡ãƒ«ï¼ˆ4å¤‰æ•°ã€ç›¸äº’ä½œç”¨ã€æœ€é©å€¤ã‚ã‚Šï¼‰
yield_reactor = (
    25
    + 0.18 * temp_reactor
    - 0.00025 * temp_reactor**2
    + 6.0 * press_reactor
    - 0.3 * press_reactor**2
    + 4.0 * cat_reactor
    - 0.4 * cat_reactor**2
    + 1.5 * residence_time
    - 0.03 * residence_time**2
    + 0.015 * temp_reactor * press_reactor
    + 0.008 * cat_reactor * residence_time
    + np.random.normal(0, 2.5, n_reactor)
)

reactor_data = pd.DataFrame({
    'temperature': temp_reactor,
    'pressure': press_reactor,
    'catalyst': cat_reactor,
    'residence_time': residence_time,
    'yield': yield_reactor
})

print("===== åå¿œå™¨ãƒ‡ãƒ¼ã‚¿ =====")
print(reactor_data.describe())
```

**Step 2: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**
```python
# ç‰¹å¾´é‡ã‚’è¿½åŠ 
reactor_data['temp_press'] = reactor_data['temperature'] * reactor_data['pressure']
reactor_data['cat_time'] = reactor_data['catalyst'] * reactor_data['residence_time']
reactor_data['temp_sq'] = reactor_data['temperature'] ** 2
reactor_data['press_sq'] = reactor_data['pressure'] ** 2

X_reactor = reactor_data.drop('yield', axis=1)
y_reactor = reactor_data['yield']

X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
    X_reactor, y_reactor, test_size=0.2, random_state=42
)
```

**Step 3: ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆLightGBMï¼‰**
```python
model_reactor = lgb.LGBMRegressor(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=15,
    random_state=42,
    verbose=-1
)
model_reactor.fit(X_train_r, y_train_r)

y_pred_r = model_reactor.predict(X_test_r)
r2_reactor = r2_score(y_test_r, y_pred_r)
mae_reactor = mean_absolute_error(y_test_r, y_pred_r)

print(f"\n===== åå¿œå™¨ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ =====")
print(f"RÂ²: {r2_reactor:.3f}")
print(f"MAE: {mae_reactor:.2f}%")

if r2_reactor > 0.85:
    print("ğŸ‰ ç›®æ¨™é”æˆï¼ï¼ˆRÂ² > 0.85ï¼‰")
```

**Step 4: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§æ¡ä»¶æ¢ç´¢**
```python
# æœ€é©æ¡ä»¶ã‚’æ¢ç´¢
space_reactor = [
    Real(320, 480, name='temperature'),
    Real(2, 12, name='pressure'),
    Real(0.5, 6.0, name='catalyst'),
    Real(5, 30, name='residence_time')
]

@use_named_args(space_reactor)
def objective_reactor(**params):
    temp = params['temperature']
    press = params['pressure']
    cat = params['catalyst']
    res_time = params['residence_time']

    features = np.array([[
        temp, press, cat, res_time,
        temp * press,
        cat * res_time,
        temp**2,
        press**2
    ]])

    predicted_yield = model_reactor.predict(features)[0]
    return -predicted_yield

result_reactor = gp_minimize(
    objective_reactor,
    space_reactor,
    n_calls=50,
    random_state=42,
    verbose=False
)

print(f"\n===== æœ€é©æ¡ä»¶ =====")
print(f"æ¸©åº¦: {result_reactor.x[0]:.1f} K")
print(f"åœ§åŠ›: {result_reactor.x[1]:.2f} bar")
print(f"è§¦åª’é‡: {result_reactor.x[2]:.2f} wt%")
print(f"æ»ç•™æ™‚é–“: {result_reactor.x[3]:.1f} min")
print(f"\næœ€å¤§äºˆæ¸¬åç‡: {-result_reactor.fun:.2f}%")

if -result_reactor.fun > 90:
    print("ğŸ‰ ç›®æ¨™é”æˆï¼ï¼ˆåç‡ > 90%ï¼‰")
```

---

## 8. ã¾ã¨ã‚

### ã“ã®ç« ã§å­¦ã‚“ã ã“ã¨

1. **ç’°å¢ƒæ§‹ç¯‰**
   - Anacondaã€venvã€Google Colabã®3ã¤ã®é¸æŠè‚¢
   - PIç‰¹æœ‰ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆscikit-optimizeã€pymooï¼‰ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

2. **ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿å‡¦ç†**
   - ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¨å¯è¦–åŒ–ï¼ˆæ•£å¸ƒå›³è¡Œåˆ—ã€ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼‰
   - å‰å‡¦ç†ï¼ˆæ¬ æå€¤è£œå®Œã€å¤–ã‚Œå€¤é™¤å»ã€æ¨™æº–åŒ–ï¼‰
   - ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆç›¸äº’ä½œç”¨é …ã€äºŒæ¬¡é …ï¼‰

3. **5ã¤ã®å›å¸°ãƒ¢ãƒ‡ãƒ«**
   - ç·šå½¢å›å¸°ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€LightGBMã€SVRã€MLP
   - ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®æ¯”è¼ƒï¼ˆMAEã€RMSEã€RÂ²ï¼‰

4. **ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–æ‰‹æ³•**
   - ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã€ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã€DoEã€å¿œç­”æ›²é¢æ³•
   - åˆ¶ç´„ä»˜ãæœ€é©åŒ–ã€å¤šç›®çš„æœ€é©åŒ–ï¼ˆNSGA-IIï¼‰

5. **é«˜åº¦ãªæ‰‹æ³•**
   - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
   - ç•°å¸¸æ¤œçŸ¥ï¼ˆIsolation Forestï¼‰
   - è§£é‡ˆæ€§åˆ†æï¼ˆSHAPå€¤ï¼‰
   - PIDåˆ¶å¾¡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’çµ‚ãˆãŸã‚ãªãŸã¯ï¼š**
- âœ… ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨å¯è¦–åŒ–ãŒã§ãã‚‹
- âœ… 5ç¨®é¡ä»¥ä¸Šã®å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹
- âœ… ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§ãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶ã‚’æœ€é©åŒ–ã§ãã‚‹
- âœ… å¤šç›®çš„æœ€é©åŒ–ã§ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è§£æã§ãã‚‹

**æ¬¡ã«å­¦ã¶ã¹ãå†…å®¹ï¼š**
1. **å®Ÿãƒ—ãƒ­ã‚»ã‚¹ã¸ã®é©ç”¨**
   - DCSï¼ˆåˆ†æ•£åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼‰ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
   - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æœ€é©åŒ–

2. **æ·±å±¤å­¦ç¿’ã®å¿œç”¨**
   - LSTMï¼ˆæ™‚ç³»åˆ—äºˆæ¸¬ï¼‰
   - ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆç•°å¸¸æ¤œçŸ¥ï¼‰

3. **è‡ªå¾‹ãƒ—ãƒ­ã‚»ã‚¹åˆ¶å¾¡**
   - å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹åˆ¶å¾¡
   - Model Predictive Controlï¼ˆMPCï¼‰

---

## æ¼”ç¿’å•é¡Œ

### å•é¡Œ1ï¼ˆé›£æ˜“åº¦ï¼šeasyï¼‰

ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–ã§ã€ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ãŒã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ç†ç”±ã‚’3ã¤æŒ™ã’ã¦ãã ã•ã„ã€‚

<details>
<summary>è§£ç­”ä¾‹</summary>

**ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®åˆ©ç‚¹ï¼š**

1. **è©•ä¾¡å›æ•°ãŒå°‘ãªã„**
   - ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ: ã™ã¹ã¦ã®çµ„ã¿åˆã‚ã›ã‚’è©¦ã™ï¼ˆä¾‹: 10Ã—10Ã—10 = 1000å›ï¼‰
   - ãƒ™ã‚¤ã‚ºæœ€é©åŒ–: 30-50å›ç¨‹åº¦ã§æœ€é©è§£ã«åˆ°é”

2. **æ¢ç´¢ãŒè³¢ã„**
   - éå»ã®è©•ä¾¡çµæœã‚’æ´»ç”¨ã—ã€æœ‰æœ›ãªé ˜åŸŸã‚’å„ªå…ˆçš„ã«æ¢ç´¢
   - ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã¯ç›²ç›®çš„ã«å…¨æ¢ç´¢

3. **å®Ÿé¨“ã‚³ã‚¹ãƒˆã®å‰Šæ¸›**
   - åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ã§ã¯1å›ã®å®Ÿé¨“ã«æ•°æ™‚é–“-æ•°æ—¥ã‹ã‹ã‚‹
   - è©•ä¾¡å›æ•°ãŒå°‘ãªã„ãŸã‚ã€ç·å®Ÿé¨“æ™‚é–“ã‚’å¤§å¹…ã«å‰Šæ¸›

**å®Ÿä¾‹ï¼š**
- ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ: 1000å®Ÿé¨“ Ã— 3æ™‚é–“ = 3000æ™‚é–“ï¼ˆ125æ—¥ï¼‰
- ãƒ™ã‚¤ã‚ºæœ€é©åŒ–: 50å®Ÿé¨“ Ã— 3æ™‚é–“ = 150æ™‚é–“ï¼ˆ6.25æ—¥ï¼‰

**ç´„20å€ã®æ™‚é–“çŸ­ç¸®ï¼**

</details>

### å•é¡Œ2ï¼ˆé›£æ˜“åº¦ï¼šmediumï¼‰

å¤šç›®çš„æœ€é©åŒ–ï¼ˆNSGA-IIï¼‰ã§å¾—ã‚‰ã‚Œã‚‹ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆã¨ã¯ä½•ã‹èª¬æ˜ã—ã€åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ã§ã®å¿œç”¨ä¾‹ã‚’1ã¤æŒ™ã’ã¦ãã ã•ã„ã€‚

<details>
<summary>è§£ç­”ä¾‹</summary>

**ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆã¨ã¯ï¼š**

è¤‡æ•°ã®ç›®çš„é–¢æ•°ã‚’åŒæ™‚ã«æœ€é©åŒ–ã™ã‚‹éš›ã€ä¸€æ–¹ã‚’æ”¹å–„ã™ã‚‹ã¨ä»–æ–¹ãŒæ‚ªåŒ–ã™ã‚‹é–¢ä¿‚ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰ãŒå­˜åœ¨ã—ã¾ã™ã€‚ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆã¯ã€ã€Œã©ã®ç›®çš„ã‚‚æ”¹å–„ã§ããªã„è§£ã®é›†åˆã€ã§ã™ã€‚

**ç‰¹å¾´ï¼š**
- ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆä¸Šã®è§£ã¯ã™ã¹ã¦ã€Œæœ€é©è§£ã€
- ã©ã®è§£ã‚’é¸ã¶ã‹ã¯ã€æ„æ€æ±ºå®šè€…ã®å„ªå…ˆåº¦ã«ã‚ˆã‚‹

**åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ã§ã®å¿œç”¨ä¾‹ï¼šè’¸ç•™å¡”ã®æœ€é©åŒ–**

**ç›®çš„1**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚³ã‚¹ãƒˆã‚’æœ€å°åŒ–
**ç›®çš„2**: è£½å“ç´”åº¦ã‚’æœ€å¤§åŒ–

**ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆã®ä¾‹ï¼š**

| è§£ | ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚³ã‚¹ãƒˆ | è£½å“ç´”åº¦ |
|----|------------------|----------|
| A  | ä½ï¼ˆ100å††/kgï¼‰   | ä½ï¼ˆ95%ï¼‰ |
| B  | ä¸­ï¼ˆ150å††/kgï¼‰   | ä¸­ï¼ˆ98%ï¼‰ |
| C  | é«˜ï¼ˆ200å††/kgï¼‰   | é«˜ï¼ˆ99.5%ï¼‰ |

**é¸æŠåŸºæº–ï¼š**
- ã‚³ã‚¹ãƒˆé‡è¦– â†’ è§£Aï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚³ã‚¹ãƒˆæœ€å°ï¼‰
- å“è³ªé‡è¦– â†’ è§£Cï¼ˆç´”åº¦æœ€é«˜ï¼‰
- ãƒãƒ©ãƒ³ã‚¹é‡è¦– â†’ è§£Bï¼ˆä¸­é–“ï¼‰

NSGA-IIã¯ã“ã®ã‚ˆã†ãªãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆã‚’è‡ªå‹•çš„ã«ç™ºè¦‹ã—ã¾ã™ã€‚

</details>

### å•é¡Œ3ï¼ˆé›£æ˜“åº¦ï¼šhardï¼‰

PIDåˆ¶å¾¡ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆKp, Ki, Kdï¼‰ãŒæ¸©åº¦åˆ¶å¾¡ã®æŒ™å‹•ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¬æ˜ã—ã€ãã‚Œãã‚Œã‚’å¤§ããã—ãŸå ´åˆã®åˆ©ç‚¹ã¨æ¬ ç‚¹ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚

<details>
<summary>è§£ç­”ä¾‹</summary>

**PIDãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ï¼š**

**1. Kpï¼ˆæ¯”ä¾‹ã‚²ã‚¤ãƒ³ï¼‰ã‚’å¤§ããã—ãŸå ´åˆ**

**åˆ©ç‚¹ï¼š**
- å¿œç­”ãŒé€Ÿããªã‚‹ï¼ˆç›®æ¨™å€¤ã«æ—©ãåˆ°é”ï¼‰
- å®šå¸¸åå·®ãŒå°ã•ããªã‚‹

**æ¬ ç‚¹ï¼š**
- ã‚ªãƒ¼ãƒãƒ¼ã‚·ãƒ¥ãƒ¼ãƒˆãŒå¤§ãããªã‚‹ï¼ˆç›®æ¨™å€¤ã‚’è¶…ãˆã¦æŒ¯å‹•ï¼‰
- å®‰å®šæ€§ãŒä½ä¸‹ï¼ˆæŒ¯å‹•çš„ãªæŒ™å‹•ï¼‰

**2. Kiï¼ˆç©åˆ†ã‚²ã‚¤ãƒ³ï¼‰ã‚’å¤§ããã—ãŸå ´åˆ**

**åˆ©ç‚¹ï¼š**
- å®šå¸¸åå·®ã‚’å®Œå…¨ã«é™¤å»ã§ãã‚‹
- é•·æœŸçš„ãªç²¾åº¦ãŒå‘ä¸Š

**æ¬ ç‚¹ï¼š**
- å¿œç­”ãŒé…ããªã‚‹ï¼ˆç©åˆ†é …ã®è“„ç©ã«æ™‚é–“ãŒã‹ã‹ã‚‹ï¼‰
- ãƒ¯ã‚¤ãƒ³ãƒ‰ã‚¢ãƒƒãƒ—ç¾è±¡ï¼ˆç©åˆ†é …ãŒç•°å¸¸ã«å¤§ãããªã‚‹ï¼‰
- ã‚ªãƒ¼ãƒãƒ¼ã‚·ãƒ¥ãƒ¼ãƒˆãŒå¢—åŠ 

**3. Kdï¼ˆå¾®åˆ†ã‚²ã‚¤ãƒ³ï¼‰ã‚’å¤§ããã—ãŸå ´åˆ**

**åˆ©ç‚¹ï¼š**
- ã‚ªãƒ¼ãƒãƒ¼ã‚·ãƒ¥ãƒ¼ãƒˆã‚’æŠ‘åˆ¶ï¼ˆå¤‰åŒ–ã‚’å…ˆèª­ã¿ã—ã¦åˆ¶å¾¡ï¼‰
- å®‰å®šæ€§ãŒå‘ä¸Š
- æŒ¯å‹•ã‚’æ¸›è¡°

**æ¬ ç‚¹ï¼š**
- ãƒã‚¤ã‚ºã«æ•æ„Ÿï¼ˆæ¸¬å®šå€¤ã®å°ã•ãªå¤‰å‹•ã‚’å¢—å¹…ï¼‰
- é«˜å‘¨æ³¢æŒ¯å‹•ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§

**æœ€é©ãªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ï¼š**

1. **Ziegler-Nicholsæ³•**ï¼ˆå¤å…¸çš„ï¼‰
2. **è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**ï¼ˆç¾ä»£çš„ï¼‰
3. **ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–**ï¼ˆæœ¬ç« ã§å­¦ã‚“ã ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’å¿œç”¨ï¼‰

**å®Ÿè£…ä¾‹ï¼š**
```python
# ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§PIDãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è‡ªå‹•èª¿æ•´
space_pid = [
    Real(0.1, 10.0, name='Kp'),
    Real(0.01, 1.0, name='Ki'),
    Real(0.01, 5.0, name='Kd')
]

@use_named_args(space_pid)
def objective_pid(**params):
    # PIDã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
    # ç›®æ¨™: ã‚ªãƒ¼ãƒãƒ¼ã‚·ãƒ¥ãƒ¼ãƒˆæœ€å° + æ•´å®šæ™‚é–“æœ€çŸ­
    overshoot, settling_time = simulate_pid(
        Kp=params['Kp'],
        Ki=params['Ki'],
        Kd=params['Kd']
    )
    return overshoot + 0.1 * settling_time

# æœ€é©åŒ–å®Ÿè¡Œ
result_pid = gp_minimize(objective_pid, space_pid, n_calls=50)
```

</details>

---

## å‚è€ƒæ–‡çŒ®

1. Pedregosa, F., et al. (2011). "Scikit-learn: Machine Learning in Python." *Journal of Machine Learning Research*, 12, 2825-2830.
   URL: https://scikit-learn.org

2. Brochu, E., Cora, V. M., & de Freitas, N. (2010). "A Tutorial on Bayesian Optimization of Expensive Cost Functions." arXiv:1012.2599.
   URL: https://arxiv.org/abs/1012.2599

3. Deb, K., et al. (2002). "A fast and elitist multiobjective genetic algorithm: NSGA-II." *IEEE Transactions on Evolutionary Computation*, 6(2), 182-197.
   DOI: [10.1109/4235.996017](https://doi.org/10.1109/4235.996017)

4. Shahriari, B., et al. (2016). "Taking the Human Out of the Loop: A Review of Bayesian Optimization." *Proceedings of the IEEE*, 104(1), 148-175.
   DOI: [10.1109/JPROC.2015.2494218](https://doi.org/10.1109/JPROC.2015.2494218)

5. Lundberg, S. M., & Lee, S. I. (2017). "A Unified Approach to Interpreting Model Predictions." *Advances in Neural Information Processing Systems*, 30.
   URL: https://github.com/slundberg/shap

6. Ã…strÃ¶m, K. J., & HÃ¤gglund, T. (2006). *Advanced PID Control*. ISA-The Instrumentation, Systems, and Automation Society.
   ISBN: 978-1556179426

7. scikit-optimize Documentation. (2024). "Bayesian Optimization."
   URL: https://scikit-optimize.github.io/stable/

8. pymoo Documentation. (2024). "Multi-objective Optimization."
   URL: https://pymoo.org/

---

**ä½œæˆæ—¥**: 2025-10-16
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0
**ã‚·ãƒªãƒ¼ã‚º**: PIå…¥é–€ã‚·ãƒªãƒ¼ã‚º v1.0
**è‘—è€…**: MI Knowledge Hub ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ